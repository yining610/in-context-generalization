PYTHONPATH=/home/ylu130/workspace/in-context-generalization
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model/llama-2-7b --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --seed 42 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i0-s42-rTrue --rationales --num-in-domain 0
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
using world size: 4
[2023-08-07 20:00:45,474] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model/llama-2-7b
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  model_parallel ............... False
  model_parallel_size .......... None
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i0-s42-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 2048
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i0-s42-rTrue
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  gradient_accumulation_steps .. 1
  batch_size ................... 5
  clip_grad .................... 1.0
  seed ......................... 42
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading Data
  0%|                                                                                                        | 0/9741 [00:00<?, ?it/s]  3%|███                                                                                         | 324/9741 [00:00<00:02, 3236.57it/s]  7%|██████▏                                                                                     | 649/9741 [00:00<00:02, 3240.08it/s] 10%|█████████▏                                                                                  | 977/9741 [00:00<00:02, 3257.09it/s] 13%|████████████▏                                                                              | 1305/9741 [00:00<00:02, 3263.06it/s] 17%|███████████████▏                                                                           | 1632/9741 [00:00<00:02, 3262.39it/s] 20%|██████████████████▎                                                                        | 1959/9741 [00:00<00:02, 3243.62it/s] 23%|█████████████████████▎                                                                     | 2284/9741 [00:00<00:02, 3244.13it/s] 27%|████████████████████████▍                                                                  | 2611/9741 [00:00<00:02, 3250.05it/s] 30%|███████████████████████████▍                                                               | 2937/9741 [00:00<00:02, 3177.76it/s] 33%|██████████████████████████████▍                                                            | 3261/9741 [00:01<00:02, 3195.26it/s] 37%|█████████████████████████████████▍                                                         | 3583/9741 [00:01<00:01, 3201.61it/s] 40%|████████████████████████████████████▌                                                      | 3909/9741 [00:01<00:01, 3216.87it/s] 43%|███████████████████████████████████████▌                                                   | 4233/9741 [00:01<00:01, 3223.77it/s] 47%|██████████████████████████████████████████▌                                                | 4556/9741 [00:01<00:01, 3194.01it/s] 50%|█████████████████████████████████████████████▌                                             | 4876/9741 [00:01<00:02, 2385.56it/s] 53%|████████████████████████████████████████████████▌                                          | 5200/9741 [00:01<00:01, 2590.40it/s] 58%|████████████████████████████████████████████████████▌                                      | 5620/9741 [00:01<00:01, 3001.89it/s] 63%|█████████████████████████████████████████████████████████▎                                 | 6137/9741 [00:01<00:01, 3581.32it/s] 68%|██████████████████████████████████████████████████████████████                             | 6650/9741 [00:02<00:00, 4009.67it/s] 74%|███████████████████████████████████████████████████████████████████                        | 7173/9741 [00:02<00:00, 4352.96it/s] 79%|███████████████████████████████████████████████████████████████████████▋                   | 7679/9741 [00:02<00:00, 4556.22it/s] 84%|████████████████████████████████████████████████████████████████████████████▌              | 8198/9741 [00:02<00:00, 4739.50it/s] 89%|█████████████████████████████████████████████████████████████████████████████████▍         | 8712/9741 [00:02<00:00, 4855.05it/s] 95%|██████████████████████████████████████████████████████████████████████████████████████▏    | 9232/9741 [00:02<00:00, 4955.57it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████| 9741/9741 [00:02<00:00, 3707.27it/s]
Load End
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.09s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:04<00:09,  4.96s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.33s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:11,  5.55s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:09<00:04,  4.91s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.01s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.44s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.39s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.26s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.45s/it]
[2023-08-07 20:01:38,521] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.33s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.51s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.74s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.92s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.73s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.92s/it]
[2023-08-07 20:01:47,458] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-07 20:01:47,459] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:
[2023-08-07 20:01:47,459] [INFO] [config.py:1012:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-07 20:01:47,459] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-07 20:01:47,459] [INFO] [config.py:1012:print]   amp_enabled .................. False
[2023-08-07 20:01:47,459] [INFO] [config.py:1012:print]   amp_params ................... False
[2023-08-07 20:01:47,459] [INFO] [config.py:1012:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-07 20:01:47,459] [INFO] [config.py:1012:print]   bfloat16_enabled ............. False
[2023-08-07 20:01:47,459] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False
[2023-08-07 20:01:47,459] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True
[2023-08-07 20:01:47,459] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False
[2023-08-07 20:01:47,459] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff9aca6e7c0>
[2023-08-07 20:01:47,459] [INFO] [config.py:1012:print]   communication_data_type ...... None
[2023-08-07 20:01:47,459] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-07 20:01:47,459] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False
[2023-08-07 20:01:47,459] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False
[2023-08-07 20:01:47,459] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-07 20:01:47,459] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   disable_allgather ............ False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   dump_state ................... False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'min_scale': 1}
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   elasticity_enabled ........... False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   fp16_auto_cast ............... False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   fp16_enabled ................. True
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   global_rank .................. 0
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 2048
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   loss_scale ................... 0
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   memory_breakdown ............. False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7ff9aca6eeb0>
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   optimizer_name ............... None
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   optimizer_params ............. None
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   pld_enabled .................. False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   pld_params ................... False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   prescale_gradients ........... False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   scheduler_name ............... None
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   scheduler_params ............. None
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   sparse_attention ............. None
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   steps_per_print .............. 1
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   train_batch_size ............. 20
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  5
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   use_node_local_storage ....... False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   world_size ................... 4
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  True
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   zero_enabled ................. False
[2023-08-07 20:01:47,460] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 0
[2023-08-07 20:01:47,461] [INFO] [config.py:997:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 5, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 1
}
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Emitting ninja build file /home/ylu130/.cache/torch_extensions/py39_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.32462310791015625 seconds
Model mem
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   12918 MB |   12918 MB |   12918 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      66 MB |      66 MB |      66 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |
|       from small pool |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|---------------------------------------------------------------------------|
| Allocations           |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     259    |     259    |     259    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |      33    |      33    |      33    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |      33    |      31    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       2    |       2    |      33    |      31    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Evaluating commonsenseqa :   0%|                                                                               | 0/50 [00:00<?, ?it/s]############### Example ###############
Loading extension module utils...
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:Answer the following multiple choice question.

### Demonstration:


### Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid

### Response:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i0-s42-rTrue
Time to load utils op: 0.20418620109558105 seconds
Loading extension module utils...
Time to load utils op: 0.3044095039367676 seconds
Loading extension module utils...
Time to load utils op: 0.30446505546569824 seconds
Evaluating commonsenseqa :   2%|█▍                                                                     | 1/50 [00:41<33:46, 41.35s/it]Evaluating commonsenseqa :   4%|██▊                                                                    | 2/50 [01:37<39:58, 49.96s/it]Evaluating commonsenseqa :   6%|████▎                                                                  | 3/50 [02:37<42:56, 54.82s/it]Evaluating commonsenseqa :   8%|█████▋                                                                 | 4/50 [03:16<37:15, 48.60s/it]Evaluating commonsenseqa :  10%|███████                                                                | 5/50 [03:32<27:29, 36.65s/it]Evaluating commonsenseqa :  12%|████████▌                                                              | 6/50 [04:29<31:50, 43.42s/it]Evaluating commonsenseqa :  14%|█████████▉                                                             | 7/50 [05:05<29:36, 41.31s/it]Evaluating commonsenseqa :  16%|███████████▎                                                           | 8/50 [05:28<24:39, 35.22s/it]Evaluating commonsenseqa :  18%|████████████▊                                                          | 9/50 [06:27<29:08, 42.65s/it]Evaluating commonsenseqa :  20%|██████████████                                                        | 10/50 [07:27<32:02, 48.07s/it]Evaluating commonsenseqa :  22%|███████████████▍                                                      | 11/50 [08:08<29:56, 46.06s/it]Evaluating commonsenseqa :  24%|████████████████▊                                                     | 12/50 [09:08<31:44, 50.12s/it]Evaluating commonsenseqa :  26%|██████████████████▏                                                   | 13/50 [09:59<31:04, 50.40s/it]Evaluating commonsenseqa :  28%|███████████████████▌                                                  | 14/50 [10:39<28:22, 47.28s/it]Evaluating commonsenseqa :  30%|█████████████████████                                                 | 15/50 [11:32<28:38, 49.11s/it]Evaluating commonsenseqa :  32%|██████████████████████▍                                               | 16/50 [11:49<22:17, 39.35s/it]Evaluating commonsenseqa :  34%|███████████████████████▊                                              | 17/50 [12:49<25:06, 45.67s/it]Evaluating commonsenseqa :  36%|█████████████████████████▏                                            | 18/50 [13:42<25:26, 47.71s/it]Evaluating commonsenseqa :  38%|██████████████████████████▌                                           | 19/50 [14:40<26:21, 51.02s/it]Evaluating commonsenseqa :  40%|████████████████████████████                                          | 20/50 [15:16<23:14, 46.50s/it]Evaluating commonsenseqa :  42%|█████████████████████████████▍                                        | 21/50 [16:00<22:05, 45.72s/it]Evaluating commonsenseqa :  44%|██████████████████████████████▊                                       | 22/50 [16:21<17:46, 38.11s/it]Evaluating commonsenseqa :  46%|████████████████████████████████▏                                     | 23/50 [16:34<13:46, 30.62s/it]Evaluating commonsenseqa :  48%|█████████████████████████████████▌                                    | 24/50 [17:13<14:25, 33.28s/it]Evaluating commonsenseqa :  50%|███████████████████████████████████                                   | 25/50 [18:13<17:11, 41.28s/it]Evaluating commonsenseqa :  52%|████████████████████████████████████▍                                 | 26/50 [19:12<18:37, 46.55s/it]Evaluating commonsenseqa :  54%|█████████████████████████████████████▊                                | 27/50 [19:29<14:25, 37.61s/it]Evaluating commonsenseqa :  56%|███████████████████████████████████████▏                              | 28/50 [20:08<13:55, 37.99s/it]Evaluating commonsenseqa :  58%|████████████████████████████████████████▌                             | 29/50 [21:02<14:59, 42.82s/it]Evaluating commonsenseqa :  60%|██████████████████████████████████████████                            | 30/50 [21:55<15:18, 45.90s/it]Evaluating commonsenseqa :  62%|███████████████████████████████████████████▍                          | 31/50 [22:54<15:46, 49.81s/it]Evaluating commonsenseqa :  64%|████████████████████████████████████████████▊                         | 32/50 [23:54<15:53, 52.97s/it]Evaluating commonsenseqa :  66%|██████████████████████████████████████████████▏                       | 33/50 [24:05<11:24, 40.28s/it]Evaluating commonsenseqa :  68%|███████████████████████████████████████████████▌                      | 34/50 [24:16<08:24, 31.51s/it]Evaluating commonsenseqa :  70%|█████████████████████████████████████████████████                     | 35/50 [24:50<08:04, 32.29s/it]Evaluating commonsenseqa :  72%|██████████████████████████████████████████████████▍                   | 36/50 [25:50<09:29, 40.66s/it]Evaluating commonsenseqa :  74%|███████████████████████████████████████████████████▊                  | 37/50 [26:51<10:06, 46.65s/it]Evaluating commonsenseqa :  76%|█████████████████████████████████████████████████████▏                | 38/50 [27:24<08:30, 42.57s/it]Evaluating commonsenseqa :  78%|██████████████████████████████████████████████████████▌               | 39/50 [27:37<06:11, 33.74s/it]Evaluating commonsenseqa :  80%|████████████████████████████████████████████████████████              | 40/50 [27:59<05:00, 30.08s/it]Evaluating commonsenseqa :  82%|█████████████████████████████████████████████████████████▍            | 41/50 [28:21<04:09, 27.75s/it]Evaluating commonsenseqa :  84%|██████████████████████████████████████████████████████████▊           | 42/50 [29:22<05:00, 37.62s/it]Evaluating commonsenseqa :  86%|████████████████████████████████████████████████████████████▏         | 43/50 [29:35<03:31, 30.26s/it]Evaluating commonsenseqa :  88%|█████████████████████████████████████████████████████████████▌        | 44/50 [30:33<03:52, 38.83s/it]Evaluating commonsenseqa :  90%|███████████████████████████████████████████████████████████████       | 45/50 [31:32<03:43, 44.67s/it]Evaluating commonsenseqa :  92%|████████████████████████████████████████████████████████████████▍     | 46/50 [31:55<02:32, 38.11s/it]Evaluating commonsenseqa :  94%|█████████████████████████████████████████████████████████████████▊    | 47/50 [32:54<02:13, 44.40s/it]Evaluating commonsenseqa :  96%|███████████████████████████████████████████████████████████████████▏  | 48/50 [33:15<01:15, 37.61s/it]Evaluating commonsenseqa :  98%|████████████████████████████████████████████████████████████████████▌ | 49/50 [33:35<00:32, 32.33s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████████████████████████████████████████| 50/50 [33:56<00:00, 28.85s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████████████████████████████████████████| 50/50 [33:56<00:00, 40.73s/it]
name: commonsenseqa | {'exact_match': 0.0, 'rougeL': 2.4229} | lm_loss 7.4626 | avg. gen lenth: 137.688
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model/llama-2-7b --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --seed 42 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s42-rTrue --rationales --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
using world size: 4
[2023-08-07 20:36:39,120] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model/llama-2-7b
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  model_parallel ............... False
  model_parallel_size .......... None
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s42-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 1
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 2048
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s42-rTrue
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  gradient_accumulation_steps .. 1
  batch_size ................... 5
  clip_grad .................... 1.0
  seed ......................... 42
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading Data
  0%|                                                                                                        | 0/9740 [00:00<?, ?it/s]  1%|▊                                                                                             | 82/9740 [00:00<00:11, 810.47it/s]  2%|█▌                                                                                           | 165/9740 [00:00<00:11, 817.49it/s]  3%|██▎                                                                                          | 247/9740 [00:00<00:11, 815.68it/s]  3%|███▏                                                                                         | 330/9740 [00:00<00:11, 818.11it/s]  4%|███▉                                                                                         | 413/9740 [00:00<00:11, 818.86it/s]  5%|████▋                                                                                        | 495/9740 [00:00<00:11, 818.15it/s]  6%|█████▌                                                                                       | 578/9740 [00:00<00:11, 819.52it/s]  7%|██████▎                                                                                      | 660/9740 [00:00<00:11, 801.90it/s]  8%|███████                                                                                      | 742/9740 [00:00<00:11, 805.11it/s]  8%|███████▊                                                                                     | 824/9740 [00:01<00:11, 809.49it/s]  9%|████████▋                                                                                    | 906/9740 [00:01<00:10, 811.75it/s] 10%|█████████▍                                                                                   | 988/9740 [00:01<00:10, 813.32it/s] 11%|██████████                                                                                  | 1071/9740 [00:01<00:10, 815.72it/s] 12%|██████████▉                                                                                 | 1153/9740 [00:01<00:10, 815.70it/s] 13%|███████████▋                                                                                | 1235/9740 [00:01<00:10, 813.58it/s] 14%|████████████▍                                                                               | 1317/9740 [00:01<00:10, 815.33it/s] 14%|█████████████▏                                                                              | 1399/9740 [00:01<00:10, 813.66it/s] 15%|█████████████▉                                                                              | 1481/9740 [00:01<00:10, 809.80it/s] 16%|██████████████▊                                                                             | 1563/9740 [00:01<00:10, 810.81it/s] 17%|███████████████▌                                                                            | 1645/9740 [00:02<00:09, 810.15it/s] 18%|████████████████▎                                                                           | 1727/9740 [00:02<00:09, 811.38it/s] 19%|█████████████████▎                                                                          | 1827/9740 [00:02<00:09, 866.77it/s] 20%|██████████████████▍                                                                         | 1949/9740 [00:02<00:08, 971.71it/s] 21%|███████████████████▎                                                                       | 2071/9740 [00:02<00:07, 1045.71it/s] 23%|████████████████████▍                                                                      | 2193/9740 [00:02<00:06, 1096.86it/s] 24%|█████████████████████▋                                                                     | 2315/9740 [00:02<00:06, 1131.06it/s] 25%|██████████████████████▊                                                                    | 2439/9740 [00:02<00:06, 1161.75it/s] 26%|███████████████████████▉                                                                   | 2562/9740 [00:02<00:06, 1179.69it/s] 28%|█████████████████████████                                                                  | 2686/9740 [00:02<00:05, 1195.50it/s] 29%|██████████████████████████▏                                                                | 2808/9740 [00:03<00:05, 1201.93it/s] 30%|███████████████████████████▎                                                               | 2929/9740 [00:03<00:05, 1200.68it/s] 31%|████████████████████████████▌                                                              | 3053/9740 [00:03<00:05, 1209.88it/s] 33%|█████████████████████████████▋                                                             | 3177/9740 [00:03<00:05, 1216.98it/s] 34%|██████████████████████████████▊                                                            | 3302/9740 [00:03<00:05, 1223.71it/s] 35%|████████████████████████████████                                                           | 3426/9740 [00:03<00:05, 1226.30it/s] 36%|█████████████████████████████████▏                                                         | 3550/9740 [00:03<00:05, 1227.77it/s] 38%|██████████████████████████████████▎                                                        | 3674/9740 [00:03<00:04, 1229.01it/s] 39%|███████████████████████████████████▌                                                       | 3800/9740 [00:03<00:04, 1235.38it/s] 40%|████████████████████████████████████▋                                                      | 3925/9740 [00:03<00:04, 1238.55it/s] 42%|█████████████████████████████████████▊                                                     | 4051/9740 [00:04<00:04, 1242.35it/s] 43%|███████████████████████████████████████                                                    | 4176/9740 [00:04<00:04, 1243.20it/s] 44%|████████████████████████████████████████▏                                                  | 4301/9740 [00:04<00:04, 1242.67it/s] 45%|█████████████████████████████████████████▎                                                 | 4426/9740 [00:04<00:04, 1239.28it/s] 47%|██████████████████████████████████████████▌                                                | 4550/9740 [00:04<00:04, 1172.48it/s] 48%|███████████████████████████████████████████▋                                               | 4675/9740 [00:04<00:04, 1194.40it/s] 49%|█████████████████████████████████████████████▎                                              | 4796/9740 [00:04<00:05, 911.42it/s] 51%|██████████████████████████████████████████████▍                                             | 4920/9740 [00:04<00:04, 990.26it/s] 52%|███████████████████████████████████████████████▏                                           | 5045/9740 [00:04<00:04, 1055.85it/s] 53%|████████████████████████████████████████████████▎                                          | 5169/9740 [00:05<00:04, 1104.60it/s] 54%|█████████████████████████████████████████████████▍                                         | 5294/9740 [00:05<00:03, 1142.71it/s] 56%|██████████████████████████████████████████████████▋                                        | 5419/9740 [00:05<00:03, 1170.50it/s] 57%|███████████████████████████████████████████████████▊                                       | 5548/9740 [00:05<00:03, 1202.33it/s] 58%|█████████████████████████████████████████████████████                                      | 5677/9740 [00:05<00:03, 1225.97it/s] 60%|██████████████████████████████████████████████████████▎                                    | 5807/9740 [00:05<00:03, 1247.38it/s] 61%|███████████████████████████████████████████████████████▍                                   | 5938/9740 [00:05<00:03, 1263.26it/s] 62%|████████████████████████████████████████████████████████▋                                  | 6068/9740 [00:05<00:02, 1272.30it/s] 64%|█████████████████████████████████████████████████████████▉                                 | 6198/9740 [00:05<00:02, 1280.36it/s] 65%|███████████████████████████████████████████████████████████                                | 6328/9740 [00:05<00:02, 1284.68it/s] 66%|████████████████████████████████████████████████████████████▎                              | 6459/9740 [00:06<00:02, 1289.40it/s] 68%|█████████████████████████████████████████████████████████████▌                             | 6589/9740 [00:06<00:02, 1291.53it/s] 69%|██████████████████████████████████████████████████████████████▊                            | 6720/9740 [00:06<00:02, 1295.60it/s] 70%|████████████████████████████████████████████████████████████████                           | 6851/9740 [00:06<00:02, 1297.57it/s] 72%|█████████████████████████████████████████████████████████████████▏                         | 6982/9740 [00:06<00:02, 1300.10it/s] 73%|██████████████████████████████████████████████████████████████████▍                        | 7113/9740 [00:06<00:02, 1298.81it/s] 74%|███████████████████████████████████████████████████████████████████▋                       | 7244/9740 [00:06<00:01, 1300.14it/s] 76%|████████████████████████████████████████████████████████████████████▉                      | 7375/9740 [00:06<00:01, 1296.72it/s] 77%|██████████████████████████████████████████████████████████████████████                     | 7505/9740 [00:06<00:01, 1266.23it/s] 78%|███████████████████████████████████████████████████████████████████████▎                   | 7635/9740 [00:06<00:01, 1274.56it/s] 80%|████████████████████████████████████████████████████████████████████████▌                  | 7763/9740 [00:07<00:01, 1275.17it/s] 81%|█████████████████████████████████████████████████████████████████████████▊                 | 7894/9740 [00:07<00:01, 1284.72it/s] 82%|██████████████████████████████████████████████████████████████████████████▉                | 8024/9740 [00:07<00:01, 1287.70it/s] 84%|████████████████████████████████████████████████████████████████████████████▏              | 8154/9740 [00:07<00:01, 1288.80it/s] 85%|█████████████████████████████████████████████████████████████████████████████▍             | 8283/9740 [00:07<00:01, 1284.00it/s] 86%|██████████████████████████████████████████████████████████████████████████████▌            | 8412/9740 [00:07<00:01, 1284.26it/s] 88%|███████████████████████████████████████████████████████████████████████████████▊           | 8542/9740 [00:07<00:00, 1288.40it/s] 89%|█████████████████████████████████████████████████████████████████████████████████          | 8671/9740 [00:07<00:00, 1287.97it/s] 90%|██████████████████████████████████████████████████████████████████████████████████▏        | 8800/9740 [00:07<00:00, 1287.70it/s] 92%|███████████████████████████████████████████████████████████████████████████████████▍       | 8929/9740 [00:07<00:00, 1286.15it/s] 93%|████████████████████████████████████████████████████████████████████████████████████▋      | 9058/9740 [00:08<00:00, 1284.97it/s] 94%|█████████████████████████████████████████████████████████████████████████████████████▊     | 9188/9740 [00:08<00:00, 1289.35it/s] 96%|███████████████████████████████████████████████████████████████████████████████████████    | 9318/9740 [00:08<00:00, 1290.41it/s] 97%|████████████████████████████████████████████████████████████████████████████████████████▎  | 9448/9740 [00:08<00:00, 1292.07it/s] 98%|█████████████████████████████████████████████████████████████████████████████████████████▍ | 9578/9740 [00:08<00:00, 1291.11it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████▋| 9708/9740 [00:08<00:00, 1291.91it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████| 9740/9740 [00:08<00:00, 1130.25it/s]
Load End
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.02s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.33s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.37s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.49s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:09<00:04,  4.91s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.30s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.36s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.49s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.33s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.50s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.69s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.86s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.76s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.92s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.79s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.98s/it]
[2023-08-07 20:37:40,165] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown
[2023-08-07 20:37:50,382] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-07 20:37:50,383] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:
[2023-08-07 20:37:50,383] [INFO] [config.py:1012:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-07 20:37:50,383] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-07 20:37:50,383] [INFO] [config.py:1012:print]   amp_enabled .................. False
[2023-08-07 20:37:50,383] [INFO] [config.py:1012:print]   amp_params ................... False
[2023-08-07 20:37:50,383] [INFO] [config.py:1012:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-07 20:37:50,383] [INFO] [config.py:1012:print]   bfloat16_enabled ............. False
[2023-08-07 20:37:50,383] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False
[2023-08-07 20:37:50,383] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True
[2023-08-07 20:37:50,383] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff2318431f0>
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   communication_data_type ...... None
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   disable_allgather ............ False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   dump_state ................... False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'min_scale': 1}
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   elasticity_enabled ........... False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   fp16_auto_cast ............... False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   fp16_enabled ................. True
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   global_rank .................. 0
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 2048
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   loss_scale ................... 0
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   memory_breakdown ............. False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7ff231843dc0>
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   optimizer_name ............... None
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   optimizer_params ............. None
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   pld_enabled .................. False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   pld_params ................... False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   prescale_gradients ........... False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   scheduler_name ............... None
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   scheduler_params ............. None
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   sparse_attention ............. None
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   steps_per_print .............. 1
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   train_batch_size ............. 20
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  5
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   use_node_local_storage ....... False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False
[2023-08-07 20:37:50,384] [INFO] [config.py:1012:print]   world_size ................... 4
[2023-08-07 20:37:50,385] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  True
[2023-08-07 20:37:50,385] [INFO] [config.py:1012:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-08-07 20:37:50,385] [INFO] [config.py:1012:print]   zero_enabled ................. False
[2023-08-07 20:37:50,385] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 0
[2023-08-07 20:37:50,385] [INFO] [config.py:997:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 5, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 1
}
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Emitting ninja build file /home/ylu130/.cache/torch_extensions/py39_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.37487220764160156 seconds
Model mem
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   12918 MB |   12918 MB |   12918 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      66 MB |      66 MB |      66 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |
|       from small pool |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|---------------------------------------------------------------------------|
| Allocations           |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     259    |     259    |     259    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |      33    |      33    |      33    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |      33    |      31    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       2    |       2    |      33    |      31    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Evaluating commonsenseqa :   0%|                                                                               | 0/50 [00:00<?, ?it/s]############### Example ###############
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:Answer the following multiple choice question.

### Demonstration:
Input: He fantasied about getting a what while driving to work and the pros and cons of the extra responsibilities and benefits? Choices:  A: new car B: promotion C: boredom D: impatience E: pressure
Rationales: 1. The question tells us that the subject is fantasizing about something while driving to work. This suggests that whatever they are fantasizing about is future-oriented and probably related to their job or work life.
2. The phrase "pros and cons of the extra responsibilities and benefits" also provides a substantial hint. We can analyze it piece by piece:
    - "extra responsibilities" suggests an increase in the level of accountability or tasks, what would not arise from getting a new car, experiencing boredom, impatience, or pressure.
    - "benefits" often refers to advantageous or positive results from something, and is commonly used in a work context to refer to employee benefits. This narrows our possible answers even more, leaning towards something related to career advancement.
3. Taking into account the context and the analysis of the clues given in the problem, the most logical answer would be "promotion," as this is something one would fantasize about during a commute and indeed has both extra responsibilities and potential benefits. Hence, the correct answer is B: promotion.
Answer: B: promotion

### Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid

### Response:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s42-rTrue
Loading extension module utils...
Time to load utils op: 0.40451693534851074 seconds
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.30624818801879883 seconds
Time to load utils op: 0.3043949604034424 seconds
Evaluating commonsenseqa :   2%|█▍                                                                     | 1/50 [00:53<43:39, 53.46s/it]Evaluating commonsenseqa :   4%|██▊                                                                    | 2/50 [01:46<42:24, 53.01s/it]Evaluating commonsenseqa :   6%|████▎                                                                  | 3/50 [02:38<41:27, 52.92s/it]Evaluating commonsenseqa :   8%|█████▋                                                                 | 4/50 [03:08<33:22, 43.53s/it]Evaluating commonsenseqa :  10%|███████                                                                | 5/50 [03:31<27:16, 36.37s/it]Evaluating commonsenseqa :  12%|████████▌                                                              | 6/50 [04:24<30:48, 42.00s/it]Evaluating commonsenseqa :  14%|█████████▉                                                             | 7/50 [05:16<32:24, 45.22s/it]Evaluating commonsenseqa :  16%|███████████▎                                                           | 8/50 [06:08<33:05, 47.27s/it]Evaluating commonsenseqa :  18%|████████████▊                                                          | 9/50 [06:59<33:12, 48.61s/it]Evaluating commonsenseqa :  20%|██████████████                                                        | 10/50 [07:52<33:13, 49.84s/it]Evaluating commonsenseqa :  22%|███████████████▍                                                      | 11/50 [08:45<33:03, 50.87s/it]Evaluating commonsenseqa :  24%|████████████████▊                                                     | 12/50 [09:38<32:32, 51.38s/it]Evaluating commonsenseqa :  26%|██████████████████▏                                                   | 13/50 [10:30<31:49, 51.60s/it]Evaluating commonsenseqa :  28%|███████████████████▌                                                  | 14/50 [11:22<31:08, 51.90s/it]Evaluating commonsenseqa :  30%|█████████████████████                                                 | 15/50 [12:14<30:14, 51.84s/it]Evaluating commonsenseqa :  32%|██████████████████████▍                                               | 16/50 [13:05<29:17, 51.70s/it]Evaluating commonsenseqa :  34%|███████████████████████▊                                              | 17/50 [13:57<28:27, 51.76s/it]Evaluating commonsenseqa :  36%|█████████████████████████▏                                            | 18/50 [14:50<27:45, 52.05s/it]Evaluating commonsenseqa :  38%|██████████████████████████▌                                           | 19/50 [15:43<27:00, 52.26s/it]Evaluating commonsenseqa :  40%|████████████████████████████                                          | 20/50 [16:40<26:53, 53.77s/it]Evaluating commonsenseqa :  42%|█████████████████████████████▍                                        | 21/50 [17:32<25:46, 53.32s/it]Evaluating commonsenseqa :  44%|██████████████████████████████▊                                       | 22/50 [17:54<20:30, 43.94s/it]Evaluating commonsenseqa :  46%|████████████████████████████████▏                                     | 23/50 [18:47<20:57, 46.56s/it]Evaluating commonsenseqa :  48%|█████████████████████████████████▌                                    | 24/50 [19:38<20:48, 48.01s/it]Evaluating commonsenseqa :  50%|███████████████████████████████████                                   | 25/50 [20:32<20:38, 49.55s/it]Evaluating commonsenseqa :  52%|████████████████████████████████████▍                                 | 26/50 [21:24<20:12, 50.50s/it]Evaluating commonsenseqa :  54%|█████████████████████████████████████▊                                | 27/50 [22:17<19:36, 51.14s/it]Evaluating commonsenseqa :  56%|███████████████████████████████████████▏                              | 28/50 [23:09<18:53, 51.53s/it]Evaluating commonsenseqa :  58%|████████████████████████████████████████▌                             | 29/50 [24:01<18:03, 51.60s/it]Evaluating commonsenseqa :  60%|██████████████████████████████████████████                            | 30/50 [24:55<17:27, 52.38s/it]Evaluating commonsenseqa :  62%|███████████████████████████████████████████▍                          | 31/50 [25:48<16:36, 52.47s/it]Evaluating commonsenseqa :  64%|████████████████████████████████████████████▊                         | 32/50 [26:41<15:47, 52.62s/it]Evaluating commonsenseqa :  66%|██████████████████████████████████████████████▏                       | 33/50 [27:28<14:27, 51.02s/it]Evaluating commonsenseqa :  68%|███████████████████████████████████████████████▌                      | 34/50 [28:21<13:44, 51.52s/it]Evaluating commonsenseqa :  70%|█████████████████████████████████████████████████                     | 35/50 [29:13<12:55, 51.67s/it]Evaluating commonsenseqa :  72%|██████████████████████████████████████████████████▍                   | 36/50 [29:58<11:35, 49.67s/it]Evaluating commonsenseqa :  74%|███████████████████████████████████████████████████▊                  | 37/50 [30:50<10:55, 50.40s/it]Evaluating commonsenseqa :  76%|█████████████████████████████████████████████████████▏                | 38/50 [31:42<10:11, 50.97s/it]Evaluating commonsenseqa :  78%|██████████████████████████████████████████████████████▌               | 39/50 [32:35<09:26, 51.50s/it]Evaluating commonsenseqa :  80%|████████████████████████████████████████████████████████              | 40/50 [33:04<07:26, 44.64s/it]Evaluating commonsenseqa :  82%|█████████████████████████████████████████████████████████▍            | 41/50 [33:57<07:05, 47.28s/it]Evaluating commonsenseqa :  84%|██████████████████████████████████████████████████████████▊           | 42/50 [34:50<06:32, 49.00s/it]Evaluating commonsenseqa :  86%|████████████████████████████████████████████████████████████▏         | 43/50 [35:42<05:49, 49.97s/it]Evaluating commonsenseqa :  88%|█████████████████████████████████████████████████████████████▌        | 44/50 [36:35<05:05, 50.88s/it]Evaluating commonsenseqa :  90%|███████████████████████████████████████████████████████████████       | 45/50 [37:28<04:16, 51.38s/it]Evaluating commonsenseqa :  92%|████████████████████████████████████████████████████████████████▍     | 46/50 [38:21<03:27, 51.90s/it]Evaluating commonsenseqa :  94%|█████████████████████████████████████████████████████████████████▊    | 47/50 [39:15<02:37, 52.63s/it]Evaluating commonsenseqa :  96%|███████████████████████████████████████████████████████████████████▏  | 48/50 [40:09<01:45, 52.82s/it]Evaluating commonsenseqa :  98%|████████████████████████████████████████████████████████████████████▌ | 49/50 [41:01<00:52, 52.56s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████████████████████████████████████████| 50/50 [41:53<00:00, 52.58s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████████████████████████████████████████| 50/50 [41:53<00:00, 50.28s/it]
name: commonsenseqa | {'exact_match': 0.0, 'rougeL': 1.1514} | lm_loss 7.4712 | avg. gen lenth: 295.16
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model/llama-2-7b --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --seed 42 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s42-rTrue --rationales --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
using world size: 4
[2023-08-07 21:19:58,070] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model/llama-2-7b
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  model_parallel ............... False
  model_parallel_size .......... None
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s42-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 2
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 2048
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s42-rTrue
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  gradient_accumulation_steps .. 1
  batch_size ................... 5
  clip_grad .................... 1.0
  seed ......................... 42
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading Data
  0%|                                                                                                        | 0/9739 [00:00<?, ?it/s]  1%|▌                                                                                             | 55/9739 [00:00<00:17, 546.77it/s]  1%|█                                                                                            | 111/9739 [00:00<00:17, 551.47it/s]  2%|█▌                                                                                           | 167/9739 [00:00<00:17, 551.71it/s]  2%|██▏                                                                                          | 223/9739 [00:00<00:17, 549.82it/s]  3%|██▋                                                                                          | 279/9739 [00:00<00:17, 551.06it/s]  3%|███▏                                                                                         | 335/9739 [00:00<00:17, 551.50it/s]  4%|███▋                                                                                         | 391/9739 [00:00<00:17, 543.07it/s]  5%|████▎                                                                                        | 446/9739 [00:00<00:17, 540.62it/s]  5%|████▊                                                                                        | 502/9739 [00:00<00:16, 543.63it/s]  6%|█████▎                                                                                       | 558/9739 [00:01<00:16, 545.74it/s]  6%|█████▊                                                                                       | 613/9739 [00:01<00:16, 546.90it/s]  7%|██████▍                                                                                      | 668/9739 [00:01<00:16, 546.48it/s]  7%|██████▉                                                                                      | 724/9739 [00:01<00:16, 547.77it/s]  8%|███████▍                                                                                     | 780/9739 [00:01<00:16, 548.51it/s]  9%|███████▉                                                                                     | 835/9739 [00:01<00:16, 548.17it/s]  9%|████████▍                                                                                    | 890/9739 [00:01<00:16, 548.08it/s] 10%|█████████                                                                                    | 945/9739 [00:01<00:16, 546.52it/s] 10%|█████████▍                                                                                  | 1000/9739 [00:01<00:15, 546.79it/s] 11%|█████████▉                                                                                  | 1055/9739 [00:01<00:15, 547.72it/s] 11%|██████████▍                                                                                 | 1111/9739 [00:02<00:15, 549.28it/s] 12%|███████████                                                                                 | 1166/9739 [00:02<00:15, 548.27it/s] 13%|███████████▌                                                                                | 1221/9739 [00:02<00:15, 548.05it/s] 13%|████████████                                                                                | 1276/9739 [00:02<00:15, 547.56it/s] 14%|████████████▌                                                                               | 1331/9739 [00:02<00:15, 547.98it/s] 14%|█████████████                                                                               | 1386/9739 [00:02<00:15, 546.00it/s] 15%|█████████████▊                                                                              | 1465/9739 [00:02<00:13, 616.88it/s] 16%|██████████████▋                                                                             | 1549/9739 [00:02<00:11, 683.05it/s] 17%|███████████████▍                                                                            | 1632/9739 [00:02<00:11, 726.45it/s] 18%|████████████████▏                                                                           | 1716/9739 [00:02<00:10, 759.76it/s] 18%|████████████████▉                                                                           | 1795/9739 [00:03<00:10, 763.60it/s] 19%|█████████████████▊                                                                          | 1879/9739 [00:03<00:10, 784.19it/s] 20%|██████████████████▍                                                                         | 1958/9739 [00:03<00:09, 778.40it/s] 21%|███████████████████▎                                                                        | 2041/9739 [00:03<00:09, 792.91it/s] 22%|████████████████████                                                                        | 2124/9739 [00:03<00:09, 802.58it/s] 23%|████████████████████▊                                                                       | 2208/9739 [00:03<00:09, 811.58it/s] 24%|█████████████████████▋                                                                      | 2291/9739 [00:03<00:09, 815.27it/s] 24%|██████████████████████▍                                                                     | 2374/9739 [00:03<00:08, 819.34it/s] 25%|███████████████████████▏                                                                    | 2458/9739 [00:03<00:08, 822.75it/s] 26%|████████████████████████                                                                    | 2541/9739 [00:03<00:08, 803.33it/s] 27%|████████████████████████▊                                                                   | 2624/9739 [00:04<00:08, 809.93it/s] 28%|█████████████████████████▌                                                                  | 2706/9739 [00:04<00:08, 811.15it/s] 29%|██████████████████████████▎                                                                 | 2789/9739 [00:04<00:08, 814.05it/s] 29%|███████████████████████████                                                                 | 2871/9739 [00:04<00:08, 813.13it/s] 30%|███████████████████████████▉                                                                | 2953/9739 [00:04<00:08, 814.10it/s] 31%|████████████████████████████▋                                                               | 3035/9739 [00:04<00:08, 801.12it/s] 32%|█████████████████████████████▍                                                              | 3117/9739 [00:04<00:08, 805.73it/s] 33%|██████████████████████████████▏                                                             | 3199/9739 [00:04<00:08, 808.94it/s] 34%|███████████████████████████████                                                             | 3282/9739 [00:04<00:07, 813.55it/s] 35%|███████████████████████████████▊                                                            | 3365/9739 [00:04<00:07, 816.73it/s] 35%|████████████████████████████████▌                                                           | 3447/9739 [00:05<00:07, 817.44it/s] 36%|█████████████████████████████████▎                                                          | 3530/9739 [00:05<00:07, 819.16it/s] 37%|██████████████████████████████████                                                          | 3612/9739 [00:05<00:07, 799.10it/s] 38%|██████████████████████████████████▉                                                         | 3695/9739 [00:05<00:07, 806.95it/s] 39%|███████████████████████████████████▋                                                        | 3778/9739 [00:05<00:07, 811.53it/s] 40%|████████████████████████████████████▍                                                       | 3860/9739 [00:05<00:07, 813.66it/s] 40%|█████████████████████████████████████▏                                                      | 3942/9739 [00:05<00:07, 809.97it/s] 41%|██████████████████████████████████████                                                      | 4025/9739 [00:05<00:07, 814.72it/s] 42%|██████████████████████████████████████▊                                                     | 4107/9739 [00:05<00:06, 816.00it/s] 43%|███████████████████████████████████████▌                                                    | 4189/9739 [00:05<00:06, 802.11it/s] 44%|████████████████████████████████████████▎                                                   | 4272/9739 [00:06<00:06, 808.68it/s] 45%|█████████████████████████████████████████▏                                                  | 4354/9739 [00:06<00:06, 810.65it/s] 46%|█████████████████████████████████████████▉                                                  | 4437/9739 [00:06<00:06, 815.24it/s] 46%|██████████████████████████████████████████▋                                                 | 4519/9739 [00:06<00:06, 788.19it/s] 47%|███████████████████████████████████████████▍                                                | 4602/9739 [00:06<00:06, 798.53it/s] 48%|████████████████████████████████████████████▏                                               | 4683/9739 [00:06<00:06, 789.03it/s] 49%|████████████████████████████████████████████▉                                               | 4763/9739 [00:06<00:08, 620.47it/s] 50%|█████████████████████████████████████████████▊                                              | 4847/9739 [00:06<00:07, 672.92it/s] 51%|██████████████████████████████████████████████▌                                             | 4928/9739 [00:07<00:06, 708.37it/s] 51%|███████████████████████████████████████████████▎                                            | 5010/9739 [00:07<00:06, 738.20it/s] 52%|████████████████████████████████████████████████                                            | 5094/9739 [00:07<00:06, 765.47it/s] 53%|████████████████████████████████████████████████▊                                           | 5173/9739 [00:07<00:05, 768.91it/s] 54%|█████████████████████████████████████████████████▋                                          | 5256/9739 [00:07<00:05, 785.98it/s] 55%|██████████████████████████████████████████████████▍                                         | 5340/9739 [00:07<00:05, 799.56it/s] 56%|███████████████████████████████████████████████████▏                                        | 5423/9739 [00:07<00:05, 807.42it/s] 57%|████████████████████████████████████████████████████                                        | 5509/9739 [00:07<00:05, 821.32it/s] 57%|████████████████████████████████████████████████████▊                                       | 5597/9739 [00:07<00:04, 835.84it/s] 58%|█████████████████████████████████████████████████████▋                                      | 5682/9739 [00:07<00:04, 838.43it/s] 59%|██████████████████████████████████████████████████████▍                                     | 5767/9739 [00:08<00:04, 834.57it/s] 60%|███████████████████████████████████████████████████████▎                                    | 5855/9739 [00:08<00:04, 845.29it/s] 61%|████████████████████████████████████████████████████████▏                                   | 5942/9739 [00:08<00:04, 850.06it/s] 62%|████████████████████████████████████████████████████████▉                                   | 6029/9739 [00:08<00:04, 854.31it/s] 63%|█████████████████████████████████████████████████████████▊                                  | 6116/9739 [00:08<00:04, 856.91it/s] 64%|██████████████████████████████████████████████████████████▌                                 | 6204/9739 [00:08<00:04, 861.12it/s] 65%|███████████████████████████████████████████████████████████▍                                | 6291/9739 [00:08<00:04, 847.11it/s] 65%|████████████████████████████████████████████████████████████▏                               | 6377/9739 [00:08<00:03, 849.09it/s] 66%|█████████████████████████████████████████████████████████████                               | 6464/9739 [00:08<00:03, 854.27it/s] 67%|█████████████████████████████████████████████████████████████▊                              | 6550/9739 [00:08<00:03, 855.72it/s] 68%|██████████████████████████████████████████████████████████████▋                             | 6638/9739 [00:09<00:03, 860.05it/s] 69%|███████████████████████████████████████████████████████████████▌                            | 6726/9739 [00:09<00:03, 863.45it/s] 70%|████████████████████████████████████████████████████████████████▎                           | 6813/9739 [00:09<00:03, 862.57it/s] 71%|█████████████████████████████████████████████████████████████████▏                          | 6900/9739 [00:09<00:03, 844.85it/s] 72%|██████████████████████████████████████████████████████████████████                          | 6988/9739 [00:09<00:03, 852.42it/s] 73%|██████████████████████████████████████████████████████████████████▊                         | 7074/9739 [00:09<00:03, 853.48it/s] 74%|███████████████████████████████████████████████████████████████████▋                        | 7161/9739 [00:09<00:03, 856.86it/s] 74%|████████████████████████████████████████████████████████████████████▍                       | 7248/9739 [00:09<00:02, 857.97it/s] 75%|█████████████████████████████████████████████████████████████████████▎                      | 7336/9739 [00:09<00:02, 862.10it/s] 76%|██████████████████████████████████████████████████████████████████████▏                     | 7424/9739 [00:09<00:02, 865.73it/s] 77%|██████████████████████████████████████████████████████████████████████▉                     | 7511/9739 [00:10<00:02, 824.86it/s] 78%|███████████████████████████████████████████████████████████████████████▊                    | 7598/9739 [00:10<00:02, 836.26it/s] 79%|████████████████████████████████████████████████████████████████████████▌                   | 7685/9739 [00:10<00:02, 843.81it/s] 80%|█████████████████████████████████████████████████████████████████████████▍                  | 7772/9739 [00:10<00:02, 851.13it/s] 81%|██████████████████████████████████████████████████████████████████████████▏                 | 7858/9739 [00:10<00:02, 851.80it/s] 82%|███████████████████████████████████████████████████████████████████████████                 | 7944/9739 [00:10<00:02, 842.89it/s] 82%|███████████████████████████████████████████████████████████████████████████▊                | 8031/9739 [00:10<00:02, 850.68it/s] 83%|████████████████████████████████████████████████████████████████████████████▋               | 8118/9739 [00:10<00:01, 855.19it/s] 84%|█████████████████████████████████████████████████████████████████████████████▍              | 8204/9739 [00:10<00:01, 856.29it/s] 85%|██████████████████████████████████████████████████████████████████████████████▎             | 8291/9739 [00:10<00:01, 859.67it/s] 86%|███████████████████████████████████████████████████████████████████████████████▏            | 8378/9739 [00:11<00:01, 859.36it/s] 87%|███████████████████████████████████████████████████████████████████████████████▉            | 8464/9739 [00:11<00:01, 842.86it/s] 88%|████████████████████████████████████████████████████████████████████████████████▊           | 8551/9739 [00:11<00:01, 850.58it/s] 89%|█████████████████████████████████████████████████████████████████████████████████▌          | 8637/9739 [00:11<00:01, 846.17it/s] 90%|██████████████████████████████████████████████████████████████████████████████████▍         | 8724/9739 [00:11<00:01, 853.15it/s] 90%|███████████████████████████████████████████████████████████████████████████████████▏        | 8811/9739 [00:11<00:01, 856.29it/s] 91%|████████████████████████████████████████████████████████████████████████████████████        | 8898/9739 [00:11<00:00, 859.74it/s] 92%|████████████████████████████████████████████████████████████████████████████████████▉       | 8985/9739 [00:11<00:00, 856.68it/s] 93%|█████████████████████████████████████████████████████████████████████████████████████▋      | 9071/9739 [00:11<00:00, 844.48it/s] 94%|██████████████████████████████████████████████████████████████████████████████████████▌     | 9159/9739 [00:11<00:00, 852.24it/s] 95%|███████████████████████████████████████████████████████████████████████████████████████▎    | 9246/9739 [00:12<00:00, 856.47it/s] 96%|████████████████████████████████████████████████████████████████████████████████████████▏   | 9333/9739 [00:12<00:00, 858.94it/s] 97%|████████████████████████████████████████████████████████████████████████████████████████▉   | 9420/9739 [00:12<00:00, 862.16it/s] 98%|█████████████████████████████████████████████████████████████████████████████████████████▊  | 9507/9739 [00:12<00:00, 861.16it/s] 99%|██████████████████████████████████████████████████████████████████████████████████████████▋ | 9594/9739 [00:12<00:00, 843.02it/s] 99%|███████████████████████████████████████████████████████████████████████████████████████████▍| 9681/9739 [00:12<00:00, 850.08it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████| 9739/9739 [00:12<00:00, 768.99it/s]
Load End
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.10s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:11,  5.63s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.18s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.32s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.31s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.27s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.20s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:11<00:05,  5.79s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.75s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.88s/it]
[2023-08-07 21:21:03,074] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.56s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.74s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.56s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.75s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  5.07s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  5.25s/it]
[2023-08-07 21:21:13,705] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-07 21:21:13,706] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:
[2023-08-07 21:21:13,707] [INFO] [config.py:1012:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-07 21:21:13,707] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-07 21:21:13,707] [INFO] [config.py:1012:print]   amp_enabled .................. False
[2023-08-07 21:21:13,707] [INFO] [config.py:1012:print]   amp_params ................... False
[2023-08-07 21:21:13,707] [INFO] [config.py:1012:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-07 21:21:13,707] [INFO] [config.py:1012:print]   bfloat16_enabled ............. False
[2023-08-07 21:21:13,707] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f51a5ff21f0>
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   communication_data_type ...... None
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   disable_allgather ............ False
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   dump_state ................... False
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'min_scale': 1}
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   elasticity_enabled ........... False
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   fp16_auto_cast ............... False
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   fp16_enabled ................. True
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   global_rank .................. 0
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 2048
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   loss_scale ................... 0
[2023-08-07 21:21:13,708] [INFO] [config.py:1012:print]   memory_breakdown ............. False
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f51a5ff2dc0>
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   optimizer_name ............... None
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   optimizer_params ............. None
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   pld_enabled .................. False
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   pld_params ................... False
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   prescale_gradients ........... False
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   scheduler_name ............... None
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   scheduler_params ............. None
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   sparse_attention ............. None
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   steps_per_print .............. 1
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   train_batch_size ............. 20
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  5
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   use_node_local_storage ....... False
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   world_size ................... 4
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  True
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   zero_enabled ................. False
[2023-08-07 21:21:13,709] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 0
[2023-08-07 21:21:13,709] [INFO] [config.py:997:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 5, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 1
}
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Emitting ninja build file /home/ylu130/.cache/torch_extensions/py39_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.46715688705444336 seconds
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.5046055316925049 seconds
Loading extension module utils...
Time to load utils op: 0.40421533584594727 seconds
Time to load utils op: 0.40780210494995117 seconds
Model mem
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   12918 MB |   12918 MB |   12918 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      66 MB |      66 MB |      66 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |
|       from small pool |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|---------------------------------------------------------------------------|
| Allocations           |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     259    |     259    |     259    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |      33    |      33    |      33    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |      33    |      31    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       2    |       2    |      33    |      31    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Evaluating commonsenseqa :   0%|                                                                               | 0/50 [00:00<?, ?it/s]############### Example ###############
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:Answer the following multiple choice question.

### Demonstration:
Input: He fantasied about getting a what while driving to work and the pros and cons of the extra responsibilities and benefits? Choices:  A: new car B: promotion C: boredom D: impatience E: pressure
Rationales: The statement presents a scenario where a person is daydreaming about a potential change that brings not only extra responsibilities but also benefits. 

Analyzing all the choices given: 
A: A new car would be a material possession. While it could bring along responsibilities and benefits, the phrase "extra responsibilities" is generally not associated with a car ownership. 
B: A promotion, in a work context, would bring about additional responsibilities since one would be tasked with more complex duties. However, it would also come with benefits such as higher pay and possibly more workplace perks. 
C: Boredom doesn't fit into this context since it is a feeling and does not carry responsibilities or benefits.
D: Impatience, just like boredom, is a feeling and does not involve responsibilities or benefits. 
E: Pressure could mean additional responsibilities, but it doesn't bring benefits with it, especially not in a work context. 

Among all the options, B: Promotion best fits the context and makes most sense in this scenario. Thus, the answer is B: Promotion.
Answer: B: promotion

Input: He was good at traditional science but excelled at social science, his favorite subject was what? Choices:  A: geography B: history studies C: math D: religion E: dancing
Rationales: 1. The question indicates that the person was good at traditional science.
2. However, the question also states that he excelled at social science.
3. Social science includes various subjects such as sociology, anthropology, political science, psychology, social work, and history.
4. The question does not specify the exact social science where he excelled, but it indicates it was his favorite subject.
5. Among the given options, only history studies fall under social sciences. 
6. Given this, we can determine that his favorite subject is history studies. 
7. Therefore, the answer is B: history studies.
Answer: B: history studies

### Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid

### Response:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s42-rTrue
Evaluating commonsenseqa :   2%|█▍                                                                     | 1/50 [00:49<40:34, 49.69s/it]Evaluating commonsenseqa :   4%|██▊                                                                    | 2/50 [01:40<40:12, 50.25s/it]Evaluating commonsenseqa :   6%|████▎                                                                  | 3/50 [02:29<39:08, 49.97s/it]Evaluating commonsenseqa :   8%|█████▋                                                                 | 4/50 [03:19<38:06, 49.72s/it]Evaluating commonsenseqa :  10%|███████                                                                | 5/50 [04:09<37:18, 49.74s/it]Evaluating commonsenseqa :  12%|████████▌                                                              | 6/50 [05:00<36:49, 50.22s/it]Evaluating commonsenseqa :  14%|█████████▉                                                             | 7/50 [05:49<35:44, 49.88s/it]Evaluating commonsenseqa :  16%|███████████▎                                                           | 8/50 [06:39<34:54, 49.87s/it]Evaluating commonsenseqa :  18%|████████████▊                                                          | 9/50 [07:29<34:05, 49.90s/it]Evaluating commonsenseqa :  20%|██████████████                                                        | 10/50 [08:17<33:00, 49.52s/it]Evaluating commonsenseqa :  22%|███████████████▍                                                      | 11/50 [08:52<29:13, 44.96s/it]Evaluating commonsenseqa :  24%|████████████████▊                                                     | 12/50 [09:42<29:29, 46.56s/it]Evaluating commonsenseqa :  26%|██████████████████▏                                                   | 13/50 [10:32<29:18, 47.52s/it]Evaluating commonsenseqa :  28%|███████████████████▌                                                  | 14/50 [11:22<29:00, 48.35s/it]Evaluating commonsenseqa :  30%|█████████████████████                                                 | 15/50 [11:57<25:49, 44.27s/it]Evaluating commonsenseqa :  32%|██████████████████████▍                                               | 16/50 [12:47<25:58, 45.84s/it]Evaluating commonsenseqa :  34%|███████████████████████▊                                              | 17/50 [13:37<25:53, 47.08s/it]Evaluating commonsenseqa :  36%|█████████████████████████▏                                            | 18/50 [14:26<25:29, 47.81s/it]Evaluating commonsenseqa :  38%|██████████████████████████▌                                           | 19/50 [15:17<25:08, 48.67s/it]Evaluating commonsenseqa :  40%|████████████████████████████                                          | 20/50 [16:06<24:29, 48.98s/it]Evaluating commonsenseqa :  42%|█████████████████████████████▍                                        | 21/50 [16:55<23:39, 48.94s/it]Evaluating commonsenseqa :  44%|██████████████████████████████▊                                       | 22/50 [17:24<19:58, 42.82s/it]Evaluating commonsenseqa :  46%|████████████████████████████████▏                                     | 23/50 [17:50<16:59, 37.75s/it]Evaluating commonsenseqa :  48%|█████████████████████████████████▌                                    | 24/50 [18:39<17:52, 41.25s/it]Evaluating commonsenseqa :  50%|███████████████████████████████████                                   | 25/50 [19:28<18:07, 43.50s/it]Evaluating commonsenseqa :  52%|████████████████████████████████████▍                                 | 26/50 [20:18<18:11, 45.47s/it]Evaluating commonsenseqa :  54%|█████████████████████████████████████▊                                | 27/50 [21:08<17:59, 46.95s/it]Evaluating commonsenseqa :  56%|███████████████████████████████████████▏                              | 28/50 [21:58<17:27, 47.62s/it]Evaluating commonsenseqa :  58%|████████████████████████████████████████▌                             | 29/50 [22:47<16:49, 48.05s/it]Evaluating commonsenseqa :  60%|██████████████████████████████████████████                            | 30/50 [23:36<16:08, 48.40s/it]Evaluating commonsenseqa :  62%|███████████████████████████████████████████▍                          | 31/50 [24:28<15:41, 49.55s/it]Evaluating commonsenseqa :  64%|████████████████████████████████████████████▊                         | 32/50 [25:17<14:49, 49.40s/it]Evaluating commonsenseqa :  66%|██████████████████████████████████████████████▏                       | 33/50 [26:07<14:01, 49.52s/it]Evaluating commonsenseqa :  68%|███████████████████████████████████████████████▌                      | 34/50 [26:57<13:15, 49.71s/it]Evaluating commonsenseqa :  70%|█████████████████████████████████████████████████                     | 35/50 [27:47<12:25, 49.72s/it]Evaluating commonsenseqa :  72%|██████████████████████████████████████████████████▍                   | 36/50 [28:37<11:37, 49.81s/it]Evaluating commonsenseqa :  74%|███████████████████████████████████████████████████▊                  | 37/50 [29:26<10:46, 49.70s/it]Evaluating commonsenseqa :  76%|█████████████████████████████████████████████████████▏                | 38/50 [30:15<09:54, 49.53s/it]Evaluating commonsenseqa :  78%|██████████████████████████████████████████████████████▌               | 39/50 [31:05<09:03, 49.45s/it]Evaluating commonsenseqa :  80%|████████████████████████████████████████████████████████              | 40/50 [31:55<08:16, 49.66s/it]Evaluating commonsenseqa :  82%|█████████████████████████████████████████████████████████▍            | 41/50 [32:31<06:51, 45.70s/it]Evaluating commonsenseqa :  84%|██████████████████████████████████████████████████████████▊           | 42/50 [33:21<06:14, 46.79s/it]Evaluating commonsenseqa :  86%|████████████████████████████████████████████████████████████▏         | 43/50 [34:10<05:33, 47.59s/it]Evaluating commonsenseqa :  88%|█████████████████████████████████████████████████████████████▌        | 44/50 [35:00<04:50, 48.38s/it]Evaluating commonsenseqa :  90%|███████████████████████████████████████████████████████████████       | 45/50 [35:50<04:03, 48.78s/it]Evaluating commonsenseqa :  92%|████████████████████████████████████████████████████████████████▍     | 46/50 [36:40<03:16, 49.07s/it]Evaluating commonsenseqa :  94%|█████████████████████████████████████████████████████████████████▊    | 47/50 [37:30<02:28, 49.35s/it]Evaluating commonsenseqa :  96%|███████████████████████████████████████████████████████████████████▏  | 48/50 [38:20<01:38, 49.50s/it]Evaluating commonsenseqa :  98%|████████████████████████████████████████████████████████████████████▌ | 49/50 [38:59<00:46, 46.52s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████████████████████████████████████████| 50/50 [39:49<00:00, 47.45s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████████████████████████████████████████| 50/50 [39:49<00:00, 47.79s/it]
name: commonsenseqa | {'exact_match': 0.0, 'rougeL': 1.2728} | lm_loss 7.589 | avg. gen lenth: 305.876
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model/llama-2-7b --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --seed 42 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s42-rTrue --rationales --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
using world size: 4
[2023-08-07 22:02:01,852] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model/llama-2-7b
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  model_parallel ............... False
  model_parallel_size .......... None
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s42-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 3
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 2048
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i3-s42-rTrue
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  gradient_accumulation_steps .. 1
  batch_size ................... 5
  clip_grad .................... 1.0
  seed ......................... 42
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading Data
  0%|                                                                                                        | 0/9738 [00:00<?, ?it/s]  0%|▎                                                                                             | 29/9738 [00:00<00:34, 283.57it/s]  1%|▌                                                                                             | 61/9738 [00:00<00:31, 303.94it/s]  1%|▉                                                                                             | 94/9738 [00:00<00:30, 315.37it/s]  1%|█▏                                                                                           | 127/9738 [00:00<00:29, 320.57it/s]  2%|█▌                                                                                           | 160/9738 [00:00<00:29, 323.80it/s]  2%|█▊                                                                                           | 193/9738 [00:00<00:29, 324.98it/s]  2%|██▏                                                                                          | 226/9738 [00:00<00:29, 324.98it/s]  3%|██▍                                                                                          | 259/9738 [00:00<00:30, 308.71it/s]  3%|██▊                                                                                          | 292/9738 [00:00<00:30, 314.34it/s]  3%|███                                                                                          | 325/9738 [00:01<00:29, 318.53it/s]  4%|███▍                                                                                         | 358/9738 [00:01<00:29, 320.91it/s]  4%|███▋                                                                                         | 391/9738 [00:01<00:28, 323.39it/s]  4%|████                                                                                         | 424/9738 [00:01<00:28, 324.70it/s]  5%|████▎                                                                                        | 457/9738 [00:01<00:28, 324.30it/s]  5%|████▋                                                                                        | 490/9738 [00:01<00:29, 311.06it/s]  5%|████▉                                                                                        | 523/9738 [00:01<00:29, 316.20it/s]  6%|█████▎                                                                                       | 556/9738 [00:01<00:28, 319.84it/s]  6%|█████▋                                                                                       | 589/9738 [00:01<00:28, 320.65it/s]  6%|█████▉                                                                                       | 622/9738 [00:01<00:28, 322.26it/s]  7%|██████▎                                                                                      | 655/9738 [00:02<00:28, 324.14it/s]  7%|██████▌                                                                                      | 688/9738 [00:02<00:27, 324.41it/s]  7%|██████▉                                                                                      | 721/9738 [00:02<00:28, 314.36it/s]  8%|███████▏                                                                                     | 754/9738 [00:02<00:28, 318.55it/s]  8%|███████▌                                                                                     | 787/9738 [00:02<00:27, 321.57it/s]  8%|███████▊                                                                                     | 820/9738 [00:02<00:27, 323.62it/s]  9%|████████▏                                                                                    | 853/9738 [00:02<00:27, 325.19it/s]  9%|████████▍                                                                                    | 886/9738 [00:02<00:27, 326.24it/s]  9%|████████▊                                                                                    | 919/9738 [00:02<00:28, 312.48it/s] 10%|█████████▏                                                                                   | 966/9738 [00:02<00:24, 355.96it/s] 10%|█████████▌                                                                                  | 1016/9738 [00:03<00:22, 395.90it/s] 11%|██████████                                                                                  | 1066/9738 [00:03<00:20, 426.22it/s] 11%|██████████▌                                                                                 | 1116/9738 [00:03<00:19, 446.16it/s] 12%|███████████                                                                                 | 1166/9738 [00:03<00:18, 461.95it/s] 12%|███████████▍                                                                                | 1213/9738 [00:03<00:19, 445.82it/s] 13%|███████████▉                                                                                | 1264/9738 [00:03<00:18, 461.72it/s] 14%|████████████▍                                                                               | 1315/9738 [00:03<00:17, 473.09it/s] 14%|████████████▉                                                                               | 1365/9738 [00:03<00:17, 479.30it/s] 15%|█████████████▎                                                                              | 1415/9738 [00:03<00:17, 484.96it/s] 15%|█████████████▊                                                                              | 1465/9738 [00:04<00:16, 489.37it/s] 16%|██████████████▎                                                                             | 1515/9738 [00:04<00:16, 492.08it/s] 16%|██████████████▊                                                                             | 1565/9738 [00:04<00:17, 466.49it/s] 17%|███████████████▏                                                                            | 1614/9738 [00:04<00:17, 473.16it/s] 17%|███████████████▋                                                                            | 1663/9738 [00:04<00:16, 475.46it/s] 18%|████████████████▏                                                                           | 1712/9738 [00:04<00:16, 478.98it/s] 18%|████████████████▋                                                                           | 1763/9738 [00:04<00:16, 485.44it/s] 19%|█████████████████                                                                           | 1812/9738 [00:04<00:17, 461.10it/s] 19%|█████████████████▌                                                                          | 1860/9738 [00:04<00:17, 460.45it/s] 20%|██████████████████                                                                          | 1907/9738 [00:04<00:17, 455.91it/s] 20%|██████████████████▍                                                                         | 1957/9738 [00:05<00:16, 467.70it/s] 21%|██████████████████▉                                                                         | 2007/9738 [00:05<00:16, 476.50it/s] 21%|███████████████████▍                                                                        | 2057/9738 [00:05<00:15, 480.71it/s] 22%|███████████████████▉                                                                        | 2107/9738 [00:05<00:15, 485.34it/s] 22%|████████████████████▍                                                                       | 2157/9738 [00:05<00:15, 488.96it/s] 23%|████████████████████▊                                                                       | 2206/9738 [00:05<00:16, 468.29it/s] 23%|█████████████████████▎                                                                      | 2256/9738 [00:05<00:15, 475.57it/s] 24%|█████████████████████▊                                                                      | 2306/9738 [00:05<00:15, 482.27it/s] 24%|██████████████████████▎                                                                     | 2356/9738 [00:05<00:15, 486.58it/s] 25%|██████████████████████▋                                                                     | 2406/9738 [00:05<00:14, 490.29it/s] 25%|███████████████████████▏                                                                    | 2456/9738 [00:06<00:14, 491.47it/s] 26%|███████████████████████▋                                                                    | 2506/9738 [00:06<00:15, 463.48it/s] 26%|████████████████████████▏                                                                   | 2556/9738 [00:06<00:15, 472.80it/s] 27%|████████████████████████▌                                                                   | 2606/9738 [00:06<00:14, 480.54it/s] 27%|█████████████████████████                                                                   | 2656/9738 [00:06<00:14, 484.93it/s] 28%|█████████████████████████▌                                                                  | 2705/9738 [00:06<00:14, 486.02it/s] 28%|██████████████████████████                                                                  | 2755/9738 [00:06<00:14, 488.81it/s] 29%|██████████████████████████▌                                                                 | 2805/9738 [00:06<00:14, 490.15it/s] 29%|██████████████████████████▉                                                                 | 2855/9738 [00:06<00:14, 464.29it/s] 30%|███████████████████████████▍                                                                | 2905/9738 [00:07<00:14, 473.29it/s] 30%|███████████████████████████▉                                                                | 2954/9738 [00:07<00:14, 477.46it/s] 31%|████████████████████████████▍                                                               | 3004/9738 [00:07<00:13, 482.91it/s] 31%|████████████████████████████▊                                                               | 3054/9738 [00:07<00:13, 486.08it/s] 32%|█████████████████████████████▎                                                              | 3104/9738 [00:07<00:13, 488.83it/s] 32%|█████████████████████████████▊                                                              | 3154/9738 [00:07<00:13, 490.76it/s] 33%|██████████████████████████████▎                                                             | 3204/9738 [00:07<00:14, 462.98it/s] 33%|██████████████████████████████▋                                                             | 3254/9738 [00:07<00:13, 472.65it/s] 34%|███████████████████████████████▏                                                            | 3303/9738 [00:07<00:13, 476.91it/s] 34%|███████████████████████████████▋                                                            | 3353/9738 [00:07<00:13, 481.39it/s] 35%|████████████████████████████████▏                                                           | 3402/9738 [00:08<00:13, 483.34it/s] 35%|████████████████████████████████▌                                                           | 3452/9738 [00:08<00:12, 486.29it/s] 36%|█████████████████████████████████                                                           | 3501/9738 [00:08<00:13, 465.73it/s] 36%|█████████████████████████████████▌                                                          | 3550/9738 [00:08<00:13, 470.13it/s] 37%|██████████████████████████████████                                                          | 3599/9738 [00:08<00:12, 474.42it/s] 37%|██████████████████████████████████▍                                                         | 3648/9738 [00:08<00:12, 478.09it/s] 38%|██████████████████████████████████▉                                                         | 3696/9738 [00:08<00:12, 476.49it/s] 38%|███████████████████████████████████▎                                                        | 3744/9738 [00:08<00:12, 472.48it/s] 39%|███████████████████████████████████▊                                                        | 3793/9738 [00:08<00:12, 476.95it/s] 39%|████████████████████████████████████▎                                                       | 3841/9738 [00:09<00:12, 454.65it/s] 40%|████████████████████████████████████▊                                                       | 3891/9738 [00:09<00:12, 465.55it/s] 40%|█████████████████████████████████████▏                                                      | 3941/9738 [00:09<00:12, 473.30it/s] 41%|█████████████████████████████████████▋                                                      | 3990/9738 [00:09<00:12, 476.35it/s] 41%|██████████████████████████████████████▏                                                     | 4040/9738 [00:09<00:11, 481.03it/s] 42%|██████████████████████████████████████▋                                                     | 4089/9738 [00:09<00:11, 474.72it/s] 42%|███████████████████████████████████████                                                     | 4137/9738 [00:09<00:12, 454.75it/s] 43%|███████████████████████████████████████▌                                                    | 4187/9738 [00:09<00:11, 465.39it/s] 44%|████████████████████████████████████████                                                    | 4237/9738 [00:09<00:11, 473.42it/s] 44%|████████████████████████████████████████▍                                                   | 4286/9738 [00:09<00:11, 478.17it/s] 45%|████████████████████████████████████████▉                                                   | 4334/9738 [00:10<00:11, 473.90it/s] 45%|█████████████████████████████████████████▍                                                  | 4383/9738 [00:10<00:11, 476.69it/s] 46%|█████████████████████████████████████████▉                                                  | 4433/9738 [00:10<00:11, 480.90it/s] 46%|██████████████████████████████████████████▎                                                 | 4482/9738 [00:10<00:11, 458.51it/s] 47%|██████████████████████████████████████████▊                                                 | 4529/9738 [00:10<00:11, 442.97it/s] 47%|███████████████████████████████████████████▎                                                | 4579/9738 [00:10<00:11, 456.90it/s] 48%|███████████████████████████████████████████▋                                                | 4629/9738 [00:10<00:10, 467.46it/s] 48%|████████████████████████████████████████████▏                                               | 4679/9738 [00:10<00:10, 475.32it/s] 49%|████████████████████████████████████████████▋                                               | 4729/9738 [00:10<00:10, 480.92it/s] 49%|█████████████████████████████████████████████▏                                              | 4778/9738 [00:11<00:13, 373.38it/s] 50%|█████████████████████████████████████████████▌                                              | 4828/9738 [00:11<00:12, 402.88it/s] 50%|██████████████████████████████████████████████                                              | 4877/9738 [00:11<00:11, 425.18it/s] 51%|██████████████████████████████████████████████▌                                             | 4927/9738 [00:11<00:10, 443.34it/s] 51%|███████████████████████████████████████████████                                             | 4976/9738 [00:11<00:10, 454.88it/s] 52%|███████████████████████████████████████████████▍                                            | 5025/9738 [00:11<00:10, 464.64it/s] 52%|███████████████████████████████████████████████▉                                            | 5073/9738 [00:11<00:10, 451.74it/s] 53%|████████████████████████████████████████████████▍                                           | 5123/9738 [00:11<00:09, 463.23it/s] 53%|████████████████████████████████████████████████▊                                           | 5173/9738 [00:11<00:09, 471.50it/s] 54%|█████████████████████████████████████████████████▎                                          | 5222/9738 [00:12<00:09, 475.85it/s] 54%|█████████████████████████████████████████████████▊                                          | 5272/9738 [00:12<00:09, 480.38it/s] 55%|██████████████████████████████████████████████████▎                                         | 5322/9738 [00:12<00:09, 483.92it/s] 55%|██████████████████████████████████████████████████▋                                         | 5371/9738 [00:12<00:09, 463.54it/s] 56%|███████████████████████████████████████████████████▏                                        | 5419/9738 [00:12<00:09, 465.96it/s] 56%|███████████████████████████████████████████████████▋                                        | 5469/9738 [00:12<00:09, 473.68it/s] 57%|████████████████████████████████████████████████████▏                                       | 5520/9738 [00:12<00:08, 482.02it/s] 57%|████████████████████████████████████████████████████▋                                       | 5571/9738 [00:12<00:08, 488.54it/s] 58%|█████████████████████████████████████████████████████                                       | 5622/9738 [00:12<00:08, 493.26it/s] 58%|█████████████████████████████████████████████████████▌                                      | 5672/9738 [00:12<00:08, 491.23it/s] 59%|██████████████████████████████████████████████████████                                      | 5722/9738 [00:13<00:08, 466.45it/s] 59%|██████████████████████████████████████████████████████▌                                     | 5773/9738 [00:13<00:08, 477.08it/s] 60%|███████████████████████████████████████████████████████                                     | 5824/9738 [00:13<00:08, 484.85it/s] 60%|███████████████████████████████████████████████████████▍                                    | 5874/9738 [00:13<00:07, 488.87it/s] 61%|███████████████████████████████████████████████████████▉                                    | 5925/9738 [00:13<00:07, 493.71it/s] 61%|████████████████████████████████████████████████████████▍                                   | 5976/9738 [00:13<00:07, 497.26it/s] 62%|████████████████████████████████████████████████████████▉                                   | 6027/9738 [00:13<00:07, 499.68it/s] 62%|█████████████████████████████████████████████████████████▍                                  | 6078/9738 [00:13<00:07, 476.17it/s] 63%|█████████████████████████████████████████████████████████▉                                  | 6128/9738 [00:13<00:07, 482.85it/s] 63%|██████████████████████████████████████████████████████████▍                                 | 6179/9738 [00:13<00:07, 489.85it/s] 64%|██████████████████████████████████████████████████████████▊                                 | 6230/9738 [00:14<00:07, 493.94it/s] 64%|███████████████████████████████████████████████████████████▎                                | 6281/9738 [00:14<00:06, 497.03it/s] 65%|███████████████████████████████████████████████████████████▊                                | 6331/9738 [00:14<00:06, 497.84it/s] 66%|████████████████████████████████████████████████████████████▎                               | 6381/9738 [00:14<00:07, 477.39it/s] 66%|████████████████████████████████████████████████████████████▊                               | 6432/9738 [00:14<00:06, 485.62it/s] 67%|█████████████████████████████████████████████████████████████▏                              | 6482/9738 [00:14<00:06, 487.13it/s] 67%|█████████████████████████████████████████████████████████████▋                              | 6533/9738 [00:14<00:06, 492.24it/s] 68%|██████████████████████████████████████████████████████████████▏                             | 6583/9738 [00:14<00:06, 493.42it/s] 68%|██████████████████████████████████████████████████████████████▋                             | 6634/9738 [00:14<00:06, 497.13it/s] 69%|███████████████████████████████████████████████████████████████▏                            | 6684/9738 [00:15<00:06, 484.35it/s] 69%|███████████████████████████████████████████████████████████████▌                            | 6733/9738 [00:15<00:06, 477.93it/s] 70%|████████████████████████████████████████████████████████████████                            | 6783/9738 [00:15<00:06, 483.88it/s] 70%|████████████████████████████████████████████████████████████████▌                           | 6834/9738 [00:15<00:05, 489.70it/s] 71%|█████████████████████████████████████████████████████████████████                           | 6885/9738 [00:15<00:05, 493.79it/s] 71%|█████████████████████████████████████████████████████████████████▌                          | 6936/9738 [00:15<00:05, 496.41it/s] 72%|██████████████████████████████████████████████████████████████████                          | 6987/9738 [00:15<00:05, 498.47it/s] 72%|██████████████████████████████████████████████████████████████████▍                         | 7037/9738 [00:15<00:05, 472.39it/s] 73%|██████████████████████████████████████████████████████████████████▉                         | 7088/9738 [00:15<00:05, 481.42it/s] 73%|███████████████████████████████████████████████████████████████████▍                        | 7139/9738 [00:15<00:05, 488.04it/s] 74%|███████████████████████████████████████████████████████████████████▉                        | 7190/9738 [00:16<00:05, 492.66it/s] 74%|████████████████████████████████████████████████████████████████████▍                       | 7240/9738 [00:16<00:05, 494.18it/s] 75%|████████████████████████████████████████████████████████████████████▉                       | 7291/9738 [00:16<00:04, 497.22it/s] 75%|█████████████████████████████████████████████████████████████████████▎                      | 7342/9738 [00:16<00:04, 498.45it/s] 76%|█████████████████████████████████████████████████████████████████████▊                      | 7392/9738 [00:16<00:04, 476.97it/s] 76%|██████████████████████████████████████████████████████████████████████▎                     | 7443/9738 [00:16<00:04, 485.32it/s] 77%|██████████████████████████████████████████████████████████████████████▊                     | 7492/9738 [00:16<00:04, 458.37it/s] 77%|███████████████████████████████████████████████████████████████████████▎                    | 7543/9738 [00:16<00:04, 471.59it/s] 78%|███████████████████████████████████████████████████████████████████████▋                    | 7594/9738 [00:16<00:04, 481.75it/s] 79%|████████████████████████████████████████████████████████████████████████▏                   | 7645/9738 [00:16<00:04, 488.88it/s] 79%|████████████████████████████████████████████████████████████████████████▋                   | 7695/9738 [00:17<00:04, 467.91it/s] 80%|█████████████████████████████████████████████████████████████████████████▏                  | 7746/9738 [00:17<00:04, 479.39it/s] 80%|█████████████████████████████████████████████████████████████████████████▋                  | 7797/9738 [00:17<00:03, 487.12it/s] 81%|██████████████████████████████████████████████████████████████████████████▏                 | 7848/9738 [00:17<00:03, 492.52it/s] 81%|██████████████████████████████████████████████████████████████████████████▋                 | 7899/9738 [00:17<00:03, 496.62it/s] 82%|███████████████████████████████████████████████████████████████████████████                 | 7950/9738 [00:17<00:03, 498.26it/s] 82%|███████████████████████████████████████████████████████████████████████████▌                | 8000/9738 [00:17<00:03, 473.44it/s] 83%|████████████████████████████████████████████████████████████████████████████                | 8051/9738 [00:17<00:03, 483.60it/s] 83%|████████████████████████████████████████████████████████████████████████████▌               | 8103/9738 [00:17<00:03, 491.89it/s] 84%|█████████████████████████████████████████████████████████████████████████████               | 8154/9738 [00:18<00:03, 496.16it/s] 84%|█████████████████████████████████████████████████████████████████████████████▌              | 8206/9738 [00:18<00:03, 500.60it/s] 85%|██████████████████████████████████████████████████████████████████████████████              | 8257/9738 [00:18<00:02, 503.32it/s] 85%|██████████████████████████████████████████████████████████████████████████████▍             | 8308/9738 [00:18<00:02, 505.04it/s] 86%|██████████████████████████████████████████████████████████████████████████████▉             | 8359/9738 [00:18<00:02, 480.00it/s] 86%|███████████████████████████████████████████████████████████████████████████████▍            | 8410/9738 [00:18<00:02, 486.97it/s] 87%|███████████████████████████████████████████████████████████████████████████████▉            | 8462/9738 [00:18<00:02, 494.21it/s] 87%|████████████████████████████████████████████████████████████████████████████████▍           | 8514/9738 [00:18<00:02, 499.31it/s] 88%|████████████████████████████████████████████████████████████████████████████████▉           | 8565/9738 [00:18<00:02, 502.40it/s] 88%|█████████████████████████████████████████████████████████████████████████████████▍          | 8616/9738 [00:18<00:02, 503.14it/s] 89%|█████████████████████████████████████████████████████████████████████████████████▉          | 8667/9738 [00:19<00:02, 483.09it/s] 90%|██████████████████████████████████████████████████████████████████████████████████▎         | 8719/9738 [00:19<00:02, 491.06it/s] 90%|██████████████████████████████████████████████████████████████████████████████████▊         | 8771/9738 [00:19<00:01, 496.74it/s] 91%|███████████████████████████████████████████████████████████████████████████████████▎        | 8822/9738 [00:19<00:01, 498.93it/s] 91%|███████████████████████████████████████████████████████████████████████████████████▊        | 8873/9738 [00:19<00:01, 502.17it/s] 92%|████████████████████████████████████████████████████████████████████████████████████▎       | 8925/9738 [00:19<00:01, 505.08it/s] 92%|████████████████████████████████████████████████████████████████████████████████████▊       | 8976/9738 [00:19<00:01, 493.74it/s] 93%|█████████████████████████████████████████████████████████████████████████████████████▎      | 9026/9738 [00:19<00:01, 484.10it/s] 93%|█████████████████████████████████████████████████████████████████████████████████████▊      | 9077/9738 [00:19<00:01, 489.95it/s] 94%|██████████████████████████████████████████████████████████████████████████████████████▏     | 9129/9738 [00:20<00:01, 495.48it/s] 94%|██████████████████████████████████████████████████████████████████████████████████████▋     | 9181/9738 [00:20<00:01, 500.05it/s] 95%|███████████████████████████████████████████████████████████████████████████████████████▏    | 9233/9738 [00:20<00:01, 503.47it/s] 95%|███████████████████████████████████████████████████████████████████████████████████████▋    | 9284/9738 [00:20<00:00, 504.66it/s] 96%|████████████████████████████████████████████████████████████████████████████████████████▏   | 9335/9738 [00:20<00:00, 480.58it/s] 96%|████████████████████████████████████████████████████████████████████████████████████████▋   | 9387/9738 [00:20<00:00, 489.31it/s] 97%|█████████████████████████████████████████████████████████████████████████████████████████▏  | 9438/9738 [00:20<00:00, 495.14it/s] 97%|█████████████████████████████████████████████████████████████████████████████████████████▋  | 9489/9738 [00:20<00:00, 496.95it/s] 98%|██████████████████████████████████████████████████████████████████████████████████████████▏ | 9541/9738 [00:20<00:00, 500.93it/s] 99%|██████████████████████████████████████████████████████████████████████████████████████████▌ | 9592/9738 [00:20<00:00, 503.57it/s] 99%|███████████████████████████████████████████████████████████████████████████████████████████ | 9644/9738 [00:21<00:00, 505.55it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████▌| 9695/9738 [00:21<00:00, 481.70it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████| 9738/9738 [00:21<00:00, 458.62it/s]
Load End
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:07<00:15,  7.82s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:07<00:14,  7.06s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:07<00:15,  7.59s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:07<00:15,  7.78s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:13<00:06,  6.56s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:13<00:06,  6.47s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:13<00:06,  6.65s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:14<00:06,  6.98s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:17<00:00,  5.54s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:17<00:00,  5.94s/it]
[2023-08-07 22:03:23,239] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:17<00:00,  5.47s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:17<00:00,  5.80s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:18<00:00,  5.71s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:18<00:00,  6.11s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:18<00:00,  5.70s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:18<00:00,  6.07s/it]
[2023-08-07 22:03:34,609] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-07 22:03:34,610] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:
[2023-08-07 22:03:34,610] [INFO] [config.py:1012:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-07 22:03:34,610] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-07 22:03:34,610] [INFO] [config.py:1012:print]   amp_enabled .................. False
[2023-08-07 22:03:34,610] [INFO] [config.py:1012:print]   amp_params ................... False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   bfloat16_enabled ............. False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f50a73261f0>
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   communication_data_type ...... None
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   disable_allgather ............ False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   dump_state ................... False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'min_scale': 1}
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   elasticity_enabled ........... False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   fp16_auto_cast ............... False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   fp16_enabled ................. True
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   global_rank .................. 0
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 2048
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   loss_scale ................... 0
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   memory_breakdown ............. False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f50a7326dc0>
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   optimizer_name ............... None
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   optimizer_params ............. None
[2023-08-07 22:03:34,611] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   pld_enabled .................. False
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   pld_params ................... False
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   prescale_gradients ........... False
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   scheduler_name ............... None
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   scheduler_params ............. None
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   sparse_attention ............. None
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   steps_per_print .............. 1
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   train_batch_size ............. 20
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  5
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   use_node_local_storage ....... False
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   world_size ................... 4
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  True
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   zero_enabled ................. False
[2023-08-07 22:03:34,612] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 0
[2023-08-07 22:03:34,612] [INFO] [config.py:997:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 5, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 1
}
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Emitting ninja build file /home/ylu130/.cache/torch_extensions/py39_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.5292520523071289 seconds
Time to load utils op: 0.5046231746673584 seconds
Loading extension module utils...
Time to load utils op: 0.6048049926757812 seconds
Model mem
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   12918 MB |   12918 MB |   12918 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      66 MB |      66 MB |      66 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |
|       from small pool |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|---------------------------------------------------------------------------|
| Allocations           |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     259    |     259    |     259    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |      33    |      33    |      33    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |      33    |      31    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       2    |       2    |      33    |      31    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Loading extension module utils...
Evaluating commonsenseqa :   0%|                                                                               | 0/50 [00:00<?, ?it/s]Time to load utils op: 0.5043716430664062 seconds
############### Example ###############
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:Answer the following multiple choice question.

### Demonstration:
Input: He fantasied about getting a what while driving to work and the pros and cons of the extra responsibilities and benefits? Choices:  A: new car B: promotion C: boredom D: impatience E: pressure
Rationales: 1. The question talks about someone fantasizing about something while driving to work. This suggests that this person is thinking about something that has to do with his job or professional life.
2. The phrases "extra responsibilities and benefits" are key clues given in the problem. These typically apply to situations involving advancements or progressions in a professional setting.
3. Looking at the options, 'new car', 'boredom', 'impatience', and 'pressure' do not typically correlate with extra responsibilities and benefits at a workplace setting.
4. The only option left is 'promotion' which directly correlates with acquiring extra responsibilities and benefits in a work context.
5. Hence, the answer is B: Promotion.
Answer: B: promotion

Input: He was good at traditional science but excelled at social science, his favorite subject was what? Choices:  A: geography B: history studies C: math D: religion E: dancing
Rationales: The statement mentions that he was good at traditional science, which include areas like math, biology, chemistry but it distinctly mentions that he excelled in social science. 

The social sciences include fields like sociology, anthropology, economics, political science, and history. Out of the given options (geography, history studies, math, religion, and dancing), we can eliminate math because it's not a social science. 

We also eliminate dancing since it's not typically classified as a social science. 

That leaves us with geography, history studies, and religion. Even though all of these could technically be considered within the realm of social sciences, the statement said he "excelled at social science," suggesting the subject he favored most was within this area. 

Since the answer is provided as B: history studies, we can conclude that History is his favorite subject. This makes sense as history is a fundamental discipline within the social sciences.
Answer: B: history studies

Input: Jan wasn't very good at studying.  What might help him study better? Choices:  A: having a bigger brain B: headaches C: inspiration D: more intelligence E: understanding
Rationales: 1. The question asks what could help Jan study better. 
2. We can begin to analyze each answer. Option A, 'having a bigger brain' is not necessarily going to improve studying skills. The size of the brain does not determine its functionality or capability.
3. Option B, 'headaches' would generally make studying more difficult, not easier. It wouldn't improve studying skills.
4. Option C, 'inspiration,' can motivate someone to study, but the inspiration itself does not give the skills or understanding needed to study effectively.
5. Option E, 'understanding,' while closely related to studying, isn't necessarily what would improve Jan's studying. He could understand something but still not be good at studying as understanding involves comprehending what's already learned while studying involves learning new things.
6. This leaves us with option D,'more intelligence.' Being intelligent typically means that an individual can learn, understand, and apply knowledge effectively, which are all critical skills when studying. Consequently, if Jan were more intelligent, he might be better at studying. 
7. Therefore, the answer is D: more intelligence.
Answer: D: more intelligence

### Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid

### Response:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i3-s42-rTrue
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 125, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 122, in main
    evaluate_main(args, tokenizer, model, dataset["test"], device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 137, in evaluate_main
    lm_loss, query_ids, response_ids, rest_ids = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 101, in run_model
    out = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return func(*args, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1836, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 727, in forward
    outputs = self.model(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 617, in forward
    layer_outputs = decoder_layer(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 328, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 266, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/functional.py", line 1836, in softmax
    ret = input.softmax(dim, dtype=dtype)
RuntimeError: CUDA out of memory. Tried to allocate 2.51 GiB (GPU 1; 47.54 GiB total capacity; 14.54 GiB already allocated; 1.49 GiB free; 17.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 432841 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 432843 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 432844 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 432842) of binary: /home/ylu130/.conda/envs/distllm/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.conda/envs/distllm/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/run.py", line 761, in main
    run(args)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-07_22:03:39
  host      : ia1.wse.jhu.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 432842)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model/llama-2-7b --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --seed 42 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s42-rTrue --rationales --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
using world size: 4
[2023-08-07 22:03:43,572] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model/llama-2-7b
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  model_parallel ............... False
  model_parallel_size .......... None
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s42-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 4
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 2048
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i4-s42-rTrue
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  gradient_accumulation_steps .. 1
  batch_size ................... 5
  clip_grad .................... 1.0
  seed ......................... 42
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading Data
  0%|                                                                                                        | 0/9737 [00:00<?, ?it/s]  0%|▎                                                                                             | 37/9737 [00:00<00:26, 367.91it/s]  1%|▋                                                                                             | 75/9737 [00:00<00:26, 370.55it/s]  1%|█                                                                                            | 113/9737 [00:00<00:25, 370.76it/s]  2%|█▍                                                                                           | 151/9737 [00:00<00:27, 348.31it/s]  2%|█▊                                                                                           | 189/9737 [00:00<00:26, 355.85it/s]  2%|██▏                                                                                          | 225/9737 [00:00<00:27, 340.32it/s]  3%|██▌                                                                                          | 263/9737 [00:00<00:27, 350.12it/s]  3%|██▊                                                                                          | 301/9737 [00:00<00:26, 356.44it/s]  3%|███▏                                                                                         | 338/9737 [00:00<00:26, 359.99it/s]  4%|███▌                                                                                         | 375/9737 [00:01<00:25, 362.57it/s]  4%|███▉                                                                                         | 412/9737 [00:01<00:27, 342.85it/s]  5%|████▎                                                                                        | 447/9737 [00:01<00:27, 334.55it/s]  5%|████▌                                                                                        | 484/9737 [00:01<00:26, 342.85it/s]  5%|████▉                                                                                        | 522/9737 [00:01<00:26, 351.55it/s]  6%|█████▎                                                                                       | 559/9737 [00:01<00:25, 356.62it/s]  6%|█████▋                                                                                       | 597/9737 [00:01<00:25, 360.78it/s]  7%|██████                                                                                       | 635/9737 [00:01<00:25, 363.58it/s]  7%|██████▍                                                                                      | 672/9737 [00:01<00:26, 336.17it/s]  7%|██████▊                                                                                      | 709/9737 [00:02<00:26, 343.13it/s]  8%|███████▏                                                                                     | 746/9737 [00:02<00:25, 350.69it/s]  8%|███████▍                                                                                     | 782/9737 [00:02<00:25, 352.45it/s]  8%|███████▊                                                                                     | 819/9737 [00:02<00:25, 356.55it/s]  9%|████████▏                                                                                    | 856/9737 [00:02<00:24, 360.40it/s]  9%|████████▌                                                                                    | 893/9737 [00:02<00:25, 346.37it/s] 10%|████████▊                                                                                    | 928/9737 [00:02<00:26, 337.13it/s] 10%|█████████▏                                                                                   | 965/9737 [00:02<00:25, 346.17it/s] 10%|█████████▍                                                                                  | 1002/9737 [00:02<00:24, 352.71it/s] 11%|█████████▊                                                                                  | 1039/9737 [00:02<00:24, 357.33it/s] 11%|██████████▏                                                                                 | 1076/9737 [00:03<00:24, 360.36it/s] 11%|██████████▌                                                                                 | 1113/9737 [00:03<00:23, 363.11it/s] 12%|██████████▊                                                                                 | 1150/9737 [00:03<00:24, 344.26it/s] 12%|███████████▏                                                                                | 1185/9737 [00:03<00:25, 338.60it/s] 13%|███████████▌                                                                                | 1222/9737 [00:03<00:24, 345.59it/s] 13%|███████████▉                                                                                | 1259/9737 [00:03<00:24, 351.05it/s] 13%|████████████▏                                                                               | 1296/9737 [00:03<00:23, 355.16it/s] 14%|████████████▌                                                                               | 1333/9737 [00:03<00:23, 357.89it/s] 14%|████████████▉                                                                               | 1369/9737 [00:03<00:23, 357.84it/s] 14%|█████████████▎                                                                              | 1405/9737 [00:03<00:24, 345.86it/s] 15%|█████████████▌                                                                              | 1440/9737 [00:04<00:24, 341.36it/s] 15%|█████████████▉                                                                              | 1477/9737 [00:04<00:23, 348.11it/s] 16%|██████████████▎                                                                             | 1514/9737 [00:04<00:23, 352.29it/s] 16%|██████████████▋                                                                             | 1551/9737 [00:04<00:23, 355.72it/s] 16%|██████████████▉                                                                             | 1587/9737 [00:04<00:22, 356.19it/s] 17%|███████████████▎                                                                            | 1624/9737 [00:04<00:22, 359.53it/s] 17%|███████████████▋                                                                            | 1660/9737 [00:04<00:22, 357.05it/s] 17%|████████████████                                                                            | 1696/9737 [00:04<00:23, 347.95it/s] 18%|████████████████▎                                                                           | 1733/9737 [00:04<00:22, 353.81it/s] 18%|████████████████▋                                                                           | 1770/9737 [00:05<00:22, 357.87it/s] 19%|█████████████████                                                                           | 1806/9737 [00:05<00:23, 340.11it/s] 19%|█████████████████▍                                                                          | 1843/9737 [00:05<00:22, 345.84it/s] 19%|█████████████████▊                                                                          | 1880/9737 [00:05<00:22, 350.81it/s] 20%|██████████████████                                                                          | 1916/9737 [00:05<00:23, 336.69it/s] 20%|██████████████████▍                                                                         | 1953/9737 [00:05<00:22, 343.82it/s] 20%|██████████████████▊                                                                         | 1990/9737 [00:05<00:22, 349.31it/s] 21%|███████████████████▏                                                                        | 2026/9737 [00:05<00:21, 350.90it/s] 21%|███████████████████▍                                                                        | 2063/9737 [00:05<00:21, 353.76it/s] 22%|███████████████████▊                                                                        | 2100/9737 [00:05<00:21, 356.09it/s] 22%|████████████████████▏                                                                       | 2136/9737 [00:06<00:21, 350.83it/s] 22%|████████████████████▌                                                                       | 2172/9737 [00:06<00:22, 339.49it/s] 23%|████████████████████▊                                                                       | 2208/9737 [00:06<00:21, 345.05it/s] 23%|█████████████████████▏                                                                      | 2245/9737 [00:06<00:21, 349.59it/s] 23%|█████████████████████▌                                                                      | 2281/9737 [00:06<00:21, 351.39it/s] 24%|█████████████████████▉                                                                      | 2318/9737 [00:06<00:20, 354.30it/s] 24%|██████████████████████▏                                                                     | 2354/9737 [00:06<00:20, 355.56it/s] 25%|██████████████████████▌                                                                     | 2390/9737 [00:06<00:20, 352.73it/s] 25%|██████████████████████▉                                                                     | 2426/9737 [00:06<00:21, 342.38it/s] 25%|███████████████████████▎                                                                    | 2461/9737 [00:07<00:21, 344.36it/s] 26%|███████████████████████▌                                                                    | 2497/9737 [00:07<00:20, 347.46it/s] 26%|███████████████████████▉                                                                    | 2534/9737 [00:07<00:20, 351.25it/s] 26%|████████████████████████▎                                                                   | 2571/9737 [00:07<00:20, 354.10it/s] 27%|████████████████████████▋                                                                   | 2607/9737 [00:07<00:20, 348.40it/s] 27%|████████████████████████▉                                                                   | 2644/9737 [00:07<00:20, 352.03it/s] 28%|█████████████████████████▎                                                                  | 2681/9737 [00:07<00:19, 354.83it/s] 28%|█████████████████████████▋                                                                  | 2717/9737 [00:07<00:20, 337.48it/s] 28%|██████████████████████████                                                                  | 2754/9737 [00:07<00:20, 344.32it/s] 29%|██████████████████████████▎                                                                 | 2791/9737 [00:07<00:19, 349.17it/s] 29%|██████████████████████████▋                                                                 | 2828/9737 [00:08<00:19, 352.46it/s] 29%|███████████████████████████                                                                 | 2864/9737 [00:08<00:19, 348.44it/s] 30%|███████████████████████████▍                                                                | 2901/9737 [00:08<00:19, 351.98it/s] 30%|███████████████████████████▊                                                                | 2937/9737 [00:08<00:19, 352.66it/s] 31%|████████████████████████████                                                                | 2973/9737 [00:08<00:19, 339.43it/s] 31%|████████████████████████████▍                                                               | 3010/9737 [00:08<00:19, 346.05it/s] 31%|████████████████████████████▊                                                               | 3046/9737 [00:08<00:19, 348.26it/s] 32%|█████████████████████████████                                                               | 3081/9737 [00:08<00:19, 344.75it/s] 32%|█████████████████████████████▍                                                              | 3118/9737 [00:08<00:18, 349.61it/s] 32%|█████████████████████████████▊                                                              | 3155/9737 [00:09<00:18, 352.87it/s] 33%|██████████████████████████████▏                                                             | 3191/9737 [00:09<00:18, 353.28it/s] 33%|██████████████████████████████▍                                                             | 3227/9737 [00:09<00:19, 337.90it/s] 34%|██████████████████████████████▊                                                             | 3264/9737 [00:09<00:18, 344.49it/s] 34%|███████████████████████████████▏                                                            | 3301/9737 [00:09<00:18, 349.18it/s] 34%|███████████████████████████████▌                                                            | 3337/9737 [00:09<00:18, 346.00it/s] 35%|███████████████████████████████▉                                                            | 3374/9737 [00:09<00:18, 350.46it/s] 35%|████████████████████████████████▏                                                           | 3410/9737 [00:09<00:17, 351.63it/s] 35%|████████████████████████████████▌                                                           | 3446/9737 [00:09<00:17, 353.69it/s] 36%|████████████████████████████████▉                                                           | 3482/9737 [00:09<00:18, 337.32it/s] 36%|█████████████████████████████████▏                                                          | 3518/9737 [00:10<00:18, 343.43it/s] 36%|█████████████████████████████████▌                                                          | 3553/9737 [00:10<00:18, 342.46it/s] 37%|█████████████████████████████████▉                                                          | 3589/9737 [00:10<00:17, 346.39it/s] 37%|██████████████████████████████████▎                                                         | 3625/9737 [00:10<00:17, 348.55it/s] 38%|██████████████████████████████████▌                                                         | 3661/9737 [00:10<00:17, 351.48it/s] 38%|██████████████████████████████████▉                                                         | 3697/9737 [00:10<00:17, 353.93it/s] 38%|███████████████████████████████████▎                                                        | 3733/9737 [00:10<00:16, 355.57it/s] 39%|███████████████████████████████████▌                                                        | 3769/9737 [00:10<00:17, 338.83it/s] 39%|███████████████████████████████████▉                                                        | 3804/9737 [00:10<00:17, 338.60it/s] 39%|████████████████████████████████████▎                                                       | 3840/9737 [00:10<00:17, 343.27it/s] 40%|████████████████████████████████████▋                                                       | 3877/9737 [00:11<00:16, 348.45it/s] 40%|████████████████████████████████████▉                                                       | 3913/9737 [00:11<00:16, 351.56it/s] 41%|█████████████████████████████████████▎                                                      | 3949/9737 [00:11<00:16, 353.80it/s] 41%|█████████████████████████████████████▋                                                      | 3985/9737 [00:11<00:16, 355.12it/s] 41%|█████████████████████████████████████▉                                                      | 4021/9737 [00:11<00:16, 339.14it/s] 42%|██████████████████████████████████████▎                                                     | 4056/9737 [00:11<00:16, 340.28it/s] 42%|██████████████████████████████████████▋                                                     | 4092/9737 [00:11<00:16, 344.43it/s] 42%|███████████████████████████████████████                                                     | 4128/9737 [00:11<00:16, 348.78it/s] 43%|███████████████████████████████████████▎                                                    | 4164/9737 [00:11<00:15, 351.78it/s] 43%|███████████████████████████████████████▋                                                    | 4200/9737 [00:12<00:15, 353.45it/s] 44%|████████████████████████████████████████                                                    | 4236/9737 [00:12<00:15, 354.01it/s] 44%|████████████████████████████████████████▎                                                   | 4272/9737 [00:12<00:15, 355.29it/s] 44%|████████████████████████████████████████▋                                                   | 4308/9737 [00:12<00:16, 336.88it/s] 45%|█████████████████████████████████████████                                                   | 4344/9737 [00:12<00:15, 343.27it/s] 45%|█████████████████████████████████████████▍                                                  | 4380/9737 [00:12<00:15, 347.68it/s] 45%|█████████████████████████████████████████▋                                                  | 4416/9737 [00:12<00:15, 351.07it/s] 46%|██████████████████████████████████████████                                                  | 4452/9737 [00:12<00:14, 353.28it/s] 46%|██████████████████████████████████████████▍                                                 | 4488/9737 [00:12<00:14, 355.01it/s] 46%|██████████████████████████████████████████▋                                                 | 4524/9737 [00:12<00:16, 314.46it/s] 47%|███████████████████████████████████████████                                                 | 4558/9737 [00:13<00:16, 319.66it/s] 47%|███████████████████████████████████████████▍                                                | 4595/9737 [00:13<00:15, 332.37it/s] 48%|███████████████████████████████████████████▊                                                | 4632/9737 [00:13<00:14, 341.89it/s] 48%|████████████████████████████████████████████                                                | 4669/9737 [00:13<00:14, 348.85it/s] 48%|████████████████████████████████████████████▍                                               | 4706/9737 [00:13<00:14, 354.03it/s] 49%|████████████████████████████████████████████▊                                               | 4742/9737 [00:13<00:14, 341.80it/s] 49%|█████████████████████████████████████████████▏                                              | 4777/9737 [00:13<00:20, 242.61it/s] 49%|█████████████████████████████████████████████▍                                              | 4814/9737 [00:13<00:18, 270.47it/s] 50%|█████████████████████████████████████████████▊                                              | 4851/9737 [00:14<00:16, 293.67it/s] 50%|██████████████████████████████████████████████▏                                             | 4888/9737 [00:14<00:15, 312.27it/s] 51%|██████████████████████████████████████████████▌                                             | 4922/9737 [00:14<00:15, 311.75it/s] 51%|██████████████████████████████████████████████▊                                             | 4959/9737 [00:14<00:14, 325.42it/s] 51%|███████████████████████████████████████████████▏                                            | 4996/9737 [00:14<00:14, 336.72it/s] 52%|███████████████████████████████████████████████▌                                            | 5031/9737 [00:14<00:13, 338.24it/s] 52%|███████████████████████████████████████████████▉                                            | 5068/9737 [00:14<00:13, 346.18it/s] 52%|████████████████████████████████████████████████▏                                           | 5105/9737 [00:14<00:13, 352.46it/s] 53%|████████████████████████████████████████████████▌                                           | 5142/9737 [00:14<00:12, 356.76it/s] 53%|████████████████████████████████████████████████▉                                           | 5178/9737 [00:14<00:13, 342.99it/s] 54%|█████████████████████████████████████████████████▎                                          | 5215/9737 [00:15<00:12, 349.79it/s] 54%|█████████████████████████████████████████████████▌                                          | 5252/9737 [00:15<00:12, 354.07it/s] 54%|█████████████████████████████████████████████████▉                                          | 5288/9737 [00:15<00:12, 348.64it/s] 55%|██████████████████████████████████████████████████▎                                         | 5325/9737 [00:15<00:12, 353.41it/s] 55%|██████████████████████████████████████████████████▋                                         | 5362/9737 [00:15<00:12, 355.97it/s] 55%|███████████████████████████████████████████████████                                         | 5398/9737 [00:15<00:12, 344.41it/s] 56%|███████████████████████████████████████████████████▎                                        | 5435/9737 [00:15<00:12, 349.25it/s] 56%|███████████████████████████████████████████████████▋                                        | 5472/9737 [00:15<00:12, 354.06it/s] 57%|████████████████████████████████████████████████████                                        | 5510/9737 [00:15<00:11, 360.73it/s] 57%|████████████████████████████████████████████████████▍                                       | 5547/9737 [00:16<00:11, 358.82it/s] 57%|████████████████████████████████████████████████████▊                                       | 5585/9737 [00:16<00:11, 363.81it/s] 58%|█████████████████████████████████████████████████████▏                                      | 5623/9737 [00:16<00:11, 367.85it/s] 58%|█████████████████████████████████████████████████████▍                                      | 5660/9737 [00:16<00:11, 360.63it/s] 59%|█████████████████████████████████████████████████████▊                                      | 5698/9737 [00:16<00:11, 365.69it/s] 59%|██████████████████████████████████████████████████████▏                                     | 5736/9737 [00:16<00:10, 369.46it/s] 59%|██████████████████████████████████████████████████████▌                                     | 5774/9737 [00:16<00:10, 371.57it/s] 60%|██████████████████████████████████████████████████████▉                                     | 5812/9737 [00:16<00:10, 366.19it/s] 60%|███████████████████████████████████████████████████████▎                                    | 5850/9737 [00:16<00:10, 369.41it/s] 60%|███████████████████████████████████████████████████████▌                                    | 5887/9737 [00:16<00:10, 366.88it/s] 61%|███████████████████████████████████████████████████████▉                                    | 5924/9737 [00:17<00:10, 365.46it/s] 61%|████████████████████████████████████████████████████████▎                                   | 5962/9737 [00:17<00:10, 369.28it/s] 62%|████████████████████████████████████████████████████████▋                                   | 5999/9737 [00:17<00:10, 369.24it/s] 62%|█████████████████████████████████████████████████████████                                   | 6037/9737 [00:17<00:09, 371.84it/s] 62%|█████████████████████████████████████████████████████████▍                                  | 6075/9737 [00:17<00:10, 365.66it/s] 63%|█████████████████████████████████████████████████████████▊                                  | 6113/9737 [00:17<00:09, 367.21it/s] 63%|██████████████████████████████████████████████████████████                                  | 6150/9737 [00:17<00:09, 362.78it/s] 64%|██████████████████████████████████████████████████████████▍                                 | 6188/9737 [00:17<00:09, 366.42it/s] 64%|██████████████████████████████████████████████████████████▊                                 | 6226/9737 [00:17<00:09, 369.43it/s] 64%|███████████████████████████████████████████████████████████▏                                | 6264/9737 [00:17<00:09, 371.31it/s] 65%|███████████████████████████████████████████████████████████▌                                | 6302/9737 [00:18<00:09, 373.17it/s] 65%|███████████████████████████████████████████████████████████▉                                | 6340/9737 [00:18<00:09, 365.36it/s] 66%|████████████████████████████████████████████████████████████▎                               | 6378/9737 [00:18<00:09, 368.71it/s] 66%|████████████████████████████████████████████████████████████▌                               | 6415/9737 [00:18<00:09, 364.05it/s] 66%|████████████████████████████████████████████████████████████▉                               | 6453/9737 [00:18<00:08, 368.14it/s] 67%|█████████████████████████████████████████████████████████████▎                              | 6491/9737 [00:18<00:08, 370.67it/s] 67%|█████████████████████████████████████████████████████████████▋                              | 6529/9737 [00:18<00:08, 372.47it/s] 67%|██████████████████████████████████████████████████████████████                              | 6567/9737 [00:18<00:08, 372.06it/s] 68%|██████████████████████████████████████████████████████████████▍                             | 6605/9737 [00:18<00:08, 368.35it/s] 68%|██████████████████████████████████████████████████████████████▊                             | 6642/9737 [00:18<00:08, 361.74it/s] 69%|███████████████████████████████████████████████████████████████                             | 6679/9737 [00:19<00:08, 363.31it/s] 69%|███████████████████████████████████████████████████████████████▍                            | 6717/9737 [00:19<00:08, 367.45it/s] 69%|███████████████████████████████████████████████████████████████▊                            | 6755/9737 [00:19<00:08, 370.63it/s] 70%|████████████████████████████████████████████████████████████████▏                           | 6793/9737 [00:19<00:07, 370.94it/s] 70%|████████████████████████████████████████████████████████████████▌                           | 6831/9737 [00:19<00:07, 372.98it/s] 71%|████████████████████████████████████████████████████████████████▉                           | 6869/9737 [00:19<00:07, 374.37it/s] 71%|█████████████████████████████████████████████████████████████████▎                          | 6907/9737 [00:19<00:07, 358.59it/s] 71%|█████████████████████████████████████████████████████████████████▌                          | 6945/9737 [00:19<00:07, 363.95it/s] 72%|█████████████████████████████████████████████████████████████████▉                          | 6983/9737 [00:19<00:07, 367.61it/s] 72%|██████████████████████████████████████████████████████████████████▎                         | 7021/9737 [00:20<00:07, 368.57it/s] 72%|██████████████████████████████████████████████████████████████████▋                         | 7059/9737 [00:20<00:07, 371.31it/s] 73%|███████████████████████████████████████████████████████████████████                         | 7097/9737 [00:20<00:07, 373.26it/s] 73%|███████████████████████████████████████████████████████████████████▍                        | 7135/9737 [00:20<00:07, 364.37it/s] 74%|███████████████████████████████████████████████████████████████████▊                        | 7172/9737 [00:20<00:07, 357.24it/s] 74%|████████████████████████████████████████████████████████████████████                        | 7210/9737 [00:20<00:06, 362.85it/s] 74%|████████████████████████████████████████████████████████████████████▍                       | 7247/9737 [00:20<00:06, 360.81it/s] 75%|████████████████████████████████████████████████████████████████████▊                       | 7285/9737 [00:20<00:06, 365.66it/s] 75%|█████████████████████████████████████████████████████████████████████▏                      | 7323/9737 [00:20<00:06, 368.74it/s] 76%|█████████████████████████████████████████████████████████████████████▌                      | 7361/9737 [00:20<00:06, 371.33it/s] 76%|█████████████████████████████████████████████████████████████████████▉                      | 7399/9737 [00:21<00:06, 359.08it/s] 76%|██████████████████████████████████████████████████████████████████████▎                     | 7436/9737 [00:21<00:06, 356.57it/s] 77%|██████████████████████████████████████████████████████████████████████▌                     | 7472/9737 [00:21<00:06, 325.02it/s] 77%|██████████████████████████████████████████████████████████████████████▉                     | 7510/9737 [00:21<00:06, 339.15it/s] 78%|███████████████████████████████████████████████████████████████████████▎                    | 7548/9737 [00:21<00:06, 349.74it/s] 78%|███████████████████████████████████████████████████████████████████████▋                    | 7586/9737 [00:21<00:06, 357.50it/s] 78%|████████████████████████████████████████████████████████████████████████                    | 7623/9737 [00:21<00:06, 348.66it/s] 79%|████████████████████████████████████████████████████████████████████████▍                   | 7661/9737 [00:21<00:05, 356.30it/s] 79%|████████████████████████████████████████████████████████████████████████▋                   | 7697/9737 [00:21<00:05, 353.20it/s] 79%|█████████████████████████████████████████████████████████████████████████                   | 7735/9737 [00:22<00:05, 360.19it/s] 80%|█████████████████████████████████████████████████████████████████████████▍                  | 7773/9737 [00:22<00:05, 365.16it/s] 80%|█████████████████████████████████████████████████████████████████████████▊                  | 7811/9737 [00:22<00:05, 368.74it/s] 81%|██████████████████████████████████████████████████████████████████████████▏                 | 7848/9737 [00:22<00:05, 360.35it/s] 81%|██████████████████████████████████████████████████████████████████████████▌                 | 7885/9737 [00:22<00:05, 359.02it/s] 81%|██████████████████████████████████████████████████████████████████████████▊                 | 7923/9737 [00:22<00:05, 362.80it/s] 82%|███████████████████████████████████████████████████████████████████████████▏                | 7960/9737 [00:22<00:04, 360.18it/s] 82%|███████████████████████████████████████████████████████████████████████████▌                | 7998/9737 [00:22<00:04, 363.74it/s] 83%|███████████████████████████████████████████████████████████████████████████▉                | 8036/9737 [00:22<00:04, 367.63it/s] 83%|████████████████████████████████████████████████████████████████████████████▎               | 8074/9737 [00:22<00:04, 370.36it/s] 83%|████████████████████████████████████████████████████████████████████████████▋               | 8112/9737 [00:23<00:04, 355.74it/s] 84%|█████████████████████████████████████████████████████████████████████████████               | 8150/9737 [00:23<00:04, 360.39it/s] 84%|█████████████████████████████████████████████████████████████████████████████▎              | 8188/9737 [00:23<00:04, 365.19it/s] 84%|█████████████████████████████████████████████████████████████████████████████▋              | 8226/9737 [00:23<00:04, 368.70it/s] 85%|██████████████████████████████████████████████████████████████████████████████              | 8263/9737 [00:23<00:04, 363.50it/s] 85%|██████████████████████████████████████████████████████████████████████████████▍             | 8301/9737 [00:23<00:03, 367.06it/s] 86%|██████████████████████████████████████████████████████████████████████████████▊             | 8338/9737 [00:23<00:03, 355.95it/s] 86%|███████████████████████████████████████████████████████████████████████████████▏            | 8375/9737 [00:23<00:03, 357.52it/s] 86%|███████████████████████████████████████████████████████████████████████████████▍            | 8413/9737 [00:23<00:03, 363.18it/s] 87%|███████████████████████████████████████████████████████████████████████████████▊            | 8451/9737 [00:23<00:03, 367.33it/s] 87%|████████████████████████████████████████████████████████████████████████████████▏           | 8489/9737 [00:24<00:03, 370.27it/s] 88%|████████████████████████████████████████████████████████████████████████████████▌           | 8527/9737 [00:24<00:03, 363.91it/s] 88%|████████████████████████████████████████████████████████████████████████████████▉           | 8565/9737 [00:24<00:03, 365.97it/s] 88%|█████████████████████████████████████████████████████████████████████████████████▎          | 8602/9737 [00:24<00:03, 350.49it/s] 89%|█████████████████████████████████████████████████████████████████████████████████▋          | 8640/9737 [00:24<00:03, 357.66it/s] 89%|█████████████████████████████████████████████████████████████████████████████████▉          | 8678/9737 [00:24<00:02, 362.89it/s] 90%|██████████████████████████████████████████████████████████████████████████████████▎         | 8716/9737 [00:24<00:02, 366.80it/s] 90%|██████████████████████████████████████████████████████████████████████████████████▋         | 8754/9737 [00:24<00:02, 369.67it/s] 90%|███████████████████████████████████████████████████████████████████████████████████         | 8792/9737 [00:24<00:02, 369.86it/s] 91%|███████████████████████████████████████████████████████████████████████████████████▍        | 8830/9737 [00:25<00:02, 357.21it/s] 91%|███████████████████████████████████████████████████████████████████████████████████▊        | 8866/9737 [00:25<00:02, 349.00it/s] 91%|████████████████████████████████████████████████████████████████████████████████████▏       | 8904/9737 [00:25<00:02, 356.97it/s] 92%|████████████████████████████████████████████████████████████████████████████████████▍       | 8942/9737 [00:25<00:02, 362.70it/s] 92%|████████████████████████████████████████████████████████████████████████████████████▊       | 8980/9737 [00:25<00:02, 366.60it/s] 93%|█████████████████████████████████████████████████████████████████████████████████████▏      | 9018/9737 [00:25<00:01, 369.13it/s] 93%|█████████████████████████████████████████████████████████████████████████████████████▌      | 9055/9737 [00:25<00:01, 369.22it/s] 93%|█████████████████████████████████████████████████████████████████████████████████████▉      | 9092/9737 [00:25<00:01, 347.76it/s] 94%|██████████████████████████████████████████████████████████████████████████████████████▎     | 9130/9737 [00:25<00:01, 355.87it/s] 94%|██████████████████████████████████████████████████████████████████████████████████████▌     | 9168/9737 [00:25<00:01, 362.07it/s] 95%|██████████████████████████████████████████████████████████████████████████████████████▉     | 9206/9737 [00:26<00:01, 366.36it/s] 95%|███████████████████████████████████████████████████████████████████████████████████████▎    | 9244/9737 [00:26<00:01, 369.50it/s] 95%|███████████████████████████████████████████████████████████████████████████████████████▋    | 9282/9737 [00:26<00:01, 370.27it/s] 96%|████████████████████████████████████████████████████████████████████████████████████████    | 9320/9737 [00:26<00:01, 371.66it/s] 96%|████████████████████████████████████████████████████████████████████████████████████████▍   | 9358/9737 [00:26<00:01, 351.55it/s] 96%|████████████████████████████████████████████████████████████████████████████████████████▊   | 9396/9737 [00:26<00:00, 358.34it/s] 97%|█████████████████████████████████████████████████████████████████████████████████████████▏  | 9434/9737 [00:26<00:00, 363.61it/s] 97%|█████████████████████████████████████████████████████████████████████████████████████████▍  | 9472/9737 [00:26<00:00, 367.21it/s] 98%|█████████████████████████████████████████████████████████████████████████████████████████▊  | 9510/9737 [00:26<00:00, 368.30it/s] 98%|██████████████████████████████████████████████████████████████████████████████████████████▏ | 9548/9737 [00:27<00:00, 371.00it/s] 98%|██████████████████████████████████████████████████████████████████████████████████████████▌ | 9586/9737 [00:27<00:00, 372.59it/s] 99%|██████████████████████████████████████████████████████████████████████████████████████████▉ | 9624/9737 [00:27<00:00, 350.10it/s] 99%|███████████████████████████████████████████████████████████████████████████████████████████▎| 9661/9737 [00:27<00:00, 354.89it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████▋| 9699/9737 [00:27<00:00, 360.93it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████▉| 9736/9737 [00:27<00:00, 363.43it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████| 9737/9737 [00:27<00:00, 353.56it/s]
Load End
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.42s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:11,  5.84s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:06<00:12,  6.11s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:11,  5.68s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.32s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:11<00:05,  5.56s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:11<00:05,  5.79s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:11<00:05,  5.54s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.77s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.93s/it]
[2023-08-07 22:05:05,579] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  4.88s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  5.09s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.26s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.43s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  4.86s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  5.06s/it]
[2023-08-07 22:05:19,032] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-07 22:05:19,033] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:
[2023-08-07 22:05:19,034] [INFO] [config.py:1012:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-07 22:05:19,034] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-07 22:05:19,034] [INFO] [config.py:1012:print]   amp_enabled .................. False
[2023-08-07 22:05:19,034] [INFO] [config.py:1012:print]   amp_params ................... False
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   bfloat16_enabled ............. False
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff3f62a17c0>
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   communication_data_type ...... None
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   disable_allgather ............ False
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   dump_state ................... False
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'min_scale': 1}
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False
[2023-08-07 22:05:19,035] [INFO] [config.py:1012:print]   elasticity_enabled ........... False
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   fp16_auto_cast ............... False
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   fp16_enabled ................. True
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   global_rank .................. 0
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 2048
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   loss_scale ................... 0
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   memory_breakdown ............. False
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7ff3f62a1eb0>
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   optimizer_name ............... None
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   optimizer_params ............. None
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   pld_enabled .................. False
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   pld_params ................... False
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   prescale_gradients ........... False
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   scheduler_name ............... None
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   scheduler_params ............. None
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   sparse_attention ............. None
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   steps_per_print .............. 1
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   train_batch_size ............. 20
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  5
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   use_node_local_storage ....... False
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   world_size ................... 4
[2023-08-07 22:05:19,036] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  True
[2023-08-07 22:05:19,037] [INFO] [config.py:1012:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-08-07 22:05:19,037] [INFO] [config.py:1012:print]   zero_enabled ................. False
[2023-08-07 22:05:19,037] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 0
[2023-08-07 22:05:19,037] [INFO] [config.py:997:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 5, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 1
}
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Emitting ninja build file /home/ylu130/.cache/torch_extensions/py39_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.48435425758361816 seconds
Loading extension module utils...
Time to load utils op: 0.40455055236816406 seconds
Loading extension module utils...
Time to load utils op: 0.40433549880981445 seconds
Model mem
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   12918 MB |   12918 MB |   12918 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      66 MB |      66 MB |      66 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |
|       from small pool |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|---------------------------------------------------------------------------|
| Allocations           |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     259    |     259    |     259    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |      33    |      33    |      33    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |      33    |      31    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       2    |       2    |      33    |      31    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Loading extension module utils...
Time to load utils op: 0.4051635265350342 seconds
Evaluating commonsenseqa :   0%|                                                                               | 0/50 [00:00<?, ?it/s]############### Example ###############
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:Answer the following multiple choice question.

### Demonstration:
Input: He fantasied about getting a what while driving to work and the pros and cons of the extra responsibilities and benefits? Choices:  A: new car B: promotion C: boredom D: impatience E: pressure
Rationales: 1. The question refers to someone fantasizing about something while driving to work, so the answer is likely related to work.
2. The mention of "extra responsibilities" and "benefits" suggest that the answer should involve an increase in status or role. This further supports that the answer would be related to the person's job.
3. Options A, C, D, and E do not directly connect to the concepts of work, responsibilities, and benefits.
4. E: "pressure" does relate to work, but it doesn't suggest an increase in roles or benefits. Instead, it relates to stress or overwork.
5. Only option B, which is "promotion," properly fits all aspects of the question. A promotion at work often comes with both increased responsibilities and benefits, and it is something one might fantasize about on the way to work.
6. Hence, the answer is B: promotion.
Answer: B: promotion

Input: He was good at traditional science but excelled at social science, his favorite subject was what? Choices:  A: geography B: history studies C: math D: religion E: dancing
Rationales: 1. The question provides information about the person's academic preferences. 
2. It is mentioned that he was good at traditional science but this isn't his favorite subject.
3. Then it is mentioned that he excelled at social science. So, we can rule out choices that are outside the field of social science such as geography and math.
4. From the remaining choices, religion and history studies are considered social sciences.
5. However, it is also mentioned that his favorite subject was what he excelled at, and if we assume traditional science to refer to natural sciences, this rules out "religion" because theoretical studies about religion is not a natural or traditional science.
6. Hence, by process of elimination and contextual clues, we can infer that his favorite subject was "B: history studies".
Answer: B: history studies

Input: Jan wasn't very good at studying.  What might help him study better? Choices:  A: having a bigger brain B: headaches C: inspiration D: more intelligence E: understanding
Rationales: 1. Let's analyze each choice in relation to Jan's capacity to study better.
2. Option A: A bigger brain does not necessarily correlate with better studying skills. The size of the brain does not directly impact one's capacity to study.
3. Option B: Headaches would likely hinder studying rather than enhance it. Pain and discomfort normally distract from focus, which is essential for effective learning.
4. Option C: While inspiration might motivate Jan to study, it does not inherently improve his ability to understand or recall the information, which are important aspects of studying.
5. Option D: More intelligence could potentially help Jan study better. Increased intelligence often comes with improved comprehension, memory, and problem-solving abilities, which are crucial for studying effectively.
6. Option E: Understanding is a part of studying, not something that improves studying. It's a result, not a tool. 
7. Therefore, after analyzing all the options, we can confirm that the most suitable choice is 'D: more intelligence'.
Answer: D: more intelligence

Input: The cat was wild but like all others he was always read for a cat what? Choices:  A: stealing B: four legs C: tuna fish D: food now E: nap
Rationales: 1. The question or sentence provided is talking about a cat in general.
2. The term "wild" depicts the cat's natural behavior or instincts.
3. The structure of the question suggests that it's looking for a behavior or characteristic that is common to all cats.
4. The options given are A: stealing B: four legs C: tuna fish D: food now E: nap. 
5. Although all cats have four legs, this is not a behavior but a physical characteristic.
6. Cats do often like to eat fish, including tuna, but not all cats universally exhibit this behavior as some might be allergic to fish or simply not like it.
7. "Stealing" isn't a universal cat behavior, even though some cats might occasionally exhibit this kind of behavior.
8. Although cats do eat food, the phrase "food now" doesn't fit well within the context of the question.
9. However, all cats do take naps, as it's a universal behavior for this species.
10. So, out of all the options given, the only one that correctly fits the context and represents a universal cat behavior is E: nap. Hence, nap is the right answer.
Answer: E: nap

### Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid

### Response:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i4-s42-rTrue
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 125, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 122, in main
    evaluate_main(args, tokenizer, model, dataset["test"], device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 137, in evaluate_main
    lm_loss, query_ids, response_ids, rest_ids = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 101, in run_model
    out = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return func(*args, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1836, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 727, in forward
    outputs = self.model(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 617, in forward
    layer_outputs = decoder_layer(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 328, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 266, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
RuntimeError: CUDA out of memory. Tried to allocate 1.26 GiB (GPU 1; 47.54 GiB total capacity; 16.81 GiB already allocated; 1.17 GiB free; 16.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating commonsenseqa :   0%|                                                                               | 0/50 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 125, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 122, in main
    evaluate_main(args, tokenizer, model, dataset["test"], device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 137, in evaluate_main
    lm_loss, query_ids, response_ids, rest_ids = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 101, in run_model
    out = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return func(*args, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1836, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 727, in forward
    outputs = self.model(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 617, in forward
    layer_outputs = decoder_layer(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 328, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 266, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/functional.py", line 1836, in softmax
    ret = input.softmax(dim, dtype=dtype)
RuntimeError: CUDA out of memory. Tried to allocate 2.51 GiB (GPU 0; 47.54 GiB total capacity; 15.63 GiB already allocated; 1.06 GiB free; 19.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 434537 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 434538 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 434535) of binary: /home/ylu130/.conda/envs/distllm/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.conda/envs/distllm/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/run.py", line 761, in main
    run(args)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-08-07_22:05:26
  host      : ia1.wse.jhu.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 434536)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-07_22:05:26
  host      : ia1.wse.jhu.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 434535)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model/llama-2-7b --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --seed 42 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s42-rFalse --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
using world size: 4
[2023-08-07 22:05:31,099] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model/llama-2-7b
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  model_parallel ............... False
  model_parallel_size .......... None
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s42-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 1
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 2048
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s42-rFalse
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  gradient_accumulation_steps .. 1
  batch_size ................... 5
  clip_grad .................... 1.0
  seed ......................... 42
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading Data
  0%|                                                                                                        | 0/9740 [00:00<?, ?it/s]  2%|██                                                                                          | 218/9740 [00:00<00:04, 2173.72it/s]  4%|████▏                                                                                       | 437/9740 [00:00<00:04, 2180.39it/s]  7%|██████▏                                                                                     | 658/9740 [00:00<00:04, 2190.46it/s]  9%|████████▎                                                                                   | 881/9740 [00:00<00:04, 2206.01it/s] 11%|██████████▎                                                                                | 1105/9740 [00:00<00:03, 2215.30it/s] 14%|████████████▍                                                                              | 1328/9740 [00:00<00:03, 2218.20it/s] 16%|██████████████▍                                                                            | 1550/9740 [00:00<00:03, 2217.22it/s] 18%|████████████████▌                                                                          | 1772/9740 [00:00<00:03, 2180.03it/s] 20%|██████████████████▌                                                                        | 1991/9740 [00:00<00:03, 2147.86it/s] 23%|████████████████████▋                                                                      | 2210/9740 [00:01<00:03, 2158.34it/s] 25%|██████████████████████▋                                                                    | 2433/9740 [00:01<00:03, 2176.93it/s] 27%|████████████████████████▊                                                                  | 2653/9740 [00:01<00:03, 2182.78it/s] 29%|██████████████████████████▊                                                                | 2873/9740 [00:01<00:03, 2185.29it/s] 32%|████████████████████████████▉                                                              | 3095/9740 [00:01<00:03, 2193.51it/s] 34%|██████████████████████████████▉                                                            | 3318/9740 [00:01<00:02, 2203.66it/s] 36%|█████████████████████████████████                                                          | 3539/9740 [00:01<00:02, 2201.62it/s] 39%|███████████████████████████████████▏                                                       | 3760/9740 [00:01<00:02, 2202.25it/s] 41%|█████████████████████████████████████▏                                                     | 3981/9740 [00:01<00:02, 2197.63it/s] 43%|███████████████████████████████████████▎                                                   | 4202/9740 [00:01<00:02, 2200.66it/s] 45%|█████████████████████████████████████████▎                                                 | 4424/9740 [00:02<00:02, 2205.15it/s] 48%|███████████████████████████████████████████▍                                               | 4645/9740 [00:02<00:02, 2185.26it/s] 50%|█████████████████████████████████████████████▍                                             | 4864/9740 [00:02<00:02, 1805.11it/s] 52%|███████████████████████████████████████████████▍                                           | 5082/9740 [00:02<00:02, 1902.02it/s] 54%|█████████████████████████████████████████████████▌                                         | 5301/9740 [00:02<00:02, 1979.26it/s] 57%|███████████████████████████████████████████████████▌                                       | 5523/9740 [00:02<00:02, 2044.42it/s] 59%|█████████████████████████████████████████████████████▋                                     | 5753/9740 [00:02<00:01, 2116.47it/s] 61%|███████████████████████████████████████████████████████▉                                   | 5983/9740 [00:02<00:01, 2168.63it/s] 64%|██████████████████████████████████████████████████████████                                 | 6214/9740 [00:02<00:01, 2208.17it/s] 66%|████████████████████████████████████████████████████████████▏                              | 6444/9740 [00:02<00:01, 2232.58it/s] 68%|██████████████████████████████████████████████████████████████▎                            | 6669/9740 [00:03<00:01, 2221.30it/s] 71%|████████████████████████████████████████████████████████████████▍                          | 6898/9740 [00:03<00:01, 2240.62it/s] 73%|██████████████████████████████████████████████████████████████████▌                        | 7127/9740 [00:03<00:01, 2253.45it/s] 76%|████████████████████████████████████████████████████████████████████▋                      | 7358/9740 [00:03<00:01, 2268.97it/s] 78%|██████████████████████████████████████████████████████████████████████▉                    | 7586/9740 [00:03<00:00, 2233.42it/s] 80%|█████████████████████████████████████████████████████████████████████████                  | 7817/9740 [00:03<00:00, 2255.14it/s] 83%|███████████████████████████████████████████████████████████████████████████▏               | 8048/9740 [00:03<00:00, 2269.27it/s] 85%|█████████████████████████████████████████████████████████████████████████████▎             | 8280/9740 [00:03<00:00, 2284.25it/s] 87%|███████████████████████████████████████████████████████████████████████████████▌           | 8511/9740 [00:03<00:00, 2291.86it/s] 91%|██████████████████████████████████████████████████████████████████████████████████▍        | 8825/9740 [00:04<00:00, 2544.27it/s] 94%|█████████████████████████████████████████████████████████████████████████████████████▊     | 9180/9740 [00:04<00:00, 2842.80it/s] 98%|█████████████████████████████████████████████████████████████████████████████████████████  | 9532/9740 [00:04<00:00, 3043.65it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████| 9740/9740 [00:04<00:00, 2285.10it/s]
Load End
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.14s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.32s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.37s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:11,  5.53s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.19s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.34s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.30s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:11<00:05,  5.54s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.57s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.73s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.67s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.85s/it]
[2023-08-07 22:06:28,502] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.63s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.82s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  4.84s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  5.03s/it]
[2023-08-07 22:06:42,018] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-07 22:06:42,019] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:
[2023-08-07 22:06:42,019] [INFO] [config.py:1012:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-07 22:06:42,019] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-07 22:06:42,019] [INFO] [config.py:1012:print]   amp_enabled .................. False
[2023-08-07 22:06:42,019] [INFO] [config.py:1012:print]   amp_params ................... False
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   bfloat16_enabled ............. False
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa63e34e7c0>
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   communication_data_type ...... None
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   disable_allgather ............ False
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   dump_state ................... False
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'min_scale': 1}
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False
[2023-08-07 22:06:42,020] [INFO] [config.py:1012:print]   elasticity_enabled ........... False
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   fp16_auto_cast ............... False
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   fp16_enabled ................. True
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   global_rank .................. 0
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 2048
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   loss_scale ................... 0
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   memory_breakdown ............. False
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7fa63e34eeb0>
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   optimizer_name ............... None
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   optimizer_params ............. None
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   pld_enabled .................. False
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   pld_params ................... False
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   prescale_gradients ........... False
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   scheduler_name ............... None
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   scheduler_params ............. None
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   sparse_attention ............. None
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   steps_per_print .............. 1
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   train_batch_size ............. 20
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  5
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   use_node_local_storage ....... False
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   world_size ................... 4
[2023-08-07 22:06:42,021] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  True
[2023-08-07 22:06:42,022] [INFO] [config.py:1012:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-08-07 22:06:42,022] [INFO] [config.py:1012:print]   zero_enabled ................. False
[2023-08-07 22:06:42,022] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 0
[2023-08-07 22:06:42,022] [INFO] [config.py:997:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 5, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 1
}
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Emitting ninja build file /home/ylu130/.cache/torch_extensions/py39_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.3422539234161377 seconds
Loading extension module utils...
Time to load utils op: 0.3045961856842041 seconds
Loading extension module utils...
Time to load utils op: 0.4038107395172119 seconds
Loading extension module utils...
Time to load utils op: 0.4044063091278076 seconds
Model mem
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   12918 MB |   12918 MB |   12918 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      66 MB |      66 MB |      66 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |
|       from small pool |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|---------------------------------------------------------------------------|
| Allocations           |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     259    |     259    |     259    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |      33    |      33    |      33    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |      33    |      31    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       2    |       2    |      33    |      31    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Evaluating commonsenseqa :   0%|                                                                               | 0/50 [00:00<?, ?it/s]############### Example ###############
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:Answer the following multiple choice question.

### Demonstration:
Input: He fantasied about getting a what while driving to work and the pros and cons of the extra responsibilities and benefits? Choices:  A: new car B: promotion C: boredom D: impatience E: pressure
Answer: B: promotion

### Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid

### Response:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s42-rFalse
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 125, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 122, in main
    evaluate_main(args, tokenizer, model, dataset["test"], device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 137, in evaluate_main
    lm_loss, query_ids, response_ids, rest_ids = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 101, in run_model
    out = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return func(*args, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1836, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 727, in forward
    outputs = self.model(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 617, in forward
    layer_outputs = decoder_layer(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 328, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 266, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
RuntimeError: CUDA out of memory. Tried to allocate 1.26 GiB (GPU 1; 47.54 GiB total capacity; 16.81 GiB already allocated; 1.17 GiB free; 16.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating commonsenseqa :   0%|                                                                               | 0/50 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 125, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 122, in main
    evaluate_main(args, tokenizer, model, dataset["test"], device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 137, in evaluate_main
    lm_loss, query_ids, response_ids, rest_ids = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 101, in run_model
    out = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return func(*args, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1836, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 727, in forward
    outputs = self.model(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 617, in forward
    layer_outputs = decoder_layer(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 328, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 266, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/functional.py", line 1836, in softmax
    ret = input.softmax(dim, dtype=dtype)
RuntimeError: CUDA out of memory. Tried to allocate 2.51 GiB (GPU 0; 47.54 GiB total capacity; 15.63 GiB already allocated; 1.05 GiB free; 19.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 435252 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 435253 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 435250) of binary: /home/ylu130/.conda/envs/distllm/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.conda/envs/distllm/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/run.py", line 761, in main
    run(args)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-08-07_22:06:48
  host      : ia1.wse.jhu.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 435251)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-07_22:06:48
  host      : ia1.wse.jhu.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 435250)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model/llama-2-7b --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --seed 42 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s42-rFalse --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date![nltk_data]   Package punkt is already up-to-date!

[nltk_data]   Package punkt is already up-to-date!
using world size: 4
[2023-08-07 22:06:53,013] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model/llama-2-7b
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  model_parallel ............... False
  model_parallel_size .......... None
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s42-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 2
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 2048
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s42-rFalse
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  gradient_accumulation_steps .. 1
  batch_size ................... 5
  clip_grad .................... 1.0
  seed ......................... 42
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading Data
  0%|                                                                                                        | 0/9739 [00:00<?, ?it/s]  1%|█▎                                                                                          | 144/9739 [00:00<00:06, 1436.92it/s]  3%|██▊                                                                                         | 295/9739 [00:00<00:06, 1478.06it/s]  5%|████▍                                                                                       | 466/9739 [00:00<00:05, 1583.17it/s]  7%|██████                                                                                      | 638/9739 [00:00<00:05, 1635.81it/s]  8%|███████▋                                                                                    | 811/9739 [00:00<00:05, 1667.47it/s] 10%|█████████▎                                                                                  | 984/9739 [00:00<00:05, 1688.54it/s] 12%|██████████▊                                                                                | 1158/9739 [00:00<00:05, 1702.66it/s] 14%|████████████▍                                                                              | 1329/9739 [00:00<00:05, 1661.65it/s] 15%|█████████████▉                                                                             | 1496/9739 [00:00<00:05, 1562.10it/s] 17%|███████████████▌                                                                           | 1668/9739 [00:01<00:05, 1607.00it/s] 19%|█████████████████                                                                          | 1830/9739 [00:01<00:04, 1610.47it/s] 21%|██████████████████▋                                                                        | 2002/9739 [00:01<00:04, 1640.64it/s] 22%|████████████████████▎                                                                      | 2172/9739 [00:01<00:04, 1657.34it/s] 24%|█████████████████████▉                                                                     | 2344/9739 [00:01<00:04, 1674.79it/s] 26%|███████████████████████▌                                                                   | 2517/9739 [00:01<00:04, 1689.05it/s] 28%|█████████████████████████                                                                  | 2687/9739 [00:01<00:04, 1630.54it/s] 29%|██████████████████████████▋                                                                | 2858/9739 [00:01<00:04, 1653.30it/s] 31%|████████████████████████████▎                                                              | 3030/9739 [00:01<00:04, 1671.91it/s] 33%|█████████████████████████████▉                                                             | 3203/9739 [00:01<00:03, 1686.65it/s] 35%|███████████████████████████████▌                                                           | 3375/9739 [00:02<00:03, 1693.75it/s] 36%|█████████████████████████████████▏                                                         | 3546/9739 [00:02<00:03, 1697.97it/s] 38%|██████████████████████████████████▋                                                        | 3718/9739 [00:02<00:03, 1703.72it/s] 40%|████████████████████████████████████▎                                                      | 3889/9739 [00:02<00:03, 1645.88it/s] 42%|█████████████████████████████████████▉                                                     | 4061/9739 [00:02<00:03, 1666.07it/s] 43%|███████████████████████████████████████▌                                                   | 4232/9739 [00:02<00:03, 1678.51it/s] 45%|█████████████████████████████████████████▏                                                 | 4405/9739 [00:02<00:03, 1692.58it/s] 48%|███████████████████████████████████████████▍                                               | 4644/9739 [00:02<00:02, 1898.87it/s] 50%|█████████████████████████████████████████████▏                                             | 4835/9739 [00:02<00:03, 1564.64it/s] 52%|███████████████████████████████████████████████▍                                           | 5078/9739 [00:03<00:02, 1787.24it/s] 54%|█████████████████████████████████████████████████▍                                         | 5296/9739 [00:03<00:02, 1891.75it/s] 57%|███████████████████████████████████████████████████▉                                       | 5559/9739 [00:03<00:01, 2095.12it/s] 60%|██████████████████████████████████████████████████████▍                                    | 5830/9739 [00:03<00:01, 2267.81it/s] 63%|█████████████████████████████████████████████████████████                                  | 6102/9739 [00:03<00:01, 2395.98it/s] 65%|███████████████████████████████████████████████████████████▌                               | 6371/9739 [00:03<00:01, 2479.83it/s] 68%|██████████████████████████████████████████████████████████████                             | 6640/9739 [00:03<00:01, 2540.80it/s] 71%|████████████████████████████████████████████████████████████████▍                          | 6897/9739 [00:03<00:01, 2544.97it/s] 73%|██████████████████████████████████████████████████████████████████▊                        | 7154/9739 [00:03<00:01, 2480.52it/s] 76%|█████████████████████████████████████████████████████████████████████▍                     | 7426/9739 [00:03<00:00, 2549.63it/s] 79%|███████████████████████████████████████████████████████████████████████▊                   | 7684/9739 [00:04<00:00, 2556.42it/s] 82%|██████████████████████████████████████████████████████████████████████████▎                | 7955/9739 [00:04<00:00, 2599.32it/s] 84%|████████████████████████████████████████████████████████████████████████████▊              | 8227/9739 [00:04<00:00, 2634.89it/s] 87%|███████████████████████████████████████████████████████████████████████████████▎           | 8492/9739 [00:04<00:00, 2614.03it/s] 90%|█████████████████████████████████████████████████████████████████████████████████▊         | 8762/9739 [00:04<00:00, 2637.85it/s] 93%|████████████████████████████████████████████████████████████████████████████████████▎      | 9027/9739 [00:04<00:00, 2533.69it/s] 95%|██████████████████████████████████████████████████████████████████████████████████████▊    | 9296/9739 [00:04<00:00, 2577.68it/s] 98%|█████████████████████████████████████████████████████████████████████████████████████████▍ | 9569/9739 [00:04<00:00, 2621.27it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████| 9739/9739 [00:04<00:00, 2017.70it/s]
Load End
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.34s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.46s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:11,  5.65s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:06<00:12,  6.13s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.26s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.30s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.44s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:12<00:06,  6.43s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.63s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.81s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.64s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.84s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.79s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.99s/it]
[2023-08-07 22:07:52,831] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.41s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.65s/it]
[2023-08-07 22:08:12,166] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-07 22:08:12,167] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:
[2023-08-07 22:08:12,168] [INFO] [config.py:1012:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-07 22:08:12,168] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-07 22:08:12,168] [INFO] [config.py:1012:print]   amp_enabled .................. False
[2023-08-07 22:08:12,168] [INFO] [config.py:1012:print]   amp_params ................... False
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   bfloat16_enabled ............. False
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1d081867c0>
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   communication_data_type ...... None
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   disable_allgather ............ False
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   dump_state ................... False
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'min_scale': 1}
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01
[2023-08-07 22:08:12,169] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   elasticity_enabled ........... False
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   fp16_auto_cast ............... False
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   fp16_enabled ................. True
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   global_rank .................. 0
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 2048
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   loss_scale ................... 0
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   memory_breakdown ............. False
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f1d08186eb0>
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   optimizer_name ............... None
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   optimizer_params ............. None
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   pld_enabled .................. False
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   pld_params ................... False
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   prescale_gradients ........... False
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   scheduler_name ............... None
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   scheduler_params ............. None
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   sparse_attention ............. None
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   steps_per_print .............. 1
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   train_batch_size ............. 20
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  5
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   use_node_local_storage ....... False
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   world_size ................... 4
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  True
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   zero_enabled ................. False
[2023-08-07 22:08:12,170] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 0
[2023-08-07 22:08:12,170] [INFO] [config.py:997:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 5, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 1
}
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Emitting ninja build file /home/ylu130/.cache/torch_extensions/py39_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.37487244606018066 seconds
Model mem
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   12918 MB |   12918 MB |   12918 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      66 MB |      66 MB |      66 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |
|       from small pool |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|---------------------------------------------------------------------------|
| Allocations           |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     259    |     259    |     259    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |      33    |      33    |      33    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |      33    |      31    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       2    |       2    |      33    |      31    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Evaluating commonsenseqa :   0%|                                                                               | 0/50 [00:00<?, ?it/s]############### Example ###############
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:Answer the following multiple choice question.

### Demonstration:
Input: He fantasied about getting a what while driving to work and the pros and cons of the extra responsibilities and benefits? Choices:  A: new car B: promotion C: boredom D: impatience E: pressure
Answer: B: promotion

Input: He was good at traditional science but excelled at social science, his favorite subject was what? Choices:  A: geography B: history studies C: math D: religion E: dancing
Answer: B: history studies

### Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid

### Response:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s42-rFalse
Loading extension module utils...
Time to load utils op: 0.30437779426574707 seconds
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.40512752532958984 seconds
Time to load utils op: 0.4044971466064453 seconds
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 125, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 122, in main
    evaluate_main(args, tokenizer, model, dataset["test"], device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 137, in evaluate_main
    lm_loss, query_ids, response_ids, rest_ids = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 101, in run_model
    out = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return func(*args, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1836, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 727, in forward
    outputs = self.model(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 617, in forward
    layer_outputs = decoder_layer(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 328, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 266, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
RuntimeError: CUDA out of memory. Tried to allocate 1.26 GiB (GPU 1; 47.54 GiB total capacity; 16.81 GiB already allocated; 1.16 GiB free; 16.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 435967 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 435969 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 435970 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 435968) of binary: /home/ylu130/.conda/envs/distllm/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.conda/envs/distllm/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/run.py", line 761, in main
    run(args)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-07_22:08:15
  host      : ia1.wse.jhu.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 435968)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model/llama-2-7b --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --seed 42 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s42-rFalse --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
using world size: 4
[2023-08-07 22:08:19,603] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model/llama-2-7b
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  model_parallel ............... False
  model_parallel_size .......... None
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s42-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 3
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 2048
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i3-s42-rFalse
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  gradient_accumulation_steps .. 1
  batch_size ................... 5
  clip_grad .................... 1.0
  seed ......................... 42
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading Data
  0%|                                                                                                        | 0/9738 [00:00<?, ?it/s]  1%|█▎                                                                                          | 134/9738 [00:00<00:07, 1339.59it/s]  3%|██▌                                                                                         | 269/9738 [00:00<00:07, 1344.04it/s]  4%|███▊                                                                                        | 406/9738 [00:00<00:06, 1353.11it/s]  6%|█████                                                                                       | 542/9738 [00:00<00:06, 1353.61it/s]  7%|██████▍                                                                                     | 678/9738 [00:00<00:06, 1352.73it/s]  8%|███████▋                                                                                    | 815/9738 [00:00<00:06, 1356.05it/s] 10%|████████▉                                                                                   | 952/9738 [00:00<00:06, 1357.86it/s] 11%|██████████▏                                                                                | 1088/9738 [00:00<00:06, 1296.52it/s] 13%|███████████▍                                                                               | 1224/9738 [00:00<00:06, 1314.25it/s] 14%|████████████▋                                                                              | 1360/9738 [00:01<00:06, 1326.37it/s] 15%|█████████████▉                                                                             | 1493/9738 [00:01<00:06, 1308.36it/s] 17%|███████████████▏                                                                           | 1627/9738 [00:01<00:06, 1315.25it/s] 18%|████████████████▍                                                                          | 1763/9738 [00:01<00:06, 1327.10it/s] 19%|█████████████████▋                                                                         | 1896/9738 [00:01<00:06, 1304.20it/s] 21%|██████████████████▉                                                                        | 2027/9738 [00:01<00:06, 1254.61it/s] 22%|████████████████████▏                                                                      | 2163/9738 [00:01<00:05, 1282.62it/s] 24%|█████████████████████▍                                                                     | 2299/9738 [00:01<00:05, 1302.52it/s] 25%|██████████████████████▊                                                                    | 2436/9738 [00:01<00:05, 1321.03it/s] 26%|████████████████████████                                                                   | 2570/9738 [00:01<00:05, 1326.61it/s] 28%|█████████████████████████▎                                                                 | 2705/9738 [00:02<00:05, 1331.89it/s] 29%|██████████████████████████▌                                                                | 2839/9738 [00:02<00:05, 1331.01it/s] 31%|███████████████████████████▊                                                               | 2975/9738 [00:02<00:05, 1336.82it/s] 32%|█████████████████████████████                                                              | 3109/9738 [00:02<00:05, 1280.49it/s] 33%|██████████████████████████████▎                                                            | 3244/9738 [00:02<00:04, 1300.18it/s] 35%|███████████████████████████████▌                                                           | 3380/9738 [00:02<00:04, 1317.09it/s] 36%|████████████████████████████████▊                                                          | 3515/9738 [00:02<00:04, 1324.76it/s] 37%|██████████████████████████████████                                                         | 3650/9738 [00:02<00:04, 1329.72it/s] 39%|███████████████████████████████████▍                                                       | 3787/9738 [00:02<00:04, 1339.00it/s] 40%|████████████████████████████████████▋                                                      | 3923/9738 [00:02<00:04, 1344.36it/s] 42%|█████████████████████████████████████▉                                                     | 4058/9738 [00:03<00:04, 1316.34it/s] 43%|███████████████████████████████████████▏                                                   | 4190/9738 [00:03<00:04, 1315.97it/s] 44%|████████████████████████████████████████▍                                                  | 4325/9738 [00:03<00:04, 1325.74it/s] 46%|█████████████████████████████████████████▋                                                 | 4461/9738 [00:03<00:03, 1333.58it/s] 47%|██████████████████████████████████████████▉                                                | 4595/9738 [00:03<00:03, 1315.67it/s] 49%|████████████████████████████████████████████▏                                              | 4732/9738 [00:03<00:03, 1329.92it/s] 50%|█████████████████████████████████████████████▍                                             | 4866/9738 [00:03<00:04, 1040.70it/s] 52%|███████████████████████████████████████████████                                            | 5033/9738 [00:03<00:03, 1196.44it/s] 54%|████████████████████████████████████████████████▉                                          | 5238/9738 [00:03<00:03, 1419.63it/s] 56%|██████████████████████████████████████████████████▊                                        | 5443/9738 [00:04<00:02, 1589.06it/s] 58%|████████████████████████████████████████████████████▊                                      | 5655/9738 [00:04<00:02, 1736.98it/s] 60%|██████████████████████████████████████████████████████▊                                    | 5871/9738 [00:04<00:02, 1855.86it/s] 62%|████████████████████████████████████████████████████████▊                                  | 6086/9738 [00:04<00:01, 1939.07it/s] 65%|██████████████████████████████████████████████████████████▊                                | 6296/9738 [00:04<00:01, 1985.06it/s] 67%|████████████████████████████████████████████████████████████▋                              | 6498/9738 [00:04<00:01, 1978.39it/s] 69%|██████████████████████████████████████████████████████████████▌                            | 6698/9738 [00:04<00:01, 1930.35it/s] 71%|████████████████████████████████████████████████████████████████▌                          | 6913/9738 [00:04<00:01, 1993.18it/s] 73%|██████████████████████████████████████████████████████████████████▋                        | 7130/9738 [00:04<00:01, 2042.50it/s] 75%|████████████████████████████████████████████████████████████████████▋                      | 7346/9738 [00:04<00:01, 2076.91it/s] 78%|██████████████████████████████████████████████████████████████████████▌                    | 7555/9738 [00:05<00:01, 2068.66it/s] 80%|████████████████████████████████████████████████████████████████████████▌                  | 7763/9738 [00:05<00:00, 2044.07it/s] 82%|██████████████████████████████████████████████████████████████████████████▌                | 7978/9738 [00:05<00:00, 2075.01it/s] 84%|████████████████████████████████████████████████████████████████████████████▍              | 8186/9738 [00:05<00:00, 2057.36it/s] 86%|██████████████████████████████████████████████████████████████████████████████▌            | 8401/9738 [00:05<00:00, 2082.11it/s] 88%|████████████████████████████████████████████████████████████████████████████████▌          | 8617/9738 [00:05<00:00, 2102.08it/s] 91%|██████████████████████████████████████████████████████████████████████████████████▌        | 8832/9738 [00:05<00:00, 2116.00it/s] 93%|████████████████████████████████████████████████████████████████████████████████████▌      | 9048/9738 [00:05<00:00, 2127.35it/s] 95%|██████████████████████████████████████████████████████████████████████████████████████▌    | 9261/9738 [00:05<00:00, 2090.61it/s] 97%|████████████████████████████████████████████████████████████████████████████████████████▌  | 9471/9738 [00:06<00:00, 2081.84it/s] 99%|██████████████████████████████████████████████████████████████████████████████████████████▍| 9680/9738 [00:06<00:00, 2049.06it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████| 9738/9738 [00:06<00:00, 1585.33it/s]
Load End
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.23s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.27s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:11,  5.66s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.46s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.42s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.27s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:11<00:05,  5.53s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:12<00:06,  6.08s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.62s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.82s/it]
[2023-08-07 22:09:19,057] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.54s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.74s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  4.84s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  5.02s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.17s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.37s/it]
[2023-08-07 22:09:37,040] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-07 22:09:37,040] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   amp_enabled .................. False
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   amp_params ................... False
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   bfloat16_enabled ............. False
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fdc86ea61f0>
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   communication_data_type ...... None
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False
[2023-08-07 22:09:37,041] [INFO] [config.py:1012:print]   disable_allgather ............ False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   dump_state ................... False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'min_scale': 1}
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   elasticity_enabled ........... False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   fp16_auto_cast ............... False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   fp16_enabled ................. True
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   global_rank .................. 0
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 2048
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   loss_scale ................... 0
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   memory_breakdown ............. False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7fdc86ea6d30>
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   optimizer_name ............... None
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   optimizer_params ............. None
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   pld_enabled .................. False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   pld_params ................... False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   prescale_gradients ........... False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   scheduler_name ............... None
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   scheduler_params ............. None
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   sparse_attention ............. None
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   steps_per_print .............. 1
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   train_batch_size ............. 20
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  5
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   use_node_local_storage ....... False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   world_size ................... 4
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  True
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   zero_enabled ................. False
[2023-08-07 22:09:37,042] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 0
[2023-08-07 22:09:37,042] [INFO] [config.py:997:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 5, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 1
}
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Emitting ninja build file /home/ylu130/.cache/torch_extensions/py39_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.40256428718566895 seconds
Loading extension module utils...
Model mem
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   12918 MB |   12918 MB |   12918 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      66 MB |      66 MB |      66 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |
|       from small pool |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|---------------------------------------------------------------------------|
| Allocations           |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     259    |     259    |     259    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |      33    |      33    |      33    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |      33    |      31    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       2    |       2    |      33    |      31    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Time to load utils op: 0.3039233684539795 seconds
Evaluating commonsenseqa :   0%|                                                                               | 0/50 [00:00<?, ?it/s]############### Example ###############
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:Answer the following multiple choice question.

### Demonstration:
Input: He fantasied about getting a what while driving to work and the pros and cons of the extra responsibilities and benefits? Choices:  A: new car B: promotion C: boredom D: impatience E: pressure
Answer: B: promotion

Input: He was good at traditional science but excelled at social science, his favorite subject was what? Choices:  A: geography B: history studies C: math D: religion E: dancing
Answer: B: history studies

Input: Jan wasn't very good at studying.  What might help him study better? Choices:  A: having a bigger brain B: headaches C: inspiration D: more intelligence E: understanding
Answer: D: more intelligence

### Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid

### Response:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i3-s42-rFalse
Loading extension module utils...
Time to load utils op: 0.20400476455688477 seconds
Loading extension module utils...
Time to load utils op: 0.30573534965515137 seconds
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 125, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 122, in main
    evaluate_main(args, tokenizer, model, dataset["test"], device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 137, in evaluate_main
    lm_loss, query_ids, response_ids, rest_ids = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 101, in run_model
    out = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return func(*args, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1836, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 727, in forward
    outputs = self.model(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 617, in forward
    layer_outputs = decoder_layer(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 328, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 266, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
RuntimeError: CUDA out of memory. Tried to allocate 1.26 GiB (GPU 1; 47.54 GiB total capacity; 16.81 GiB already allocated; 1.16 GiB free; 16.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating commonsenseqa :   0%|                                                                               | 0/50 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 125, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 122, in main
    evaluate_main(args, tokenizer, model, dataset["test"], device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 137, in evaluate_main
    lm_loss, query_ids, response_ids, rest_ids = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 101, in run_model
    out = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return func(*args, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1836, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 727, in forward
    outputs = self.model(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 617, in forward
    layer_outputs = decoder_layer(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 328, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 266, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/functional.py", line 1836, in softmax
    ret = input.softmax(dim, dtype=dtype)
RuntimeError: CUDA out of memory. Tried to allocate 2.51 GiB (GPU 0; 47.54 GiB total capacity; 15.63 GiB already allocated; 1.05 GiB free; 19.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 436716 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 436718 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 436719 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 436717) of binary: /home/ylu130/.conda/envs/distllm/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.conda/envs/distllm/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/run.py", line 761, in main
    run(args)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-07_22:09:42
  host      : ia1.wse.jhu.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 436717)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model/llama-2-7b --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --seed 42 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s42-rFalse --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
using world size: 4
[2023-08-07 22:09:47,058] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model/llama-2-7b
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  model_parallel ............... False
  model_parallel_size .......... None
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s42-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 4
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 2048
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i4-s42-rFalse
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  gradient_accumulation_steps .. 1
  batch_size ................... 5
  clip_grad .................... 1.0
  seed ......................... 42
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading Data
  0%|                                                                                                        | 0/9737 [00:00<?, ?it/s]  1%|█                                                                                           | 118/9737 [00:00<00:08, 1171.65it/s]  2%|██▏                                                                                         | 236/9737 [00:00<00:08, 1173.74it/s]  4%|███▎                                                                                        | 355/9737 [00:00<00:07, 1177.85it/s]  5%|████▍                                                                                       | 473/9737 [00:00<00:08, 1125.70it/s]  6%|█████▌                                                                                      | 591/9737 [00:00<00:07, 1143.61it/s]  7%|██████▋                                                                                     | 710/9737 [00:00<00:07, 1156.24it/s]  9%|███████▊                                                                                    | 830/9737 [00:00<00:07, 1167.97it/s] 10%|████████▉                                                                                   | 947/9737 [00:00<00:07, 1147.98it/s] 11%|█████████▉                                                                                 | 1065/9737 [00:00<00:07, 1157.37it/s] 12%|███████████                                                                                | 1183/9737 [00:01<00:07, 1163.91it/s] 13%|████████████▏                                                                              | 1300/9737 [00:01<00:07, 1128.72it/s] 15%|█████████████▏                                                                             | 1417/9737 [00:01<00:07, 1140.82it/s] 16%|██████████████▎                                                                            | 1536/9737 [00:01<00:07, 1153.60it/s] 17%|███████████████▍                                                                           | 1655/9737 [00:01<00:06, 1162.15it/s] 18%|████████████████▌                                                                          | 1774/9737 [00:01<00:06, 1169.19it/s] 19%|█████████████████▋                                                                         | 1892/9737 [00:01<00:06, 1147.20it/s] 21%|██████████████████▊                                                                        | 2011/9737 [00:01<00:06, 1157.86it/s] 22%|███████████████████▉                                                                       | 2127/9737 [00:01<00:06, 1118.40it/s] 23%|████████████████████▉                                                                      | 2245/9737 [00:01<00:06, 1136.16it/s] 24%|██████████████████████                                                                     | 2363/9737 [00:02<00:06, 1147.83it/s] 25%|███████████████████████▏                                                                   | 2482/9737 [00:02<00:06, 1157.85it/s] 27%|████████████████████████▎                                                                  | 2601/9737 [00:02<00:06, 1164.76it/s] 28%|█████████████████████████▍                                                                 | 2719/9737 [00:02<00:06, 1167.61it/s] 29%|██████████████████████████▌                                                                | 2838/9737 [00:02<00:05, 1172.07it/s] 30%|███████████████████████████▋                                                               | 2956/9737 [00:02<00:06, 1121.40it/s] 32%|████████████████████████████▋                                                              | 3074/9737 [00:02<00:05, 1136.78it/s] 33%|█████████████████████████████▊                                                             | 3192/9737 [00:02<00:05, 1148.58it/s] 34%|██████████████████████████████▉                                                            | 3310/9737 [00:02<00:05, 1157.19it/s] 35%|████████████████████████████████                                                           | 3428/9737 [00:02<00:05, 1162.64it/s] 36%|█████████████████████████████████▏                                                         | 3545/9737 [00:03<00:05, 1155.55it/s] 38%|██████████████████████████████████▏                                                        | 3662/9737 [00:03<00:05, 1157.85it/s] 39%|███████████████████████████████████▎                                                       | 3781/9737 [00:03<00:05, 1166.16it/s] 40%|████████████████████████████████████▌                                                      | 3906/9737 [00:03<00:04, 1190.82it/s] 42%|██████████████████████████████████████▏                                                    | 4086/9737 [00:03<00:04, 1370.28it/s] 44%|███████████████████████████████████████▊                                                   | 4266/9737 [00:03<00:03, 1496.18it/s] 46%|█████████████████████████████████████████▌                                                 | 4444/9737 [00:03<00:03, 1579.48it/s] 47%|███████████████████████████████████████████                                                | 4608/9737 [00:03<00:03, 1596.34it/s] 49%|████████████████████████████████████████████▌                                              | 4768/9737 [00:03<00:03, 1303.93it/s] 50%|█████████████████████████████████████████████▉                                             | 4910/9737 [00:04<00:03, 1333.07it/s] 52%|███████████████████████████████████████████████▌                                           | 5088/9737 [00:04<00:03, 1452.59it/s] 54%|█████████████████████████████████████████████████▏                                         | 5267/9737 [00:04<00:02, 1544.30it/s] 56%|██████████████████████████████████████████████████▉                                        | 5445/9737 [00:04<00:02, 1611.16it/s] 58%|████████████████████████████████████████████████████▌                                      | 5623/9737 [00:04<00:02, 1658.74it/s] 60%|██████████████████████████████████████████████████████▎                                    | 5810/9737 [00:04<00:02, 1718.92it/s] 61%|███████████████████████████████████████████████████████▉                                   | 5988/9737 [00:04<00:02, 1735.80it/s] 63%|█████████████████████████████████████████████████████████▋                                 | 6171/9737 [00:04<00:02, 1757.33it/s] 65%|███████████████████████████████████████████████████████████▎                               | 6348/9737 [00:04<00:01, 1700.40it/s] 67%|█████████████████████████████████████████████████████████████                              | 6535/9737 [00:04<00:01, 1747.98it/s] 69%|██████████████████████████████████████████████████████████████▊                            | 6723/9737 [00:05<00:01, 1784.23it/s] 71%|████████████████████████████████████████████████████████████████▌                          | 6911/9737 [00:05<00:01, 1811.69it/s] 73%|██████████████████████████████████████████████████████████████████▎                        | 7096/9737 [00:05<00:01, 1822.26it/s] 75%|████████████████████████████████████████████████████████████████████                       | 7279/9737 [00:05<00:01, 1814.96it/s] 77%|█████████████████████████████████████████████████████████████████████▋                     | 7461/9737 [00:05<00:01, 1788.90it/s] 78%|███████████████████████████████████████████████████████████████████████▍                   | 7641/9737 [00:05<00:01, 1702.12it/s] 80%|█████████████████████████████████████████████████████████████████████████▏                 | 7825/9737 [00:05<00:01, 1739.70it/s] 82%|██████████████████████████████████████████████████████████████████████████▊                | 8010/9737 [00:05<00:00, 1770.73it/s] 84%|████████████████████████████████████████████████████████████████████████████▌              | 8196/9737 [00:05<00:00, 1795.89it/s] 86%|██████████████████████████████████████████████████████████████████████████████▎            | 8377/9737 [00:05<00:00, 1785.59it/s] 88%|████████████████████████████████████████████████████████████████████████████████           | 8562/9737 [00:06<00:00, 1803.89it/s] 90%|█████████████████████████████████████████████████████████████████████████████████▊         | 8749/9737 [00:06<00:00, 1821.12it/s] 92%|███████████████████████████████████████████████████████████████████████████████████▍       | 8932/9737 [00:06<00:00, 1724.76it/s] 94%|█████████████████████████████████████████████████████████████████████████████████████▏     | 9117/9737 [00:06<00:00, 1759.61it/s] 96%|██████████████████████████████████████████████████████████████████████████████████████▉    | 9305/9737 [00:06<00:00, 1792.15it/s] 97%|████████████████████████████████████████████████████████████████████████████████████████▋  | 9488/9737 [00:06<00:00, 1801.01it/s] 99%|██████████████████████████████████████████████████████████████████████████████████████████▎| 9669/9737 [00:06<00:00, 1797.83it/s]100%|███████████████████████████████████████████████████████████████████████████████████████████| 9737/9737 [00:06<00:00, 1440.70it/s]
Load End
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.43s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.32s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.45s/it]Loading checkpoint shards:  33%|████████████████████████                                                | 1/3 [00:05<00:10,  5.45s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.23s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.36s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:10<00:05,  5.41s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████                        | 2/3 [00:11<00:05,  5.86s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.62s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.80s/it]
[2023-08-07 22:10:47,167] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.82s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.98s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.78s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.94s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.23s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.36s/it]
[2023-08-07 22:10:58,090] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-07 22:10:58,092] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:
[2023-08-07 22:10:58,092] [INFO] [config.py:1012:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-07 22:10:58,092] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-07 22:10:58,092] [INFO] [config.py:1012:print]   amp_enabled .................. False
[2023-08-07 22:10:58,092] [INFO] [config.py:1012:print]   amp_params ................... False
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   bfloat16_enabled ............. False
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1169b717c0>
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   communication_data_type ...... None
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   disable_allgather ............ False
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   dump_state ................... False
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'min_scale': 1}
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   elasticity_enabled ........... False
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-07 22:10:58,093] [INFO] [config.py:1012:print]   fp16_auto_cast ............... False
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   fp16_enabled ................. True
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   global_rank .................. 0
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 2048
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   loss_scale ................... 0
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   memory_breakdown ............. False
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f1169b71eb0>
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   optimizer_name ............... None
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   optimizer_params ............. None
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   pld_enabled .................. False
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   pld_params ................... False
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   prescale_gradients ........... False
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   scheduler_name ............... None
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   scheduler_params ............. None
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   sparse_attention ............. None
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   steps_per_print .............. 1
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   train_batch_size ............. 20
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  5
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   use_node_local_storage ....... False
[2023-08-07 22:10:58,094] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False
[2023-08-07 22:10:58,095] [INFO] [config.py:1012:print]   world_size ................... 4
[2023-08-07 22:10:58,095] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  True
[2023-08-07 22:10:58,095] [INFO] [config.py:1012:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-08-07 22:10:58,095] [INFO] [config.py:1012:print]   zero_enabled ................. False
[2023-08-07 22:10:58,095] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 0
[2023-08-07 22:10:58,095] [INFO] [config.py:997:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 5, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 1
}
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Emitting ninja build file /home/ylu130/.cache/torch_extensions/py39_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.3651895523071289 seconds
Loading extension module utils...
Time to load utils op: 0.30438685417175293 seconds
Loading extension module utils...
Time to load utils op: 0.3042745590209961 seconds
Model mem
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   12918 MB |   12918 MB |   12918 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      66 MB |      66 MB |      66 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |
|       from small pool |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|---------------------------------------------------------------------------|
| Allocations           |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     259    |     259    |     259    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |      33    |      33    |      33    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |      33    |      31    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       2    |       2    |      33    |      31    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Loading extension module utils...
Time to load utils op: 0.30416059494018555 seconds
Evaluating commonsenseqa :   0%|                                                                               | 0/50 [00:00<?, ?it/s]############### Example ###############
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:Answer the following multiple choice question.

### Demonstration:
Input: He fantasied about getting a what while driving to work and the pros and cons of the extra responsibilities and benefits? Choices:  A: new car B: promotion C: boredom D: impatience E: pressure
Answer: B: promotion

Input: He was good at traditional science but excelled at social science, his favorite subject was what? Choices:  A: geography B: history studies C: math D: religion E: dancing
Answer: B: history studies

Input: Jan wasn't very good at studying.  What might help him study better? Choices:  A: having a bigger brain B: headaches C: inspiration D: more intelligence E: understanding
Answer: D: more intelligence

Input: The cat was wild but like all others he was always read for a cat what? Choices:  A: stealing B: four legs C: tuna fish D: food now E: nap
Answer: E: nap

### Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid

### Response:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i4-s42-rFalse
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 125, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 122, in main
    evaluate_main(args, tokenizer, model, dataset["test"], device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 137, in evaluate_main
    lm_loss, query_ids, response_ids, rest_ids = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 101, in run_model
    out = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return func(*args, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1836, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 727, in forward
    outputs = self.model(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 617, in forward
    layer_outputs = decoder_layer(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 328, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 266, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
RuntimeError: CUDA out of memory. Tried to allocate 1.26 GiB (GPU 1; 47.54 GiB total capacity; 16.81 GiB already allocated; 1.16 GiB free; 16.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Evaluating commonsenseqa :   0%|                                                                               | 0/50 [00:05<?, ?it/s]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 125, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 122, in main
    evaluate_main(args, tokenizer, model, dataset["test"], device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 137, in evaluate_main
    lm_loss, query_ids, response_ids, rest_ids = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 101, in run_model
    out = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return func(*args, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1836, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 727, in forward
    outputs = self.model(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 617, in forward
    layer_outputs = decoder_layer(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 328, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ylu130/workspace/minillm/transformers/src/transformers/models/llama/modeling_llama.py", line 266, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/nn/functional.py", line 1836, in softmax
    ret = input.softmax(dim, dtype=dtype)
RuntimeError: CUDA out of memory. Tried to allocate 2.51 GiB (GPU 0; 47.54 GiB total capacity; 15.63 GiB already allocated; 1.05 GiB free; 19.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 437509 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 437510 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 437507) of binary: /home/ylu130/.conda/envs/distllm/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.conda/envs/distllm/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/run.py", line 761, in main
    run(args)
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.conda/envs/distllm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-08-07_22:11:04
  host      : ia1.wse.jhu.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 437508)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-07_22:11:04
  host      : ia1.wse.jhu.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 437507)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
