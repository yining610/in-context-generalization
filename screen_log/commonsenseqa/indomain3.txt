PYTHONPATH=/home/ylu130/workspace/in-context-generalization
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 29501 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model/llama-2-7b --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i0-s1-rTrue --seed 1 --max-prompt-length 2048 --rationales --num-in-domain 0
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date![nltk_data]   Package punkt is already up-to-date!

[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
using world size: 4
[2023-08-21 11:42:39,283] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model/llama-2-7b
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  model_parallel ............... False
  model_parallel_size .......... None
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i0-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 2048
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i0-s1-rTrue
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  gradient_accumulation_steps .. 1
  batch_size ................... 5
  clip_grad .................... 1.0
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading Data
  0%|                                                                | 0/9741 [00:00<?, ?it/s]  4%|██▎                                                 | 429/9741 [00:00<00:02, 4283.35it/s]  9%|████▌                                               | 866/9741 [00:00<00:02, 4331.34it/s] 13%|██████▊                                            | 1303/9741 [00:00<00:01, 4343.70it/s] 18%|█████████                                          | 1738/9741 [00:00<00:01, 4334.85it/s] 22%|███████████▎                                       | 2172/9741 [00:00<00:01, 4284.39it/s] 27%|█████████████▌                                     | 2601/9741 [00:00<00:01, 4160.18it/s] 31%|███████████████▊                                   | 3024/9741 [00:00<00:01, 4181.76it/s] 35%|██████████████████                                 | 3454/9741 [00:00<00:01, 4217.65it/s] 40%|████████████████████▎                              | 3881/9741 [00:00<00:01, 4232.04it/s] 44%|██████████████████████▌                            | 4311/9741 [00:01<00:01, 4250.10it/s] 49%|████████████████████████▊                          | 4737/9741 [00:01<00:01, 4231.50it/s] 53%|███████████████████████████                        | 5161/9741 [00:01<00:01, 2749.23it/s] 57%|█████████████████████████████▎                     | 5592/9741 [00:01<00:01, 3090.15it/s] 62%|███████████████████████████████▋                   | 6045/9741 [00:01<00:01, 3431.27it/s] 67%|█████████████████████████████████▉                 | 6488/9741 [00:01<00:00, 3684.07it/s] 71%|████████████████████████████████████▎              | 6936/9741 [00:01<00:00, 3895.25it/s] 76%|██████████████████████████████████████▋            | 7392/9741 [00:01<00:00, 4076.29it/s] 80%|█████████████████████████████████████████          | 7841/9741 [00:01<00:00, 4190.85it/s] 85%|███████████████████████████████████████████▌       | 8309/9741 [00:02<00:00, 4329.93it/s] 90%|█████████████████████████████████████████████▉     | 8774/9741 [00:02<00:00, 4422.83it/s] 95%|████████████████████████████████████████████████▎  | 9239/9741 [00:02<00:00, 4487.01it/s]100%|██████████████████████████████████████████████████▉| 9724/9741 [00:02<00:00, 4593.93it/s]100%|███████████████████████████████████████████████████| 9741/9741 [00:02<00:00, 4061.21it/s]
Load End
Num instances: 1000
Loading checkpoint shards:   0%|                                        | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                        | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                        | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|██████████▋                     | 1/3 [00:04<00:09,  4.82s/it]Loading checkpoint shards:  33%|██████████▋                     | 1/3 [00:05<00:11,  5.51s/it]Loading checkpoint shards:  33%|██████████▋                     | 1/3 [00:05<00:10,  5.48s/it]Loading checkpoint shards:   0%|                                        | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  67%|█████████████████████▎          | 2/3 [00:09<00:04,  4.96s/it]Loading checkpoint shards:  67%|█████████████████████▎          | 2/3 [00:10<00:05,  5.38s/it]Loading checkpoint shards:  67%|█████████████████████▎          | 2/3 [00:10<00:05,  5.34s/it]Loading checkpoint shards:  33%|██████████▋                     | 1/3 [00:05<00:10,  5.26s/it]Loading checkpoint shards: 100%|████████████████████████████████| 3/3 [00:13<00:00,  4.38s/it]Loading checkpoint shards: 100%|████████████████████████████████| 3/3 [00:13<00:00,  4.53s/it]
Loading checkpoint shards: 100%|████████████████████████████████| 3/3 [00:14<00:00,  4.75s/it]Loading checkpoint shards: 100%|████████████████████████████████| 3/3 [00:14<00:00,  4.93s/it]
Loading checkpoint shards: 100%|████████████████████████████████| 3/3 [00:14<00:00,  4.71s/it]Loading checkpoint shards: 100%|████████████████████████████████| 3/3 [00:14<00:00,  4.89s/it]
[2023-08-21 11:43:33,836] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards:  67%|█████████████████████▎          | 2/3 [00:10<00:05,  5.23s/it]Loading checkpoint shards: 100%|████████████████████████████████| 3/3 [00:14<00:00,  4.65s/it]Loading checkpoint shards: 100%|████████████████████████████████| 3/3 [00:14<00:00,  4.81s/it]
[2023-08-21 11:43:47,306] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-21 11:43:47,307] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:
[2023-08-21 11:43:47,307] [INFO] [config.py:1012:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   amp_enabled .................. False
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   amp_params ................... False
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   bfloat16_enabled ............. False
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f42d7637f40>
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   communication_data_type ...... None
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   disable_allgather ............ False
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   dump_state ................... False
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'min_scale': 1}
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False
[2023-08-21 11:43:47,308] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   elasticity_enabled ........... False
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   fp16_auto_cast ............... False
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   fp16_enabled ................. True
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   global_rank .................. 0
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 2048
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   loss_scale ................... 0
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   memory_breakdown ............. False
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f42d50dcdc0>
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   optimizer_name ............... None
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   optimizer_params ............. None
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   pld_enabled .................. False
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   pld_params ................... False
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   prescale_gradients ........... False
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   scheduler_name ............... None
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   scheduler_params ............. None
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   sparse_attention ............. None
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   steps_per_print .............. 1
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   train_batch_size ............. 20
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  5
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   use_node_local_storage ....... False
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False
[2023-08-21 11:43:47,309] [INFO] [config.py:1012:print]   world_size ................... 4
[2023-08-21 11:43:47,310] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  True
[2023-08-21 11:43:47,310] [INFO] [config.py:1012:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-08-21 11:43:47,310] [INFO] [config.py:1012:print]   zero_enabled ................. False
[2023-08-21 11:43:47,310] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 0
[2023-08-21 11:43:47,310] [INFO] [config.py:997:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 5, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 1
}
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Emitting ninja build file /home/ylu130/.cache/torch_extensions/py39_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.3605022430419922 seconds
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.3051154613494873 seconds
Time to load utils op: 0.3043360710144043 seconds
Model mem
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   12918 MB |   12918 MB |   12918 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      66 MB |      66 MB |      66 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |
|       from small pool |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|---------------------------------------------------------------------------|
| Allocations           |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     259    |     259    |     259    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |      33    |      33    |      33    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |      33    |      31    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       2    |       2    |      33    |      31    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Loading extension module utils...
Evaluating commonsenseqa :   0%|                                       | 0/50 [00:00<?, ?it/s]Time to load utils op: 0.3044140338897705 seconds
############### Example ###############
### Instruction:Answer the following multiple choice question.



Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i0-s1-rTrue
Evaluating commonsenseqa :   2%|▌                              | 1/50 [00:59<48:49, 59.78s/it]Evaluating commonsenseqa :   4%|█▏                             | 2/50 [01:45<41:02, 51.29s/it]Evaluating commonsenseqa :   6%|█▊                             | 3/50 [02:46<43:40, 55.76s/it]Evaluating commonsenseqa :   8%|██▍                            | 4/50 [03:45<43:42, 57.00s/it]Evaluating commonsenseqa :  10%|███                            | 5/50 [04:45<43:41, 58.27s/it]Evaluating commonsenseqa :  12%|███▋                           | 6/50 [05:40<41:57, 57.23s/it]Evaluating commonsenseqa :  14%|████▎                          | 7/50 [06:41<41:42, 58.20s/it]Evaluating commonsenseqa :  16%|████▉                          | 8/50 [07:41<41:13, 58.88s/it]Evaluating commonsenseqa :  18%|█████▌                         | 9/50 [08:40<40:22, 59.08s/it]Evaluating commonsenseqa :  20%|██████                        | 10/50 [09:40<39:24, 59.11s/it]Evaluating commonsenseqa :  22%|██████▌                       | 11/50 [10:39<38:28, 59.20s/it]Evaluating commonsenseqa :  24%|███████▏                      | 12/50 [11:39<37:34, 59.33s/it]Evaluating commonsenseqa :  26%|███████▊                      | 13/50 [12:39<36:43, 59.55s/it]Evaluating commonsenseqa :  28%|████████▍                     | 14/50 [13:37<35:35, 59.32s/it]Evaluating commonsenseqa :  30%|█████████                     | 15/50 [14:38<34:54, 59.84s/it]Evaluating commonsenseqa :  32%|█████████▌                    | 16/50 [15:38<33:50, 59.71s/it]Evaluating commonsenseqa :  34%|██████████▏                   | 17/50 [16:38<32:54, 59.85s/it]Evaluating commonsenseqa :  36%|██████████▊                   | 18/50 [17:22<29:23, 55.12s/it]Evaluating commonsenseqa :  38%|███████████▍                  | 19/50 [17:58<25:25, 49.20s/it]Evaluating commonsenseqa :  40%|████████████                  | 20/50 [18:32<22:22, 44.76s/it]Evaluating commonsenseqa :  42%|████████████▌                 | 21/50 [19:10<20:35, 42.60s/it]Evaluating commonsenseqa :  44%|█████████████▏                | 22/50 [20:08<22:08, 47.46s/it]Evaluating commonsenseqa :  46%|█████████████▊                | 23/50 [21:08<22:59, 51.11s/it]Evaluating commonsenseqa :  48%|██████████████▍               | 24/50 [21:53<21:21, 49.30s/it]Evaluating commonsenseqa :  50%|███████████████               | 25/50 [22:40<20:13, 48.55s/it]Evaluating commonsenseqa :  52%|███████████████▌              | 26/50 [23:40<20:46, 51.93s/it]Evaluating commonsenseqa :  54%|████████████████▏             | 27/50 [24:40<20:53, 54.50s/it]Evaluating commonsenseqa :  56%|████████████████▊             | 28/50 [25:40<20:32, 56.01s/it]Evaluating commonsenseqa :  58%|█████████████████▍            | 29/50 [26:41<20:07, 57.51s/it]Evaluating commonsenseqa :  60%|██████████████████            | 30/50 [27:10<16:22, 49.10s/it]Evaluating commonsenseqa :  62%|██████████████████▌           | 31/50 [27:52<14:53, 47.00s/it]Evaluating commonsenseqa :  64%|███████████████████▏          | 32/50 [28:25<12:51, 42.85s/it]Evaluating commonsenseqa :  66%|███████████████████▊          | 33/50 [29:25<13:31, 47.74s/it]Evaluating commonsenseqa :  68%|████████████████████▍         | 34/50 [29:47<10:42, 40.13s/it]Evaluating commonsenseqa :  70%|█████████████████████         | 35/50 [30:28<10:04, 40.28s/it]Evaluating commonsenseqa :  72%|█████████████████████▌        | 36/50 [31:28<10:47, 46.23s/it]Evaluating commonsenseqa :  74%|██████████████████████▏       | 37/50 [32:03<09:18, 42.98s/it]Evaluating commonsenseqa :  76%|██████████████████████▊       | 38/50 [33:02<09:33, 47.80s/it]Evaluating commonsenseqa :  78%|███████████████████████▍      | 39/50 [34:02<09:23, 51.27s/it]Evaluating commonsenseqa :  80%|████████████████████████      | 40/50 [34:32<07:31, 45.10s/it]Evaluating commonsenseqa :  82%|████████████████████████▌     | 41/50 [35:33<07:29, 49.93s/it]Evaluating commonsenseqa :  84%|█████████████████████████▏    | 42/50 [36:33<07:01, 52.71s/it]Evaluating commonsenseqa :  86%|█████████████████████████▊    | 43/50 [37:19<05:55, 50.82s/it]Evaluating commonsenseqa :  88%|██████████████████████████▍   | 44/50 [38:19<05:21, 53.51s/it]Evaluating commonsenseqa :  90%|███████████████████████████   | 45/50 [39:19<04:37, 55.51s/it]Evaluating commonsenseqa :  92%|███████████████████████████▌  | 46/50 [40:18<03:45, 56.42s/it]Evaluating commonsenseqa :  94%|████████████████████████████▏ | 47/50 [41:12<02:47, 55.73s/it]Evaluating commonsenseqa :  96%|████████████████████████████▊ | 48/50 [42:10<01:53, 56.55s/it]Evaluating commonsenseqa :  98%|█████████████████████████████▍| 49/50 [43:09<00:57, 57.24s/it]Evaluating commonsenseqa : 100%|██████████████████████████████| 50/50 [43:33<00:00, 47.19s/it]Evaluating commonsenseqa : 100%|██████████████████████████████| 50/50 [43:33<00:00, 52.27s/it]
name: commonsenseqa | {'exact_match': 0.0, 'rougeL': 4.8648} | lm_loss 7.4377 | avg. gen lenth: 216.032
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 29501 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model/llama-2-7b --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s1-rTrue --seed 1 --max-prompt-length 2048 --rationales --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data] Downloading package punkt to /home/ylu130/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
using world size: 4
[2023-08-21 12:28:18,140] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model/llama-2-7b
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  model_parallel ............... False
  model_parallel_size .......... None
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 1
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 2048
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s1-rTrue
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  gradient_accumulation_steps .. 1
  batch_size ................... 5
  clip_grad .................... 1.0
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading Data
  0%|                                                                | 0/9740 [00:00<?, ?it/s]  1%|▍                                                     | 78/9740 [00:00<00:12, 778.28it/s]  2%|▊                                                    | 157/9740 [00:00<00:12, 784.95it/s]  2%|█▎                                                   | 236/9740 [00:00<00:12, 781.34it/s]  3%|█▋                                                   | 315/9740 [00:00<00:12, 783.47it/s]  4%|██▏                                                  | 394/9740 [00:00<00:11, 785.70it/s]  5%|██▌                                                  | 473/9740 [00:00<00:11, 782.80it/s]  6%|███                                                  | 552/9740 [00:00<00:11, 784.63it/s]  6%|███▍                                                 | 631/9740 [00:00<00:11, 784.11it/s]  7%|███▊                                                 | 710/9740 [00:00<00:11, 766.85it/s]  8%|████▎                                                | 789/9740 [00:01<00:11, 771.38it/s]  9%|████▋                                                | 868/9740 [00:01<00:11, 776.89it/s] 10%|█████▏                                               | 947/9740 [00:01<00:11, 778.20it/s] 11%|█████▍                                              | 1026/9740 [00:01<00:11, 781.57it/s] 11%|█████▉                                              | 1106/9740 [00:01<00:11, 784.59it/s] 12%|██████▎                                             | 1185/9740 [00:01<00:10, 784.16it/s] 13%|██████▋                                             | 1264/9740 [00:01<00:10, 783.94it/s] 14%|███████▏                                            | 1343/9740 [00:01<00:10, 783.08it/s] 15%|███████▌                                            | 1422/9740 [00:01<00:10, 782.92it/s] 15%|████████                                            | 1501/9740 [00:01<00:10, 778.83it/s] 16%|████████▍                                           | 1579/9740 [00:02<00:10, 777.64it/s] 17%|████████▊                                           | 1658/9740 [00:02<00:10, 780.02it/s] 18%|█████████▎                                          | 1737/9740 [00:02<00:10, 782.10it/s] 19%|█████████▋                                          | 1816/9740 [00:02<00:10, 762.17it/s] 19%|██████████                                          | 1896/9740 [00:02<00:10, 773.04it/s] 21%|██████████▊                                         | 2016/9740 [00:02<00:08, 895.99it/s] 22%|███████████▍                                        | 2135/9740 [00:02<00:07, 982.02it/s] 23%|███████████▊                                       | 2254/9740 [00:02<00:07, 1043.39it/s] 24%|████████████▍                                      | 2374/9740 [00:02<00:06, 1088.31it/s] 26%|█████████████                                      | 2494/9740 [00:02<00:06, 1121.10it/s] 27%|█████████████▋                                     | 2614/9740 [00:03<00:06, 1142.78it/s] 28%|██████████████▎                                    | 2731/9740 [00:03<00:06, 1148.10it/s] 29%|██████████████▉                                    | 2850/9740 [00:03<00:05, 1159.77it/s] 30%|███████████████▌                                   | 2969/9740 [00:03<00:05, 1167.41it/s] 32%|████████████████▏                                  | 3089/9740 [00:03<00:05, 1175.30it/s] 33%|████████████████▊                                  | 3208/9740 [00:03<00:05, 1178.44it/s] 34%|█████████████████▍                                 | 3328/9740 [00:03<00:05, 1183.28it/s] 35%|██████████████████                                 | 3447/9740 [00:03<00:05, 1184.75it/s] 37%|██████████████████▋                                | 3567/9740 [00:03<00:05, 1186.90it/s] 38%|███████████████████▎                               | 3686/9740 [00:03<00:05, 1185.31it/s] 39%|███████████████████▉                               | 3807/9740 [00:04<00:04, 1189.55it/s] 40%|████████████████████▌                              | 3926/9740 [00:04<00:04, 1189.58it/s] 42%|█████████████████████▏                             | 4046/9740 [00:04<00:04, 1191.28it/s] 43%|█████████████████████▊                             | 4166/9740 [00:04<00:04, 1189.12it/s] 44%|██████████████████████▍                            | 4285/9740 [00:04<00:04, 1180.72it/s] 45%|███████████████████████                            | 4404/9740 [00:04<00:04, 1178.53it/s] 46%|███████████████████████▋                           | 4522/9740 [00:04<00:04, 1124.14it/s] 48%|████████████████████████▎                          | 4642/9740 [00:04<00:04, 1144.46it/s] 49%|█████████████████████████▍                          | 4757/9740 [00:04<00:05, 873.90it/s] 50%|██████████████████████████                          | 4876/9740 [00:05<00:05, 949.62it/s] 51%|██████████████████████████▏                        | 4995/9740 [00:05<00:04, 1010.64it/s] 53%|██████████████████████████▊                        | 5115/9740 [00:05<00:04, 1059.88it/s] 54%|███████████████████████████▍                       | 5234/9740 [00:05<00:04, 1094.52it/s] 55%|████████████████████████████                       | 5353/9740 [00:05<00:03, 1120.86it/s] 56%|████████████████████████████▋                      | 5472/9740 [00:05<00:03, 1139.21it/s] 57%|█████████████████████████████▎                     | 5596/9740 [00:05<00:03, 1168.03it/s] 59%|█████████████████████████████▉                     | 5719/9740 [00:05<00:03, 1183.93it/s] 60%|██████████████████████████████▌                    | 5843/9740 [00:05<00:03, 1199.14it/s] 61%|███████████████████████████████▏                   | 5967/9740 [00:05<00:03, 1209.21it/s] 63%|███████████████████████████████▉                   | 6090/9740 [00:06<00:03, 1214.69it/s] 64%|████████████████████████████████▌                  | 6214/9740 [00:06<00:02, 1219.74it/s] 65%|█████████████████████████████████▏                 | 6337/9740 [00:06<00:02, 1220.16it/s] 66%|█████████████████████████████████▊                 | 6461/9740 [00:06<00:02, 1225.71it/s] 68%|██████████████████████████████████▍                | 6584/9740 [00:06<00:02, 1225.53it/s] 69%|███████████████████████████████████▏               | 6709/9740 [00:06<00:02, 1230.19it/s] 70%|███████████████████████████████████▊               | 6833/9740 [00:06<00:02, 1231.43it/s] 71%|████████████████████████████████████▍              | 6958/9740 [00:06<00:02, 1235.13it/s] 73%|█████████████████████████████████████              | 7082/9740 [00:06<00:02, 1234.47it/s] 74%|█████████████████████████████████████▋             | 7207/9740 [00:06<00:02, 1236.00it/s] 75%|██████████████████████████████████████▍            | 7331/9740 [00:07<00:01, 1234.02it/s] 77%|███████████████████████████████████████            | 7455/9740 [00:07<00:01, 1204.03it/s] 78%|███████████████████████████████████████▋           | 7579/9740 [00:07<00:01, 1211.90it/s] 79%|████████████████████████████████████████▎          | 7702/9740 [00:07<00:01, 1216.71it/s] 80%|████████████████████████████████████████▉          | 7825/9740 [00:07<00:01, 1219.33it/s] 82%|█████████████████████████████████████████▌         | 7948/9740 [00:07<00:01, 1174.22it/s] 83%|██████████████████████████████████████████▎        | 8072/9740 [00:07<00:01, 1191.94it/s] 84%|██████████████████████████████████████████▉        | 8196/9740 [00:07<00:01, 1205.37it/s] 85%|███████████████████████████████████████████▌       | 8319/9740 [00:07<00:01, 1211.06it/s] 87%|████████████████████████████████████████████▏      | 8443/9740 [00:08<00:01, 1218.43it/s] 88%|████████████████████████████████████████████▊      | 8567/9740 [00:08<00:00, 1223.13it/s] 89%|█████████████████████████████████████████████▌     | 8690/9740 [00:08<00:00, 1224.57it/s] 90%|██████████████████████████████████████████████▏    | 8813/9740 [00:08<00:00, 1224.88it/s] 92%|██████████████████████████████████████████████▊    | 8938/9740 [00:08<00:00, 1229.97it/s] 93%|███████████████████████████████████████████████▍   | 9062/9740 [00:08<00:00, 1229.49it/s] 94%|████████████████████████████████████████████████   | 9187/9740 [00:08<00:00, 1232.91it/s] 96%|████████████████████████████████████████████████▊  | 9311/9740 [00:08<00:00, 1230.96it/s] 97%|█████████████████████████████████████████████████▍ | 9435/9740 [00:08<00:00, 1232.12it/s] 98%|██████████████████████████████████████████████████ | 9559/9740 [00:08<00:00, 1231.03it/s] 99%|██████████████████████████████████████████████████▋| 9683/9740 [00:09<00:00, 1233.36it/s]100%|███████████████████████████████████████████████████| 9740/9740 [00:09<00:00, 1075.00it/s]
Load End
Num instances: 1000
Loading checkpoint shards:   0%|                                        | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                        | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                        | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                        | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|██████████▋                     | 1/3 [00:05<00:10,  5.20s/it]Loading checkpoint shards:  33%|██████████▋                     | 1/3 [00:05<00:10,  5.12s/it]Loading checkpoint shards:  33%|██████████▋                     | 1/3 [00:05<00:10,  5.40s/it]Loading checkpoint shards:  33%|██████████▋                     | 1/3 [00:05<00:10,  5.42s/it]Loading checkpoint shards:  67%|█████████████████████▎          | 2/3 [00:10<00:05,  5.06s/it]Loading checkpoint shards:  67%|█████████████████████▎          | 2/3 [00:10<00:05,  5.17s/it]Loading checkpoint shards:  67%|█████████████████████▎          | 2/3 [00:11<00:05,  5.52s/it]Loading checkpoint shards:  67%|█████████████████████▎          | 2/3 [00:11<00:05,  5.60s/it]Loading checkpoint shards: 100%|████████████████████████████████| 3/3 [00:13<00:00,  4.47s/it]Loading checkpoint shards: 100%|████████████████████████████████| 3/3 [00:13<00:00,  4.65s/it]
Loading checkpoint shards: 100%|████████████████████████████████| 3/3 [00:14<00:00,  4.50s/it]Loading checkpoint shards: 100%|████████████████████████████████| 3/3 [00:14<00:00,  4.67s/it]
Loading checkpoint shards: 100%|████████████████████████████████| 3/3 [00:14<00:00,  4.80s/it]Loading checkpoint shards: 100%|████████████████████████████████| 3/3 [00:14<00:00,  4.99s/it]
Loading checkpoint shards: 100%|████████████████████████████████| 3/3 [00:15<00:00,  5.10s/it]Loading checkpoint shards: 100%|████████████████████████████████| 3/3 [00:15<00:00,  5.21s/it]
[2023-08-21 12:29:20,218] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown
[2023-08-21 12:29:30,138] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-21 12:29:30,140] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:
[2023-08-21 12:29:30,140] [INFO] [config.py:1012:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-21 12:29:30,140] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-21 12:29:30,140] [INFO] [config.py:1012:print]   amp_enabled .................. False
[2023-08-21 12:29:30,140] [INFO] [config.py:1012:print]   amp_params ................... False
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   bfloat16_enabled ............. False
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc7401c0f40>
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   communication_data_type ...... None
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   disable_allgather ............ False
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   dump_state ................... False
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'min_scale': 1}
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   elasticity_enabled ........... False
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-21 12:29:30,141] [INFO] [config.py:1012:print]   fp16_auto_cast ............... False
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   fp16_enabled ................. True
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   global_rank .................. 0
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 2048
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   loss_scale ................... 0
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   memory_breakdown ............. False
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7fc71ac01dc0>
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   optimizer_name ............... None
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   optimizer_params ............. None
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   pld_enabled .................. False
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   pld_params ................... False
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   prescale_gradients ........... False
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   scheduler_name ............... None
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   scheduler_params ............. None
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   sparse_attention ............. None
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   steps_per_print .............. 1
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   train_batch_size ............. 20
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  5
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   use_node_local_storage ....... False
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   world_size ................... 4
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  True
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   zero_enabled ................. False
[2023-08-21 12:29:30,142] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 0
[2023-08-21 12:29:30,143] [INFO] [config.py:997:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 5, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 1
}
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Using /home/ylu130/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Emitting ninja build file /home/ylu130/.cache/torch_extensions/py39_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.40443897247314453 seconds
Loading extension module utils...
Time to load utils op: 0.30432772636413574 seconds
Loading extension module utils...
Time to load utils op: 0.3046755790710449 seconds
Loading extension module utils...
Time to load utils op: 0.3043360710144043 seconds
Model mem
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |   12916 MB |   12916 MB |   12916 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      64 MB |      64 MB |      64 MB |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   12918 MB |   12918 MB |   12918 MB |       0 B  |
|       from large pool |   12852 MB |   12852 MB |   12852 MB |       0 B  |
|       from small pool |      66 MB |      66 MB |      66 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |
|       from small pool |    1512 KB |    2047 KB |   34815 KB |   33303 KB |
|---------------------------------------------------------------------------|
| Allocations           |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |     387    |     387    |     387    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |     161    |     161    |     161    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     259    |     259    |     259    |       0    |
|       from large pool |     226    |     226    |     226    |       0    |
|       from small pool |      33    |      33    |      33    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |      33    |      31    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       2    |       2    |      33    |      31    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Evaluating commonsenseqa :   0%|                                       | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Where could you find some plumbing that would not be of use to you if you are thirsty? Choices:  A: oil refineries B: wall C: show D: own home E: water fountain
Output: 1. The question is asking where you could find some plumbing that would not be of use to you if you are thirsty.
2. We then consider each of the given options.
3. Option A: oil refineries. Even though there may be water available in oil refineries, it is not safe to drink as it may be contaminated with oil.
4. Option B: wall. In most cases, there may be water pipes inside walls of buildings that supplies water. Therefore, if you are thirsty, these pipes in the wall may be of use to you as they may contain drinkable water.
5. Option C: show. It doesn't specify what kind of "show" it is, but if it's a TV show, there could be no actual plumbing. If it's a trade show, you could find usable plumbing in display booths or restroom areas. Therefore, the possibility of getting drinkable water from a "show" depends largely on context and cannot be completely ruled out.
6. Option D: own home. This should not be the correct answer because in your own home, the plumbing serves you with fresh water, which you can drink when you're thirsty.
7. Option E: water fountain. Plumbing for a water fountain usually includes potable water that is treated and safe to drink.
8. After considering all the options, it is clear that option A, oil refineries, is the only one where you would not access potable water through the plumbing.
So the final answer is A: oil refineries

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s1-rTrue
Evaluating commonsenseqa :   2%|▌                              | 1/50 [00:52<42:54, 52.55s/it]Evaluating commonsenseqa :   4%|█▏                             | 2/50 [01:43<41:21, 51.69s/it]Evaluating commonsenseqa :   6%|█▊                             | 3/50 [02:34<40:04, 51.16s/it]Evaluating commonsenseqa :   8%|██▍                            | 4/50 [03:24<39:00, 50.89s/it]Evaluating commonsenseqa :  10%|███                            | 5/50 [04:02<34:38, 46.19s/it]Evaluating commonsenseqa :  12%|███▋                           | 6/50 [04:53<34:59, 47.71s/it]Evaluating commonsenseqa :  14%|████▎                          | 7/50 [05:44<34:58, 48.81s/it]Evaluating commonsenseqa :  16%|████▉                          | 8/50 [06:35<34:42, 49.58s/it]Evaluating commonsenseqa :  18%|█████▌                         | 9/50 [07:26<34:16, 50.15s/it]Evaluating commonsenseqa :  20%|██████                        | 10/50 [08:17<33:34, 50.37s/it]Evaluating commonsenseqa :  22%|██████▌                       | 11/50 [09:09<33:05, 50.90s/it]Evaluating commonsenseqa :  24%|███████▏                      | 12/50 [10:01<32:21, 51.10s/it]Evaluating commonsenseqa :  26%|███████▊                      | 13/50 [10:53<31:40, 51.36s/it]Evaluating commonsenseqa :  28%|████████▍                     | 14/50 [11:45<30:55, 51.53s/it]Evaluating commonsenseqa :  30%|█████████                     | 15/50 [12:36<30:03, 51.53s/it]Evaluating commonsenseqa :  32%|█████████▌                    | 16/50 [13:28<29:14, 51.61s/it]Evaluating commonsenseqa :  34%|██████████▏                   | 17/50 [14:19<28:20, 51.53s/it]Evaluating commonsenseqa :  36%|██████████▊                   | 18/50 [15:11<27:30, 51.58s/it]Evaluating commonsenseqa :  38%|███████████▍                  | 19/50 [16:02<26:32, 51.39s/it]Evaluating commonsenseqa :  40%|████████████                  | 20/50 [16:53<25:40, 51.35s/it]Evaluating commonsenseqa :  42%|████████████▌                 | 21/50 [17:44<24:40, 51.07s/it]Evaluating commonsenseqa :  44%|█████████████▏                | 22/50 [18:34<23:46, 50.94s/it]Evaluating commonsenseqa :  46%|█████████████▊                | 23/50 [19:25<22:52, 50.85s/it]Evaluating commonsenseqa :  48%|██████████████▍               | 24/50 [20:16<22:00, 50.77s/it]Evaluating commonsenseqa :  50%|███████████████               | 25/50 [21:06<21:09, 50.78s/it]Evaluating commonsenseqa :  52%|███████████████▌              | 26/50 [21:58<20:21, 50.90s/it]Evaluating commonsenseqa :  54%|████████████████▏             | 27/50 [22:49<19:34, 51.08s/it]Evaluating commonsenseqa :  56%|████████████████▊             | 28/50 [23:40<18:40, 50.92s/it]Evaluating commonsenseqa :  58%|█████████████████▍            | 29/50 [24:31<17:50, 50.96s/it]Evaluating commonsenseqa :  60%|██████████████████            | 30/50 [25:22<16:58, 50.94s/it]Evaluating commonsenseqa :  62%|██████████████████▌           | 31/50 [26:12<16:07, 50.93s/it]Evaluating commonsenseqa :  64%|███████████████████▏          | 32/50 [27:04<15:20, 51.13s/it]Evaluating commonsenseqa :  66%|███████████████████▊          | 33/50 [27:55<14:28, 51.07s/it]Evaluating commonsenseqa :  68%|████████████████████▍         | 34/50 [28:46<13:37, 51.09s/it]Evaluating commonsenseqa :  70%|█████████████████████         | 35/50 [29:37<12:46, 51.08s/it]