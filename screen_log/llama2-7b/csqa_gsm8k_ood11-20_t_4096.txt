WORLD_SIZE=1
MASTER_ADDR=icgpu06
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 18963 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 8 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o16-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 16 16 17 18 19 20 True 4096 8
[2023-09-06 15:03:22,535] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o16-tgsm8k-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 16
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o16-tgsm8k-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 8
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 311807.16it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.97s/it]
 > number of parameters: 6738415616
[2023-09-06 15:03:38,277] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-06 15:03:38,504] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-06 15:03:38,505] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554b5f46490>
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-06 15:03:38,506] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-06 15:03:38,507] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-06 15:03:38,507] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/125 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: Mary added a total of 7 stickers/page * 6 pages= <<7*6=42>>42 stickers to the 6 other pages.
In total, Mary added 3 large stickers + 42 stickers = <<3+42=45>>45 stickers to her journal.
Since she started with 89 stickers, she now has 89 - 45 = <<89-45=44>>44 stickers left.
So the final answer is 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: If he babysits for 2 hours at $7 per hour, he will earn 2*7 = $<<2*7=14>>14
This week he will earn $5 allowance, $10 mowing the lawn and $14 from babysitting for a total of 5+10+14 = $<<5+10+14=29>>29
If we add the $29 he will earn to his $65 savings, he will have a total of 29 + 65 = $<<29+65=94>>94
The bike costs $100 and he will have $94 leaving $100-$94 = $<<100-94=6>>6 more that he will need to earn
So the final answer is 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: His kangaroos have a total of 23*2=<<23*2=46>>46 legs
He has 23*3=<<23*3=69>>69 goats
The goats have 69*4=<<69*4=276>>276 legs
So in total his animals have 276+46=<<276+46=322>>322 legs
So the final answer is 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: After buying a hat, Josh has $20 - $10 = $<<20-10=10>>10
After buying a pencil, Josh has $10 - $2 = $<<10-2=8>>8
The total cost of cookies is 4 * $1.25 = $<<4*1.25=5>>5
After buying the cookies, Josh has $8 - $5 = $<<8-5=3>>3
So the final answer is 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: The old team per round record is 1,148 because 287 x 4 = <<1148=1148>>1,148
The team season record is 11,480 because 10 x 1,248 = 11,480
They need 1,040 points in the final round to tie the record because 11,480 - 10,440 = <<11480-10440=1040>>1,040
They need to average 260 points each because 1,040 / 4 = <<1040/4=260>>260
This is 27 points less than the current record average because 287 - 260 = <<27=27>>27
So the final answer is 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: Max finished history in 20 * 2 = <<20*2=40>>40 minutes.
Finishing geography took the most time, which is 40 * 3 = <<40*3=120>>120 minutes.
In total, for all three subjects, Max needed 20 + 40 + 120 = <<20+40+120=180>>180 minutes.
So the final answer is 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: If Sophia ate 1/6 of the pie, then 6/6 - 1/6 = 5/6 is left in the fridge.
Let x be the pie's original weight.
The current weight of the pie can be described by 5x/6=1200 grams
So, 5x=7200.
And, the original weight is x = <<1440=1440>>1440 grams.
So, Sophia ate 1440 grams original - 1200 grams after= <<1440-1200=240>>240 grams of pie.
So the final answer is 240

Input: Sarah, Mary, and Tuan decided to go to the restaurant for a meal. They decided to split the cost of the meal evenly. If the total price of the meal comes to $67 and they have a coupon for $4, how much does each person need to contribute to the bill?
Output: After using the coupon, the final price comes to 67 - 4 = <<67-4=63>>63 dollars.
With three people, they each need to pay 63 / 3 = <<63/3=21>>21 dollars each.
So the final answer is 21

Input: Tom's brother is 4 times as old as Tom's dog. If in 6 years, Tom's brother will be 30 years, how old is Tom's dog going to be in six years?
Output: If in six years Tom's brother will be 30 years old, he is currently 30-6 = <<30-6=24>>24 years old.
Since Tom's brother is 4 times as old as Tom's dog, Tom's dog is 24/4 = <<24/4=6>>6 years old currently.
Tom's dog will be 6+6 = <<6+6=12>>12 years old in six years.
So the final answer is 12

Input: There are 50 children at the party. Three-fifths of them are boys. How many of the children are girls?
Output: 50 x 3/5 = <<50*3/5=30>>30 children are boys.
So, there are 50 - 30 = <<50-30=20>>20 children that are girls.
So the final answer is 20

Input: Gail has some bills in her wallet which amount to $100. She has four $5 bills and three $20 bills, and the rest are $10 bills. How many $10 bills are in her wallet?
Output: Four $5 bills amount to $5 x 4 = $<<5*4=20>>20.
Three $20 bills amount to $20 x 3 = $<<20*3=60>>60.
So Gail's $5 and $20 bills amount to $20 + $60 = $80.
Thus, her $10 bills amount to $100 - $80 = $20.
Therefore, she has $20/$10 = <<20/10=2>>2 $10 bills.
So the final answer is 2

Input: A 220-liter barrel has a small leak. It lost 10% of its contents before anyone noticed. How many liters are left in the barrel?
Output: The barrel lost 220 * 10 / 100 = <<220*10/100=22>>22 liters before anyone noticed.
So, 220 – 22 = <<220-22=198>>198 liters are left in the barrel.
So the final answer is 198

Input: Markese earned 5 fewer dollars than Evan. Together they earned $37. How many dollars did Markese earn? Use E to represent how many dollars Evan earned.
Output: E = Evan's earnings
Markese's earnings = E - 5
E + E - 5 = 37
2E = 42
E = <<21=21>>21
Evan made $21 so Markese earned 21 - 5 = <<21-5=16>>16
Markese earned $<<16=16>>16.
So the final answer is 16

Input: Lou Senior took 3 cookies out of the cookie jar and ate them.  Since he didn't get caught by his wife, he went back the next day and took another 3 cookies out of the jar.  But after eating just one of the cookies, he felt guilty about it and put the other two cookies back.  His son, Louie Junior saw that his Dad was eating cookies.  So, Louie Junior took seven cookies out of the jar and hid them in his bedroom for later.  The next morning, Debra, Lou's wife looked into the cookie jar and reacted by accusing her husband of eating half of the cookies out of the cookie jar.  How many cookies remained in the jar?
Output: Lou Sr 3 cookies from the jar, then another three, but then put two back, for a total of 3+3-2=<<3+3-2=4>>4 cookies removed.
Louie Jr took seven more out of the jar, for a total of 4+7=<<4+7=11>>11 cookies removed.
If Debra thought half were gone, then the number of missing cookies would equal the number of cookies that remain, or 11 missing=11 remaining cookies
So the final answer is 11

Input: John had $200. He gave 3/8 of his money to his mother and 3/10 to his father. How much money did John have left?
Output: John's mother received 3/8 x $200 = $<<3/8*200=75>>75.
His father got 3/10 x $200 = $<<3/10*200=60>>60.
Thus, John gave a total of $75 + $60 = $<<75+60=135>>135.
Therefore, John had $200 - $135 = $<<200-135=65>>65 left.
So the final answer is 65

Input: Tonya has $150.00 on her credit card.  If she leaves any balance on her card at the end of the month, she is charged 20% interest.  If she makes a $50.00 payment on her card, what will be the new balance?
Output: Her card has a $150.00 balance and she makes a $50.00 payment so the new balance is 150-50 = $<<150-50=100.00>>100.00
She didn't pay it off so she is charged 20% interest on her $100.00 balance so the interest is.20*100 = $20.00
Her balance was $100.00 and she was charged $20.00 in interest so her new balance is 100+20 = $<<100+20=120.00>>120.00
So the final answer is 120

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o16-tgsm8k-s1-rTrue-m4096
Evaluating commonsenseqa :   0%|          | 0/125 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 97, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 94, in main
    inference_main(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 90, in inference_main
    query_ids, response_ids, answers, indices = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 65, in run_model
    gen_out = model.generate(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 1454, in generate
    return self.sample(
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 2568, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3500537) of binary: /home/ylu130/.conda/envs/ood/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.conda/envs/ood/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-06_15:03:50
  host      : icgpu06.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3500537)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 18963 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 8 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o17-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 17 16 17 18 19 20 True 4096 8
[2023-09-06 15:03:53,945] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o17-tgsm8k-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 17
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o17-tgsm8k-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 8
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 286033.33it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.19s/it]
 > number of parameters: 6738415616
[2023-09-06 15:04:02,035] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-06 15:04:02,239] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-06 15:04:02,240] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-06 15:04:02,240] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554b5f44490>
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-06 15:04:02,241] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-06 15:04:02,242] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-06 15:04:02,242] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/125 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: Mary added a total of 7 stickers/page * 6 pages= <<7*6=42>>42 stickers to the 6 other pages.
In total, Mary added 3 large stickers + 42 stickers = <<3+42=45>>45 stickers to her journal.
Since she started with 89 stickers, she now has 89 - 45 = <<89-45=44>>44 stickers left.
So the final answer is 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: If he babysits for 2 hours at $7 per hour, he will earn 2*7 = $<<2*7=14>>14
This week he will earn $5 allowance, $10 mowing the lawn and $14 from babysitting for a total of 5+10+14 = $<<5+10+14=29>>29
If we add the $29 he will earn to his $65 savings, he will have a total of 29 + 65 = $<<29+65=94>>94
The bike costs $100 and he will have $94 leaving $100-$94 = $<<100-94=6>>6 more that he will need to earn
So the final answer is 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: His kangaroos have a total of 23*2=<<23*2=46>>46 legs
He has 23*3=<<23*3=69>>69 goats
The goats have 69*4=<<69*4=276>>276 legs
So in total his animals have 276+46=<<276+46=322>>322 legs
So the final answer is 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: After buying a hat, Josh has $20 - $10 = $<<20-10=10>>10
After buying a pencil, Josh has $10 - $2 = $<<10-2=8>>8
The total cost of cookies is 4 * $1.25 = $<<4*1.25=5>>5
After buying the cookies, Josh has $8 - $5 = $<<8-5=3>>3
So the final answer is 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: The old team per round record is 1,148 because 287 x 4 = <<1148=1148>>1,148
The team season record is 11,480 because 10 x 1,248 = 11,480
They need 1,040 points in the final round to tie the record because 11,480 - 10,440 = <<11480-10440=1040>>1,040
They need to average 260 points each because 1,040 / 4 = <<1040/4=260>>260
This is 27 points less than the current record average because 287 - 260 = <<27=27>>27
So the final answer is 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: Max finished history in 20 * 2 = <<20*2=40>>40 minutes.
Finishing geography took the most time, which is 40 * 3 = <<40*3=120>>120 minutes.
In total, for all three subjects, Max needed 20 + 40 + 120 = <<20+40+120=180>>180 minutes.
So the final answer is 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: If Sophia ate 1/6 of the pie, then 6/6 - 1/6 = 5/6 is left in the fridge.
Let x be the pie's original weight.
The current weight of the pie can be described by 5x/6=1200 grams
So, 5x=7200.
And, the original weight is x = <<1440=1440>>1440 grams.
So, Sophia ate 1440 grams original - 1200 grams after= <<1440-1200=240>>240 grams of pie.
So the final answer is 240

Input: Sarah, Mary, and Tuan decided to go to the restaurant for a meal. They decided to split the cost of the meal evenly. If the total price of the meal comes to $67 and they have a coupon for $4, how much does each person need to contribute to the bill?
Output: After using the coupon, the final price comes to 67 - 4 = <<67-4=63>>63 dollars.
With three people, they each need to pay 63 / 3 = <<63/3=21>>21 dollars each.
So the final answer is 21

Input: Tom's brother is 4 times as old as Tom's dog. If in 6 years, Tom's brother will be 30 years, how old is Tom's dog going to be in six years?
Output: If in six years Tom's brother will be 30 years old, he is currently 30-6 = <<30-6=24>>24 years old.
Since Tom's brother is 4 times as old as Tom's dog, Tom's dog is 24/4 = <<24/4=6>>6 years old currently.
Tom's dog will be 6+6 = <<6+6=12>>12 years old in six years.
So the final answer is 12

Input: There are 50 children at the party. Three-fifths of them are boys. How many of the children are girls?
Output: 50 x 3/5 = <<50*3/5=30>>30 children are boys.
So, there are 50 - 30 = <<50-30=20>>20 children that are girls.
So the final answer is 20

Input: Gail has some bills in her wallet which amount to $100. She has four $5 bills and three $20 bills, and the rest are $10 bills. How many $10 bills are in her wallet?
Output: Four $5 bills amount to $5 x 4 = $<<5*4=20>>20.
Three $20 bills amount to $20 x 3 = $<<20*3=60>>60.
So Gail's $5 and $20 bills amount to $20 + $60 = $80.
Thus, her $10 bills amount to $100 - $80 = $20.
Therefore, she has $20/$10 = <<20/10=2>>2 $10 bills.
So the final answer is 2

Input: A 220-liter barrel has a small leak. It lost 10% of its contents before anyone noticed. How many liters are left in the barrel?
Output: The barrel lost 220 * 10 / 100 = <<220*10/100=22>>22 liters before anyone noticed.
So, 220 – 22 = <<220-22=198>>198 liters are left in the barrel.
So the final answer is 198

Input: Markese earned 5 fewer dollars than Evan. Together they earned $37. How many dollars did Markese earn? Use E to represent how many dollars Evan earned.
Output: E = Evan's earnings
Markese's earnings = E - 5
E + E - 5 = 37
2E = 42
E = <<21=21>>21
Evan made $21 so Markese earned 21 - 5 = <<21-5=16>>16
Markese earned $<<16=16>>16.
So the final answer is 16

Input: Lou Senior took 3 cookies out of the cookie jar and ate them.  Since he didn't get caught by his wife, he went back the next day and took another 3 cookies out of the jar.  But after eating just one of the cookies, he felt guilty about it and put the other two cookies back.  His son, Louie Junior saw that his Dad was eating cookies.  So, Louie Junior took seven cookies out of the jar and hid them in his bedroom for later.  The next morning, Debra, Lou's wife looked into the cookie jar and reacted by accusing her husband of eating half of the cookies out of the cookie jar.  How many cookies remained in the jar?
Output: Lou Sr 3 cookies from the jar, then another three, but then put two back, for a total of 3+3-2=<<3+3-2=4>>4 cookies removed.
Louie Jr took seven more out of the jar, for a total of 4+7=<<4+7=11>>11 cookies removed.
If Debra thought half were gone, then the number of missing cookies would equal the number of cookies that remain, or 11 missing=11 remaining cookies
So the final answer is 11

Input: John had $200. He gave 3/8 of his money to his mother and 3/10 to his father. How much money did John have left?
Output: John's mother received 3/8 x $200 = $<<3/8*200=75>>75.
His father got 3/10 x $200 = $<<3/10*200=60>>60.
Thus, John gave a total of $75 + $60 = $<<75+60=135>>135.
Therefore, John had $200 - $135 = $<<200-135=65>>65 left.
So the final answer is 65

Input: Tonya has $150.00 on her credit card.  If she leaves any balance on her card at the end of the month, she is charged 20% interest.  If she makes a $50.00 payment on her card, what will be the new balance?
Output: Her card has a $150.00 balance and she makes a $50.00 payment so the new balance is 150-50 = $<<150-50=100.00>>100.00
She didn't pay it off so she is charged 20% interest on her $100.00 balance so the interest is.20*100 = $20.00
Her balance was $100.00 and she was charged $20.00 in interest so her new balance is 100+20 = $<<100+20=120.00>>120.00
So the final answer is 120

Input: In her first term, Governor Sandoval gave twice as many commencement addresses as Governor Hawkins. Governor Sloan gave ten more commencement addresses than Governor Sandoval in the same amount of time. If Governor Sandoval gave 12 commencement addresses, how many commencement addresses did the three of them give altogether?
Output: Since Governor Sandoval gave 12 commencement addresses, twice the number that Governor Hawkins gave, Governor Hawkins gave 12/2 = <<12/2=6>>6 commencement addresses.
The two of them gave 6+12 = <<6+12=18>>18 commencement addresses in total.
Governor Sloan gave ten more commencement addresses than Governor Sandoval, a total of 12+10 = <<12+10=22>>22 commencement addresses.
The three of them gave 22+18 = <<22+18=40>>40 commencement addresses.
So the final answer is 40

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o17-tgsm8k-s1-rTrue-m4096
Evaluating commonsenseqa :   0%|          | 0/125 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 97, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 94, in main
    inference_main(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 90, in inference_main
    query_ids, response_ids, answers, indices = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 65, in run_model
    gen_out = model.generate(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 1454, in generate
    return self.sample(
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 2568, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3501329) of binary: /home/ylu130/.conda/envs/ood/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.conda/envs/ood/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-06_15:04:11
  host      : icgpu06.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3501329)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 18963 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 8 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o18-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 18 16 17 18 19 20 True 4096 8
[2023-09-06 15:04:15,838] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o18-tgsm8k-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 18
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o18-tgsm8k-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 8
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 274104.96it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.93s/it]
 > number of parameters: 6738415616
[2023-09-06 15:04:25,418] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-06 15:04:25,609] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-06 15:04:25,611] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-06 15:04:25,611] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-06 15:04:25,611] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-06 15:04:25,611] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-06 15:04:25,611] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-06 15:04:25,611] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-06 15:04:25,611] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-06 15:04:25,611] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-06 15:04:25,611] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-06 15:04:25,611] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-06 15:04:25,611] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554b5f46490>
[2023-09-06 15:04:25,611] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-06 15:04:25,612] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-06 15:04:25,613] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-06 15:04:25,613] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-06 15:04:25,613] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-06 15:04:25,613] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-06 15:04:25,613] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-06 15:04:25,613] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-06 15:04:25,613] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-06 15:04:25,613] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-06 15:04:25,613] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-06 15:04:25,613] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-06 15:04:25,613] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-06 15:04:25,613] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-06 15:04:25,613] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-06 15:04:25,613] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-06 15:04:25,613] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-06 15:04:25,613] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/125 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: Mary added a total of 7 stickers/page * 6 pages= <<7*6=42>>42 stickers to the 6 other pages.
In total, Mary added 3 large stickers + 42 stickers = <<3+42=45>>45 stickers to her journal.
Since she started with 89 stickers, she now has 89 - 45 = <<89-45=44>>44 stickers left.
So the final answer is 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: If he babysits for 2 hours at $7 per hour, he will earn 2*7 = $<<2*7=14>>14
This week he will earn $5 allowance, $10 mowing the lawn and $14 from babysitting for a total of 5+10+14 = $<<5+10+14=29>>29
If we add the $29 he will earn to his $65 savings, he will have a total of 29 + 65 = $<<29+65=94>>94
The bike costs $100 and he will have $94 leaving $100-$94 = $<<100-94=6>>6 more that he will need to earn
So the final answer is 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: His kangaroos have a total of 23*2=<<23*2=46>>46 legs
He has 23*3=<<23*3=69>>69 goats
The goats have 69*4=<<69*4=276>>276 legs
So in total his animals have 276+46=<<276+46=322>>322 legs
So the final answer is 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: After buying a hat, Josh has $20 - $10 = $<<20-10=10>>10
After buying a pencil, Josh has $10 - $2 = $<<10-2=8>>8
The total cost of cookies is 4 * $1.25 = $<<4*1.25=5>>5
After buying the cookies, Josh has $8 - $5 = $<<8-5=3>>3
So the final answer is 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: The old team per round record is 1,148 because 287 x 4 = <<1148=1148>>1,148
The team season record is 11,480 because 10 x 1,248 = 11,480
They need 1,040 points in the final round to tie the record because 11,480 - 10,440 = <<11480-10440=1040>>1,040
They need to average 260 points each because 1,040 / 4 = <<1040/4=260>>260
This is 27 points less than the current record average because 287 - 260 = <<27=27>>27
So the final answer is 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: Max finished history in 20 * 2 = <<20*2=40>>40 minutes.
Finishing geography took the most time, which is 40 * 3 = <<40*3=120>>120 minutes.
In total, for all three subjects, Max needed 20 + 40 + 120 = <<20+40+120=180>>180 minutes.
So the final answer is 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: If Sophia ate 1/6 of the pie, then 6/6 - 1/6 = 5/6 is left in the fridge.
Let x be the pie's original weight.
The current weight of the pie can be described by 5x/6=1200 grams
So, 5x=7200.
And, the original weight is x = <<1440=1440>>1440 grams.
So, Sophia ate 1440 grams original - 1200 grams after= <<1440-1200=240>>240 grams of pie.
So the final answer is 240

Input: Sarah, Mary, and Tuan decided to go to the restaurant for a meal. They decided to split the cost of the meal evenly. If the total price of the meal comes to $67 and they have a coupon for $4, how much does each person need to contribute to the bill?
Output: After using the coupon, the final price comes to 67 - 4 = <<67-4=63>>63 dollars.
With three people, they each need to pay 63 / 3 = <<63/3=21>>21 dollars each.
So the final answer is 21

Input: Tom's brother is 4 times as old as Tom's dog. If in 6 years, Tom's brother will be 30 years, how old is Tom's dog going to be in six years?
Output: If in six years Tom's brother will be 30 years old, he is currently 30-6 = <<30-6=24>>24 years old.
Since Tom's brother is 4 times as old as Tom's dog, Tom's dog is 24/4 = <<24/4=6>>6 years old currently.
Tom's dog will be 6+6 = <<6+6=12>>12 years old in six years.
So the final answer is 12

Input: There are 50 children at the party. Three-fifths of them are boys. How many of the children are girls?
Output: 50 x 3/5 = <<50*3/5=30>>30 children are boys.
So, there are 50 - 30 = <<50-30=20>>20 children that are girls.
So the final answer is 20

Input: Gail has some bills in her wallet which amount to $100. She has four $5 bills and three $20 bills, and the rest are $10 bills. How many $10 bills are in her wallet?
Output: Four $5 bills amount to $5 x 4 = $<<5*4=20>>20.
Three $20 bills amount to $20 x 3 = $<<20*3=60>>60.
So Gail's $5 and $20 bills amount to $20 + $60 = $80.
Thus, her $10 bills amount to $100 - $80 = $20.
Therefore, she has $20/$10 = <<20/10=2>>2 $10 bills.
So the final answer is 2

Input: A 220-liter barrel has a small leak. It lost 10% of its contents before anyone noticed. How many liters are left in the barrel?
Output: The barrel lost 220 * 10 / 100 = <<220*10/100=22>>22 liters before anyone noticed.
So, 220 – 22 = <<220-22=198>>198 liters are left in the barrel.
So the final answer is 198

Input: Markese earned 5 fewer dollars than Evan. Together they earned $37. How many dollars did Markese earn? Use E to represent how many dollars Evan earned.
Output: E = Evan's earnings
Markese's earnings = E - 5
E + E - 5 = 37
2E = 42
E = <<21=21>>21
Evan made $21 so Markese earned 21 - 5 = <<21-5=16>>16
Markese earned $<<16=16>>16.
So the final answer is 16

Input: Lou Senior took 3 cookies out of the cookie jar and ate them.  Since he didn't get caught by his wife, he went back the next day and took another 3 cookies out of the jar.  But after eating just one of the cookies, he felt guilty about it and put the other two cookies back.  His son, Louie Junior saw that his Dad was eating cookies.  So, Louie Junior took seven cookies out of the jar and hid them in his bedroom for later.  The next morning, Debra, Lou's wife looked into the cookie jar and reacted by accusing her husband of eating half of the cookies out of the cookie jar.  How many cookies remained in the jar?
Output: Lou Sr 3 cookies from the jar, then another three, but then put two back, for a total of 3+3-2=<<3+3-2=4>>4 cookies removed.
Louie Jr took seven more out of the jar, for a total of 4+7=<<4+7=11>>11 cookies removed.
If Debra thought half were gone, then the number of missing cookies would equal the number of cookies that remain, or 11 missing=11 remaining cookies
So the final answer is 11

Input: John had $200. He gave 3/8 of his money to his mother and 3/10 to his father. How much money did John have left?
Output: John's mother received 3/8 x $200 = $<<3/8*200=75>>75.
His father got 3/10 x $200 = $<<3/10*200=60>>60.
Thus, John gave a total of $75 + $60 = $<<75+60=135>>135.
Therefore, John had $200 - $135 = $<<200-135=65>>65 left.
So the final answer is 65

Input: Tonya has $150.00 on her credit card.  If she leaves any balance on her card at the end of the month, she is charged 20% interest.  If she makes a $50.00 payment on her card, what will be the new balance?
Output: Her card has a $150.00 balance and she makes a $50.00 payment so the new balance is 150-50 = $<<150-50=100.00>>100.00
She didn't pay it off so she is charged 20% interest on her $100.00 balance so the interest is.20*100 = $20.00
Her balance was $100.00 and she was charged $20.00 in interest so her new balance is 100+20 = $<<100+20=120.00>>120.00
So the final answer is 120

Input: In her first term, Governor Sandoval gave twice as many commencement addresses as Governor Hawkins. Governor Sloan gave ten more commencement addresses than Governor Sandoval in the same amount of time. If Governor Sandoval gave 12 commencement addresses, how many commencement addresses did the three of them give altogether?
Output: Since Governor Sandoval gave 12 commencement addresses, twice the number that Governor Hawkins gave, Governor Hawkins gave 12/2 = <<12/2=6>>6 commencement addresses.
The two of them gave 6+12 = <<6+12=18>>18 commencement addresses in total.
Governor Sloan gave ten more commencement addresses than Governor Sandoval, a total of 12+10 = <<12+10=22>>22 commencement addresses.
The three of them gave 22+18 = <<22+18=40>>40 commencement addresses.
So the final answer is 40

Input: If Buzz bought a pizza with 78 slices at a restaurant and then decided to share it with the waiter in the ratio of 5:8, with Buzz's ratio being 5, what's twenty less the number of slices of pizza that the waiter ate?
Output: The total ratio representing the slices of pizza that Buzz bought is 5+8=<<5+8=13>>13
If he shared the slices of pizza with the waiter, the waiter received a fraction of 8/13 of the total number of slices, which totals 8/13 * 78 = <<8/13*78=48>>48 slices
Twenty less the number of slices of pizza that the waiter ate is 48-20 = <<48-20=28>>28
So the final answer is 28

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o18-tgsm8k-s1-rTrue-m4096
Evaluating commonsenseqa :   1%|          | 1/125 [01:15<2:36:54, 75.92s/it]Evaluating commonsenseqa :   2%|▏         | 2/125 [02:31<2:34:47, 75.51s/it]Evaluating commonsenseqa :   2%|▏         | 3/125 [03:44<2:31:49, 74.67s/it]Evaluating commonsenseqa :   3%|▎         | 4/125 [04:58<2:29:53, 74.33s/it]Evaluating commonsenseqa :   4%|▍         | 5/125 [06:12<2:28:21, 74.18s/it]Evaluating commonsenseqa :   5%|▍         | 6/125 [07:26<2:27:09, 74.20s/it]Evaluating commonsenseqa :   6%|▌         | 7/125 [08:40<2:25:49, 74.15s/it]Evaluating commonsenseqa :   6%|▋         | 8/125 [09:55<2:24:49, 74.27s/it]Evaluating commonsenseqa :   7%|▋         | 9/125 [11:09<2:23:24, 74.17s/it]Evaluating commonsenseqa :   8%|▊         | 10/125 [12:23<2:22:01, 74.10s/it]Evaluating commonsenseqa :   9%|▉         | 11/125 [13:37<2:20:42, 74.06s/it]Evaluating commonsenseqa :  10%|▉         | 12/125 [14:51<2:19:32, 74.09s/it]Evaluating commonsenseqa :  10%|█         | 13/125 [16:05<2:18:19, 74.10s/it]Evaluating commonsenseqa :  11%|█         | 14/125 [17:19<2:16:57, 74.03s/it]Evaluating commonsenseqa :  12%|█▏        | 15/125 [18:32<2:15:26, 73.88s/it]Evaluating commonsenseqa :  13%|█▎        | 16/125 [19:46<2:14:16, 73.91s/it]Evaluating commonsenseqa :  14%|█▎        | 17/125 [21:00<2:13:06, 73.95s/it]Evaluating commonsenseqa :  14%|█▍        | 18/125 [22:15<2:11:57, 73.99s/it]Evaluating commonsenseqa :  15%|█▌        | 19/125 [23:29<2:10:47, 74.04s/it]Evaluating commonsenseqa :  16%|█▌        | 20/125 [24:42<2:09:23, 73.94s/it]Evaluating commonsenseqa :  17%|█▋        | 21/125 [25:57<2:08:18, 74.03s/it]Evaluating commonsenseqa :  18%|█▊        | 22/125 [27:11<2:07:11, 74.09s/it]Evaluating commonsenseqa :  18%|█▊        | 23/125 [28:24<2:05:42, 73.95s/it]Evaluating commonsenseqa :  19%|█▉        | 24/125 [29:38<2:04:27, 73.94s/it]Evaluating commonsenseqa :  20%|██        | 25/125 [30:53<2:03:21, 74.01s/it]Evaluating commonsenseqa :  21%|██        | 26/125 [32:07<2:02:07, 74.01s/it]Evaluating commonsenseqa :  22%|██▏       | 27/125 [33:21<2:00:54, 74.03s/it]Evaluating commonsenseqa :  22%|██▏       | 28/125 [34:35<2:00:03, 74.26s/it]Evaluating commonsenseqa :  23%|██▎       | 29/125 [35:50<1:58:45, 74.23s/it]Evaluating commonsenseqa :  24%|██▍       | 30/125 [37:04<1:57:28, 74.19s/it]Evaluating commonsenseqa :  25%|██▍       | 31/125 [38:18<1:56:07, 74.12s/it]Evaluating commonsenseqa :  26%|██▌       | 32/125 [39:32<1:54:57, 74.16s/it]Evaluating commonsenseqa :  26%|██▋       | 33/125 [40:46<1:53:32, 74.05s/it]Evaluating commonsenseqa :  27%|██▋       | 34/125 [42:00<1:52:17, 74.03s/it]Evaluating commonsenseqa :  28%|██▊       | 35/125 [43:13<1:50:54, 73.94s/it]Evaluating commonsenseqa :  29%|██▉       | 36/125 [44:28<1:49:44, 73.98s/it]Evaluating commonsenseqa :  30%|██▉       | 37/125 [45:42<1:48:45, 74.16s/it]Evaluating commonsenseqa :  30%|███       | 38/125 [46:56<1:47:29, 74.13s/it]Evaluating commonsenseqa :  31%|███       | 39/125 [48:10<1:46:08, 74.05s/it]Evaluating commonsenseqa :  32%|███▏      | 40/125 [49:24<1:45:04, 74.17s/it]Evaluating commonsenseqa :  33%|███▎      | 41/125 [50:38<1:43:38, 74.03s/it]Evaluating commonsenseqa :  34%|███▎      | 42/125 [51:52<1:42:31, 74.12s/it]Evaluating commonsenseqa :  34%|███▍      | 43/125 [53:06<1:41:06, 73.98s/it]Evaluating commonsenseqa :  35%|███▌      | 44/125 [54:20<1:39:59, 74.06s/it]Evaluating commonsenseqa :  36%|███▌      | 45/125 [55:35<1:38:59, 74.25s/it]Evaluating commonsenseqa :  37%|███▋      | 46/125 [56:49<1:37:47, 74.27s/it]Evaluating commonsenseqa :  38%|███▊      | 47/125 [58:04<1:36:35, 74.30s/it]Evaluating commonsenseqa :  38%|███▊      | 48/125 [59:18<1:35:23, 74.33s/it]Evaluating commonsenseqa :  39%|███▉      | 49/125 [1:00:33<1:34:14, 74.40s/it]Evaluating commonsenseqa :  40%|████      | 50/125 [1:01:47<1:32:47, 74.24s/it]Evaluating commonsenseqa :  41%|████      | 51/125 [1:03:01<1:31:45, 74.40s/it]Evaluating commonsenseqa :  42%|████▏     | 52/125 [1:04:16<1:30:32, 74.42s/it]Evaluating commonsenseqa :  42%|████▏     | 53/125 [1:05:30<1:29:19, 74.44s/it]Evaluating commonsenseqa :  43%|████▎     | 54/125 [1:06:44<1:27:59, 74.37s/it]Evaluating commonsenseqa :  44%|████▍     | 55/125 [1:07:59<1:26:44, 74.35s/it]Evaluating commonsenseqa :  45%|████▍     | 56/125 [1:09:13<1:25:32, 74.38s/it]Evaluating commonsenseqa :  46%|████▌     | 57/125 [1:10:27<1:24:11, 74.28s/it]Evaluating commonsenseqa :  46%|████▋     | 58/125 [1:11:42<1:22:56, 74.27s/it]Evaluating commonsenseqa :  47%|████▋     | 59/125 [1:12:56<1:21:41, 74.26s/it]Evaluating commonsenseqa :  48%|████▊     | 60/125 [1:14:10<1:20:22, 74.20s/it]Evaluating commonsenseqa :  49%|████▉     | 61/125 [1:15:24<1:19:05, 74.14s/it]Evaluating commonsenseqa :  50%|████▉     | 62/125 [1:16:38<1:17:55, 74.22s/it]Evaluating commonsenseqa :  50%|█████     | 63/125 [1:17:52<1:16:35, 74.13s/it]Evaluating commonsenseqa :  51%|█████     | 64/125 [1:19:06<1:15:23, 74.16s/it]Evaluating commonsenseqa :  52%|█████▏    | 65/125 [1:20:21<1:14:10, 74.18s/it]Evaluating commonsenseqa :  53%|█████▎    | 66/125 [1:21:35<1:12:55, 74.17s/it]Evaluating commonsenseqa :  54%|█████▎    | 67/125 [1:22:50<1:11:55, 74.40s/it]Evaluating commonsenseqa :  54%|█████▍    | 68/125 [1:24:03<1:10:26, 74.16s/it]Evaluating commonsenseqa :  55%|█████▌    | 69/125 [1:25:18<1:09:14, 74.18s/it]Evaluating commonsenseqa :  56%|█████▌    | 70/125 [1:26:32<1:08:01, 74.21s/it]Evaluating commonsenseqa :  57%|█████▋    | 71/125 [1:27:45<1:06:37, 74.02s/it]Evaluating commonsenseqa :  58%|█████▊    | 72/125 [1:28:59<1:05:23, 74.03s/it]Evaluating commonsenseqa :  58%|█████▊    | 73/125 [1:30:14<1:04:12, 74.08s/it]Evaluating commonsenseqa :  59%|█████▉    | 74/125 [1:31:28<1:02:56, 74.05s/it]Evaluating commonsenseqa :  60%|██████    | 75/125 [1:32:42<1:01:44, 74.09s/it]Evaluating commonsenseqa :  61%|██████    | 76/125 [1:33:55<1:00:21, 73.91s/it]Evaluating commonsenseqa :  62%|██████▏   | 77/125 [1:35:09<59:11, 73.99s/it]  Evaluating commonsenseqa :  62%|██████▏   | 78/125 [1:36:23<57:52, 73.89s/it]Evaluating commonsenseqa :  63%|██████▎   | 79/125 [1:37:37<56:37, 73.86s/it]Evaluating commonsenseqa :  64%|██████▍   | 80/125 [1:38:51<55:23, 73.85s/it]Evaluating commonsenseqa :  65%|██████▍   | 81/125 [1:40:05<54:17, 74.04s/it]Evaluating commonsenseqa :  66%|██████▌   | 82/125 [1:41:19<53:03, 74.05s/it]Evaluating commonsenseqa :  66%|██████▋   | 83/125 [1:42:34<51:55, 74.18s/it]Evaluating commonsenseqa :  67%|██████▋   | 84/125 [1:43:48<50:36, 74.06s/it]Evaluating commonsenseqa :  68%|██████▊   | 85/125 [1:45:01<49:14, 73.86s/it]Evaluating commonsenseqa :  69%|██████▉   | 86/125 [1:46:15<47:59, 73.85s/it]Evaluating commonsenseqa :  70%|██████▉   | 87/125 [1:47:28<46:43, 73.78s/it]Evaluating commonsenseqa :  70%|███████   | 88/125 [1:48:42<45:28, 73.74s/it]Evaluating commonsenseqa :  71%|███████   | 89/125 [1:49:56<44:12, 73.67s/it]Evaluating commonsenseqa :  72%|███████▏  | 90/125 [1:51:09<42:54, 73.55s/it]Evaluating commonsenseqa :  73%|███████▎  | 91/125 [1:52:23<41:46, 73.72s/it]Evaluating commonsenseqa :  74%|███████▎  | 92/125 [1:53:37<40:32, 73.73s/it]Evaluating commonsenseqa :  74%|███████▍  | 93/125 [1:54:50<39:17, 73.67s/it]Evaluating commonsenseqa :  75%|███████▌  | 94/125 [1:56:04<38:01, 73.60s/it]Evaluating commonsenseqa :  76%|███████▌  | 95/125 [1:57:17<36:48, 73.63s/it]Evaluating commonsenseqa :  77%|███████▋  | 96/125 [1:58:31<35:37, 73.71s/it]Evaluating commonsenseqa :  78%|███████▊  | 97/125 [1:59:45<34:26, 73.81s/it]Evaluating commonsenseqa :  78%|███████▊  | 98/125 [2:00:59<33:08, 73.65s/it]Evaluating commonsenseqa :  79%|███████▉  | 99/125 [2:02:13<31:59, 73.81s/it]Evaluating commonsenseqa :  80%|████████  | 100/125 [2:03:27<30:46, 73.88s/it]Evaluating commonsenseqa :  81%|████████  | 101/125 [2:04:41<29:36, 74.02s/it]Evaluating commonsenseqa :  82%|████████▏ | 102/125 [2:05:55<28:22, 74.00s/it]Evaluating commonsenseqa :  82%|████████▏ | 103/125 [2:07:09<27:05, 73.87s/it]Evaluating commonsenseqa :  83%|████████▎ | 104/125 [2:08:22<25:49, 73.76s/it]Evaluating commonsenseqa :  84%|████████▍ | 105/125 [2:09:37<24:40, 74.00s/it]Evaluating commonsenseqa :  85%|████████▍ | 106/125 [2:10:50<23:23, 73.87s/it]Evaluating commonsenseqa :  86%|████████▌ | 107/125 [2:12:04<22:09, 73.87s/it]Evaluating commonsenseqa :  86%|████████▋ | 108/125 [2:13:18<20:55, 73.87s/it]Evaluating commonsenseqa :  87%|████████▋ | 109/125 [2:14:32<19:41, 73.86s/it]Evaluating commonsenseqa :  88%|████████▊ | 110/125 [2:15:46<18:28, 73.91s/it]Evaluating commonsenseqa :  89%|████████▉ | 111/125 [2:16:59<17:12, 73.74s/it]Evaluating commonsenseqa :  90%|████████▉ | 112/125 [2:18:13<15:57, 73.66s/it]Evaluating commonsenseqa :  90%|█████████ | 113/125 [2:19:26<14:43, 73.65s/it]Evaluating commonsenseqa :  91%|█████████ | 114/125 [2:20:40<13:29, 73.61s/it]Evaluating commonsenseqa :  92%|█████████▏| 115/125 [2:21:53<12:15, 73.56s/it]Evaluating commonsenseqa :  93%|█████████▎| 116/125 [2:23:07<11:02, 73.60s/it]Evaluating commonsenseqa :  94%|█████████▎| 117/125 [2:24:21<09:49, 73.70s/it]Evaluating commonsenseqa :  94%|█████████▍| 118/125 [2:25:34<08:35, 73.65s/it]Evaluating commonsenseqa :  95%|█████████▌| 119/125 [2:26:48<07:21, 73.66s/it]Evaluating commonsenseqa :  96%|█████████▌| 120/125 [2:28:02<06:07, 73.59s/it]Evaluating commonsenseqa :  97%|█████████▋| 121/125 [2:29:16<04:54, 73.72s/it]Evaluating commonsenseqa :  98%|█████████▊| 122/125 [2:30:29<03:40, 73.63s/it]Evaluating commonsenseqa :  98%|█████████▊| 123/125 [2:31:43<02:27, 73.58s/it]Evaluating commonsenseqa :  99%|█████████▉| 124/125 [2:32:56<01:13, 73.63s/it]Evaluating commonsenseqa : 100%|██████████| 125/125 [2:34:10<00:00, 73.73s/it]Evaluating commonsenseqa : 100%|██████████| 125/125 [2:34:10<00:00, 74.01s/it]
name: commonsenseqa | avg. gen lenth: 393.399 | time: 9253.844531297684s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 18963 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 8 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o19-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 19 16 17 18 19 20 True 4096 8
[2023-09-06 17:38:47,324] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o19-tgsm8k-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 19
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o19-tgsm8k-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 8
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 256968.55it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.63s/it]
 > number of parameters: 6738415616
[2023-09-06 17:39:00,698] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-06 17:39:00,906] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-06 17:39:00,908] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-06 17:39:00,908] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-06 17:39:00,908] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-06 17:39:00,908] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-06 17:39:00,908] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-06 17:39:00,908] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-06 17:39:00,908] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-06 17:39:00,908] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-06 17:39:00,908] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-06 17:39:00,908] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-06 17:39:00,908] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba358310>
[2023-09-06 17:39:00,908] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-06 17:39:00,909] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-06 17:39:00,910] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-06 17:39:00,910] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-06 17:39:00,910] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-06 17:39:00,910] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-06 17:39:00,910] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-06 17:39:00,910] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-06 17:39:00,910] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-06 17:39:00,910] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-06 17:39:00,910] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-06 17:39:00,910] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-06 17:39:00,910] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-06 17:39:00,910] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-06 17:39:00,910] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-06 17:39:00,910] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-06 17:39:00,910] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-06 17:39:00,910] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/125 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: Mary added a total of 7 stickers/page * 6 pages= <<7*6=42>>42 stickers to the 6 other pages.
In total, Mary added 3 large stickers + 42 stickers = <<3+42=45>>45 stickers to her journal.
Since she started with 89 stickers, she now has 89 - 45 = <<89-45=44>>44 stickers left.
So the final answer is 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: If he babysits for 2 hours at $7 per hour, he will earn 2*7 = $<<2*7=14>>14
This week he will earn $5 allowance, $10 mowing the lawn and $14 from babysitting for a total of 5+10+14 = $<<5+10+14=29>>29
If we add the $29 he will earn to his $65 savings, he will have a total of 29 + 65 = $<<29+65=94>>94
The bike costs $100 and he will have $94 leaving $100-$94 = $<<100-94=6>>6 more that he will need to earn
So the final answer is 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: His kangaroos have a total of 23*2=<<23*2=46>>46 legs
He has 23*3=<<23*3=69>>69 goats
The goats have 69*4=<<69*4=276>>276 legs
So in total his animals have 276+46=<<276+46=322>>322 legs
So the final answer is 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: After buying a hat, Josh has $20 - $10 = $<<20-10=10>>10
After buying a pencil, Josh has $10 - $2 = $<<10-2=8>>8
The total cost of cookies is 4 * $1.25 = $<<4*1.25=5>>5
After buying the cookies, Josh has $8 - $5 = $<<8-5=3>>3
So the final answer is 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: The old team per round record is 1,148 because 287 x 4 = <<1148=1148>>1,148
The team season record is 11,480 because 10 x 1,248 = 11,480
They need 1,040 points in the final round to tie the record because 11,480 - 10,440 = <<11480-10440=1040>>1,040
They need to average 260 points each because 1,040 / 4 = <<1040/4=260>>260
This is 27 points less than the current record average because 287 - 260 = <<27=27>>27
So the final answer is 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: Max finished history in 20 * 2 = <<20*2=40>>40 minutes.
Finishing geography took the most time, which is 40 * 3 = <<40*3=120>>120 minutes.
In total, for all three subjects, Max needed 20 + 40 + 120 = <<20+40+120=180>>180 minutes.
So the final answer is 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: If Sophia ate 1/6 of the pie, then 6/6 - 1/6 = 5/6 is left in the fridge.
Let x be the pie's original weight.
The current weight of the pie can be described by 5x/6=1200 grams
So, 5x=7200.
And, the original weight is x = <<1440=1440>>1440 grams.
So, Sophia ate 1440 grams original - 1200 grams after= <<1440-1200=240>>240 grams of pie.
So the final answer is 240

Input: Sarah, Mary, and Tuan decided to go to the restaurant for a meal. They decided to split the cost of the meal evenly. If the total price of the meal comes to $67 and they have a coupon for $4, how much does each person need to contribute to the bill?
Output: After using the coupon, the final price comes to 67 - 4 = <<67-4=63>>63 dollars.
With three people, they each need to pay 63 / 3 = <<63/3=21>>21 dollars each.
So the final answer is 21

Input: Tom's brother is 4 times as old as Tom's dog. If in 6 years, Tom's brother will be 30 years, how old is Tom's dog going to be in six years?
Output: If in six years Tom's brother will be 30 years old, he is currently 30-6 = <<30-6=24>>24 years old.
Since Tom's brother is 4 times as old as Tom's dog, Tom's dog is 24/4 = <<24/4=6>>6 years old currently.
Tom's dog will be 6+6 = <<6+6=12>>12 years old in six years.
So the final answer is 12

Input: There are 50 children at the party. Three-fifths of them are boys. How many of the children are girls?
Output: 50 x 3/5 = <<50*3/5=30>>30 children are boys.
So, there are 50 - 30 = <<50-30=20>>20 children that are girls.
So the final answer is 20

Input: Gail has some bills in her wallet which amount to $100. She has four $5 bills and three $20 bills, and the rest are $10 bills. How many $10 bills are in her wallet?
Output: Four $5 bills amount to $5 x 4 = $<<5*4=20>>20.
Three $20 bills amount to $20 x 3 = $<<20*3=60>>60.
So Gail's $5 and $20 bills amount to $20 + $60 = $80.
Thus, her $10 bills amount to $100 - $80 = $20.
Therefore, she has $20/$10 = <<20/10=2>>2 $10 bills.
So the final answer is 2

Input: A 220-liter barrel has a small leak. It lost 10% of its contents before anyone noticed. How many liters are left in the barrel?
Output: The barrel lost 220 * 10 / 100 = <<220*10/100=22>>22 liters before anyone noticed.
So, 220 – 22 = <<220-22=198>>198 liters are left in the barrel.
So the final answer is 198

Input: Markese earned 5 fewer dollars than Evan. Together they earned $37. How many dollars did Markese earn? Use E to represent how many dollars Evan earned.
Output: E = Evan's earnings
Markese's earnings = E - 5
E + E - 5 = 37
2E = 42
E = <<21=21>>21
Evan made $21 so Markese earned 21 - 5 = <<21-5=16>>16
Markese earned $<<16=16>>16.
So the final answer is 16

Input: Lou Senior took 3 cookies out of the cookie jar and ate them.  Since he didn't get caught by his wife, he went back the next day and took another 3 cookies out of the jar.  But after eating just one of the cookies, he felt guilty about it and put the other two cookies back.  His son, Louie Junior saw that his Dad was eating cookies.  So, Louie Junior took seven cookies out of the jar and hid them in his bedroom for later.  The next morning, Debra, Lou's wife looked into the cookie jar and reacted by accusing her husband of eating half of the cookies out of the cookie jar.  How many cookies remained in the jar?
Output: Lou Sr 3 cookies from the jar, then another three, but then put two back, for a total of 3+3-2=<<3+3-2=4>>4 cookies removed.
Louie Jr took seven more out of the jar, for a total of 4+7=<<4+7=11>>11 cookies removed.
If Debra thought half were gone, then the number of missing cookies would equal the number of cookies that remain, or 11 missing=11 remaining cookies
So the final answer is 11

Input: John had $200. He gave 3/8 of his money to his mother and 3/10 to his father. How much money did John have left?
Output: John's mother received 3/8 x $200 = $<<3/8*200=75>>75.
His father got 3/10 x $200 = $<<3/10*200=60>>60.
Thus, John gave a total of $75 + $60 = $<<75+60=135>>135.
Therefore, John had $200 - $135 = $<<200-135=65>>65 left.
So the final answer is 65

Input: Tonya has $150.00 on her credit card.  If she leaves any balance on her card at the end of the month, she is charged 20% interest.  If she makes a $50.00 payment on her card, what will be the new balance?
Output: Her card has a $150.00 balance and she makes a $50.00 payment so the new balance is 150-50 = $<<150-50=100.00>>100.00
She didn't pay it off so she is charged 20% interest on her $100.00 balance so the interest is.20*100 = $20.00
Her balance was $100.00 and she was charged $20.00 in interest so her new balance is 100+20 = $<<100+20=120.00>>120.00
So the final answer is 120

Input: In her first term, Governor Sandoval gave twice as many commencement addresses as Governor Hawkins. Governor Sloan gave ten more commencement addresses than Governor Sandoval in the same amount of time. If Governor Sandoval gave 12 commencement addresses, how many commencement addresses did the three of them give altogether?
Output: Since Governor Sandoval gave 12 commencement addresses, twice the number that Governor Hawkins gave, Governor Hawkins gave 12/2 = <<12/2=6>>6 commencement addresses.
The two of them gave 6+12 = <<6+12=18>>18 commencement addresses in total.
Governor Sloan gave ten more commencement addresses than Governor Sandoval, a total of 12+10 = <<12+10=22>>22 commencement addresses.
The three of them gave 22+18 = <<22+18=40>>40 commencement addresses.
So the final answer is 40

Input: If Buzz bought a pizza with 78 slices at a restaurant and then decided to share it with the waiter in the ratio of 5:8, with Buzz's ratio being 5, what's twenty less the number of slices of pizza that the waiter ate?
Output: The total ratio representing the slices of pizza that Buzz bought is 5+8=<<5+8=13>>13
If he shared the slices of pizza with the waiter, the waiter received a fraction of 8/13 of the total number of slices, which totals 8/13 * 78 = <<8/13*78=48>>48 slices
Twenty less the number of slices of pizza that the waiter ate is 48-20 = <<48-20=28>>28
So the final answer is 28

Input: Jolene and Phil have four children, each with the same birthday.  They gave birth to their first child exactly 15 years ago.  They gave birth to their second child exactly one year after the birth of their first child.  They gave birth to their third child on the fourth birthday of their second child. Two years after the birth of their third child, they gave birth to their fourth child.  How old, in years, is their fourth child?
Output: Their 1st child was born 15 years ago, and therefore is <<15=15>>15 years old.
Their 2nd child was born 1 year after their 15-year-old child, and therefore is 15-1=<<15-1=14>>14 years old.
Their 3rd child was born 4 years after their 14-year-old child, and therefore is 14-4=10 years old.
Their 4th child was born 2 years after their 10-year-old child, and therefore is 10-2=8 years old.
So the final answer is 8

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o19-tgsm8k-s1-rTrue-m4096
Evaluating commonsenseqa :   1%|          | 1/125 [01:14<2:34:45, 74.89s/it]Evaluating commonsenseqa :   1%|          | 1/125 [01:21<2:47:29, 81.04s/it]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 97, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 94, in main
    inference_main(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 90, in inference_main
    query_ids, response_ids, answers, indices = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 65, in run_model
    gen_out = model.generate(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 1454, in generate
    return self.sample(
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 2568, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3516304) of binary: /home/ylu130/.conda/envs/ood/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.conda/envs/ood/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-06_17:40:25
  host      : icgpu06.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3516304)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 18963 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 8 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o20-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 20 16 17 18 19 20 True 4096 8
[2023-09-06 17:40:29,491] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o20-tgsm8k-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 20
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o20-tgsm8k-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 8
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 246243.46it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]
 > number of parameters: 6738415616
[2023-09-06 17:40:38,695] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-06 17:40:38,903] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-06 17:40:38,905] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554b5f46490>
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-06 17:40:38,906] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-06 17:40:38,907] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-06 17:40:38,908] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-06 17:40:38,908] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-06 17:40:38,908] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-06 17:40:38,908] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/125 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: Mary added a total of 7 stickers/page * 6 pages= <<7*6=42>>42 stickers to the 6 other pages.
In total, Mary added 3 large stickers + 42 stickers = <<3+42=45>>45 stickers to her journal.
Since she started with 89 stickers, she now has 89 - 45 = <<89-45=44>>44 stickers left.
So the final answer is 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: If he babysits for 2 hours at $7 per hour, he will earn 2*7 = $<<2*7=14>>14
This week he will earn $5 allowance, $10 mowing the lawn and $14 from babysitting for a total of 5+10+14 = $<<5+10+14=29>>29
If we add the $29 he will earn to his $65 savings, he will have a total of 29 + 65 = $<<29+65=94>>94
The bike costs $100 and he will have $94 leaving $100-$94 = $<<100-94=6>>6 more that he will need to earn
So the final answer is 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: His kangaroos have a total of 23*2=<<23*2=46>>46 legs
He has 23*3=<<23*3=69>>69 goats
The goats have 69*4=<<69*4=276>>276 legs
So in total his animals have 276+46=<<276+46=322>>322 legs
So the final answer is 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: After buying a hat, Josh has $20 - $10 = $<<20-10=10>>10
After buying a pencil, Josh has $10 - $2 = $<<10-2=8>>8
The total cost of cookies is 4 * $1.25 = $<<4*1.25=5>>5
After buying the cookies, Josh has $8 - $5 = $<<8-5=3>>3
So the final answer is 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: The old team per round record is 1,148 because 287 x 4 = <<1148=1148>>1,148
The team season record is 11,480 because 10 x 1,248 = 11,480
They need 1,040 points in the final round to tie the record because 11,480 - 10,440 = <<11480-10440=1040>>1,040
They need to average 260 points each because 1,040 / 4 = <<1040/4=260>>260
This is 27 points less than the current record average because 287 - 260 = <<27=27>>27
So the final answer is 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: Max finished history in 20 * 2 = <<20*2=40>>40 minutes.
Finishing geography took the most time, which is 40 * 3 = <<40*3=120>>120 minutes.
In total, for all three subjects, Max needed 20 + 40 + 120 = <<20+40+120=180>>180 minutes.
So the final answer is 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: If Sophia ate 1/6 of the pie, then 6/6 - 1/6 = 5/6 is left in the fridge.
Let x be the pie's original weight.
The current weight of the pie can be described by 5x/6=1200 grams
So, 5x=7200.
And, the original weight is x = <<1440=1440>>1440 grams.
So, Sophia ate 1440 grams original - 1200 grams after= <<1440-1200=240>>240 grams of pie.
So the final answer is 240

Input: Sarah, Mary, and Tuan decided to go to the restaurant for a meal. They decided to split the cost of the meal evenly. If the total price of the meal comes to $67 and they have a coupon for $4, how much does each person need to contribute to the bill?
Output: After using the coupon, the final price comes to 67 - 4 = <<67-4=63>>63 dollars.
With three people, they each need to pay 63 / 3 = <<63/3=21>>21 dollars each.
So the final answer is 21

Input: Tom's brother is 4 times as old as Tom's dog. If in 6 years, Tom's brother will be 30 years, how old is Tom's dog going to be in six years?
Output: If in six years Tom's brother will be 30 years old, he is currently 30-6 = <<30-6=24>>24 years old.
Since Tom's brother is 4 times as old as Tom's dog, Tom's dog is 24/4 = <<24/4=6>>6 years old currently.
Tom's dog will be 6+6 = <<6+6=12>>12 years old in six years.
So the final answer is 12

Input: There are 50 children at the party. Three-fifths of them are boys. How many of the children are girls?
Output: 50 x 3/5 = <<50*3/5=30>>30 children are boys.
So, there are 50 - 30 = <<50-30=20>>20 children that are girls.
So the final answer is 20

Input: Gail has some bills in her wallet which amount to $100. She has four $5 bills and three $20 bills, and the rest are $10 bills. How many $10 bills are in her wallet?
Output: Four $5 bills amount to $5 x 4 = $<<5*4=20>>20.
Three $20 bills amount to $20 x 3 = $<<20*3=60>>60.
So Gail's $5 and $20 bills amount to $20 + $60 = $80.
Thus, her $10 bills amount to $100 - $80 = $20.
Therefore, she has $20/$10 = <<20/10=2>>2 $10 bills.
So the final answer is 2

Input: A 220-liter barrel has a small leak. It lost 10% of its contents before anyone noticed. How many liters are left in the barrel?
Output: The barrel lost 220 * 10 / 100 = <<220*10/100=22>>22 liters before anyone noticed.
So, 220 – 22 = <<220-22=198>>198 liters are left in the barrel.
So the final answer is 198

Input: Markese earned 5 fewer dollars than Evan. Together they earned $37. How many dollars did Markese earn? Use E to represent how many dollars Evan earned.
Output: E = Evan's earnings
Markese's earnings = E - 5
E + E - 5 = 37
2E = 42
E = <<21=21>>21
Evan made $21 so Markese earned 21 - 5 = <<21-5=16>>16
Markese earned $<<16=16>>16.
So the final answer is 16

Input: Lou Senior took 3 cookies out of the cookie jar and ate them.  Since he didn't get caught by his wife, he went back the next day and took another 3 cookies out of the jar.  But after eating just one of the cookies, he felt guilty about it and put the other two cookies back.  His son, Louie Junior saw that his Dad was eating cookies.  So, Louie Junior took seven cookies out of the jar and hid them in his bedroom for later.  The next morning, Debra, Lou's wife looked into the cookie jar and reacted by accusing her husband of eating half of the cookies out of the cookie jar.  How many cookies remained in the jar?
Output: Lou Sr 3 cookies from the jar, then another three, but then put two back, for a total of 3+3-2=<<3+3-2=4>>4 cookies removed.
Louie Jr took seven more out of the jar, for a total of 4+7=<<4+7=11>>11 cookies removed.
If Debra thought half were gone, then the number of missing cookies would equal the number of cookies that remain, or 11 missing=11 remaining cookies
So the final answer is 11

Input: John had $200. He gave 3/8 of his money to his mother and 3/10 to his father. How much money did John have left?
Output: John's mother received 3/8 x $200 = $<<3/8*200=75>>75.
His father got 3/10 x $200 = $<<3/10*200=60>>60.
Thus, John gave a total of $75 + $60 = $<<75+60=135>>135.
Therefore, John had $200 - $135 = $<<200-135=65>>65 left.
So the final answer is 65

Input: Tonya has $150.00 on her credit card.  If she leaves any balance on her card at the end of the month, she is charged 20% interest.  If she makes a $50.00 payment on her card, what will be the new balance?
Output: Her card has a $150.00 balance and she makes a $50.00 payment so the new balance is 150-50 = $<<150-50=100.00>>100.00
She didn't pay it off so she is charged 20% interest on her $100.00 balance so the interest is.20*100 = $20.00
Her balance was $100.00 and she was charged $20.00 in interest so her new balance is 100+20 = $<<100+20=120.00>>120.00
So the final answer is 120

Input: In her first term, Governor Sandoval gave twice as many commencement addresses as Governor Hawkins. Governor Sloan gave ten more commencement addresses than Governor Sandoval in the same amount of time. If Governor Sandoval gave 12 commencement addresses, how many commencement addresses did the three of them give altogether?
Output: Since Governor Sandoval gave 12 commencement addresses, twice the number that Governor Hawkins gave, Governor Hawkins gave 12/2 = <<12/2=6>>6 commencement addresses.
The two of them gave 6+12 = <<6+12=18>>18 commencement addresses in total.
Governor Sloan gave ten more commencement addresses than Governor Sandoval, a total of 12+10 = <<12+10=22>>22 commencement addresses.
The three of them gave 22+18 = <<22+18=40>>40 commencement addresses.
So the final answer is 40

Input: If Buzz bought a pizza with 78 slices at a restaurant and then decided to share it with the waiter in the ratio of 5:8, with Buzz's ratio being 5, what's twenty less the number of slices of pizza that the waiter ate?
Output: The total ratio representing the slices of pizza that Buzz bought is 5+8=<<5+8=13>>13
If he shared the slices of pizza with the waiter, the waiter received a fraction of 8/13 of the total number of slices, which totals 8/13 * 78 = <<8/13*78=48>>48 slices
Twenty less the number of slices of pizza that the waiter ate is 48-20 = <<48-20=28>>28
So the final answer is 28

Input: Jolene and Phil have four children, each with the same birthday.  They gave birth to their first child exactly 15 years ago.  They gave birth to their second child exactly one year after the birth of their first child.  They gave birth to their third child on the fourth birthday of their second child. Two years after the birth of their third child, they gave birth to their fourth child.  How old, in years, is their fourth child?
Output: Their 1st child was born 15 years ago, and therefore is <<15=15>>15 years old.
Their 2nd child was born 1 year after their 15-year-old child, and therefore is 15-1=<<15-1=14>>14 years old.
Their 3rd child was born 4 years after their 14-year-old child, and therefore is 14-4=10 years old.
Their 4th child was born 2 years after their 10-year-old child, and therefore is 10-2=8 years old.
So the final answer is 8

Input: Each purple book has 230 pages. Each orange book contains 510 pages. Mirella read 5 purple books and 4 orange books. How many more orange pages did she read than purple pages?
Output: Purple = 5 * 230 = <<5*230=1150>>1150 pages
Orange = 4 * 510 = <<4*510=2040>>2040 pages
2040 - 1150 = <<2040-1150=890>>890
Mirella read 890 more orange pages than purple pages.
So the final answer is 890
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o20-tgsm8k-s1-rTrue-m4096
Evaluating commonsenseqa :   1%|          | 1/125 [01:14<2:33:55, 74.48s/it]Evaluating commonsenseqa :   2%|▏         | 2/125 [02:27<2:31:25, 73.87s/it]Evaluating commonsenseqa :   2%|▏         | 3/125 [03:40<2:29:10, 73.36s/it]Evaluating commonsenseqa :   3%|▎         | 4/125 [04:53<2:27:32, 73.16s/it]Evaluating commonsenseqa :   4%|▍         | 5/125 [06:06<2:26:03, 73.03s/it]Evaluating commonsenseqa :   5%|▍         | 6/125 [07:19<2:24:39, 72.94s/it]Evaluating commonsenseqa :   6%|▌         | 7/125 [08:32<2:23:28, 72.95s/it]Evaluating commonsenseqa :   6%|▋         | 8/125 [09:44<2:22:14, 72.95s/it]Evaluating commonsenseqa :   7%|▋         | 9/125 [10:57<2:20:59, 72.92s/it]Evaluating commonsenseqa :   8%|▊         | 10/125 [12:10<2:19:43, 72.90s/it]Evaluating commonsenseqa :   9%|▉         | 11/125 [13:23<2:18:40, 72.99s/it]Evaluating commonsenseqa :  10%|▉         | 12/125 [14:36<2:17:22, 72.95s/it]Evaluating commonsenseqa :  10%|█         | 13/125 [15:49<2:16:05, 72.91s/it]Evaluating commonsenseqa :  11%|█         | 14/125 [17:02<2:14:54, 72.93s/it]Evaluating commonsenseqa :  12%|█▏        | 15/125 [18:15<2:13:39, 72.91s/it]slurmstepd: error: *** JOB 998963 ON icgpu06 CANCELLED AT 2023-09-06T17:59:20 ***
