WORLD_SIZE=1
MASTER_ADDR=icgpu02
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu02 --master_port 19674 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o11-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 11 11 12 13 14 15 True 4096 10
[2023-09-06 18:02:52,893] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu02 --master_port 19674 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o12-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 12 11 12 13 14 15 True 4096 10
[2023-09-06 18:02:59,418] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu02 --master_port 19674 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 13 11 12 13 14 15 True 4096 10
[2023-09-06 18:03:06,385] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu02 --master_port 19674 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o14-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 14 11 12 13 14 15 True 4096 10
[2023-09-06 18:03:00,899] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-06 18:03:00,899] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-06 18:03:00,899] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-06 18:03:00,899] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-06 18:03:00,899] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-06 18:03:00,899] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-06 18:03:00,899] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-06 18:03:00,899] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-06 18:03:00,899] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0280>
[2023-09-06 18:03:00,899] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-06 18:03:00,899] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-06 18:03:00,900] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-06 18:03:00,901] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-06 18:03:00,901] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-06 18:03:00,901] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-06 18:03:00,901] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-06 18:03:00,901] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-06 18:03:00,901] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-06 18:03:00,901] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-06 18:03:00,901] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-06 18:03:00,901] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-06 18:03:00,901] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-06 18:03:00,901] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-06 18:03:00,901] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-06 18:03:00,901] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: Mary added a total of 7 stickers/page * 6 pages= <<7*6=42>>42 stickers to the 6 other pages.
In total, Mary added 3 large stickers + 42 stickers = <<3+42=45>>45 stickers to her journal.
Since she started with 89 stickers, she now has 89 - 45 = <<89-45=44>>44 stickers left.
So the final answer is 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: If he babysits for 2 hours at $7 per hour, he will earn 2*7 = $<<2*7=14>>14
This week he will earn $5 allowance, $10 mowing the lawn and $14 from babysitting for a total of 5+10+14 = $<<5+10+14=29>>29
If we add the $29 he will earn to his $65 savings, he will have a total of 29 + 65 = $<<29+65=94>>94
The bike costs $100 and he will have $94 leaving $100-$94 = $<<100-94=6>>6 more that he will need to earn
So the final answer is 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: His kangaroos have a total of 23*2=<<23*2=46>>46 legs
He has 23*3=<<23*3=69>>69 goats
The goats have 69*4=<<69*4=276>>276 legs
So in total his animals have 276+46=<<276+46=322>>322 legs
So the final answer is 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: After buying a hat, Josh has $20 - $10 = $<<20-10=10>>10
After buying a pencil, Josh has $10 - $2 = $<<10-2=8>>8
The total cost of cookies is 4 * $1.25 = $<<4*1.25=5>>5
After buying the cookies, Josh has $8 - $5 = $<<8-5=3>>3
So the final answer is 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: The old team per round record is 1,148 because 287 x 4 = <<1148=1148>>1,148
The team season record is 11,480 because 10 x 1,248 = 11,480
They need 1,040 points in the final round to tie the record because 11,480 - 10,440 = <<11480-10440=1040>>1,040
They need to average 260 points each because 1,040 / 4 = <<1040/4=260>>260
This is 27 points less than the current record average because 287 - 260 = <<27=27>>27
So the final answer is 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: Max finished history in 20 * 2 = <<20*2=40>>40 minutes.
Finishing geography took the most time, which is 40 * 3 = <<40*3=120>>120 minutes.
In total, for all three subjects, Max needed 20 + 40 + 120 = <<20+40+120=180>>180 minutes.
So the final answer is 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: If Sophia ate 1/6 of the pie, then 6/6 - 1/6 = 5/6 is left in the fridge.
Let x be the pie's original weight.
The current weight of the pie can be described by 5x/6=1200 grams
So, 5x=7200.
And, the original weight is x = <<1440=1440>>1440 grams.
So, Sophia ate 1440 grams original - 1200 grams after= <<1440-1200=240>>240 grams of pie.
So the final answer is 240

Input: Sarah, Mary, and Tuan decided to go to the restaurant for a meal. They decided to split the cost of the meal evenly. If the total price of the meal comes to $67 and they have a coupon for $4, how much does each person need to contribute to the bill?
Output: After using the coupon, the final price comes to 67 - 4 = <<67-4=63>>63 dollars.
With three people, they each need to pay 63 / 3 = <<63/3=21>>21 dollars each.
So the final answer is 21

Input: Tom's brother is 4 times as old as Tom's dog. If in 6 years, Tom's brother will be 30 years, how old is Tom's dog going to be in six years?
Output: If in six years Tom's brother will be 30 years old, he is currently 30-6 = <<30-6=24>>24 years old.
Since Tom's brother is 4 times as old as Tom's dog, Tom's dog is 24/4 = <<24/4=6>>6 years old currently.
Tom's dog will be 6+6 = <<6+6=12>>12 years old in six years.
So the final answer is 12

Input: There are 50 children at the party. Three-fifths of them are boys. How many of the children are girls?
Output: 50 x 3/5 = <<50*3/5=30>>30 children are boys.
So, there are 50 - 30 = <<50-30=20>>20 children that are girls.
So the final answer is 20

Input: Gail has some bills in her wallet which amount to $100. She has four $5 bills and three $20 bills, and the rest are $10 bills. How many $10 bills are in her wallet?
Output: Four $5 bills amount to $5 x 4 = $<<5*4=20>>20.
Three $20 bills amount to $20 x 3 = $<<20*3=60>>60.
So Gail's $5 and $20 bills amount to $20 + $60 = $80.
Thus, her $10 bills amount to $100 - $80 = $20.
Therefore, she has $20/$10 = <<20/10=2>>2 $10 bills.
So the final answer is 2

Input: A 220-liter barrel has a small leak. It lost 10% of its contents before anyone noticed. How many liters are left in the barrel?
Output: The barrel lost 220 * 10 / 100 = <<220*10/100=22>>22 liters before anyone noticed.
So, 220 – 22 = <<220-22=198>>198 liters are left in the barrel.
So the final answer is 198

Input: Markese earned 5 fewer dollars than Evan. Together they earned $37. How many dollars did Markese earn? Use E to represent how many dollars Evan earned.
Output: E = Evan's earnings
Markese's earnings = E - 5
E + E - 5 = 37
2E = 42
E = <<21=21>>21
Evan made $21 so Markese earned 21 - 5 = <<21-5=16>>16
Markese earned $<<16=16>>16.
So the final answer is 16

Input: Lou Senior took 3 cookies out of the cookie jar and ate them.  Since he didn't get caught by his wife, he went back the next day and took another 3 cookies out of the jar.  But after eating just one of the cookies, he felt guilty about it and put the other two cookies back.  His son, Louie Junior saw that his Dad was eating cookies.  So, Louie Junior took seven cookies out of the jar and hid them in his bedroom for later.  The next morning, Debra, Lou's wife looked into the cookie jar and reacted by accusing her husband of eating half of the cookies out of the cookie jar.  How many cookies remained in the jar?
Output: Lou Sr 3 cookies from the jar, then another three, but then put two back, for a total of 3+3-2=<<3+3-2=4>>4 cookies removed.
Louie Jr took seven more out of the jar, for a total of 4+7=<<4+7=11>>11 cookies removed.
If Debra thought half were gone, then the number of missing cookies would equal the number of cookies that remain, or 11 missing=11 remaining cookies
So the final answer is 11

Input: John had $200. He gave 3/8 of his money to his mother and 3/10 to his father. How much money did John have left?
Output: John's mother received 3/8 x $200 = $<<3/8*200=75>>75.
His father got 3/10 x $200 = $<<3/10*200=60>>60.
Thus, John gave a total of $75 + $60 = $<<75+60=135>>135.
Therefore, John had $200 - $135 = $<<200-135=65>>65 left.
So the final answer is 65

Input: Tonya has $150.00 on her credit card.  If she leaves any balance on her card at the end of the month, she is charged 20% interest.  If she makes a $50.00 payment on her card, what will be the new balance?
Output: Her card has a $150.00 balance and she makes a $50.00 payment so the new balance is 150-50 = $<<150-50=100.00>>100.00
She didn't pay it off so she is charged 20% interest on her $100.00 balance so the interest is.20*100 = $20.00
Her balance was $100.00 and she was charged $20.00 in interest so her new balance is 100+20 = $<<100+20=120.00>>120.00
So the final answer is 120

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o16-tgsm8k-s1-rTrue-m4096
Evaluating commonsenseqa :   0%|          | 0/100 [00:07<?, ?it/s]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 97, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 94, in main
    inference_main(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 90, in inference_main
    query_ids, response_ids, answers, indices = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 65, in run_model
    gen_out = model.generate(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 1454, in generate
    return self.sample(
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 2568, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3518759) of binary: /home/ylu130/.conda/envs/ood/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.conda/envs/ood/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-06_18:03:10
  host      : icgpu06.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3518759)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o17-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 17 16 17 18 19 20 True 4096 10
[2023-09-06 18:03:13,900] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o17-tgsm8k-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 17
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ..............accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu02 --master_port 19674 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o11-tgsm8k-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 11 11 12 13 14 15 True 4096 10
 ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 268342.23it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.18s/it]
 > number of parameters: 6738415616
[2023-09-06 18:03:22,071] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-06 18:03:22,274] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-06 18:03:22,275] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c01f0>
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-06 18:03:22,276] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-06 18:03:22,277] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-06 18:03:22,278] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-06 18:03:22,278] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-06 18:03:22,278] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-06 18:03:22,278] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-06 18:03:22,278] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: Mary added a total of 7 stickers/page * 6 pages= <<7*6=42>>42 stickers to the 6 other pages.
In total, Mary added 3 large stickers + 42 stickers = <<3+42=45>>45 stickers to her journal.
Since she started with 89 stickers, she now has 89 - 45 = <<89-45=44>>44 stickers left.
So the final answer is 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: If he babysits for 2 hours at $7 per hour, he will earn 2*7 = $<<2*7=14>>14
This week he will earn $5 allowance, $10 mowing the lawn and $14 from babysitting for a total of 5+10+14 = $<<5+10+14=29>>29
If we add the $29 he will earn to his $65 savings, he will have a total of 29 + 65 = $<<29+65=94>>94
The bike costs $100 and he will have $94 leaving $100-$94 = $<<100-94=6>>6 more that he will need to earn
So the final answer is 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: His kangaroos have a total of 23*2=<<23*2=46>>46 legs
He has 23*3=<<23*3=69>>69 goats
The goats have 69*4=<<69*4=276>>276 legs
So in total his animals have 276+46=<<276+46=322>>322 legs
So the final answer is 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: After buying a hat, Josh has $20 - $10 = $<<20-10=10>>10
After buying a pencil, Josh has $10 - $2 = $<<10-2=8>>8
The total cost of cookies is 4 * $1.25 = $<<4*1.25=5>>5
After buying the cookies, Josh has $8 - $5 = $<<8-5=3>>3
So the final answer is 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: The old team per round record is 1,148 because 287 x 4 = <<1148=1148>>1,148
The team season record is 11,480 because 10 x 1,248 = 11,480
They need 1,040 points in the final round to tie the record because 11,480 - 10,440 = <<11480-10440=1040>>1,040
They need to average 260 points each because 1,040 / 4 = <<1040/4=260>>260
This is 27 points less than the current record average because 287 - 260 = <<27=27>>27
So the final answer is 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: Max finished history in 20 * 2 = <<20*2=40>>40 minutes.
Finishing geography took the most time, which is 40 * 3 = <<40*3=120>>120 minutes.
In total, for all three subjects, Max needed 20 + 40 + 120 = <<20+40+120=180>>180 minutes.
So the final answer is 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: If Sophia ate 1/6 of the pie, then 6/6 - 1/6 = 5/6 is left in the fridge.
Let x be the pie's original weight.
The current weight of the pie can be described by 5x/6=1200 grams
So, 5x=7200.
And, the original weight is x = <<1440=1440>>1440 grams.
So, Sophia ate 1440 grams original - 1200 grams after= <<1440-1200=240>>240 grams of pie.
So the final answer is 240

Input: Sarah, Mary, and Tuan decided to go to the restaurant for a meal. They decided to split the cost of the meal evenly. If the total price of the meal comes to $67 and they have a coupon for $4, how much does each person need to contribute to the bill?
Output: After using the coupon, the final price comes to 67 - 4 = <<67-4=63>>63 dollars.
With three people, they each need to pay 63 / 3 = <<63/3=21>>21 dollars each.
So the final answer is 21

Input: Tom's brother is 4 times as old as Tom's dog. If in 6 years, Tom's brother will be 30 years, how old is Tom's dog going to be in six years?
Output: If in six years Tom's brother will be 30 years old, he is currently 30-6 = <<30-6=24>>24 years old.
Since Tom's brother is 4 times as old as Tom's dog, Tom's dog is 24/4 = <<24/4=6>>6 years old currently.
Tom's dog will be 6+6 = <<6+6=12>>12 years old in six years.
So the final answer is 12

Input: There are 50 children at the party. Three-fifths of them are boys. How many of the children are girls?
Output: 50 x 3/5 = <<50*3/5=30>>30 children are boys.
So, there are 50 - 30 = <<50-30=20>>20 children that are girls.
So the final answer is 20

Input: Gail has some bills in her wallet which amount to $100. She has four $5 bills and three $20 bills, and the rest are $10 bills. How many $10 bills are in her wallet?
Output: Four $5 bills amount to $5 x 4 = $<<5*4=20>>20.
Three $20 bills amount to $20 x 3 = $<<20*3=60>>60.
So Gail's $5 and $20 bills amount to $20 + $60 = $80.
Thus, her $10 bills amount to $100 - $80 = $20.
Therefore, she has $20/$10 = <<20/10=2>>2 $10 bills.
So the final answer is 2

Input: A 220-liter barrel has a small leak. It lost 10% of its contents before anyone noticed. How many liters are left in the barrel?
Output: The barrel lost 220 * 10 / 100 = <<220*10/100=22>>22 liters before anyone noticed.
So, 220 – 22 = <<220-22=198>>198 liters are left in the barrel.
So the final answer is 198

Input: Markese earned 5 fewer dollars than Evan. Together they earned $37. How many dollars did Markese earn? Use E to represent how many dollars Evan earned.
Output: E = Evan's earnings
Markese's earnings = E - 5
E + E - 5 = 37
2E = 42
E = <<21=21>>21
Evan made $21 so Markese earned 21 - 5 = <<21-5=16>>16
Markese earned $<<16=16>>16.
So the final answer is 16

Input: Lou Senior took 3 cookies out of the cookie jar and ate them.  Since he didn't get caught by his wife, he went back the next day and took another 3 cookies out of the jar.  But after eating just one of the cookies, he felt guilty about it and put the other two cookies back.  His son, Louie Junior saw that his Dad was eating cookies.  So, Louie Junior took seven cookies out of the jar and hid them in his bedroom for later.  The next morning, Debra, Lou's wife looked into the cookie jar and reacted by accusing her husband of eating half of the cookies out of the cookie jar.  How many cookies remained in the jar?
Output: Lou Sr 3 cookies from the jar, then another three, but then put two back, for a total of 3+3-2=<<3+3-2=4>>4 cookies removed.
Louie Jr took seven more out of the jar, for a total of 4+7=<<4+7=11>>11 cookies removed.
If Debra thought half were gone, then the number of missing cookies would equal the number of cookies that remain, or 11 missing=11 remaining cookies
So the final answer is 11

Input: John had $200. He gave 3/8 of his money to his mother and 3/10 to his father. How much money did John have left?
Output: John's mother received 3/8 x $200 = $<<3/8*200=75>>75.
His father got 3/10 x $200 = $<<3/10*200=60>>60.
Thus, John gave a total of $75 + $60 = $<<75+60=135>>135.
Therefore, John had $200 - $135 = $<<200-135=65>>65 left.
So the final answer is 65

Input: Tonya has $150.00 on her credit card.  If she leaves any balance on her card at the end of the month, she is charged 20% interest.  If she makes a $50.00 payment on her card, what will be the new balance?
Output: Her card has a $150.00 balance and she makes a $50.00 payment so the new balance is 150-50 = $<<150-50=100.00>>100.00
She didn't pay it off so she is charged 20% interest on her $100.00 balance so the interest is.20*100 = $20.00
Her balance was $100.00 and she was charged $20.00 in interest so her new balance is 100+20 = $<<100+20=120.00>>120.00
So the final answer is 120

Input: In her first term, Governor Sandoval gave twice as many commencement addresses as Governor Hawkins. Governor Sloan gave ten more commencement addresses than Governor Sandoval in the same amount of time. If Governor Sandoval gave 12 commencement addresses, how many commencement addresses did the three of them give altogether?
Output: Since Governor Sandoval gave 12 commencement addresses, twice the number that Governor Hawkins gave, Governor Hawkins gave 12/2 = <<12/2=6>>6 commencement addresses.
The two of them gave 6+12 = <<6+12=18>>18 commencement addresses in total.
Governor Sloan gave ten more commencement addresses than Governor Sandoval, a total of 12+10 = <<12+10=22>>22 commencement addresses.
The three of them gave 22+18 = <<22+18=40>>40 commencement addresses.
So the final answer is 40

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o17-tgsm8k-s1-rTrue-m4096
Evaluating commonsenseqa :   0%|          | 0/100 [00:07<?, ?it/s]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 97, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 94, in main
    inference_main(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 90, in inference_main
    query_ids, response_ids, answers, indices = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 65, in run_model
    gen_out = model.generate(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 1454, in generate
    return self.sample(
  File "/home[2023-09-06 18:03:33,818] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3518921) of binary: /home/ylu130/.conda/envs/ood/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.conda/envs/ood/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-06_18:03:31
  host      : icgpu06.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3518921)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o18-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 18 16 17 18 19 20 True 4096 10
[2023-09-06 18:03:35,775] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o19-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 19 16 17 18 19 20 True 4096 10
[2023-09-06 18:03:42,665] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o19-tgsm8k-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 19
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o19-tgsm8k-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 252987.46it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.22s/it]
 > number of parameters: 6738415616
[2023-09-06 18:03:51,857] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-06 18:03:52,061] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-06 18:03:52,063] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-06 18:03:52,063] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-06 18:03:52,063] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-06 18:03:52,063] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-06 18:03:52,063] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-06 18:03:52,063] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-06 18:03:52,063] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-06 18:03:52,063] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-06 18:03:52,063] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-06 18:03:52,063] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-06 18:03:52,063] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c01f0>
[2023-09-06 18:03:52,063] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-06 18:03:52,063] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-06 18:03:52,064] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-06 18:03:52,065] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-06 18:03:52,065] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-06 18:03:52,065] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-06 18:03:52,065] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-06 18:03:52,065] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-06 18:03:52,065] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-06 18:03:52,065] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-06 18:03:52,065] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-06 18:03:52,065] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-06 18:03:52,065] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-06 18:03:52,065] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-06 18:03:52,065] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-06 18:03:52,065] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-06 18:03:52,065] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: Mary added a total of 7 stickers/page * 6 pages= <<7*6=42>>42 stickers to the 6 other pages.
In total, Mary added 3 large stickers + 42 stickers = <<3+42=45>>45 stickers to her journal.
Since she started with 89 stickers, she now has 89 - 45 = <<89-45=44>>44 stickers left.
So the final answer is 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: If he babysits for 2 hours at $7 per hour, he will earn 2*7 = $<<2*7=14>>14
This week he will earn $5 allowance, $10 mowing the lawn and $14 from babysitting for a total of 5+10+14 = $<<5+10+14=29>>29
If we add the $29 he will earn to his $65 savings, he will have a total of 29 + 65 = $<<29+65=94>>94
The bike costs $100 and he will have $94 leaving $100-$94 = $<<100-94=6>>6 more that he will need to earn
So the final answer is 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: His kangaroos have a total of 23*2=<<23*2=46>>46 legs
He has 23*3=<<23*3=69>>69 goats
The goats have 69*4=<<69*4=276>>276 legs
So in total his animals have 276+46=<<276+46=322>>322 legs
So the final answer is 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: After buying a hat, Josh has $20 - $10 = $<<20-10=10>>10
After buying a pencil, Josh has $10 - $2 = $<<10-2=8>>8
The total cost of cookies is 4 * $1.25 = $<<4*1.25=5>>5
After buying the cookies, Josh has $8 - $5 = $<<8-5=3>>3
So the final answer is 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: The old team per round record is 1,148 because 287 x 4 = <<1148=1148>>1,148
The team season record is 11,480 because 10 x 1,248 = 11,480
They need 1,040 points in the final round to tie the record because 11,480 - 10,440 = <<11480-10440=1040>>1,040
They need to average 260 points each because 1,040 / 4 = <<1040/4=260>>260
This is 27 points less than the current record average because 287 - 260 = <<27=27>>27
So the final answer is 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: Max finished history in 20 * 2 = <<20*2=40>>40 minutes.
Finishing geography took the most time, which is 40 * 3 = <<40*3=120>>120 minutes.
In total, for all three subjects, Max needed 20 + 40 + 120 = <<20+40+120=180>>180 minutes.
So the final answer is 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: If Sophia ate 1/6 of the pie, then 6/6 - 1/6 = 5/6 is left in the fridge.
Let x be the pie's original weight.
The current weight of the pie can be described by 5x/6=1200 grams
So, 5x=7200.
And, the original weight is x = <<1440=1440>>1440 grams.
So, Sophia ate 1440 grams original - 1200 grams after= <<1440-1200=240>>240 grams of pie.
So the final answer is 240

Input: Sarah, Mary, and Tuan decided to go to the restaurant for a meal. They decided to split the cost of the meal evenly. If the total price of the meal comes to $67 and they have a coupon for $4, how much does each person need to contribute to the bill?
Output: After using the coupon, the final price comes to 67 - 4 = <<67-4=63>>63 dollars.
With three people, they each need to pay 63 / 3 = <<63/3=21>>21 dollars each.
So the final answer is 21

Input: Tom's brother is 4 times as old as Tom's dog. If in 6 years, Tom's brother will be 30 years, how old is Tom's dog going to be in six years?
Output: If in six years Tom's brother will be 30 years old, he is currently 30-6 = <<30-6=24>>24 years old.
Since Tom's brother is 4 times as old as Tom's dog, Tom's dog is 24/4 = <<24/4=6>>6 years old currently.
Tom's dog will be 6+6 = <<6+6=12>>12 years old in six years.
So the final answer is 12

Input: There are 50 children at the party. Three-fifths of them are boys. How many of the children are girls?
Output: 50 x 3/5 = <<50*3/5=30>>30 children are boys.
So, there are 50 - 30 = <<50-30=20>>20 children that are girls.
So the final answer is 20

Input: Gail has some bills in her wallet which amount to $100. She has four $5 bills and three $20 bills, and the rest are $10 bills. How many $10 bills are in her wallet?
Output: Four $5 bills amount to $5 x 4 = $<<5*4=20>>20.
Three $20 bills amount to $20 x 3 = $<<20*3=60>>60.
So Gail's $5 and $20 bills amount to $20 + $60 = $80.
Thus, her $10 bills amount to $100 - $80 = $20.
Therefore, she has $20/$10 = <<20/10=2>>2 $10 bills.
So the final answer is 2

Input: A 220-liter barrel has a small leak. It lost 10% of its contents before anyone noticed. How many liters are left in the barrel?
Output: The barrel lost 220 * 10 / 100 = <<220*10/100=22>>22 liters before anyone noticed.
So, 220 – 22 = <<220-22=198>>198 liters are left in the barrel.
So the final answer is 198

Input: Markese earned 5 fewer dollars than Evan. Together they earned $37. How many dollars did Markese earn? Use E to represent how many dollars Evan earned.
Output: E = Evan's earnings
Markese's earnings = E - 5
E + E - 5 = 37
2E = 42
E = <<21=21>>21
Evan made $21 so Markese earned 21 - 5 = <<21-5=16>>16
Markese earned $<<16=16>>16.
So the final answer is 16

Input: Lou Senior took 3 cookies out of the cookie jar and ate them.  Since he didn't get caught by his wife, he went back the next day and took another 3 cookies out of the jar.  But after eating just one of the cookies, he felt guilty about it and put the other two cookies back.  His son, Louie Junior saw that his Dad was eating cookies.  So, Louie Junior took seven cookies out of the jar and hid them in his bedroom for later.  The next morning, Debra, Lou's wife looked into the cookie jar and reacted by accusing her husband of eating half of the cookies out of the cookie jar.  How many cookies remained in the jar?
Output: Lou Sr 3 cookies from the jar, then another three, but then put two back, for a total of 3+3-2=<<3+3-2=4>>4 cookies removed.
Louie Jr took seven more out of the jar, for a total of 4+7=<<4+7=11>>11 cookies removed.
If Debra thought half were gone, then the number of missing cookies would equal the number of cookies that remain, or 11 missing=11 remaining cookies
So the final answer is 11

Input: John had $200. He gave 3/8 of his money to his mother and 3/10 to his father. How much money did John have left?
Output: John's mother received 3/8 x $200 = $<<3/8*200=75>>75.
His father got 3/10 x $200 = $<<3/10*200=60>>60.
Thus, John gave a total of $75 + $60 = $<<75+60=135>>135.
Therefore, John had $200 - $135 = $<<200-135=65>>65 left.
So the final answer is 65

Input: Tonya has $150.00 on her credit card.  If she leaves any balance on her card at the end of the month, she is charged 20% interest.  If she makes a $50.00 payment on her card, what will be the new balance?
Output: Her card has a $150.00 balance and she makes a $50.00 payment so the new balance is 150-50 = $<<150-50=100.00>>100.00
She didn't pay it off so she is charged 20% interest on her $100.00 balance so the interest is.20*100 = $20.00
Her balance was $100.00 and she was charged $20.00 in interest so her new balance is 100+20 = $<<100+20=120.00>>120.00
So the final answer is 120

Input: In her first term, Governor Sandoval gave twice as many commencement addresses as Governor Hawkins. Governor Sloan gave ten more commencement addresses than Governor Sandoval in the same amount of time. If Governor Sandoval gave 12 commencement addresses, how many commencement addresses did the three of them give altogether?
Output: Since Governor Sandoval gave 12 commencement addresses, twice the number that Governor Hawkins gave, Governor Hawkins gave 12/2 = <<12/2=6>>6 commencement addresses.
The two of them gave 6+12 = <<6+12=18>>18 commencement addresses in total.
Governor Sloan gave ten more commencement addresses than Governor Sandoval, a total of 12+10 = <<12+10=22>>22 commencement addresses.
The three of them gave 22+18 = <<22+18=40>>40 commencement addresses.
So the final answer is 40

Input: If Buzz bought a pizza with 78 slices at a restaurant and then decided to share it with the waiter in the ratio of 5:8, with Buzz's ratio being 5, what's twenty less the number of slices of pizza that the waiter ate?
Output: The total ratio representing the slices of pizza that Buzz bought is 5+8=<<5+8=13>>13
If he shared the slices of pizza with the waiter, the waiter received a fraction of 8/13 of the total number of slices, which totals 8/13 * 78 = <<8/13*78=48>>48 slices
Twenty less the number of slices of pizza that the waiter ate is 48-20 = <<48-20=28>>28
So the final answer is 28

Input: Jolene and Phil have four children, each with the same birthday.  They gave birth to their first child exactly 15 years ago.  They gave birth to their second child exactly one year after the birth of their first child.  They gave birth to their third child on the fourth birthday of their second child. Two years after the birth of their third child, they gave birth to their fourth child.  How old, in years, is their fourth child?
Output: Their 1st child was born 15 years ago, and therefore is <<15=15>>15 years old.
Their 2nd child was born 1 year after their 15-year-old child, and therefore is 15-1=<<15-1=14>>14 years old.
Their 3rd child was born 4 years after their 14-year-old child, and therefore is 14-4=10 years old.
Their 4th child was born 2 years after their 10-year-old child, and therefore is 10-2=8 years old.
So the final answer is 8

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o19-tgsm8k-s1-rTrue-m4096
Evaluating commonsenseqa :   1%|          | 1/100 [01:29<2:28:20, 89.90s/it]Evaluating commonsenseqa :   1%|          | 1/100 [01:37<2:40:52, 97.50s/it]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 97, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 94, in main
    inference_main(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 90, in inference_main
    query_ids, response_ids, answers, indices = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 65, in run_model
    gen_out = model.generate(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 1454, in generate
    return self.sample(
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 2568, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3519209) of binary: /home/ylu130/.conda/envs/ood/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.conda/envs/ood/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-06_18:05:30
  host      : icgpu06.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3519209)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o20-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 20 16 17 18 19 20 True 4096 10
[2023-09-06 18:05:35,001] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o20-tgsm8k-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 20
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o20-tgsm8k-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 205679.11it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.32s/it]
 > number of parameters: 6738415616
[2023-09-06 18:05:43,643] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-06 18:05:43,851] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-06 18:05:43,853] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-06 18:05:43,853] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-06 18:05:43,853] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-06 18:05:43,853] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-06 18:05:43,853] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-06 18:05:43,853] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-06 18:05:43,853] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-06 18:05:43,853] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-06 18:05:43,853] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-06 18:05:43,853] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-06 18:05:43,853] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0280>
[2023-09-06 18:05:43,853] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-06 18:05:43,854] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-06 18:05:43,855] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-06 18:05:43,855] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-06 18:05:43,855] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-06 18:05:43,855] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-06 18:05:43,855] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-06 18:05:43,855] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-06 18:05:43,855] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-06 18:05:43,855] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-06 18:05:43,855] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-06 18:05:43,855] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-06 18:05:43,855] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-06 18:05:43,855] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-06 18:05:43,855] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-06 18:05:43,855] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-06 18:05:43,855] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-06 18:05:43,855] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: Mary added a total of 7 stickers/page * 6 pages= <<7*6=42>>42 stickers to the 6 other pages.
In total, Mary added 3 large stickers + 42 stickers = <<3+42=45>>45 stickers to her journal.
Since she started with 89 stickers, she now has 89 - 45 = <<89-45=44>>44 stickers left.
So the final answer is 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: If he babysits for 2 hours at $7 per hour, he will earn 2*7 = $<<2*7=14>>14
This week he will earn $5 allowance, $10 mowing the lawn and $14 from babysitting for a total of 5+10+14 = $<<5+10+14=29>>29
If we add the $29 he will earn to his $65 savings, he will have a total of 29 + 65 = $<<29+65=94>>94
The bike costs $100 and he will have $94 leaving $100-$94 = $<<100-94=6>>6 more that he will need to earn
So the final answer is 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: His kangaroos have a total of 23*2=<<23*2=46>>46 legs
He has 23*3=<<23*3=69>>69 goats
The goats have 69*4=<<69*4=276>>276 legs
So in total his animals have 276+46=<<276+46=322>>322 legs
So the final answer is 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: After buying a hat, Josh has $20 - $10 = $<<20-10=10>>10
After buying a pencil, Josh has $10 - $2 = $<<10-2=8>>8
The total cost of cookies is 4 * $1.25 = $<<4*1.25=5>>5
After buying the cookies, Josh has $8 - $5 = $<<8-5=3>>3
So the final answer is 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: The old team per round record is 1,148 because 287 x 4 = <<1148=1148>>1,148
The team season record is 11,480 because 10 x 1,248 = 11,480
They need 1,040 points in the final round to tie the record because 11,480 - 10,440 = <<11480-10440=1040>>1,040
They need to average 260 points each because 1,040 / 4 = <<1040/4=260>>260
This is 27 points less than the current record average because 287 - 260 = <<27=27>>27
So the final answer is 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: Max finished history in 20 * 2 = <<20*2=40>>40 minutes.
Finishing geography took the most time, which is 40 * 3 = <<40*3=120>>120 minutes.
In total, for all three subjects, Max needed 20 + 40 + 120 = <<20+40+120=180>>180 minutes.
So the final answer is 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: If Sophia ate 1/6 of the pie, then 6/6 - 1/6 = 5/6 is left in the fridge.
Let x be the pie's original weight.
The current weight of the pie can be described by 5x/6=1200 grams
So, 5x=7200.
And, the original weight is x = <<1440=1440>>1440 grams.
So, Sophia ate 1440 grams original - 1200 grams after= <<1440-1200=240>>240 grams of pie.
So the final answer is 240

Input: Sarah, Mary, and Tuan decided to go to the restaurant for a meal. They decided to split the cost of the meal evenly. If the total price of the meal comes to $67 and they have a coupon for $4, how much does each person need to contribute to the bill?
Output: After using the coupon, the final price comes to 67 - 4 = <<67-4=63>>63 dollars.
With three people, they each need to pay 63 / 3 = <<63/3=21>>21 dollars each.
So the final answer is 21

Input: Tom's brother is 4 times as old as Tom's dog. If in 6 years, Tom's brother will be 30 years, how old is Tom's dog going to be in six years?
Output: If in six years Tom's brother will be 30 years old, he is currently 30-6 = <<30-6=24>>24 years old.
Since Tom's brother is 4 times as old as Tom's dog, Tom's dog is 24/4 = <<24/4=6>>6 years old currently.
Tom's dog will be 6+6 = <<6+6=12>>12 years old in six years.
So the final answer is 12

Input: There are 50 children at the party. Three-fifths of them are boys. How many of the children are girls?
Output: 50 x 3/5 = <<50*3/5=30>>30 children are boys.
So, there are 50 - 30 = <<50-30=20>>20 children that are girls.
So the final answer is 20

Input: Gail has some bills in her wallet which amount to $100. She has four $5 bills and three $20 bills, and the rest are $10 bills. How many $10 bills are in her wallet?
Output: Four $5 bills amount to $5 x 4 = $<<5*4=20>>20.
Three $20 bills amount to $20 x 3 = $<<20*3=60>>60.
So Gail's $5 and $20 bills amount to $20 + $60 = $80.
Thus, her $10 bills amount to $100 - $80 = $20.
Therefore, she has $20/$10 = <<20/10=2>>2 $10 bills.
So the final answer is 2

Input: A 220-liter barrel has a small leak. It lost 10% of its contents before anyone noticed. How many liters are left in the barrel?
Output: The barrel lost 220 * 10 / 100 = <<220*10/100=22>>22 liters before anyone noticed.
So, 220 – 22 = <<220-22=198>>198 liters are left in the barrel.
So the final answer is 198

Input: Markese earned 5 fewer dollars than Evan. Together they earned $37. How many dollars did Markese earn? Use E to represent how many dollars Evan earned.
Output: E = Evan's earnings
Markese's earnings = E - 5
E + E - 5 = 37
2E = 42
E = <<21=21>>21
Evan made $21 so Markese earned 21 - 5 = <<21-5=16>>16
Markese earned $<<16=16>>16.
So the final answer is 16

Input: Lou Senior took 3 cookies out of the cookie jar and ate them.  Since he didn't get caught by his wife, he went back the next day and took another 3 cookies out of the jar.  But after eating just one of the cookies, he felt guilty about it and put the other two cookies back.  His son, Louie Junior saw that his Dad was eating cookies.  So, Louie Junior took seven cookies out of the jar and hid them in his bedroom for later.  The next morning, Debra, Lou's wife looked into the cookie jar and reacted by accusing her husband of eating half of the cookies out of the cookie jar.  How many cookies remained in the jar?
Output: Lou Sr 3 cookies from the jar, then another three, but then put two back, for a total of 3+3-2=<<3+3-2=4>>4 cookies removed.
Louie Jr took seven more out of the jar, for a total of 4+7=<<4+7=11>>11 cookies removed.
If Debra thought half were gone, then the number of missing cookies would equal the number of cookies that remain, or 11 missing=11 remaining cookies
So the final answer is 11

Input: John had $200. He gave 3/8 of his money to his mother and 3/10 to his father. How much money did John have left?
Output: John's mother received 3/8 x $200 = $<<3/8*200=75>>75.
His father got 3/10 x $200 = $<<3/10*200=60>>60.
Thus, John gave a total of $75 + $60 = $<<75+60=135>>135.
Therefore, John had $200 - $135 = $<<200-135=65>>65 left.
So the final answer is 65

Input: Tonya has $150.00 on her credit card.  If she leaves any balance on her card at the end of the month, she is charged 20% interest.  If she makes a $50.00 payment on her card, what will be the new balance?
Output: Her card has a $150.00 balance and she makes a $50.00 payment so the new balance is 150-50 = $<<150-50=100.00>>100.00
She didn't pay it off so she is charged 20% interest on her $100.00 balance so the interest is.20*100 = $20.00
Her balance was $100.00 and she was charged $20.00 in interest so her new balance is 100+20 = $<<100+20=120.00>>120.00
So the final answer is 120

Input: In her first term, Governor Sandoval gave twice as many commencement addresses as Governor Hawkins. Governor Sloan gave ten more commencement addresses than Governor Sandoval in the same amount of time. If Governor Sandoval gave 12 commencement addresses, how many commencement addresses did the three of them give altogether?
Output: Since Governor Sandoval gave 12 commencement addresses, twice the number that Governor Hawkins gave, Governor Hawkins gave 12/2 = <<12/2=6>>6 commencement addresses.
The two of them gave 6+12 = <<6+12=18>>18 commencement addresses in total.
Governor Sloan gave ten more commencement addresses than Governor Sandoval, a total of 12+10 = <<12+10=22>>22 commencement addresses.
The three of them gave 22+18 = <<22+18=40>>40 commencement addresses.
So the final answer is 40

Input: If Buzz bought a pizza with 78 slices at a restaurant and then decided to share it with the waiter in the ratio of 5:8, with Buzz's ratio being 5, what's twenty less the number of slices of pizza that the waiter ate?
Output: The total ratio representing the slices of pizza that Buzz bought is 5+8=<<5+8=13>>13
If he shared the slices of pizza with the waiter, the waiter received a fraction of 8/13 of the total number of slices, which totals 8/13 * 78 = <<8/13*78=48>>48 slices
Twenty less the number of slices of pizza that the waiter ate is 48-20 = <<48-20=28>>28
So the final answer is 28

Input: Jolene and Phil have four children, each with the same birthday.  They gave birth to their first child exactly 15 years ago.  They gave birth to their second child exactly one year after the birth of their first child.  They gave birth to their third child on the fourth birthday of their second child. Two years after the birth of their third child, they gave birth to their fourth child.  How old, in years, is their fourth child?
Output: Their 1st child was born 15 years ago, and therefore is <<15=15>>15 years old.
Their 2nd child was born 1 year after their 15-year-old child, and therefore is 15-1=<<15-1=14>>14 years old.
Their 3rd child was born 4 years after their 14-year-old child, and therefore is 14-4=10 years old.
Their 4th child was born 2 years after their 10-year-old child, and therefore is 10-2=8 years old.
So the final answer is 8

Input: Each purple book has 230 pages. Each orange book contains 510 pages. Mirella read 5 purple books and 4 orange books. How many more orange pages did she read than purple pages?
Output: Purple = 5 * 230 = <<5*230=1150>>1150 pages
Orange = 4 * 510 = <<4*510=2040>>2040 pages
2040 - 1150 = <<2040-1150=890>>890
Mirella read 890 more orange pages than purple pages.
So the final answer is 890
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o20-tgsm8k-s1-rTrue-m4096
Evaluating commonsenseqa :   1%|          | 1/100 [01:29<2:28:01, 89.71s/it]   Evaluating commonsenseqa :   2%|▏         | 2/100 [02:57<2:24:26, 88.44s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [04:24<2:21:53, 87.77s/it] Evaluating commonsenseqa :   4%|▍         | 4/100 [05:51<2:19:54, 87.44s/it] Evaluating commonsenseqa :   5%|▌         | 5/100 [07:18<2:18:06, 87.22s/it] Evaluating commonsenseqa :   6%|▌         | 6/100 [08:44<2:16:28, 87.11s/it] Evaluating commonsenseqa :   7%|▋         | 7/100 [10:11<2:14:58, 87.08s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [11:38<2:13:26, 87.02s/it] Evaluating commonsenseqa :   9%|▉         | 9/100 [13:05<2:11:55, 86.98s/it] Evaluating commonsenseqa :  10%|█         | 10/100 [14:32<2:10:23, 86.93s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [15:59<2:08:58, 86.95s/it] Evaluating commonsenseqa :  12%|█▏        | 12/100 [17:26<2:07:29, 86.92s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [18:53<2:06:00, 86.90s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [20:20<2:04:36, 86.93s/it] Evaluating commonsenseqa :  15%|█▌        | 15/100 [21:47<2:03:08, 86.92s/it] Evaluating commonsenseqa :  16%|█▌        | 16/100 [23:14<2:01:40, 86.91s/it] Evaluating commonsenseqa :  17%|█▋        | 17/100 [24:41<2:00:18, 86.97s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [26:08<1:58:52, 86.98s/it] Evaluating commonsenseqa :  19%|█▉        | 19/100 [27:35<1:57:24, 86.97s/it] Evaluating commonsenseqa :  20%|██        | 20/100 [29:02<1:55:58, 86.98s/it] Evaluating commonsenseqa :  21%|██        | 21/100 [30:29<1:54:30, 86.97s/it] Evaluating commonsenseqa :  22%|██▏       | 22/100 [31:56<1:53:04, 86.99s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [33:22<1:51:36, 86.96s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [34:49<1:50:08, 86.96s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [36:16<1:48:44, 86.99s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [37:43<1:47:18, 87.00s/it] Evaluating commonsenseqa :  27%|██▋       | 27/100 [39:11<1:45:53, 87.03s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [40:38<1:44:26, 87.04s/it] Evaluating commonsenseqa :  29%|██▉       | 29/100 [42:05<1:42:58, 87.02s/it] Evaluating commonsenseqa :  30%|███       | 30/100 [43:32<1:41:33, 87.05s/it] Evaluating commonsenseqa :  31%|███       | 31/100 [44:59<1:40:03, 87.01s/it] Evaluating commonsenseqa :  32%|███▏      | 32/100 [46:26<1:38:33, 86.97s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [47:53<1:37:07, 86.97s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [49:19<1:35:40, 86.98s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [50:46<1:34:12, 86.97s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [52:13<1:32:45, 86.96s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [53:40<1:31:19, 86.98s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [55:08<1:29:57, 87.06s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [56:35<1:28:27, 87.01s/it] Evaluating commonsenseqa :  40%|████      | 40/100 [58:02<1:27:00, 87.00s/it] Evaluating commonsenseqa :  41%|████      | 41/100 [59:28<1:25:31, 86.97s/it]   Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:00:55<1:24:04, 86.97s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:02:22<1:22:36, 86.96s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:03:49<1:21:08, 86.94s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:05:16<1:19:44, 87.00s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [1:06:43<1:18:18, 87.00s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:08:10<1:16:52, 87.04s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:09:37<1:15:24, 87.01s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:11:04<1:13:56, 87.00s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [1:12:31<1:12:31, 87.02s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [1:13:58<1:11:02, 87.00s/it] Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:15:25<1:09:35, 86.99s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:16:52<1:08:08, 86.98s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:18:19<1:06:41, 86.98s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:19:46<1:05:13, 86.97s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:21:13<1:03:47, 86.98s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:22:40<1:02:20, 86.99s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:24:07<1:00:53, 86.99s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [1:25:34<59:28, 87.03s/it]  Evaluating commonsenseqa :  60%|██████    | 60/100 [1:27:01<58:01, 87.03s/it] Evaluating commonsenseqa :  61%|██████    | 61/100 [1:28:29<56:34, 87.04s/it] Evaluating commonsenseqa :  62%|██████▏   | 62/100 [1:29:56<55:11, 87.14s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [1:31:23<53:41, 87.08s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [1:32:50<52:13, 87.06s/it] Evaluating commonsenseqa :  65%|██████▌   | 65/100 [1:34:17<50:46, 87.05s/it] Evaluating commonsenseqa :  66%|██████▌   | 66/100 [1:35:44<49:20, 87.07s/it] Evaluating commonsenseqa :  67%|██████▋   | 67/100 [1:37:11<47:52, 87.06s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [1:38:38<46:25, 87.05s/it] Evaluating commonsenseqa :  69%|██████▉   | 69/100 [1:40:05<44:59, 87.08s/it] Evaluating commonsenseqa :  70%|███████   | 70/100 [1:41:32<43:32, 87.07s/it] Evaluating commonsenseqa :  71%|███████   | 71/100 [1:42:59<42:05, 87.09s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [1:44:27<40:39, 87.14s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [1:45:54<39:12, 87.13s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [1:47:21<37:44, 87.09s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [1:48:48<36:18, 87.12s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [1:50:15<34:50, 87.09s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [1:51:42<33:22, 87.06s/it] Evaluating commonsenseqa :  78%|███████▊  | 78/100 [1:53:09<31:55, 87.08s/it] Evaluating commonsenseqa :  79%|███████▉  | 79/100 [1:54:36<30:28, 87.07s/it] Evaluating commonsenseqa :  80%|████████  | 80/100 [1:56:03<29:01, 87.08s/it] Evaluating commonsenseqa :  81%|████████  | 81/100 [1:57:30<27:34, 87.09s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [1:58:57<26:07, 87.10s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [2:00:25<24:40, 87.09s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [2:01:52<23:13, 87.11s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [2:03:19<21:47, 87.19s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [2:04:46<20:20, 87.16s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [2:06:13<18:52, 87.15s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [2:07:40<17:25, 87.13s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [2:09:07<15:58, 87.12s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [2:10:35<14:31, 87.12s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [2:12:02<13:03, 87.09s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [2:13:29<11:36, 87.11s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [2:14:56<10:10, 87.14s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [2:16:23<08:42, 87.15s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [2:17:50<07:15, 87.13s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [2:19:18<05:48, 87.21s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [2:20:45<04:21, 87.19s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [2:22:12<02:54, 87.20s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [2:23:39<01:27, 87.17s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:25:06<00:00, 87.14s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:25:06<00:00, 87.07s/it]
name: commonsenseqa | avg. gen lenth: 349.816 | time: 8709.919579982758s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o16-tgsm8k-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 16 16 17 18 19 20 True 4096 10
[2023-09-06 20:31:00,942] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o16-tgsm8k-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 16
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o16-tgsm8k-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 320417.18it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.31s/it]
 > number of parameters: 6738415616
[2023-09-06 20:31:15,230] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-06 20:31:15,428] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-06 20:31:15,430] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-06 20:31:15,430] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-06 20:31:15,430] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-06 20:31:15,430] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-06 20:31:15,430] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0280>
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-06 20:31:15,431] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-06 20:31:15,432] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-06 20:31:15,432] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Adam bought 3 kilograms of nuts and 2.5 kilograms of dried fruits at a store. One kilogram of nuts costs $12 and one kilogram of dried fruit costs $8. How much did his purchases cost?
Output: For the nuts Adam paid 3 * $12 = $<<3*12=36>>36.
And for dried fruits Adam paid 2.5 * $8 = $<<2.5*8=20>>20.
So in total for his purchases Adam paid $36 + $20 = $<<36+20=56>>56.
So the final answer is 56

Input: Johns goes to the gym 3 times a week.  He spends 1 hour each day lifting weight. Additionally, he also spends a third of his weightlifting time warming up and doing cardio each day.  How many hours does he spend at the gym a week?
Output: He spends 60/3=<<60/3=20>>20 minutes warming up
So he spends 60+20=<<60+20=80>>80 minutes at the gym per day
That means he spends 80*3=<<80*3=240>>240 minutes at the gym
So he spends 240/60=<<240/60=4>>4 hours at the gym a week
So the final answer is 4

Input: James has to refuel his plane.  It used to cost $200 to refill the tank.  He got an extra tank to double fuel capacity.  Fuel prices also went up by 20%.  How much does he pay now for fuel?
Output: The cost to fill a tank went up 200*.2=$<<200*.2=40>>40
So it cost 200+40=$<<200+40=240>>240 to fill the tank
That means he now pays 240*2=$<<240*2=480>>480
So the final answer is 480

Input: The number of goals scored in a game against Barca by exactly two players last season accounts for 20% of all goals scored in the league. If the players scored an equal number of goals, and the total number of goals scored in the league against Barca that season is 300, calculate the number of goals each of the two players scored.
Output: If the total number of goals scored in the league that season against Barca is 300, the two players scored 20/100*300=<<300*20/100=60>>60 goals.
If the players scored an equal number of goals, each scored 60/2=<<60/2=30>>30 goals.
So the final answer is 30

Input: Every day Tom drinks 5 12-oz cans of soda plus 64 ounces of water. How many ounces of fluid does he drink a week?
Output: He drinks 12 * 5 = <<12*5=60>>60 ounces of soda a day
So he drinks 60 + 64 = <<60+64=124>>124 ounces of liquid a day
So in total he drinks 124 * 7 = <<124*7=868>>868 ounces of liquid a week
So the final answer is 868

Input: Stella and Twinkle are filling up a truck with a capacity of 6000 stone blocks at the rate of 250 blocks per hour per person. They work for four hours and are then joined by 6 other people who also work at the same rate. How many hours did filling the truck take?
Output: Stella and Twinkle filled up the truck at the rate of 250 blocks per hour per person, a total of 2*250 = <<250*2=500>>500 blocks per hour for both.
After working for four hours, Stella and Twinkle had filled 4*500 = <<4*500=2000>>2000 blocks into the truck.
The number of blocks they had to put into the truck for it to be full is 6000-2000 = <<6000-2000=4000>>4000
When 6 more people joined Stella and Twinkle, a total of 2+6 = <<2+6=8>>8 people were filling the truck now.
Working at the rate of 250 blocks per person, the eight people filled the truck with 250*8 = <<250*8=2000>>2000 blocks in one hour.
If there were 4000 blocks that still needed to be put into the truck, the 8 people took 4000/2000 = <<4000/2000=2>>2 hours to fill the truck with the blocks.
The total time it took to fill up the tank is 4+2 = <<4+2=6>>6 hours.
So the final answer is 6

Input: Elijah drank 8.5 pints of coffee yesterday. Emilio drank 9.5 pints of water yesterday. How many cups of liquid did the two boys drink yesterday?
Output: Total drunk: 8.5 + 9.5 = <<8.5+9.5=18>>18 pints
18 pints * 2 = <<18*2=36>>36 cups
The two boys drank a total of 36 cups of liquid yesterday.
So the final answer is 36

Input: Doris works at the Widget Factory in the packing department. She puts 3 widgets in each carton, which are 4 inches wide, 4 inches long, and 5 inches tall. She then packs those cartons into a shipping box before sending it to the loading bay. The shipping boxes are 20 inches wide, 20 inches long, and 20 inches high. How many widgets get shipped in each shipping box?
Output: Each carton has an area of 4*4*5 = <<4*4*5=80>>80 square inches.
Each shipping box has an area of 20*20*20 = <<20*20*20=8000>>8000 square inches
The total number of cartons that will fit into each box is 8000/80 = <<8000/80=100>>100
Since there are 3 widgets in each carton, the total number of cartons in each box will be 3*100 = <<3*100=300>>300
So the final answer is 300

Input: Queenie earns $150 a day as a part-time clerk. She earns an additional $5 per hour as overtime pay. How much will Queenie receive for working 5 days with 4 hours overtime?
Output: Queenie will earn $150 x 5 = $<<150*5=750>>750 for working 5 days.
She will receive an additional $5 x 4 = $<<5*4=20>>20 for overtime pay.
Hence, Queenie will receive a total of $750 + $20 = $<<750+20=770>>770.
So the final answer is 770

Input: Jodi starts off walking 1 mile a day for 6 days a week.  On the second week, she walks 2 miles a day, 6 days a week.  On the third week, she walks 3 miles a day, 6 days a week. Finally on the fourth week, she walks 4 miles a day, 6 days a week.  How many miles has she walked in 4 weeks?
Output: The first week she walked 1 mile, 6 days a week for a total of 1*6 = <<1*6=6>>6 miles
The second week she walked 2 miles, 6 days a week for a total of 2*6 = <<2*6=12>>12 miles
The third week she walked 3 miles, 6 days a week for a total of 3*6 = <<3*6=18>>18 miles
On the fourth week, she walked 4 miles, 6 days a week for a total of 4*6 = <<4*6=24>>24 miles
Over four weeks she has walked 6+12+18+24 = <<6+12+18+24=60>>60 miles
So the final answer is 60

Input: A club is going to get additional members so that they will have 5 more than twice their current number of their members. If the club has 10 members now, how many additional members do they need?
Output: Twice the current number of members now is 10 * 2 = <<10*2=20>>20 members
So, the club wants to have 20 + 5 = <<20+5=25>>25 members.
Therefore, they need to get 25 - 10 = <<25-10=15>>15 members.
So the final answer is 15

Input: Andrea buys herself a pony for her 30th birthday. She pays $500/month to rent a pasture for it, $10 a day for food, and $60/lesson for two lessons a week. How much does she spend on her pony in a year?
Output: First find the annual cost of food: $10/day * 365 days = $<<10*365=3650>>3650
Then find how much she spends a week on lessons: $60/lesson * 2 lessons = $<<60*2=120>>120
Then multiply that weekly cost by the number of weeks per year to find the annual cost: $120/week * 52 weeks/year = $<<120*52=6240>>6240
Then find the annual cost of the pasture: $500/month * 12 months/year = $<<500*12=6000>>6000
Then add all the costs to find the total cost: $6000 + $6240 + $3650 = $<<6000+6240+3650=15890>>15890
So the final answer is 15890

Input: A pet shop has 2 puppies and some kittens. A puppy costs $20, and a kitten costs $15. If the stock is worth $100, how many kittens does the pet shop have?
Output: The 2 puppies cost 2 * 20 = <<2*20=40>>40 dollars
The stock is worth 100 dollars, meaning that the kittens cost 100 - 40 = <<100-40=60>>60 dollars
Since the cost of a kitten is 15 dollars, the pet shop has 60/15 = <<60/15=4>>4 kittens
So the final answer is 4

Input: Noah, who loves his Grammy, calls her every week to talk about his day. If each call lasts 30 minutes and he is charged $0.05 per call minute, how much would he be billed if he makes the calls for a year?
Output: A year has 52 weeks, so if he talks for 30 minutes every week, the total number of call hours is 52*30 = <<52*30=1560>>1560 minutes.
Since he is charged 0.05 cents per call, the total cost is 1560*.05 = <<1560*.05=78>>78 dollars
So the final answer is 78

Input: A merchant bought 15 keyboards and 25 printers for a total of $2050. If a keyboard costs $20, how much does a printer cost?
Output: 15 keyboards at $20 per keyboard cost a total of 15*$20=$<<15*20=300>>300
The total cost of printers and keyboards is $2050 so 25 printers cost $2050-$300=$<<2050-300=1750>>1750
1 printer cost $1750/25=$<<1750/25=70>>70
So the final answer is 70

Input: Gina has two bank accounts. Each account has a quarter of the balance in Betty's account. If Betty's account balance is $3,456, what is the combined balance of both Gina's accounts?
Output: Each of Gina's account has $3456 / 4 accounts = $<<3456/4=864>>864/account
Her combined account balance is therefore $864 + $864 = $<<864+864=1728>>1728
So the final answer is 1728

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o16-tgsm8k-s10-rTrue-m4096
                                                                                               Evaluating commonsenseqa :   1%|          | 1/100 [01:37<2:40:35, 97.33s/it]                                                                                                 Evaluating commonsenseqa :   2%|▏         | 2/100 [03:12<2:37:02, 96.15s/it]                                                                                                 Evaluating commonsenseqa :   3%|▎         | 3/100 [04:49<2:36:11, 96.61s/it]                                                                                                 Evaluating commonsenseqa :   4%|▍         | 4/100 [06:24<2:33:06, 95.69s/it]                                                                                                 Evaluating commonsenseqa :   5%|▌         | 5/100 [07:59<2:31:17, 95.55s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [09:33<2:29:02, 95.13s/it]                  Evaluating commonsenseqa :   7%|▋         | 7/100 [11:08<2:27:11, 94.97s/it]                  Evaluating commonsenseqa :   8%|▊         | 8/100 [12:43<2:25:52, 95.14s/it]                                                                                                 Evaluating commonsenseqa :   9%|▉         | 9/100 [14:19<2:24:34, 95.32s/it]                                                                                                 Evaluating commonsenseqa :  10%|█         | 10/100 [15:54<2:22:47, 95.19s/it]                                                                                                 Evaluating commonsenseqa :  11%|█         | 11/100 [17:29<2:21:00, 95.06s/it]                                                                                                 Evaluating commonsenseqa :  12%|█▏        | 12/100 [19:04<2:19:22, 95.03s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [20:39<2:17:52, 95.09s/it]                 Evaluating commonsenseqa :  14%|█▍        | 14/100 [22:14<2:16:12, 95.03s/it]                 Evaluating commonsenseqa :  15%|█▌        | 15/100 [23:48<2:14:23, 94.86s/it]                 Evaluating commonsenseqa :  16%|█▌        | 16/100 [25:24<2:13:05, 95.07s/it]                                                                                                   Evaluating commonsenseqa :  17%|█▋        | 17/100 [26:59<2:11:38, 95.16s/it]                                                                                                   Evaluating commonsenseqa :  18%|█▊        | 18/100 [28:35<2:10:21, 95.38s/it]                                                                                                   Evaluating commonsenseqa :  19%|█▉        | 19/100 [30:10<2:08:36, 95.26s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [31:46<2:07:27, 95.59s/it]                 Evaluating commonsenseqa :  21%|██        | 21/100 [33:22<2:05:57, 95.67s/it]                 Evaluating commonsenseqa :  22%|██▏       | 22/100 [34:59<2:04:44, 95.96s/it]:00, 109.88s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [3:02:46<00:00, 109.66s/it]
name: commonsenseqa | avg. gen lenth: 375.896 | time: 10968.997445583344s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu02 --master_port 19674 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o14-tgsm8k-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 14 11 12 13 14 15 True 4096 10
[2023-09-06 21:06:43,918] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o14-tgsm8k-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 14
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o14-tgsm8k-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 342358.45it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.69s/it]
 > number of parameters: 6738415616
[2023-09-06 21:06:57,002] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-06 21:06:57,210] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-06 21:06:57,212] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-06 21:06:57,212] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-06 21:06:57,212] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-06 21:06:57,212] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-06 21:06:57,212] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0250>
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-06 21:06:57,213] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-06 21:06:57,214] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-06 21:06:57,214] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Adam bought 3 kilograms of nuts and 2.5 kilograms of dried fruits at a store. One kilogram of nuts costs $12 and one kilogram of dried fruit costs $8. How much did his purchases cost?
Output: For the nuts Adam paid 3 * $12 = $<<3*12=36>>36.
And for dried fruits Adam paid 2.5 * $8 = $<<2.5*8=20>>20.
So in total for his purchases Adam paid $36 + $20 = $<<36+20=56>>56.
So the final answer is 56

Input: Johns goes to the gym 3 times a week.  He spends 1 hour each day lifting weight. Additionally, he also spends a third of his weightlifting time warming up and doing cardio each day.  How many hours does he spend at the gym a week?
Output: He spends 60/3=<<60/3=20>>20 minutes warming up
So he spends 60+20=<<60+20=80>>80 minutes at the gym per day
That means he spends 80*3=<<80*3=240>>240 minutes at the gym
So he spends 240/60=<<240/60=4>>4 hours at the gym a week
So the final answer is 4

Input: James has to refuel his plane.  It used to cost $200 to refill the tank.  He got an extra tank to double fuel capacity.  Fuel prices also went up by 20%.  How much does he pay now for fuel?
Output: The cost to fill a tank went up 200*.2=$<<200*.2=40>>40
So it cost 200+40=$<<200+40=240>>240 to fill the tank
That means he now pays 240*2=$<<240*2=480>>480
So the final answer is 480

Input: The number of goals scored in a game against Barca by exactly two players last season accounts for 20% of all goals scored in the league. If the players scored an equal number of goals, and the total number of goals scored in the league against Barca that season is 300, calculate the number of goals each of the two players scored.
Output: If the total number of goals scored in the league that season against Barca is 300, the two players scored 20/100*300=<<300*20/100=60>>60 goals.
If the players scored an equal number of goals, each scored 60/2=<<60/2=30>>30 goals.
So the final answer is 30

Input: Every day Tom drinks 5 12-oz cans of soda plus 64 ounces of water. How many ounces of fluid does he drink a week?
Output: He drinks 12 * 5 = <<12*5=60>>60 ounces of soda a day
So he drinks 60 + 64 = <<60+64=124>>124 ounces of liquid a day
So in total he drinks 124 * 7 = <<124*7=868>>868 ounces of liquid a week
So the final answer is 868

Input: Stella and Twinkle are filling up a truck with a capacity of 6000 stone blocks at the rate of 250 blocks per hour per person. They work for four hours and are then joined by 6 other people who also work at the same rate. How many hours did filling the truck take?
Output: Stella and Twinkle filled up the truck at the rate of 250 blocks per hour per person, a total of 2*250 = <<250*2=500>>500 blocks per hour for both.
After working for four hours, Stella and Twinkle had filled 4*500 = <<4*500=2000>>2000 blocks into the truck.
The number of blocks they had to put into the truck for it to be full is 6000-2000 = <<6000-2000=4000>>4000
When 6 more people joined Stella and Twinkle, a total of 2+6 = <<2+6=8>>8 people were filling the truck now.
Working at the rate of 250 blocks per person, the eight people filled the truck with 250*8 = <<250*8=2000>>2000 blocks in one hour.
If there were 4000 blocks that still needed to be put into the truck, the 8 people took 4000/2000 = <<4000/2000=2>>2 hours to fill the truck with the blocks.
The total time it took to fill up the tank is 4+2 = <<4+2=6>>6 hours.
So the final answer is 6

Input: Elijah drank 8.5 pints of coffee yesterday. Emilio drank 9.5 pints of water yesterday. How many cups of liquid did the two boys drink yesterday?
Output: Total drunk: 8.5 + 9.5 = <<8.5+9.5=18>>18 pints
18 pints * 2 = <<18*2=36>>36 cups
The two boys drank a total of 36 cups of liquid yesterday.
So the final answer is 36

Input: Doris works at the Widget Factory in the packing department. She puts 3 widgets in each carton, which are 4 inches wide, 4 inches long, and 5 inches tall. She then packs those cartons into a shipping box before sending it to the loading bay. The shipping boxes are 20 inches wide, 20 inches long, and 20 inches high. How many widgets get shipped in each shipping box?
Output: Each carton has an area of 4*4*5 = <<4*4*5=80>>80 square inches.
Each shipping box has an area of 20*20*20 = <<20*20*20=8000>>8000 square inches
The total number of cartons that will fit into each box is 8000/80 = <<8000/80=100>>100
Since there are 3 widgets in each carton, the total number of cartons in each box will be 3*100 = <<3*100=300>>300
So the final answer is 300

Input: Queenie earns $150 a day as a part-time clerk. She earns an additional $5 per hour as overtime pay. How much will Queenie receive for working 5 days with 4 hours overtime?
Output: Queenie will earn $150 x 5 = $<<150*5=750>>750 for working 5 days.
She will receive an additional $5 x 4 = $<<5*4=20>>20 for overtime pay.
Hence, Queenie will receive a total of $750 + $20 = $<<750+20=770>>770.
So the final answer is 770

Input: Jodi starts off walking 1 mile a day for 6 days a week.  On the second week, she walks 2 miles a day, 6 days a week.  On the third week, she walks 3 miles a day, 6 days a week. Finally on the fourth week, she walks 4 miles a day, 6 days a week.  How many miles has she walked in 4 weeks?
Output: The first week she walked 1 mile, 6 days a week for a total of 1*6 = <<1*6=6>>6 miles
The second week she walked 2 miles, 6 days a week for a total of 2*6 = <<2*6=12>>12 miles
The third week she walked 3 miles, 6 days a week for a total of 3*6 = <<3*6=18>>18 miles
On the fourth week, she walked 4 miles, 6 days a week for a total of 4*6 = <<4*6=24>>24 miles
Over four weeks she has walked 6+12+18+24 = <<6+12+18+24=60>>60 miles
So the final answer is 60

Input: A club is going to get additional members so that they will have 5 more than twice their current number of their members. If the club has 10 members now, how many additional members do they need?
Output: Twice the current number of members now is 10 * 2 = <<10*2=20>>20 members
So, the club wants to have 20 + 5 = <<20+5=25>>25 members.
Therefore, they need to get 25 - 10 = <<25-10=15>>15 members.
So the final answer is 15

Input: Andrea buys herself a pony for her 30th birthday. She pays $500/month to rent a pasture for it, $10 a day for food, and $60/lesson for two lessons a week. How much does she spend on her pony in a year?
Output: First find the annual cost of food: $10/day * 365 days = $<<10*365=3650>>3650
Then find how much she spends a week on lessons: $60/lesson * 2 lessons = $<<60*2=120>>120
Then multiply that weekly cost by the number of weeks per year to find the annual cost: $120/week * 52 weeks/year = $<<120*52=6240>>6240
Then find the annual cost of the pasture: $500/month * 12 months/year = $<<500*12=6000>>6000
Then add all the costs to find the total cost: $6000 + $6240 + $3650 = $<<6000+6240+3650=15890>>15890
So the final answer is 15890

Input: A pet shop has 2 puppies and some kittens. A puppy costs $20, and a kitten costs $15. If the stock is worth $100, how many kittens does the pet shop have?
Output: The 2 puppies cost 2 * 20 = <<2*20=40>>40 dollars
The stock is worth 100 dollars, meaning that the kittens cost 100 - 40 = <<100-40=60>>60 dollars
Since the cost of a kitten is 15 dollars, the pet shop has 60/15 = <<60/15=4>>4 kittens
So the final answer is 4

Input: Noah, who loves his Grammy, calls her every week to talk about his day. If each call lasts 30 minutes and he is charged $0.05 per call minute, how much would he be billed if he makes the calls for a year?
Output: A year has 52 weeks, so if he talks for 30 minutes every week, the total number of call hours is 52*30 = <<52*30=1560>>1560 minutes.
Since he is charged 0.05 cents per call, the total cost is 1560*.05 = <<1560*.05=78>>78 dollars
So the final answer is 78

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o14-tgsm8k-s10-rTrue-m4096
                                                                                    Evaluating commonsenseqa :   1%|          | 1/100 [01:46<2:56:10, 106.77s/it]                                                                                    Evaluating commonsenseqa :   2%|▏         | 2/100 [03:31<2:52:23, 105.55s/it]                                                                                    Evaluating commonsenseqa :   3%|▎         | 3/100 [05:15<2:49:28, 104.83s/it]                                                                                    Evaluating commonsenseqa :   4%|▍         | 4/100 [06:58<2:46:21, 103.98s/it]                                                                                    Evaluating commonsenseqa :   5%|▌         | 5/100 [08:40<2:43:31, 103.28s/it]                                                                                    Evaluating commonsenseqa :   6%|▌         | 6/100 [10:23<2:41:49, 103.29s/it]                                                                                    Evaluating commonsenseqa :   7%|▋         | 7/100 [12:06<2:40:07, 103.31s/it]                                                                                    Evaluating commonsenseqa :   8%|▊         | 8/100 [13:49<2:37:53, 102.97s/it]    Evaluating commonsenseqa :   9%|▉         | 9/100 [15:33<2:36:38, 103.28s/it]      Evaluating commonsenseqa :  10%|█         | 10/100 [17:15<2:34:31, 103.02s/it]     Evaluating commonsenseqa :  11%|█         | 11/100 [18:59<2:33:06, 103.22s/it]     Evaluating commonsenseqa :  12%|█▏        | 12/100 [20:42<2:31:20, 103.18s/it]   Evaluating commonsenseqa :  13%|█▎        | 13/100 [22:25<2:29:30, 103.11s/it]   Evaluating commonsenseqa :  14%|█▍        | 14/100 [24:09<2:28:06, 103.34s/it]                                                                                           Evaluating commonsenseqa :  15%|█▌        | 15/100 [25:51<2:26:04, 103.11s/it]                                                                                        Evaluating commonsenseqa :  16%|█▌        | 16/100 [27:34<2:24:06, 102.94s/it]                                                                                        Evaluating commonsenseqa :  17%|█▋        | 17/100 [29:17<2:22:39, 103.13s/it]                                                                                        Evaluating commonsenseqa :  18%|█▊        | 18/100 [30:59<2:20:29, 102.80s/it]                                                                                          Evaluating commonsenseqa :  19%|█▉        | 19/100 [32:42<2:18:50, 102.84s/it]                                                                                          Evaluating commonsenseqa :  20%|██        | 20/100 [34:27<2:18:03, 103.55s/it]                                                                                          Evaluating commonsenseqa :  21%|██        | 21/100 [36:09<2:15:42, 103.08s/it]                                                                                          Evaluating commonsenseqa :  22%|██▏       | 22/100 [37:54<2:14:30, 103.46s/it]                                                                                          Evaluating commonsenseqa :  23%|██▎       | 23/100 [39:36<2:12:09, 102.98s/it]                                                                                          Evaluating commonsenseqa :  24%|██▍       | 24/100 [41:19<2:10:47, 103.25s/it]     Evaluating commonsenseqa :  25%|██▌       | 25/100 [43:02<2:08:44, 102.99s/it]     Evaluating commonsenseqa :  26%|██▌       | 26/100 [44:45<2:07:03, 103.01s/it]     Evaluating commonsenseqa :  27%|██▋       | 27/100 [46:28<2:05:17, 102.98s/it]                                                                                                 Evaluating commonsenseqa :  28%|██▊       | 28/100 [48:11<2:03:48, 103.17s/it]                                                                                            Evaluating commonsenseqa :  29%|██▉       | 29/100 [49:55<2:02:15, 103.32s/it]                                                                                            Evaluating commonsenseqa :  30%|███       | 30/100 [51:38<2:00:21, 103.16s/it]                                                                                            Evaluating commonsenseqa :  31%|███       | 31/100 [53:22<1:58:56, 103.43s/it]                                                                                            Evaluating commonsenseqa :  32%|███▏      | 32/100 [55:04<1:56:46, 103.04s/it]                                                                                            Evaluating commonsenseqa :  33%|███▎      | 33/100 [56:47<1:55:08, 103.11s/it]                                                                                            Evaluating commonsenseqa :  34%|███▍      | 34/100 [58:30<1:53:09, 102.87s/it]                                                                                            Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:00:12<1:51:17, 102.73s/it]                                                                                            Evaluating commonsenseqa :  36%|███▌      | 36/100 [1:01:57<1:50:14, 103.35s/it]   Evaluating commonsenseqa :  37%|███▋      | 37/100 [1:03:42<1:49:01, 103.83s/it]     Evaluating commonsenseqa :  38%|███▊      | 38/100 [1:05:25<1:47:12, 103.75s/it]     Evaluating commonsenseqa :  39%|███▉      | 39/100 [1:07:09<1:45:30, 103.78s/it]                                                                                               Evaluating commonsenseqa :  40%|████      | 40/100 [1:08:51<1:43:17, 103.28s/it]                                                                                            Evaluating commonsenseqa :  41%|████      | 41/100 [1:10:34<1:41:18, 103.02s/it]                                                                                            Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:12:16<1:39:17, 102.71s/it]                                                                                            Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:13:59<1:37:45, 102.90s/it]                                                                                            Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:15:43<1:36:26, 103.33s/it]                                                                                            Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:17:27<1:34:45, 103.37s/it]                                                                                            Evaluating commonsenseqa :  46%|████▌     | 46/100 [1:19:09<1:32:43, 103.04s/it]                                                                                              Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:20:53<1:31:16, 103.33s/it]   Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:22:36<1:29:25, 103.19s/it]   Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:24:19<1:27:39, 103.12s/it]   Evaluating commonsenseqa :  50%|█████     | 50/100 [1:26:02<1:25:48, 102.96s/it]   Evaluating commonsenseqa :  51%|█████     | 51/100 [1:27:46<1:24:24, 103.36s/it]                                                                                                 Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:29:29<1:22:40, 103.34s/it]                                                                                              Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:31:11<1:20:40, 102.99s/it]                                                                                              Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:32:53<1:18:43, 102.69s/it]                                                                                              Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:34:36<1:17:03, 102.73s/it]                                                                                                Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:36:19<1:15:27, 102.90s/it]                                                                                                Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:38:03<1:13:53, 103.10s/it]                                                                                                Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:39:45<1:11:58, 102.81s/it]                                                                                                Evaluating commonsenseqa :  59%|█████▉    | 59/100 [1:41:28<1:10:19, 102.92s/it]   Evaluating commonsenseqa :  60%|██████    | 60/100 [1:43:10<1:08:25, 102.63s/it]   Evaluating commonsenseqa :  61%|██████    | 61/100 [1:44:53<1:06:41, 102.60s/it]   Evaluating commonsenseqa :  62%|██████▏   | 62/100 [1:46:34<1:04:45, 102.25s/it] Evaluating commonsenseqa :  63%|██████▎   | 63/100 [1:48:19<1:03:29, 102.95s/it]                                                                                                 Evaluating commonsenseqa :  64%|██████▍   | 64/100 [1:50:02<1:01:46, 102.96s/it]                                                                                                  Evaluating commonsenseqa :  65%|██████▌   | 65/100 [1:51:45<1:00:10, 103.17s/it]                                                                                                  Evaluating commonsenseqa :  66%|██████▌   | 66/100 [1:53:30<58:42, 103.61s/it]                                                                                                    Evaluating commonsenseqa :  67%|██████▋   | 67/100 [1:55:14<57:03, 103.75s/it]                                                                                                  Evaluating commonsenseqa :  68%|██████▊   | 68/100 [1:56:57<55:12, 103.52s/it]                                                                                                  Evaluating commonsenseqa :  69%|██████▉   | 69/100 [1:58:40<53:21, 103.27s/it]                                                                                                  Evaluating commonsenseqa :  70%|███████   | 70/100 [2:00:23<51:33, 103.12s/it]                                                                                                  Evaluating commonsenseqa :  71%|███████   | 71/100 [2:02:06<49:52, 103.17s/it]                                                                                                                                                                                                                                                                                     Evaluating commonsenseqa :  72%|███████▏  | 72/100 [2:03:49<48:04, 103.02s/it]orkspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o17-tgsm8k-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 17 16 17 18 19 20 True 4096 10
[2023-09-06 23:10:52,611] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o17-tgsm8k-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 17
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o17-tgsm8k-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 302516.85it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.51s/it]
 > number of parameters: 6738415616
[2023-09-06 23:11:03,300] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-06 23:11:03,501] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-06 23:11:03,503] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-06 23:11:03,503] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-06 23:11:03,503] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-06 23:11:03,503] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-06 23:11:03,503] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-06 23:11:03,503] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-06 23:11:03,503] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-06 23:11:03,503] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-06 23:11:03,503] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0280>
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-06 23:11:03,504] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-06 23:11:03,505] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-06 23:11:03,505] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Adam bought 3 kilograms of nuts and 2.5 kilograms of dried fruits at a store. One kilogram of nuts costs $12 and one kilogram of dried fruit costs $8. How much did his purchases cost?
Output: For the nuts Adam paid 3 * $12 = $<<3*12=36>>36.
And for dried fruits Adam paid 2.5 * $8 = $<<2.5*8=20>>20.
So in total for his purchases Adam paid $36 + $20 = $<<36+20=56>>56.
So the final answer is 56

Input: Johns goes to the gym 3 times a week.  He spends 1 hour each day lifting weight. Additionally, he also spends a third of his weightlifting time warming up and doing cardio each day.  How many hours does he spend at the gym a week?
Output: He spends 60/3=<<60/3=20>>20 minutes warming up
So he spends 60+20=<<60+20=80>>80 minutes at the gym per day
That means he spends 80*3=<<80*3=240>>240 minutes at the gym
So he spends 240/60=<<240/60=4>>4 hours at the gym a week
So the final answer is 4

Input: James has to refuel his plane.  It used to cost $200 to refill the tank.  He got an extra tank to double fuel capacity.  Fuel prices also went up by 20%.  How much does he pay now for fuel?
Output: The cost to fill a tank went up 200*.2=$<<200*.2=40>>40
So it cost 200+40=$<<200+40=240>>240 to fill the tank
That means he now pays 240*2=$<<240*2=480>>480
So the final answer is 480

Input: The number of goals scored in a game against Barca by exactly two players last season accounts for 20% of all goals scored in the league. If the players scored an equal number of goals, and the total number of goals scored in the league against Barca that season is 300, calculate the number of goals each of the two players scored.
Output: If the total number of goals scored in the league that season against Barca is 300, the two players scored 20/100*300=<<300*20/100=60>>60 goals.
If the players scored an equal number of goals, each scored 60/2=<<60/2=30>>30 goals.
So the final answer is 30

Input: Every day Tom drinks 5 12-oz cans of soda plus 64 ounces of water. How many ounces of fluid does he drink a week?
Output: He drinks 12 * 5 = <<12*5=60>>60 ounces of soda a day
So he drinks 60 + 64 = <<60+64=124>>124 ounces of liquid a day
So in total he drinks 124 * 7 = <<124*7=868>>868 ounces of liquid a week
So the final answer is 868

Input: Stella and Twinkle are filling up a truck with a capacity of 6000 stone blocks at the rate of 250 blocks per hour per person. They work for four hours and are then joined by 6 other people who also work at the same rate. How many hours did filling the truck take?
Output: Stella and Twinkle filled up the truck at the rate of 250 blocks per hour per person, a total of 2*250 = <<250*2=500>>500 blocks per hour for both.
After working for four hours, Stella and Twinkle had filled 4*500 = <<4*500=2000>>2000 blocks into the truck.
The number of blocks they had to put into the truck for it to be full is 6000-2000 = <<6000-2000=4000>>4000
When 6 more people joined Stella and Twinkle, a total of 2+6 = <<2+6=8>>8 people were filling the truck now.
Working at the rate of 250 blocks per person, the eight people filled the truck with 250*8 = <<250*8=2000>>2000 blocks in one hour.
If there were 4000 blocks that still needed to be put into the truck, the 8 people took 4000/2000 = <<4000/2000=2>>2 hours to fill the truck with the blocks.
The total time it took to fill up the tank is 4+2 = <<4+2=6>>6 hours.
So the final answer is 6

Input: Elijah drank 8.5 pints of coffee yesterday. Emilio drank 9.5 pints of water yesterday. How many cups of liquid did the two boys drink yesterday?
Output: Total drunk: 8.5 + 9.5 = <<8.5+9.5=18>>18 pints
18 pints * 2 = <<18*2=36>>36 cups
The two boys drank a total of 36 cups of liquid yesterday.
So the final answer is 36

Input: Doris works at the Widget Factory in the packing department. She puts 3 widgets in each carton, which are 4 inches wide, 4 inches long, and 5 inches tall. She then packs those cartons into a shipping box before sending it to the loading bay. The shipping boxes are 20 inches wide, 20 inches long, and 20 inches high. How many widgets get shipped in each shipping box?
Output: Each carton has an area of 4*4*5 = <<4*4*5=80>>80 square inches.
Each shipping box has an area of 20*20*20 = <<20*20*20=8000>>8000 square inches
The total number of cartons that will fit into each box is 8000/80 = <<8000/80=100>>100
Since there are 3 widgets in each carton, the total number of cartons in each box will be 3*100 = <<3*100=300>>300
So the final answer is 300

Input: Queenie earns $150 a day as a part-time clerk. She earns an additional $5 per hour as overtime pay. How much will Queenie receive for working 5 days with 4 hours overtime?
Output: Queenie will earn $150 x 5 = $<<150*5=750>>750 for working 5 days.
She will receive an additional $5 x 4 = $<<5*4=20>>20 for overtime pay.
Hence, Queenie will receive a total of $750 + $20 = $<<750+20=770>>770.
So the final answer is 770

Input: Jodi starts off walking 1 mile a day for 6 days a week.  On the second week, she walks 2 miles a day, 6 days a week.  On the third week, she walks 3 miles a day, 6 days a week. Finally on the fourth week, she walks 4 miles a day, 6 days a week.  How many miles has she walked in 4 weeks?
Output: The first week she walked 1 mile, 6 days a week for a total of 1*6 = <<1*6=6>>6 miles
The second week she walked 2 miles, 6 days a week for a total of 2*6 = <<2*6=12>>12 miles
The third week she walked 3 miles, 6 days a week for a total of 3*6 = <<3*6=18>>18 miles
On the fourth week, she walked 4 miles, 6 days a week for a total of 4*6 = <<4*6=24>>24 miles
Over four weeks she has walked 6+12+18+24 = <<6+12+18+24=60>>60 miles
So the final answer is 60

Input: A club is going to get additional members so that they will have 5 more than twice their current number of their members. If the club has 10 members now, how many additional members do they need?
Output: Twice the current number of members now is 10 * 2 = <<10*2=20>>20 members
So, the club wants to have 20 + 5 = <<20+5=25>>25 members.
Therefore, they need to get 25 - 10 = <<25-10=15>>15 members.
So the final answer is 15

Input: Andrea buys herself a pony for her 30th birthday. She pays $500/month to rent a pasture for it, $10 a day for food, and $60/lesson for two lessons a week. How much does she spend on her pony in a year?
Output: First find the annual cost of food: $10/day * 365 days = $<<10*365=3650>>3650
Then find how much she spends a week on lessons: $60/lesson * 2 lessons = $<<60*2=120>>120
Then multiply that weekly cost by the number of weeks per year to find the annual cost: $120/week * 52 weeks/year = $<<120*52=6240>>6240
Then find the annual cost of the pasture: $500/month * 12 months/year = $<<500*12=6000>>6000
Then add all the costs to find the total cost: $6000 + $6240 + $3650 = $<<6000+6240+3650=15890>>15890
So the final answer is 15890

Input: A pet shop has 2 puppies and some kittens. A puppy costs $20, and a kitten costs $15. If the stock is worth $100, how many kittens does the pet shop have?
Output: The 2 puppies cost 2 * 20 = <<2*20=40>>40 dollars
The stock is worth 100 dollars, meaning that the kittens cost 100 - 40 = <<100-40=60>>60 dollars
Since the cost of a kitten is 15 dollars, the pet shop has 60/15 = <<60/15=4>>4 kittens
So the final answer is 4

Input: Noah, who loves his Grammy, calls her every week to talk about his day. If each call lasts 30 minutes and he is charged $0.05 per call minute, how much would he be billed if he makes the calls for a year?
Output: A year has 52 weeks, so if he talks for 30 minutes every week, the total number of call hours is 52*30 = <<52*30=1560>>1560 minutes.
Since he is charged 0.05 cents per call, the total cost is 1560*.05 = <<1560*.05=78>>78 dollars
So the final answer is 78

Input: A merchant bought 15 keyboards and 25 printers for a total of $2050. If a keyboard costs $20, how much does a printer cost?
Output: 15 keyboards at $20 per keyboard cost a total of 15*$20=$<<15*20=300>>300
The total cost of printers and keyboards is $2050 so 25 printers cost $2050-$300=$<<2050-300=1750>>1750
1 printer cost $1750/25=$<<1750/25=70>>70
So the final answer is 70

Input: Gina has two bank accounts. Each account has a quarter of the balance in Betty's account. If Betty's account balance is $3,456, what is the combined balance of both Gina's accounts?
Output: Each of Gina's account has $3456 / 4 accounts = $<<3456/4=864>>864/account
Her combined account balance is therefore $864 + $864 = $<<864+864=1728>>1728
So the final answer is 1728

Input: Zain has 10 more of each coin than Emerie. If Emerie has six quarters, seven dimes, and five nickels, how many coins does Zain have?
Output: If Emerie has six quarters, Zain has 6+10 = <<6+10=16>>16 quarters.
At the same time, Zain has 7+10 = <<7+10=17>>17 dimes, ten more than Emerie.
The total number of quarters and dimes Zain has is 17+16 = <<17+16=33>>33 coins.
Zain also has 10 more nickels than Emerie, a total of 10+5 = <<10+5=15>>15 nickels.
In total, Zain has 33+15 = <<33+15=48>>48 coins.
So the final answer is 48

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o17-tgsm8k-s10-rTrue-m4096
Evaluating commonsenseqa :   0%|          | 0/100 [00:08<?, ?it/s]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 97, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 94, in main
    inference_main(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 90, in inference_main
    query_ids, response_ids, answers, indices = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 65, in run_model
    gen_out = model.generate(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 1454, in generate
    return self.sample(
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 2568, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3547370) of binary: /home/ylu130/.conda/envs/ood/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.conda/envs/ood/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-06_23:11:15
  host      : icgpu06.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3547370)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o18-tgsm8k-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 18 16 17 18 19 20 True 4096 10
[2023-09-06 23:11:19,359] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o18-tgsm8k-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 18
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o18-tgsm8k-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 283599.18it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.72s/it]
 > number of parameters: 6738415616
[2023-09-06 23:11:28,441] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-06 23:11:28,643] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-06 23:11:28,644] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-06 23:11:28,644] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c2280>
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-06 23:11:28,645] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-06 23:11:28,646] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-06 23:11:28,646] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Adam bought 3 kilograms of nuts and 2.5 kilograms of dried fruits at a store. One kilogram of nuts costs $12 and one kilogram of dried fruit costs $8. How much did his purchases cost?
Output: For the nuts Adam paid 3 * $12 = $<<3*12=36>>36.
And for dried fruits Adam paid 2.5 * $8 = $<<2.5*8=20>>20.
So in total for his purchases Adam paid $36 + $20 = $<<36+20=56>>56.
So the final answer is 56

Input: Johns goes to the gym 3 times a week.  He spends 1 hour each day lifting weight. Additionally, he also spends a third of his weightlifting time warming up and doing cardio each day.  How many hours does he spend at the gym a week?
Output: He spends 60/3=<<60/3=20>>20 minutes warming up
So he spends 60+20=<<60+20=80>>80 minutes at the gym per day
That means he spends 80*3=<<80*3=240>>240 minutes at the gym
So he spends 240/60=<<240/60=4>>4 hours at the gym a week
So the final answer is 4

Input: James has to refuel his plane.  It used to cost $200 to refill the tank.  He got an extra tank to double fuel capacity.  Fuel prices also went up by 20%.  How much does he pay now for fuel?
Output: The cost to fill a tank went up 200*.2=$<<200*.2=40>>40
So it cost 200+40=$<<200+40=240>>240 to fill the tank
That means he now pays 240*2=$<<240*2=480>>480
So the final answer is 480

Input: The number of goals scored in a game against Barca by exactly two players last season accounts for 20% of all goals scored in the league. If the players scored an equal number of goals, and the total number of goals scored in the league against Barca that season is 300, calculate the number of goals each of the two players scored.
Output: If the total number of goals scored in the league that season against Barca is 300, the two players scored 20/100*300=<<300*20/100=60>>60 goals.
If the players scored an equal number of goals, each scored 60/2=<<60/2=30>>30 goals.
So the final answer is 30

Input: Every day Tom drinks 5 12-oz cans of soda plus 64 ounces of water. How many ounces of fluid does he drink a week?
Output: He drinks 12 * 5 = <<12*5=60>>60 ounces of soda a day
So he drinks 60 + 64 = <<60+64=124>>124 ounces of liquid a day
So in total he drinks 124 * 7 = <<124*7=868>>868 ounces of liquid a week
So the final answer is 868

Input: Stella and Twinkle are filling up a truck with a capacity of 6000 stone blocks at the rate of 250 blocks per hour per person. They work for four hours and are then joined by 6 other people who also work at the same rate. How many hours did filling the truck take?
Output: Stella and Twinkle filled up the truck at the rate of 250 blocks per hour per person, a total of 2*250 = <<250*2=500>>500 blocks per hour for both.
After working for four hours, Stella and Twinkle had filled 4*500 = <<4*500=2000>>2000 blocks into the truck.
The number of blocks they had to put into the truck for it to be full is 6000-2000 = <<6000-2000=4000>>4000
When 6 more people joined Stella and Twinkle, a total of 2+6 = <<2+6=8>>8 people were filling the truck now.
Working at the rate of 250 blocks per person, the eight people filled the truck with 250*8 = <<250*8=2000>>2000 blocks in one hour.
If there were 4000 blocks that still needed to be put into the truck, the 8 people took 4000/2000 = <<4000/2000=2>>2 hours to fill the truck with the blocks.
The total time it took to fill up the tank is 4+2 = <<4+2=6>>6 hours.
So the final answer is 6

Input: Elijah drank 8.5 pints of coffee yesterday. Emilio drank 9.5 pints of water yesterday. How many cups of liquid did the two boys drink yesterday?
Output: Total drunk: 8.5 + 9.5 = <<8.5+9.5=18>>18 pints
18 pints * 2 = <<18*2=36>>36 cups
The two boys drank a total of 36 cups of liquid yesterday.
So the final answer is 36

Input: Doris works at the Widget Factory in the packing department. She puts 3 widgets in each carton, which are 4 inches wide, 4 inches long, and 5 inches tall. She then packs those cartons into a shipping box before sending it to the loading bay. The shipping boxes are 20 inches wide, 20 inches long, and 20 inches high. How many widgets get shipped in each shipping box?
Output: Each carton has an area of 4*4*5 = <<4*4*5=80>>80 square inches.
Each shipping box has an area of 20*20*20 = <<20*20*20=8000>>8000 square inches
The total number of cartons that will fit into each box is 8000/80 = <<8000/80=100>>100
Since there are 3 widgets in each carton, the total number of cartons in each box will be 3*100 = <<3*100=300>>300
So the final answer is 300

Input: Queenie earns $150 a day as a part-time clerk. She earns an additional $5 per hour as overtime pay. How much will Queenie receive for working 5 days with 4 hours overtime?
Output: Queenie will earn $150 x 5 = $<<150*5=750>>750 for working 5 days.
She will receive an additional $5 x 4 = $<<5*4=20>>20 for overtime pay.
Hence, Queenie will receive a total of $750 + $20 = $<<750+20=770>>770.
So the final answer is 770

Input: Jodi starts off walking 1 mile a day for 6 days a week.  On the second week, she walks 2 miles a day, 6 days a week.  On the third week, she walks 3 miles a day, 6 days a week. Finally on the fourth week, she walks 4 miles a day, 6 days a week.  How many miles has she walked in 4 weeks?
Output: The first week she walked 1 mile, 6 days a week for a total of 1*6 = <<1*6=6>>6 miles
The second week she walked 2 miles, 6 days a week for a total of 2*6 = <<2*6=12>>12 miles
The third week she walked 3 miles, 6 days a week for a total of 3*6 = <<3*6=18>>18 miles
On the fourth week, she walked 4 miles, 6 days a week for a total of 4*6 = <<4*6=24>>24 miles
Over four weeks she has walked 6+12+18+24 = <<6+12+18+24=60>>60 miles
So the final answer is 60

Input: A club is going to get additional members so that they will have 5 more than twice their current number of their members. If the club has 10 members now, how many additional members do they need?
Output: Twice the current number of members now is 10 * 2 = <<10*2=20>>20 members
So, the club wants to have 20 + 5 = <<20+5=25>>25 members.
Therefore, they need to get 25 - 10 = <<25-10=15>>15 members.
So the final answer is 15

Input: Andrea buys herself a pony for her 30th birthday. She pays $500/month to rent a pasture for it, $10 a day for food, and $60/lesson for two lessons a week. How much does she spend on her pony in a year?
Output: First find the annual cost of food: $10/day * 365 days = $<<10*365=3650>>3650
Then find how much she spends a week on lessons: $60/lesson * 2 lessons = $<<60*2=120>>120
Then multiply that weekly cost by the number of weeks per year to find the annual cost: $120/week * 52 weeks/year = $<<120*52=6240>>6240
Then find the annual cost of the pasture: $500/month * 12 months/year = $<<500*12=6000>>6000
Then add all the costs to find the total cost: $6000 + $6240 + $3650 = $<<6000+6240+3650=15890>>15890
So the final answer is 15890

Input: A pet shop has 2 puppies and some kittens. A puppy costs $20, and a kitten costs $15. If the stock is worth $100, how many kittens does the pet shop have?
Output: The 2 puppies cost 2 * 20 = <<2*20=40>>40 dollars
The stock is worth 100 dollars, meaning that the kittens cost 100 - 40 = <<100-40=60>>60 dollars
Since the cost of a kitten is 15 dollars, the pet shop has 60/15 = <<60/15=4>>4 kittens
So the final answer is 4

Input: Noah, who loves his Grammy, calls her every week to talk about his day. If each call lasts 30 minutes and he is charged $0.05 per call minute, how much would he be billed if he makes the calls for a year?
Output: A year has 52 weeks, so if he talks for 30 minutes every week, the total number of call hours is 52*30 = <<52*30=1560>>1560 minutes.
Since he is charged 0.05 cents per call, the total cost is 1560*.05 = <<1560*.05=78>>78 dollars
So the final answer is 78

Input: A merchant bought 15 keyboards and 25 printers for a total of $2050. If a keyboard costs $20, how much does a printer cost?
Output: 15 keyboards at $20 per keyboard cost a total of 15*$20=$<<15*20=300>>300
The total cost of printers and keyboards is $2050 so 25 printers cost $2050-$300=$<<2050-300=1750>>1750
1 printer cost $1750/25=$<<1750/25=70>>70
So the final answer is 70

Input: Gina has two bank accounts. Each account has a quarter of the balance in Betty's account. If Betty's account balance is $3,456, what is the combined balance of both Gina's accounts?
Output: Each of Gina's account has $3456 / 4 accounts = $<<3456/4=864>>864/account
Her combined account balance is therefore $864 + $864 = $<<864+864=1728>>1728
So the final answer is 1728

Input: Zain has 10 more of each coin than Emerie. If Emerie has six quarters, seven dimes, and five nickels, how many coins does Zain have?
Output: If Emerie has six quarters, Zain has 6+10 = <<6+10=16>>16 quarters.
At the same time, Zain has 7+10 = <<7+10=17>>17 dimes, ten more than Emerie.
The total number of quarters and dimes Zain has is 17+16 = <<17+16=33>>33 coins.
Zain also has 10 more nickels than Emerie, a total of 10+5 = <<10+5=15>>15 nickels.
In total, Zain has 33+15 = <<33+15=48>>48 coins.
So the final answer is 48

Input: Adonis is playing a prank on his dad by replacing his shampoo with hot sauce. Every day, after his dad showers, Adonis replaces the shampoo with 1/2 an ounce of hot sauce. He knows his dad uses 1 oz of shampoo a day from a new 10 oz bottle that no one else uses. After 4 days, what percentage of the liquid in the bottle is hot sauce?
Output: Each day, the shampoo bottle goes down 1/2 an ounce because 1 - (1/2) = 1/2
After four days the shampoo bottle is 2 ounces lower because 4 x (1/2) = <<4*(1/2)=2>>2
After four days, the shampoo bottle has 8 ounces of liquid left because 10 -2 = <<10-2=8>>8
After four days the shampoo bottle has 2 ounces of hot sauce 4 x (1/2) = <<4*(1/2)=2>>2
The proportion of hot sauce in the bottle is.25 because 2 / 8 = <<2/8=.25>>.25
The percentage of hot sauce is 25% because.25 x 100 = <<.25*100=25>>25
So the final answer is 25

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o18-tgsm8k-s10-rTrue-m4096
                                                                                               Evaluating commonsenseqa :   1%|          | 1/100 [01:29<2:28:05, 89.75s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [02:58<2:25:10, 88.88s/it]                Evaluating commonsenseqa :   3%|▎         | 3/100 [04:26<2:23:05, 88.51s/it]                Evaluating commonsenseqa :   4%|▍         | 4/100 [05:54<2:21:20, 88.34s/it]                Evaluating commonsenseqa :   5%|▌         | 5/100 [07:21<2:19:27, 88.08s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [08:50<2:18:05, 88.14s/it]                Evaluating commonsenseqa :   7%|▋         | 7/100 [10:17<2:16:27, 88.04s/it]                Evaluating commonsenseqa :   8%|▊         | 8/100 [11:45<2:14:41, 87.84s/it]                Evaluating commonsenseqa :   9%|▉         | 9/100 [13:13<2:13:10, 87.80s/it]                Evaluating commonsenseqa :  10%|█         | 10/100 [14:40<2:11:36, 87.74s/it]               Evaluating commonsenseqa :  11%|█         | 11/100 [16:07<2:09:57, 87.61s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [17:35<2:08:32, 87.65s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [19:03<2:07:04, 87.63s/it]               Evaluating commonsenseqa :  14%|█▍        | 14/100 [20:30<2:05:35, 87.63s/it]               Evaluating commonsenseqa :  15%|█▌        | 15/100 [21:58<2:04:04, 87.58s/it]               Evaluating commonsenseqa :  16%|█▌        | 16/100 [23:25<2:02:37, 87.59s/it]               Evaluating commonsenseqa :  17%|█▋        | 17/100 [24:53<2:01:10, 87.59s/it]               Evaluating commonsenseqa :  18%|█▊        | 18/100 [26:20<1:59:36, 87.52s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [27:48<1:58:06, 87.49s/it]               Evaluating commonsenseqa :  20%|██        | 20/100 [29:15<1:56:41, 87.51s/it]               Evaluating commonsenseqa :  21%|██        | 21/100 [30:43<1:55:20, 87.60s/it]               Evaluating commonsenseqa :  22%|██▏       | 22/100 [32:11<1:53:51, 87.59s/it]             Evaluating commonsenseqa :  23%|██▎       | 23/100 [33:39<1:52:31, 87.69s/it]               Evaluating commonsenseqa :  24%|██▍       | 24/100 [35:06<1:51:03, 87.68s/it]               Evaluating commonsenseqa :  25%|██▌       | 25/100 [36:34<1:49:32, 87.64s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [38:01<1:47:58, 87.55s/it]               Evaluating commonsenseqa :  27%|██▋       | 27/100 [39:29<1:46:31, 87.56s/it]               Evaluating commonsenseqa :  28%|██▊       | 28/100 [40:56<1:45:07, 87.60s/it]               Evaluating commonsenseqa :  29%|██▉       | 29/100 [42:24<1:43:32, 87.50s/it]               Evaluating commonsenseqa :  30%|███       | 30/100 [43:51<1:42:00, 87.44s/it]               Evaluating commonsenseqa :  31%|███       | 31/100 [45:19<1:40:41, 87.56s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [46:46<1:39:13, 87.55s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [48:14<1:37:43, 87.52s/it]0, 103.03s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:52:06<00:00, 103.27s/it]
name: commonsenseqa | avg. gen lenth: 376.415 | time: 10329.489838838577s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu02 --master_port 19674 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o15-tgsm8k-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 15 11 12 13 14 15 True 4096 10
[2023-09-06 23:59:11,383] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o15-tgsm8k-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 15
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o15-tgsm8k-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 327031.63it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.82s/it]
 > number of parameters: 6738415616
[2023-09-06 23:59:22,726] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-06 23:59:22,928] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-06 23:59:22,930] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-06 23:59:22,930] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-06 23:59:22,930] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-06 23:59:22,930] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-06 23:59:22,930] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-06 23:59:22,930] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-06 23:59:22,930] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-06 23:59:22,930] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3be1f0>
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-06 23:59:22,931] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-06 23:59:22,932] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-06 23:59:22,932] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Adam bought 3 kilograms of nuts and 2.5 kilograms of dried fruits at a store. One kilogram of nuts costs $12 and one kilogram of dried fruit costs $8. How much did his purchases cost?
Output: For the nuts Adam paid 3 * $12 = $<<3*12=36>>36.
And for dried fruits Adam paid 2.5 * $8 = $<<2.5*8=20>>20.
So in total for his purchases Adam paid $36 + $20 = $<<36+20=56>>56.
So the final answer is 56

Input: Johns goes to the gym 3 times a week.  He spends 1 hour each day lifting weight. Additionally, he also spends a third of his weightlifting time warming up and doing cardio each day.  How many hours does he spend at the gym a week?
Output: He spends 60/3=<<60/3=20>>20 minutes warming up
So he spends 60+20=<<60+20=80>>80 minutes at the gym per day
That means he spends 80*3=<<80*3=240>>240 minutes at the gym
So he spends 240/60=<<240/60=4>>4 hours at the gym a week
So the final answer is 4

Input: James has to refuel his plane.  It used to cost $200 to refill the tank.  He got an extra tank to double fuel capacity.  Fuel prices also went up by 20%.  How much does he pay now for fuel?
Output: The cost to fill a tank went up 200*.2=$<<200*.2=40>>40
So it cost 200+40=$<<200+40=240>>240 to fill the tank
That means he now pays 240*2=$<<240*2=480>>480
So the final answer is 480

Input: The number of goals scored in a game against Barca by exactly two players last season accounts for 20% of all goals scored in the league. If the players scored an equal number of goals, and the total number of goals scored in the league against Barca that season is 300, calculate the number of goals each of the two players scored.
Output: If the total number of goals scored in the league that season against Barca is 300, the two players scored 20/100*300=<<300*20/100=60>>60 goals.
If the players scored an equal number of goals, each scored 60/2=<<60/2=30>>30 goals.
So the final answer is 30

Input: Every day Tom drinks 5 12-oz cans of soda plus 64 ounces of water. How many ounces of fluid does he drink a week?
Output: He drinks 12 * 5 = <<12*5=60>>60 ounces of soda a day
So he drinks 60 + 64 = <<60+64=124>>124 ounces of liquid a day
So in total he drinks 124 * 7 = <<124*7=868>>868 ounces of liquid a week
So the final answer is 868

Input: Stella and Twinkle are filling up a truck with a capacity of 6000 stone blocks at the rate of 250 blocks per hour per person. They work for four hours and are then joined by 6 other people who also work at the same rate. How many hours did filling the truck take?
Output: Stella and Twinkle filled up the truck at the rate of 250 blocks per hour per person, a total of 2*250 = <<250*2=500>>500 blocks per hour for both.
After working for four hours, Stella and Twinkle had filled 4*500 = <<4*500=2000>>2000 blocks into the truck.
The number of blocks they had to put into the truck for it to be full is 6000-2000 = <<6000-2000=4000>>4000
When 6 more people joined Stella and Twinkle, a total of 2+6 = <<2+6=8>>8 people were filling the truck now.
Working at the rate of 250 blocks per person, the eight people filled the truck with 250*8 = <<250*8=2000>>2000 blocks in one hour.
If there were 4000 blocks that still needed to be put into the truck, the 8 people took 4000/2000 = <<4000/2000=2>>2 hours to fill the truck with the blocks.
The total time it took to fill up the tank is 4+2 = <<4+2=6>>6 hours.
So the final answer is 6

Input: Elijah drank 8.5 pints of coffee yesterday. Emilio drank 9.5 pints of water yesterday. How many cups of liquid did the two boys drink yesterday?
Output: Total drunk: 8.5 + 9.5 = <<8.5+9.5=18>>18 pints
18 pints * 2 = <<18*2=36>>36 cups
The two boys drank a total of 36 cups of liquid yesterday.
So the final answer is 36

Input: Doris works at the Widget Factory in the packing department. She puts 3 widgets in each carton, which are 4 inches wide, 4 inches long, and 5 inches tall. She then packs those cartons into a shipping box before sending it to the loading bay. The shipping boxes are 20 inches wide, 20 inches long, and 20 inches high. How many widgets get shipped in each shipping box?
Output: Each carton has an area of 4*4*5 = <<4*4*5=80>>80 square inches.
Each shipping box has an area of 20*20*20 = <<20*20*20=8000>>8000 square inches
The total number of cartons that will fit into each box is 8000/80 = <<8000/80=100>>100
Since there are 3 widgets in each carton, the total number of cartons in each box will be 3*100 = <<3*100=300>>300
So the final answer is 300

Input: Queenie earns $150 a day as a part-time clerk. She earns an additional $5 per hour as overtime pay. How much will Queenie receive for working 5 days with 4 hours overtime?
Output: Queenie will earn $150 x 5 = $<<150*5=750>>750 for working 5 days.
She will receive an additional $5 x 4 = $<<5*4=20>>20 for overtime pay.
Hence, Queenie will receive a total of $750 + $20 = $<<750+20=770>>770.
So the final answer is 770

Input: Jodi starts off walking 1 mile a day for 6 days a week.  On the second week, she walks 2 miles a day, 6 days a week.  On the third week, she walks 3 miles a day, 6 days a week. Finally on the fourth week, she walks 4 miles a day, 6 days a week.  How many miles has she walked in 4 weeks?
Output: The first week she walked 1 mile, 6 days a week for a total of 1*6 = <<1*6=6>>6 miles
The second week she walked 2 miles, 6 days a week for a total of 2*6 = <<2*6=12>>12 miles
The third week she walked 3 miles, 6 days a week for a total of 3*6 = <<3*6=18>>18 miles
On the fourth week, she walked 4 miles, 6 days a week for a total of 4*6 = <<4*6=24>>24 miles
Over four weeks she has walked 6+12+18+24 = <<6+12+18+24=60>>60 miles
So the final answer is 60

Input: A club is going to get additional members so that they will have 5 more than twice their current number of their members. If the club has 10 members now, how many additional members do they need?
Output: Twice the current number of members now is 10 * 2 = <<10*2=20>>20 members
So, the club wants to have 20 + 5 = <<20+5=25>>25 members.
Therefore, they need to get 25 - 10 = <<25-10=15>>15 members.
So the final answer is 15

Input: Andrea buys herself a pony for her 30th birthday. She pays $500/month to rent a pasture for it, $10 a day for food, and $60/lesson for two lessons a week. How much does she spend on her pony in a year?
Output: First find the annual cost of food: $10/day * 365 days = $<<10*365=3650>>3650
Then find how much she spends a week on lessons: $60/lesson * 2 lessons = $<<60*2=120>>120
Then multiply that weekly cost by the number of weeks per year to find the annual cost: $120/week * 52 weeks/year = $<<120*52=6240>>6240
Then find the annual cost of the pasture: $500/month * 12 months/year = $<<500*12=6000>>6000
Then add all the costs to find the total cost: $6000 + $6240 + $3650 = $<<6000+6240+3650=15890>>15890
So the final answer is 15890

Input: A pet shop has 2 puppies and some kittens. A puppy costs $20, and a kitten costs $15. If the stock is worth $100, how many kittens does the pet shop have?
Output: The 2 puppies cost 2 * 20 = <<2*20=40>>40 dollars
The stock is worth 100 dollars, meaning that the kittens cost 100 - 40 = <<100-40=60>>60 dollars
Since the cost of a kitten is 15 dollars, the pet shop has 60/15 = <<60/15=4>>4 kittens
So the final answer is 4

Input: Noah, who loves his Grammy, calls her every week to talk about his day. If each call lasts 30 minutes and he is charged $0.05 per call minute, how much would he be billed if he makes the calls for a year?
Output: A year has 52 weeks, so if he talks for 30 minutes every week, the total number of call hours is 52*30 = <<52*30=1560>>1560 minutes.
Since he is charged 0.05 cents per call, the total cost is 1560*.05 = <<1560*.05=78>>78 dollars
So the final answer is 78

Input: A merchant bought 15 keyboards and 25 printers for a total of $2050. If a keyboard costs $20, how much does a printer cost?
Output: 15 keyboards at $20 per keyboard cost a total of 15*$20=$<<15*20=300>>300
The total cost of printers and keyboards is $2050 so 25 printers cost $2050-$300=$<<2050-300=1750>>1750
1 printer cost $1750/25=$<<1750/25=70>>70
So the final answer is 70

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o15-tgsm8k-s10-rTrue-m4096
Evaluating commonsenseqa :   1%|          | 1/100 [01:39<2:44:36, 99.76s/it]         Evaluating commonsenseqa :   2%|▏         | 2/100 [03:17<2:40:44, 98.41s/it]       Evaluating commonsenseqa :   3%|▎         | 3/100 [04:54<2:38:07, 97.81s/it]       Evaluating commonsenseqa :   4%|▍         | 4/100 [06:30<2:35:45, 97.35s/it]       Evaluating commonsenseqa :   5%|▌         | 5/100 [08:08<2:34:16, 97.44s/it]                                                                                             Evaluating commonsenseqa :   6%|▌         | 6/100 [09:47<2:33:12, 97.80s/it]       Evaluating commonsenseqa :   7%|▋         | 7/100 [11:26<2:32:19, 98.27s/it]       Evaluating commonsenseqa :   8%|▊         | 8/100 [13:03<2:30:08, 97.92s/it]           Evaluating commonsenseqa :   9%|▉         | 9/100 [14:41<2:28:39, 98.01s/it]           Evaluating commonsenseqa :  10%|█         | 10/100 [16:19<2:27:00, 98.01s/it]          Evaluating commonsenseqa :  11%|█         | 11/100 [17:56<2:24:42, 97.55s/it]          Evaluating commonsenseqa :  12%|█▏        | 12/100 [19:33<2:22:58, 97.48s/it]        Evaluating commonsenseqa :  13%|█▎        | 13/100 [21:12<2:22:03, 97.97s/it]                                                                                                  Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:11:37<1:14:35, 87.75s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [1:13:05<1:13:11, 87.83s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [1:14:32<1:11:42, 87.81s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:16:00<1:10:14, 87.81s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:17:28<1:08:45, 87.77s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:18:56<1:07:20, 87.83s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:20:23<1:05:49, 87.76s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:21:51<1:04:24, 87.84s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:23:20<1:03:01, 87.93s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:24:47<1:01:27, 87.80s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [1:26:15<59:58, 87.78s/it]  Evaluating commonsenseqa :  60%|██████    | 60/100 [1:27:43<58:30, 87.77s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [1:29:10<57:01, 87.74s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [1:30:38<55:35, 87.79s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [1:32:06<54:08, 87.81s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [1:33:34<52:42, 87.83s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [1:35:02<51:15, 87.88s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [1:36:30<49:47, 87.88s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [1:37:58<48:19, 87.88s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [1:39:25<46:52, 87.88s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [1:40:53<45:21, 87.78s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [1:42:20<43:48, 87.62s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [1:43:48<42:23, 87.70s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [1:45:16<40:56, 87.74s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [1:46:43<39:26, 87.64s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [1:48:11<38:01, 87.77s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [1:49:39<36:32, 87.70s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [1:51:07<35:07, 87.80s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [1:52:35<33:38, 87.76s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [1:54:02<32:08, 87.67s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [1:55:30<30:40, 87.66s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [1:56:57<29:11, 87.56s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [1:58:25<27:45, 87.65s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [1:59:53<26:17, 87.63s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [2:01:20<24:49, 87.64s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [2:02:48<23:22, 87.67s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [2:04:16<21:55, 87.70s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [2:05:43<20:27, 87.69s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [2:07:11<18:59, 87.68s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [2:08:39<17:31, 87.63s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [2:10:06<16:02, 87.53s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [2:11:33<14:35, 87.56s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [2:13:01<13:09, 87.67s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [2:14:29<11:40, 87.56s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [2:15:56<10:13, 87.60s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [2:17:25<08:46, 87.78s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [2:18:52<07:18, 87.72s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [2:20:20<05:50, 87.70s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [2:21:47<04:23, 87.67s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [2:23:15<02:55, 87.68s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [2:24:43<01:27, 87.63s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:26:10<00:00, 87.53s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:26:10<00:00, 87.70s/it]
name: commonsenseqa | avg. gen lenth: 378.248 | time: 8773.615997552872s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o19-tgsm8k-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 19 16 17 18 19 20 True 4096 10
[2023-09-07 01:37:50,526] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o19-tgsm8k-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 19
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o19-tgsm8k-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 262941.99it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.79s/it]
 > number of parameters: 6738415616
[2023-09-07 01:38:06,064] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 01:38:06,275] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 01:38:06,277] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 01:38:06,277] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 01:38:06,277] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 01:38:06,277] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 01:38:06,277] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c2250>
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 01:38:06,278] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 01:38:06,279] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 01:38:06,279] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Adam bought 3 kilograms of nuts and 2.5 kilograms of dried fruits at a store. One kilogram of nuts costs $12 and one kilogram of dried fruit costs $8. How much did his purchases cost?
Output: For the nuts Adam paid 3 * $12 = $<<3*12=36>>36.
And for dried fruits Adam paid 2.5 * $8 = $<<2.5*8=20>>20.
So in total for his purchases Adam paid $36 + $20 = $<<36+20=56>>56.
So the final answer is 56

Input: Johns goes to the gym 3 times a week.  He spends 1 hour each day lifting weight. Additionally, he also spends a third of his weightlifting time warming up and doing cardio each day.  How many hours does he spend at the gym a week?
Output: He spends 60/3=<<60/3=20>>20 minutes warming up
So he spends 60+20=<<60+20=80>>80 minutes at the gym per day
That means he spends 80*3=<<80*3=240>>240 minutes at the gym
So he spends 240/60=<<240/60=4>>4 hours at the gym a week
So the final answer is 4

Input: James has to refuel his plane.  It used to cost $200 to refill the tank.  He got an extra tank to double fuel capacity.  Fuel prices also went up by 20%.  How much does he pay now for fuel?
Output: The cost to fill a tank went up 200*.2=$<<200*.2=40>>40
So it cost 200+40=$<<200+40=240>>240 to fill the tank
That means he now pays 240*2=$<<240*2=480>>480
So the final answer is 480

Input: The number of goals scored in a game against Barca by exactly two players last season accounts for 20% of all goals scored in the league. If the players scored an equal number of goals, and the total number of goals scored in the league against Barca that season is 300, calculate the number of goals each of the two players scored.
Output: If the total number of goals scored in the league that season against Barca is 300, the two players scored 20/100*300=<<300*20/100=60>>60 goals.
If the players scored an equal number of goals, each scored 60/2=<<60/2=30>>30 goals.
So the final answer is 30

Input: Every day Tom drinks 5 12-oz cans of soda plus 64 ounces of water. How many ounces of fluid does he drink a week?
Output: He drinks 12 * 5 = <<12*5=60>>60 ounces of soda a day
So he drinks 60 + 64 = <<60+64=124>>124 ounces of liquid a day
So in total he drinks 124 * 7 = <<124*7=868>>868 ounces of liquid a week
So the final answer is 868

Input: Stella and Twinkle are filling up a truck with a capacity of 6000 stone blocks at the rate of 250 blocks per hour per person. They work for four hours and are then joined by 6 other people who also work at the same rate. How many hours did filling the truck take?
Output: Stella and Twinkle filled up the truck at the rate of 250 blocks per hour per person, a total of 2*250 = <<250*2=500>>500 blocks per hour for both.
After working for four hours, Stella and Twinkle had filled 4*500 = <<4*500=2000>>2000 blocks into the truck.
The number of blocks they had to put into the truck for it to be full is 6000-2000 = <<6000-2000=4000>>4000
When 6 more people joined Stella and Twinkle, a total of 2+6 = <<2+6=8>>8 people were filling the truck now.
Working at the rate of 250 blocks per person, the eight people filled the truck with 250*8 = <<250*8=2000>>2000 blocks in one hour.
If there were 4000 blocks that still needed to be put into the truck, the 8 people took 4000/2000 = <<4000/2000=2>>2 hours to fill the truck with the blocks.
The total time it took to fill up the tank is 4+2 = <<4+2=6>>6 hours.
So the final answer is 6

Input: Elijah drank 8.5 pints of coffee yesterday. Emilio drank 9.5 pints of water yesterday. How many cups of liquid did the two boys drink yesterday?
Output: Total drunk: 8.5 + 9.5 = <<8.5+9.5=18>>18 pints
18 pints * 2 = <<18*2=36>>36 cups
The two boys drank a total of 36 cups of liquid yesterday.
So the final answer is 36

Input: Doris works at the Widget Factory in the packing department. She puts 3 widgets in each carton, which are 4 inches wide, 4 inches long, and 5 inches tall. She then packs those cartons into a shipping box before sending it to the loading bay. The shipping boxes are 20 inches wide, 20 inches long, and 20 inches high. How many widgets get shipped in each shipping box?
Output: Each carton has an area of 4*4*5 = <<4*4*5=80>>80 square inches.
Each shipping box has an area of 20*20*20 = <<20*20*20=8000>>8000 square inches
The total number of cartons that will fit into each box is 8000/80 = <<8000/80=100>>100
Since there are 3 widgets in each carton, the total number of cartons in each box will be 3*100 = <<3*100=300>>300
So the final answer is 300

Input: Queenie earns $150 a day as a part-time clerk. She earns an additional $5 per hour as overtime pay. How much will Queenie receive for working 5 days with 4 hours overtime?
Output: Queenie will earn $150 x 5 = $<<150*5=750>>750 for working 5 days.
She will receive an additional $5 x 4 = $<<5*4=20>>20 for overtime pay.
Hence, Queenie will receive a total of $750 + $20 = $<<750+20=770>>770.
So the final answer is 770

Input: Jodi starts off walking 1 mile a day for 6 days a week.  On the second week, she walks 2 miles a day, 6 days a week.  On the third week, she walks 3 miles a day, 6 days a week. Finally on the fourth week, she walks 4 miles a day, 6 days a week.  How many miles has she walked in 4 weeks?
Output: The first week she walked 1 mile, 6 days a week for a total of 1*6 = <<1*6=6>>6 miles
The second week she walked 2 miles, 6 days a week for a total of 2*6 = <<2*6=12>>12 miles
The third week she walked 3 miles, 6 days a week for a total of 3*6 = <<3*6=18>>18 miles
On the fourth week, she walked 4 miles, 6 days a week for a total of 4*6 = <<4*6=24>>24 miles
Over four weeks she has walked 6+12+18+24 = <<6+12+18+24=60>>60 miles
So the final answer is 60

Input: A club is going to get additional members so that they will have 5 more than twice their current number of their members. If the club has 10 members now, how many additional members do they need?
Output: Twice the current number of members now is 10 * 2 = <<10*2=20>>20 members
So, the club wants to have 20 + 5 = <<20+5=25>>25 members.
Therefore, they need to get 25 - 10 = <<25-10=15>>15 members.
So the final answer is 15

Input: Andrea buys herself a pony for her 30th birthday. She pays $500/month to rent a pasture for it, $10 a day for food, and $60/lesson for two lessons a week. How much does she spend on her pony in a year?
Output: First find the annual cost of food: $10/day * 365 days = $<<10*365=3650>>3650
Then find how much she spends a week on lessons: $60/lesson * 2 lessons = $<<60*2=120>>120
Then multiply that weekly cost by the number of weeks per year to find the annual cost: $120/week * 52 weeks/year = $<<120*52=6240>>6240
Then find the annual cost of the pasture: $500/month * 12 months/year = $<<500*12=6000>>6000
Then add all the costs to find the total cost: $6000 + $6240 + $3650 = $<<6000+6240+3650=15890>>15890
So the final answer is 15890

Input: A pet shop has 2 puppies and some kittens. A puppy costs $20, and a kitten costs $15. If the stock is worth $100, how many kittens does the pet shop have?
Output: The 2 puppies cost 2 * 20 = <<2*20=40>>40 dollars
The stock is worth 100 dollars, meaning that the kittens cost 100 - 40 = <<100-40=60>>60 dollars
Since the cost of a kitten is 15 dollars, the pet shop has 60/15 = <<60/15=4>>4 kittens
So the final answer is 4

Input: Noah, who loves his Grammy, calls her every week to talk about his day. If each call lasts 30 minutes and he is charged $0.05 per call minute, how much would he be billed if he makes the calls for a year?
Output: A year has 52 weeks, so if he talks for 30 minutes every week, the total number of call hours is 52*30 = <<52*30=1560>>1560 minutes.
Since he is charged 0.05 cents per call, the total cost is 1560*.05 = <<1560*.05=78>>78 dollars
So the final answer is 78

Input: A merchant bought 15 keyboards and 25 printers for a total of $2050. If a keyboard costs $20, how much does a printer cost?
Output: 15 keyboards at $20 per keyboard cost a total of 15*$20=$<<15*20=300>>300
The total cost of printers and keyboards is $2050 so 25 printers cost $2050-$300=$<<2050-300=1750>>1750
1 printer cost $1750/25=$<<1750/25=70>>70
So the final answer is 70

Input: Gina has two bank accounts. Each account has a quarter of the balance in Betty's account. If Betty's account balance is $3,456, what is the combined balance of both Gina's accounts?
Output: Each of Gina's account has $3456 / 4 accounts = $<<3456/4=864>>864/account
Her combined account balance is therefore $864 + $864 = $<<864+864=1728>>1728
So the final answer is 1728

Input: Zain has 10 more of each coin than Emerie. If Emerie has six quarters, seven dimes, and five nickels, how many coins does Zain have?
Output: If Emerie has six quarters, Zain has 6+10 = <<6+10=16>>16 quarters.
At the same time, Zain has 7+10 = <<7+10=17>>17 dimes, ten more than Emerie.
The total number of quarters and dimes Zain has is 17+16 = <<17+16=33>>33 coins.
Zain also has 10 more nickels than Emerie, a total of 10+5 = <<10+5=15>>15 nickels.
In total, Zain has 33+15 = <<33+15=48>>48 coins.
So the final answer is 48

Input: Adonis is playing a prank on his dad by replacing his shampoo with hot sauce. Every day, after his dad showers, Adonis replaces the shampoo with 1/2 an ounce of hot sauce. He knows his dad uses 1 oz of shampoo a day from a new 10 oz bottle that no one else uses. After 4 days, what percentage of the liquid in the bottle is hot sauce?
Output: Each day, the shampoo bottle goes down 1/2 an ounce because 1 - (1/2) = 1/2
After four days the shampoo bottle is 2 ounces lower because 4 x (1/2) = <<4*(1/2)=2>>2
After four days, the shampoo bottle has 8 ounces of liquid left because 10 -2 = <<10-2=8>>8
After four days the shampoo bottle has 2 ounces of hot sauce 4 x (1/2) = <<4*(1/2)=2>>2
The proportion of hot sauce in the bottle is.25 because 2 / 8 = <<2/8=.25>>.25
The percentage of hot sauce is 25% because.25 x 100 = <<.25*100=25>>25
So the final answer is 25

Input: Lana and Mike are taking their dog and renting a cabin in the mountains for 2 weeks.  The daily rate is $125.00  There is a $100.00 pet fee.  There is also a 20% service/cleaning fee for the rental.  They need to pay 50% of the entire bill as a security deposit.  How much is their security deposit?
Output: There are 7 days in a week and they're going for 2 weeks so that's 7*2 = <<7*2=14>>14 days
The daily rate is $125.00 and they are staying for 14 days so that's 125*14 = $<<125*14=1750.00>>1,750.00
There is also a $100.00 pet fee on top of their rental fee of $1,750.00 so that's 100+1750 = $<<100+1750=1850.00>>1850.00
There is a 20% fee on the $1,850.00 so the fee comes to.20*1
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o19-tgsm8k-s10-rTrue-m4096
                                                                                            Evaluating commonsenseqa :   1%|          | 1/100 [01:30<2:28:46, 90.17s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [02:58<2:25:34, 89.13s/it]               Evaluating commonsenseqa :   3%|▎         | 3/100 [04:26<2:22:59, 88.45s/it]               Evaluating commonsenseqa :   4%|▍         | 4/100 [05:53<2:21:03, 88.16s/it]               Evaluating commonsenseqa :   5%|▌         | 5/100 [07:21<2:19:17, 87.98s/it]             Evaluating commonsenseqa :   6%|▌         | 6/100 [08:49<2:17:39, 87.87s/it]             Evaluating commonsenseqa :   7%|▋         | 7/100 [10:16<2:16:06, 87.81s/it]             Evaluating commonsenseqa :   8%|▊         | 8/100 [11:44<2:14:33, 87.75s/it]             Evaluating commonsenseqa :   9%|▉         | 9/100 [13:12<2:13:04, 87.74s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [14:39<2:11:33, 87.71s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [16:07<2:10:05, 87.70s/it]            Evaluating commonsenseqa :  12%|█▏        | 12/100 [17:35<2:08:38, 87.71s/it]          Evaluating commonsenseqa :  13%|█▎        | 13/100 [19:02<2:07:06, 87.66s/it]            Evaluating commonsenseqa :  14%|█▍        | 14/100 [20:30<2:05:40, 87.68s/it]            Evaluating commonsenseqa :  15%|█▌        | 15/100 [21:58<2:04:09, 87.65s/it]            Evaluating commonsenseqa :  16%|█▌        | 16/100 [23:25<2:02:44, 87.68s/it]            Evaluating commonsenseqa :  17%|█▋        | 17/100 [24:53<2:01:15, 87.66s/it]            Evaluating commonsenseqa :  18%|█▊        | 18/100 [26:21<1:59:49, 87.68s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [27:48<1:58:23, 87.69s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [29:16<1:56:57, 87.72s/it]            Evaluating commonsenseqa :  21%|██        | 21/100 [30:44<1:55:25, 87.66s/it]            Evaluating commonsenseqa :  22%|██▏       | 22/100 [32:11<1:53:50, 87.57s/it]          Evaluating commonsenseqa :  23%|██▎       | 23/100 [33:39<1:52:18, 87.52s/it]          Evaluating commonsenseqa :  24%|██▍       | 24/100 [35:06<1:50:48, 87.48s/it]            Evaluating commonsenseqa :  25%|██▌       | 25/100 [36:33<1:49:19, 87.46s/it]            Evaluating commonsenseqa :  26%|██▌       | 26/100 [38:01<1:47:50, 87.44s/it]            Evaluating commonsenseqa :  27%|██▋       | 27/100 [39:28<1:46:22, 87.43s/it]            Evaluating commonsenseqa :  28%|██▊       | 28/100 [40:55<1:44:52, 87.39s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [42:23<1:43:24, 87.38s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [43:50<1:41:55, 87.36s/it]            Evaluating commonsenseqa :  31%|███       | 31/100 [45:18<1:40:31, 87.42s/it]            Evaluating commonsenseqa :  32%|███▏      | 32/100 [46:45<1:39:03, 87.40s/it]          Evaluating commonsenseqa :  33%|███▎      | 33/100 [48:12<1:37:36, 87.41s/it]          Evaluating commonsenseqa :  34%|███▍      | 34/100 [49:40<1:36:07, 87.38s/it]          Evaluating commonsenseqa :  35%|███▌      | 35/100 [51:07<1:34:42, 87.43s/it]            Evaluating commonsenseqa :  36%|███▌      | 36/100 [52:35<1:33:13, 87.40s/it]            Evaluating commonsenseqa :  37%|███▋      | 37/100 [54:02<1:31:43, 87.36s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [55:29<1:30:13, 87.31s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [56:56<1:28:44, 87.29s/it]            Evaluating commonsenseqa :  40%|████      | 40/100 [58:23<1:27:12, 87.20s/it]            Evaluating commonsenseqa :  41%|████      | 41/100 [59:50<1:25:41, 87.14s/it]            Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:01:17<1:24:13, 87.13s/it]        Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:02:44<1:22:44, 87.10s/it]        Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:04:11<1:21:14, 87.05s/it]8.19s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:43:02<00:00, 97.82s/it]
name: commonsenseqa | avg. gen lenth: 385.039 | time: 9784.90627670288s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu02 --master_port 19674 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o11-tgsm8k-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 11 11 12 13 14 15 True 4096 10
[2023-09-07 02:42:33,252] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o11-tgsm8k-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 11
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o11-tgsm8k-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 370397.67it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.44s/it]
Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:05:38<1:19:47, 87.04s/it]st] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 02:42:46,089] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 02:42:46,090] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 02:42:46,090] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 02:42:46,090] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 02:42:46,090] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 02:42:46,090] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c01f0>
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 02:42:46,091] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 02:42:46,092] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 02:42:46,092] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Tapanga and Corey have 66 candies together. However, Tapanga has 8 more candies than Corey. How many candies does Corey have?
Output: Let x = the total number of candies Corey has.
x + 8 = the total number of candies Tapanga has.
The equation for the total number of candies is x + (x + 8) = 66
Combining like terms, we get 2x + 8 = 66
Subtracting 8 from both sides, we get 2x = 58
Dividing both sides by 2, we get x = <<29=29>>29, so Corey has 29 candies.
So the final answer is 29

Input: Freddy is calling his family on New Year's Eve. He calls his dad, who lives in the same city as him, and they talk for 45 minutes. Then he calls his brother, who lives on the other side of the world, and they talk for 31 minutes. Local calls cost 5 cents a minute, while international calls cost 25 cents a minute. How many dollars did Freddy spend calling his family on New Year's Eve?
Output: At 5 cents a minute, calling his father cost Freddy 5* 45 = <<5*45=225>>225 cents.
At 25 cents a minute, calling his brother cost Freddy 25 * 31 = <<25*31=775>>775 cents.
Adding the cost of calling his father and brother, we find that Freddy paid a total of 225 + 775 = <<225+775=1000>>1000 cents.
Since each dollar has 100 cents, Freddy paid 1000 / 100 = <<1000/100=10>>10 dollars
So the final answer is 10

Input: Lawrence worked 8 hours each day on Monday, Tuesday and Friday. He worked 5.5 hours on both Wednesday and Thursday. How many hours would Lawrence work each day if he worked the same number of hours each day?
Output: 8 hours * 3 = <<8*3=24>>24 hours
5.5 * 2 = <<5.5*2=11>>11 hours
24 + 11 = <<24+11=35>>35 hours
35/7 = <<35/7=5>>5 hours
Lawrence would work 5 hours each of the 7 days in a week.
So the final answer is 5

Input: Ali had a stock of 800 books in his Room. He sold 60 on Monday, 10 on Tuesday, 20 on Wednesday, 44 on Thursday and 66 on Friday. How many books were not sold?
Output: We look first for the total number of books that were sold: 60 + 10 + 20 + 44 + 66 = <<60+10+20+44+66=200>>200 books.
So the total number of books that were not sold is: 800 – 200 = <<800-200=600>>600 books.
So the final answer is 600

Input: Michael makes birdhouses to sell at craft shows. He charges $22 for each large birdhouse, $16 for each medium birdhouse, and $7 for each small birdhouse. This week, he sold 2 large birdhouses, 2 medium birdhouses, and 3 small birdhouses. How much money, in dollars, did he make this week?
Output: Michael sold 2 large birdhouses for $22 each, so he made 2*$22= $<<2*22=44>>44 from large birdhouse sales.
Michael also sold 2 medium birdhouses for $16 each, so he made 2*$16= $<<2*16=32>>32 from medium birdhouse sales.
Michael sold 3 small birdhouses for $7 each, so he made 3*7=$<<3*7=21>>21 from small birdhouse sales.
Since Michael made $44 from large birdhouse sales, $32 from medium birdhouse sales, and $21 for small birdhouse sales, he made $44+$32+$21= $<<44+32+21=97>>97 total this week.
So the final answer is 97

Input: Nalani had two female dogs that were expecting and after a month gave birth to 10 puppies each. She then sold 3/4 of the puppies after they came of age, each at $200. Calculate the total amount of money she received from the sale of the puppies.
Output: If the two expectant dogs gave birth to 10 puppies each, the total number of puppies Nalani had is 10+10= <<10+10=20>>20
When they came of age, Nalani sold 3/4 of the dogs, a total of 3/4*20 = <<3/4*20=15>>15 dogs.
If each dog sold for $200, Nalani received 15*200 = $<<15*200=3000>>3000 from the sale of the dogs.
So the final answer is 3000

Input: Boris has 24 books and he donates a fourth of his books to the library. Cameron has 30 books and he donates a third of his books to the library. After donating their books, how many books in total do Boris and Cameron have together?
Output: Boris donates 24 / 4 = <<24/4=6>>6 books
Then Boris has a total of 24 - 6 = <<24-6=18>>18 books
Cameron donates 30 / 3 = <<30/3=10>>10 books
Then Cameron has a total of 30 - 10 = <<30-10=20>>20 books
Altogether, Boris and Cameron have 18 + 20 = <<18+20=38>>38 books
So the final answer is 38

Input: There are 3 boxes of cereal. One box holds 14 ounces of cereal. Another box holds half the amount of the first box and 5 ounces less than the third box. How much cereal is there in all 3 cereal boxes?
Output: First = <<14=14>>14 oz
Second = (1/2) * 14 = <<(1/2)*14=7>>7 oz
Third = 7 + 5 = <<7+5=12>>12 oz
14 + 7 + 12 = <<14+7+12=33>>33 oz
There are 33 ounces of cereal in those 3 boxes.
So the final answer is 33

Input: A jug needs 40 cups of water to be full. A custodian at Truman Elementary School has to fill water jugs for 200 students, who drink 10 cups of water in a day. How many water jugs will the custodian fill with cups of water to provide the students with all the water they need in a day?
Output: Since each student needs 10 cups of water per day and there are 200 students, the custodian has to provide 200*10 = <<200*10=2000>>2000 cups of water.
A jug of water needs 40 cups to be full, so 2000 cups of water will fill 2000/40 = <<2000/40=50>>50 jugs
So the final answer is 50

Input: It takes 1 hour for refrigerated dough to come to room temperature.  It takes 15 minutes to shape the dough and 2 hours to proof.  The bread takes 30 minutes to bake and 15 minutes to cool.  If the bakery opens at 6:00 am, what is the latest time the head baker can make it to the store to start working?
Output: It takes 1 hour to bring the dough to room temp and 2 hours to proof the dough so that's 1+2 = <<1+2=3>>3 hours
It takes 15 minutes to shape the dough, 30 minutes to bake and another 15 minutes to cool for a total of 15+30+15 = <<15+30+15=60>>60 minutes
There are 60 minutes in 1 hour so that takes him 60/60 = <<60/60=1>>1 hour
In total it takes 3+1 = <<3+1=4>>4 hours from start to finish to make the bread
If the bakery opens at 6:00 am and it takes 4 hours to prep then the latest the head baker can show up is 6-4 = <<6-4=2>>2:00 am
So the final answer is 2

Input: Geric had twice as many bills as Kyla who has 2 fewer bills than Jessa.  After giving 3 bills to Geric, Jessa has 7 bills left. How many bills did Geric have at the beginning?
Output: Jessa had 7 + 3 = <<7+3=10>>10 bills before she gave 3 to Geric.
Kyla had 10 - 2 = <<10-2=8>>8 bills.
So, Geric had 8 x 2 = <<8*2=16>>16 bills.
So the final answer is 16

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o11-tgsm8k-s20-rTrue-m4096
Evaluating commonsenseqa :   1%|          | 1/100 [02:04<3:25:53, 124.79s/it]            Evaluating commonsenseqa :   2%|▏         | 2/100 [04:07<3:21:39, 123.46s/it]                                                                                                    Evaluating commonsenseqa :   3%|▎         | 3/100 [06:08<3:18:12, 122.60s/it]          Evaluating commonsenseqa :   4%|▍         | 4/100 [08:10<3:15:20, 122.09s/it]                                                                                                    Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:15:47<1:09:37, 87.03s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:17:15<1:08:10, 87.04s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:18:42<1:06:43, 87.04s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:20:09<1:05:17, 87.06s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:21:36<1:03:50, 87.06s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:23:03<1:02:24, 87.08s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:24:30<1:00:56, 87.05s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [1:25:57<59:28, 87.05s/it]  Evaluating commonsenseqa :  60%|██████    | 60/100 [1:27:24<58:01, 87.05s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [1:28:51<56:35, 87.05s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [1:30:18<55:07, 87.04s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [1:31:45<53:41, 87.06s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [1:33:12<52:15, 87.09s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [1:34:39<50:48, 87.09s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [1:36:07<49:23, 87.15s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [1:37:34<47:54, 87.12s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [1:39:01<46:27, 87.11s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [1:40:28<44:59, 87.09s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [1:41:55<43:32, 87.10s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [1:43:22<42:05, 87.09s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [1:44:49<40:40, 87.15s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [1:46:16<39:12, 87.13s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [1:47:44<37:45, 87.14s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [1:49:11<36:18, 87.13s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [1:50:38<34:55, 87.29s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [1:52:05<33:26, 87.23s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [1:53:33<31:58, 87.20s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [1:55:00<30:30, 87.15s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [1:56:27<29:03, 87.17s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [1:57:54<27:36, 87.19s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [1:59:21<26:09, 87.21s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [2:00:48<24:42, 87.19s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [2:02:16<23:15, 87.20s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [2:03:43<21:48, 87.21s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [2:05:10<20:20, 87.17s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [2:06:37<18:53, 87.18s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [2:08:04<17:26, 87.21s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [2:09:32<15:59, 87.20s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [2:10:59<14:31, 87.20s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [2:12:26<13:04, 87.20s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [2:13:53<11:37, 87.21s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [2:15:20<10:10, 87.17s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [2:16:48<08:43, 87.19s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [2:18:15<07:15, 87.18s/it]Evaluating commonsenseqa :  37%|███▋      |██▌| 96/100 [2:19:42<05:48, 87.20s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [2:21:09<04:21, 87.18s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [2:22:36<02:54, 87.19s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [2:24:04<01:27, 87.20s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:25:31<00:00, 87.19s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:25:31<00:00, 87.31s/it]
name: commonsenseqa | avg. gen lenth: 369.685 | time: 8734.27253293991s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o20-tgsm8k-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 20 16 17 18 19 20 True 4096 10
[2023-09-07 04:03:46,423] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o20-tgsm8k-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 20
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o20-tgsm8k-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 253651.83it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.68s/it]
 > number of parameters: 6738415616
[2023-09-07 04:03:59,680] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 04:03:59,876] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 04:03:59,878] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 04:03:59,878] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 04:03:59,878] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 04:03:59,878] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 04:03:59,878] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0250>
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 04:03:59,879] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 04:03:59,880] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 04:03:59,880] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Adam bought 3 kilograms of nuts and 2.5 kilograms of dried fruits at a store. One kilogram of nuts costs $12 and one kilogram of dried fruit costs $8. How much did his purchases cost?
Output: For the nuts Adam paid 3 * $12 = $<<3*12=36>>36.
And for dried fruits Adam paid 2.5 * $8 = $<<2.5*8=20>>20.
So in total for his purchases Adam paid $36 + $20 = $<<36+20=56>>56.
So the final answer is 56

Input: Johns goes to the gym 3 times a week.  He spends 1 hour each day lifting weight. Additionally, he also spends a third of his weightlifting time warming up and doing cardio each day.  How many hours does he spend at the gym a week?
Output: He spends 60/3=<<60/3=20>>20 minutes warming up
So he spends 60+20=<<60+20=80>>80 minutes at the gym per day
That means he spends 80*3=<<80*3=240>>240 minutes at the gym
So he spends 240/60=<<240/60=4>>4 hours at the gym a week
So the final answer is 4

Input: James has to refuel his plane.  It used to cost $200 to refill the tank.  He got an extra tank to double fuel capacity.  Fuel prices also went up by 20%.  How much does he pay now for fuel?
Output: The cost to fill a tank went up 200*.2=$<<200*.2=40>>40
So it cost 200+40=$<<200+40=240>>240 to fill the tank
That means he now pays 240*2=$<<240*2=480>>480
So the final answer is 480

Input: The number of goals scored in a game against Barca by exactly two players last season accounts for 20% of all goals scored in the league. If the players scored an equal number of goals, and the total number of goals scored in the league against Barca that season is 300, calculate the number of goals each of the two players scored.
Output: If the total number of goals scored in the league that season against Barca is 300, the two players scored 20/100*300=<<300*20/100=60>>60 goals.
If the players scored an equal number of goals, each scored 60/2=<<60/2=30>>30 goals.
So the final answer is 30

Input: Every day Tom drinks 5 12-oz cans of soda plus 64 ounces of water. How many ounces of fluid does he drink a week?
Output: He drinks 12 * 5 = <<12*5=60>>60 ounces of soda a day
So he drinks 60 + 64 = <<60+64=124>>124 ounces of liquid a day
So in total he drinks 124 * 7 = <<124*7=868>>868 ounces of liquid a week
So the final answer is 868

Input: Stella and Twinkle are filling up a truck with a capacity of 6000 stone blocks at the rate of 250 blocks per hour per person. They work for four hours and are then joined by 6 other people who also work at the same rate. How many hours did filling the truck take?
Output: Stella and Twinkle filled up the truck at the rate of 250 blocks per hour per person, a total of 2*250 = <<250*2=500>>500 blocks per hour for both.
After working for four hours, Stella and Twinkle had filled 4*500 = <<4*500=2000>>2000 blocks into the truck.
The number of blocks they had to put into the truck for it to be full is 6000-2000 = <<6000-2000=4000>>4000
When 6 more people joined Stella and Twinkle, a total of 2+6 = <<2+6=8>>8 people were filling the truck now.
Working at the rate of 250 blocks per person, the eight people filled the truck with 250*8 = <<250*8=2000>>2000 blocks in one hour.
If there were 4000 blocks that still needed to be put into the truck, the 8 people took 4000/2000 = <<4000/2000=2>>2 hours to fill the truck with the blocks.
The total time it took to fill up the tank is 4+2 = <<4+2=6>>6 hours.
So the final answer is 6

Input: Elijah drank 8.5 pints of coffee yesterday. Emilio drank 9.5 pints of water yesterday. How many cups of liquid did the two boys drink yesterday?
Output: Total drunk: 8.5 + 9.5 = <<8.5+9.5=18>>18 pints
18 pints * 2 = <<18*2=36>>36 cups
The two boys drank a total of 36 cups of liquid yesterday.
So the final answer is 36

Input: Doris works at the Widget Factory in the packing department. She puts 3 widgets in each carton, which are 4 inches wide, 4 inches long, and 5 inches tall. She then packs those cartons into a shipping box before sending it to the loading bay. The shipping boxes are 20 inches wide, 20 inches long, and 20 inches high. How many widgets get shipped in each shipping box?
Output: Each carton has an area of 4*4*5 = <<4*4*5=80>>80 square inches.
Each shipping box has an area of 20*20*20 = <<20*20*20=8000>>8000 square inches
The total number of cartons that will fit into each box is 8000/80 = <<8000/80=100>>100
Since there are 3 widgets in each carton, the total number of cartons in each box will be 3*100 = <<3*100=300>>300
So the final answer is 300

Input: Queenie earns $150 a day as a part-time clerk. She earns an additional $5 per hour as overtime pay. How much will Queenie receive for working 5 days with 4 hours overtime?
Output: Queenie will earn $150 x 5 = $<<150*5=750>>750 for working 5 days.
She will receive an additional $5 x 4 = $<<5*4=20>>20 for overtime pay.
Hence, Queenie will receive a total of $750 + $20 = $<<750+20=770>>770.
So the final answer is 770

Input: Jodi starts off walking 1 mile a day for 6 days a week.  On the second week, she walks 2 miles a day, 6 days a week.  On the third week, she walks 3 miles a day, 6 days a week. Finally on the fourth week, she walks 4 miles a day, 6 days a week.  How many miles has she walked in 4 weeks?
Output: The first week she walked 1 mile, 6 days a week for a total of 1*6 = <<1*6=6>>6 miles
The second week she walked 2 miles, 6 days a week for a total of 2*6 = <<2*6=12>>12 miles
The third week she walked 3 miles, 6 days a week for a total of 3*6 = <<3*6=18>>18 miles
On the fourth week, she walked 4 miles, 6 days a week for a total of 4*6 = <<4*6=24>>24 miles
Over four weeks she has walked 6+12+18+24 = <<6+12+18+24=60>>60 miles
So the final answer is 60

Input: A club is going to get additional members so that they will have 5 more than twice their current number of their members. If the club has 10 members now, how many additional members do they need?
Output: Twice the current number of members now is 10 * 2 = <<10*2=20>>20 members
So, the club wants to have 20 + 5 = <<20+5=25>>25 members.
Therefore, they need to get 25 - 10 = <<25-10=15>>15 members.
So the final answer is 15

Input: Andrea buys herself a pony for her 30th birthday. She pays $500/month to rent a pasture for it, $10 a day for food, and $60/lesson for two lessons a week. How much does she spend on her pony in a year?
Output: First find the annual cost of food: $10/day * 365 days = $<<10*365=3650>>3650
Then find how much she spends a week on lessons: $60/lesson * 2 lessons = $<<60*2=120>>120
Then multiply that weekly cost by the number of weeks per year to find the annual cost: $120/week * 52 weeks/year = $<<120*52=6240>>6240
Then find the annual cost of the pasture: $500/month * 12 months/year = $<<500*12=6000>>6000
Then add all the costs to find the total cost: $6000 + $6240 + $3650 = $<<6000+6240+3650=15890>>15890
So the final answer is 15890

Input: A pet shop has 2 puppies and some kittens. A puppy costs $20, and a kitten costs $15. If the stock is worth $100, how many kittens does the pet shop have?
Output: The 2 puppies cost 2 * 20 = <<2*20=40>>40 dollars
The stock is worth 100 dollars, meaning that the kittens cost 100 - 40 = <<100-40=60>>60 dollars
Since the cost of a kitten is 15 dollars, the pet shop has 60/15 = <<60/15=4>>4 kittens
So the final answer is 4

Input: Noah, who loves his Grammy, calls her every week to talk about his day. If each call lasts 30 minutes and he is charged $0.05 per call minute, how much would he be billed if he makes the calls for a year?
Output: A year has 52 weeks, so if he talks for 30 minutes every week, the total number of call hours is 52*30 = <<52*30=1560>>1560 minutes.
Since he is charged 0.05 cents per call, the total cost is 1560*.05 = <<1560*.05=78>>78 dollars
So the final answer is 78

Input: A merchant bought 15 keyboards and 25 printers for a total of $2050. If a keyboard costs $20, how much does a printer cost?
Output: 15 keyboards at $20 per keyboard cost a total of 15*$20=$<<15*20=300>>300
The total cost of printers and keyboards is $2050 so 25 printers cost $2050-$300=$<<2050-300=1750>>1750
1 printer cost $1750/25=$<<1750/25=70>>70
So the final answer is 70

Input: Gina has two bank accounts. Each account has a quarter of the balance in Betty's account. If Betty's account balance is $3,456, what is the combined balance of both Gina's accounts?
Output: Each of Gina's account has $3456 / 4 accounts = $<<3456/4=864>>864/account
Her combined account balance is therefore $864 + $864 = $<<864+864=1728>>1728
So the final answer is 1728

Input: Zain has 10 more of each coin than Emerie. If Emerie has six quarters, seven dimes, and five nickels, how many coins does Zain have?
Output: If Emerie has six quarters, Zain has 6+10 = <<6+10=16>>16 quarters.
At the same time, Zain has 7+10 = <<7+10=17>>17 dimes, ten more than Emerie.
The total number of quarters and dimes Zain has is 17+16 = <<17+16=33>>33 coins.
Zain also has 10 more nickels than Emerie, a total of 10+5 = <<10+5=15>>15 nickels.
In total, Zain has 33+15 = <<33+15=48>>48 coins.
So the final answer is 48

Input: Adonis is playing a prank on his dad by replacing his shampoo with hot sauce. Every day, after his dad showers, Adonis replaces the shampoo with 1/2 an ounce of hot sauce. He knows his dad uses 1 oz of shampoo a day from a new 10 oz bottle that no one else uses. After 4 days, what percentage of the liquid in the bottle is hot sauce?
Output: Each day, the shampoo bottle goes down 1/2 an ounce because 1 - (1/2) = 1/2
After four days the shampoo bottle is 2 ounces lower because 4 x (1/2) = <<4*(1/2)=2>>2
After four days, the shampoo bottle has 8 ounces of liquid left because 10 -2 = <<10-2=8>>8
After four days the shampoo bottle has 2 ounces of hot sauce 4 x (1/2) = <<4*(1/2)=2>>2
The proportion of hot sauce in the bottle is.25 because 2 / 8 = <<2/8=.25>>.25
The percentage of hot sauce is 25% because.25 x 100 = <<.25*100=25>>25
So the final answer is 25

Input: Lana and Mike are taking their dog and renting a cabin in the mountains for 2 weeks.  The daily rate is $125.00  There is a $100.00 pet fee.  There is also a 20% service/cleaning fee for the rental.  They need to pay 50% of the entire bill as a security deposit.  How much is their security deposit?
Output: There are 7 days in a week and they're going for 2 weeks so that's 7*2 = <<7*2=14>>14 days
The daily rate is $125.00 and they are staying for 14 days so that's 125*14 = $<<125*14=1750.00>>1,750.00
There is also a $100.00 pet fee on top of their rental fee of $1,750.00 so that's 100+1750 = $<<100+1750=1850.00>>1850.00
There is a 20% fee on the $1,850.00 so the fee comes to.20*1
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o20-tgsm8k-s10-rTrue-m4096
                                                                                         Evaluating commonsenseqa :   1%|          | 1/100 [01:29<2:27:44, 89.54s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [02:57<2:24:27, 88.45s/it]          Evaluating commonsenseqa :   3%|▎         | 3/100 [04:24<2:22:15, 87.99s/it]            Evaluating commonsenseqa :   4%|▍         | 4/100 [05:52<2:20:23, 87.74s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [07:19<2:18:44, 87.63s/it]            Evaluating commonsenseqa :   6%|▌         | 6/100 [08:46<2:17:09, 87.55s/it]            Evaluating commonsenseqa :   7%|▋         | 7/100 [10:14<2:15:40, 87.53s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [11:41<2:14:09, 87.50s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [13:09<2:12:40, 87.48s/it]            Evaluating commonsenseqa :  10%|█         | 10/100 [14:36<2:11:10, 87.45s/it]           Evaluating commonsenseqa :  11%|█         | 11/100 [16:04<2:09:44, 87.47s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [17:31<2:08:16, 87.46s/it]         Evaluating commonsenseqa :  13%|█▎        | 13/100 [18:58<2:06:48, 87.45s/it]         Evaluating commonsenseqa :  14%|█▍        | 14/100 [20:26<2:05:20, 87.44s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [21:53<2:03:49, 87.41s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [23:21<2:02:23, 87.42s/it]         Evaluating commonsenseqa :  17%|█▋        | 17/100 [24:48<2:00:55, 87.41s/it]           Evaluating commonsenseqa :  18%|█▊        | 18/100 [26:15<1:59:27, 87.41s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [27:43<1:58:01, 87.42s/it]           Evaluating commonsenseqa :  20%|██        | 20/100 [29:10<1:56:34, 87.43s/it]           Evaluating commonsenseqa :  21%|██        | 21/100 [30:38<1:55:08, 87.45s/it]           Evaluating commonsenseqa :  22%|██▏       | 22/100 [32:05<1:53:41, 87.45s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [33:33<1:52:13, 87.45s/it]         Evaluating commonsenseqa :  24%|██▍       | 24/100 [35:00<1:50:46, 87.45s/it]         Evaluating commonsenseqa :  25%|██▌       | 25/100 [36:28<1:49:18, 87.45s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [37:55<1:47:52, 87.46s/it]         Evaluating commonsenseqa :  27%|██▋       | 27/100 [39:23<1:46:23, 87.45s/it]         Evaluating commonsenseqa :  28%|██▊       | 28/100 [40:50<1:44:55, 87.44s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [42:17<1:43:27, 87.43s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [43:45<1:41:58, 87.41s/it]         Evaluating commonsenseqa :  31%|███       | 31/100 [45:12<1:40:33, 87.45s/it]           Evaluating commonsenseqa :  32%|███▏      | 32/100 [46:40<1:39:06, 87.45s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [48:07<1:37:39, 87.46s/it]         Evaluating commonsenseqa :  34%|███▍      | 34/100 [49:35<1:36:11, 87.44s/it]         Evaluating commonsenseqa :  35%|███▌      | 35/100 [51:02<1:34:45, 87.47s/it]         Evaluating commonsenseqa :  36%|███▌      | 36/100 [52:30<1:33:17, 87.46s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [53:57<1:31:48, 87.44s/it]         Evaluating commonsenseqa :  38%|███▊      | 38/100 [55:25<1:30:24, 87.48s/it]         Evaluating commonsenseqa :  39%|███▉      | 39/100 [56:52<1:28:57, 87.50s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [58:20<1:27:29, 87.48s/it]         Evaluating commonsenseqa :  41%|████      | 41/100 [59:47<1:25:56, 87.40s/it]         Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:01:14<1:24:25, 87.33s/it]     Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:02:41<1:22:54, 87.27s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:04:08<1:21:24, 87.23s/it]     Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:05:35<1:19:56, 87.21s/it]     Evaluating commonsenseqa :  46%|████▌     | 46/100 [1:07:02<1:18:27, 87.18s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:08:30<1:16:59, 87.16s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:09:57<1:15:32, 87.17s/it]     Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:11:24<1:14:05, 87.17s/it]     Evaluating commonsenseqa :  50%|█████     | 50/100 [1:12:51<1:12:39, 87.20s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [1:14:18<1:11:12, 87.20s/it]     Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:15:46<1:09:44, 87.18s/it]   Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:17:13<1:08:17, 87.19s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:18:40<1:06:50, 87.18s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:20:07<1:05:24, 87.20s/it]   Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:21:34<1:03:57, 87.20s/it]   Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:23:02<1:02:37, 87.38s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:24:29<1:01:07, 87.32s/it]   Evaluating commonsenseqa :  59%|█████▉    | 59/100 [1:25:57<59:38, 87.28s/it]       Evaluating commonsenseqa :  60%|██████    | 60/100 [1:27:24<58:11, 87.28s/it]       Evaluating commonsenseqa :  61%|██████    | 61/100 [1:28:51<56:42, 87.25s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [1:30:18<55:14, 87.22s/it]     Evaluating commonsenseqa :  63%|██████▎   | 63/100 [1:31:45<53:47, 87.22s/it]     Evaluating commonsenseqa :  64%|██████▍   | 64/100 [1:33:13<52:21, 87.25s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [1:34:40<50:53, 87.25s/it]     Evaluating commonsenseqa :  66%|██████▌   | 66/100 [1:36:07<49:25, 87.23s/it]     Evaluating commonsenseqa :  67%|██████▋   | 67/100 [1:37:34<47:58, 87.22s/it]     Evaluating commonsenseqa :  68%|██████▊   | 68/100 [1:39:02<46:31, 87.23s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [1:40:29<45:03, 87.22s/it]     Evaluating commonsenseqa :  70%|███████   | 70/100 [1:41:56<43:37, 87.25s/it]     Evaluating commonsenseqa :  71%|███████   | 71/100 [1:43:23<42:09, 87.22s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [1:44:50<40:42, 87.23s/it]   Evaluating commonsenseqa :  73%|███████▎  | 73/100 [1:46:18<39:14, 87.20s/it]     Evaluating commonsenseqa :  74%|███████▍  | 74/100 [1:47:45<37:47, 87.21s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [1:49:12<36:20, 87.22s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [1:50:39<34:53, 87.21s/it]     Evaluating commonsenseqa :  77%|███████▋  | 77/100 [1:52:07<33:25, 87.21s/it]     Evaluating commonsenseqa :  78%|███████▊  | 78/100 [1:53:34<31:58, 87.19s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [1:55:01<30:30, 87.16s/it]     Evaluating commonsenseqa :  80%|████████  | 80/100 [1:56:28<29:04, 87.22s/it]     Evaluating commonsenseqa :  81%|████████  | 81/100 [1:57:55<27:37, 87.26s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [1:59:23<26:10, 87.26s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [2:00:50<24:43, 87.24s/it]   Evaluating commonsenseqa :  84%|████████▍ | 84/100 [2:02:17<23:16, 87.27s/it]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Evaluating commonsenseqa :  85%|████████▌ | 85/100 [2:03:44<21:48, 87.26s/it]r to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o12-tgsm8k-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 12
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o12-tgsm8k-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 350538.94it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.50s/it]
 > number of parameters: 6738415616
[2023-09-07 06:06:50,312] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 06:06:50,625] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 06:06:50,627] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c01f0>
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 06:06:50,627] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 06:06:50,628] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 06:06:50,629] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 06:06:50,629] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 06:06:50,629] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 06:06:50,629] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 06:06:50,629] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 06:06:50,629] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Tapanga and Corey have 66 candies together. However, Tapanga has 8 more candies than Corey. How many candies does Corey have?
Output: Let x = the total number of candies Corey has.
x + 8 = the total number of candies Tapanga has.
The equation for the total number of candies is x + (x + 8) = 66
Combining like terms, we get 2x + 8 = 66
Subtracting 8 from both sides, we get 2x = 58
Dividing both sides by 2, we get x = <<29=29>>29, so Corey has 29 candies.
So the final answer is 29

Input: Freddy is calling his family on New Year's Eve. He calls his dad, who lives in the same city as him, and they talk for 45 minutes. Then he calls his brother, who lives on the other side of the world, and they talk for 31 minutes. Local calls cost 5 cents a minute, while international calls cost 25 cents a minute. How many dollars did Freddy spend calling his family on New Year's Eve?
Output: At 5 cents a minute, calling his father cost Freddy 5* 45 = <<5*45=225>>225 cents.
At 25 cents a minute, calling his brother cost Freddy 25 * 31 = <<25*31=775>>775 cents.
Adding the cost of calling his father and brother, we find that Freddy paid a total of 225 + 775 = <<225+775=1000>>1000 cents.
Since each dollar has 100 cents, Freddy paid 1000 / 100 = <<1000/100=10>>10 dollars
So the final answer is 10

Input: Lawrence worked 8 hours each day on Monday, Tuesday and Friday. He worked 5.5 hours on both Wednesday and Thursday. How many hours would Lawrence work each day if he worked the same number of hours each day?
Output: 8 hours * 3 = <<8*3=24>>24 hours
5.5 * 2 = <<5.5*2=11>>11 hours
24 + 11 = <<24+11=35>>35 hours
35/7 = <<35/7=5>>5 hours
Lawrence would work 5 hours each of the 7 days in a week.
So the final answer is 5

Input: Ali had a stock of 800 books in his Room. He sold 60 on Monday, 10 on Tuesday, 20 on Wednesday, 44 on Thursday and 66 on Friday. How many books were not sold?
Output: We look first for the total number of books that were sold: 60 + 10 + 20 + 44 + 66 = <<60+10+20+44+66=200>>200 books.
So the total number of books that were not sold is: 800 – 200 = <<800-200=600>>600 books.
So the final answer is 600

Input: Michael makes birdhouses to sell at craft shows. He charges $22 for each large birdhouse, $16 for each medium birdhouse, and $7 for each small birdhouse. This week, he sold 2 large birdhouses, 2 medium birdhouses, and 3 small birdhouses. How much money, in dollars, did he make this week?
Output: Michael sold 2 large birdhouses for $22 each, so he made 2*$22= $<<2*22=44>>44 from large birdhouse sales.
Michael also sold 2 medium birdhouses for $16 each, so he made 2*$16= $<<2*16=32>>32 from medium birdhouse sales.
Michael sold 3 small birdhouses for $7 each, so he made 3*7=$<<3*7=21>>21 from small birdhouse sales.
Since Michael made $44 from large birdhouse sales, $32 from medium birdhouse sales, and $21 for small birdhouse sales, he made $44+$32+$21= $<<44+32+21=97>>97 total this week.
So the final answer is 97

Input: Nalani had two female dogs that were expecting and after a month gave birth to 10 puppies each. She then sold 3/4 of the puppies after they came of age, each at $200. Calculate the total amount of money she received from the sale of the puppies.
Output: If the two expectant dogs gave birth to 10 puppies each, the total number of puppies Nalani had is 10+10= <<10+10=20>>20
When they came of age, Nalani sold 3/4 of the dogs, a total of 3/4*20 = <<3/4*20=15>>15 dogs.
If each dog sold for $200, Nalani received 15*200 = $<<15*200=3000>>3000 from the sale of the dogs.
So the final answer is 3000

Input: Boris has 24 books and he donates a fourth of his books to the library. Cameron has 30 books and he donates a third of his books to the library. After donating their books, how many books in total do Boris and Cameron have together?
Output: Boris donates 24 / 4 = <<24/4=6>>6 books
Then Boris has a total of 24 - 6 = <<24-6=18>>18 books
Cameron donates 30 / 3 = <<30/3=10>>10 books
Then Cameron has a total of 30 - 10 = <<30-10=20>>20 books
Altogether, Boris and Cameron have 18 + 20 = <<18+20=38>>38 books
So the final answer is 38

Input: There are 3 boxes of cereal. One box holds 14 ounces of cereal. Another box holds half the amount of the first box and 5 ounces less than the third box. How much cereal is there in all 3 cereal boxes?
Output: First = <<14=14>>14 oz
Second = (1/2) * 14 = <<(1/2)*14=7>>7 oz
Third = 7 + 5 = <<7+5=12>>12 oz
14 + 7 + 12 = <<14+7+12=33>>33 oz
There are 33 ounces of cereal in those 3 boxes.
So the final answer is 33

Input: A jug needs 40 cups of water to be full. A custodian at Truman Elementary School has to fill water jugs for 200 students, who drink 10 cups of water in a day. How many water jugs will the custodian fill with cups of water to provide the students with all the water they need in a day?
Output: Since each student needs 10 cups of water per day and there are 200 students, the custodian has to provide 200*10 = <<200*10=2000>>2000 cups of water.
A jug of water needs 40 cups to be full, so 2000 cups of water will fill 2000/40 = <<2000/40=50>>50 jugs
So the final answer is 50

Input: It takes 1 hour for refrigerated dough to come to room temperature.  It takes 15 minutes to shape the dough and 2 hours to proof.  The bread takes 30 minutes to bake and 15 minutes to cool.  If the bakery opens at 6:00 am, what is the latest time the head baker can make it to the store to start working?
Output: It takes 1 hour to bring the dough to room temp and 2 hours to proof the dough so that's 1+2 = <<1+2=3>>3 hours
It takes 15 minutes to shape the dough, 30 minutes to bake and another 15 minutes to cool for a total of 15+30+15 = <<15+30+15=60>>60 minutes
There are 60 minutes in 1 hour so that takes him 60/60 = <<60/60=1>>1 hour
In total it takes 3+1 = <<3+1=4>>4 hours from start to finish to make the bread
If the bakery opens at 6:00 am and it takes 4 hours to prep then the latest the head baker can show up is 6-4 = <<6-4=2>>2:00 am
So the final answer is 2

Input: Geric had twice as many bills as Kyla who has 2 fewer bills than Jessa.  After giving 3 bills to Geric, Jessa has 7 bills left. How many bills did Geric have at the beginning?
Output: Jessa had 7 + 3 = <<7+3=10>>10 bills before she gave 3 to Geric.
Kyla had 10 - 2 = <<10-2=8>>8 bills.
So, Geric had 8 x 2 = <<8*2=16>>16 bills.
So the final answer is 16

Input: They say the first year of a dog's life equals 15 human years. The second year of a dog's life equals 9 human years and after that, every year of a dog's life equals 5 human years. According to this logic, how many human years has my 10-year-old dog lived?
Output: If your dog is 10 years old then in his first year of life he lived 1*15 = 15 human years
In his second year of life, he lived 1*9 = <<1*9=9>>9 human years
We need to calculate his remaining years or 10-2 = <<10-2=8>>8 years of dog life into human years
If 1 year of dog life after the 2 years equates to 5 human years, then 8 years of dog life equals 8*5 = <<8*5=40>>40 human years
In total, your dog has lived 15 + 9 + 40 = <<15+9+40=64>>64 human years
So the final answer is 64

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o12-tgsm8k-s20-rTrue-m4096
Evaluating commonsenseqa :   1%|          | 1/100 [01:53<3:07:46, 113.81s/it]                  Evaluating commonsenseqa :   2%|▏         | 2/100 [03:45<3:04:16, 112.82s/it]                Evaluating commonsenseqa :   3%|▎         | 3/100 [05:37<3:01:08, 112.05s/it]                                                                                                                Evaluating commonsenseqa :   4%|▍         | 4/100 [07:28<2:58:54, 111.82s/it]                Evaluating commonsenseqa :   5%|▌         | 5/100 [09:20<2:57:23, 112.03s/it]                Evaluating commonsenseqa :  92%|█████████▏| 92/100 [2:13:55<11:37, 87.17s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [2:15:22<10:09, 87.08s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [2:16:49<08:42, 87.08s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [2:18:16<07:15, 87.06s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [2:19:43<05:48, 87.05s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [2:21:10<04:21, 87.02s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [2:22:37<02:53, 86.99s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [2:24:04<01:26, 87.00s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:25:31<00:00, 86.95s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:25:31<00:00, 87.31s/it]
name: commonsenseqa | avg. gen lenth: 369.685 | time: 8734.411449432373s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o16-tgsm8k-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 16 16 17 18 19 20 True 4096 10
[2023-09-07 06:29:42,344] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o16-tgsm8k-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 16
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o16-tgsm8k-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 290770.29it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.61s/it]
 > number of parameters: 6738415616
[2023-09-07 06:29:57,396] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 06:29:57,600] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 06:29:57,602] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 06:29:57,602] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0130>
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 06:29:57,603] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 06:29:57,604] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 06:29:57,605] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 06:29:57,605] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Tapanga and Corey have 66 candies together. However, Tapanga has 8 more candies than Corey. How many candies does Corey have?
Output: Let x = the total number of candies Corey has.
x + 8 = the total number of candies Tapanga has.
The equation for the total number of candies is x + (x + 8) = 66
Combining like terms, we get 2x + 8 = 66
Subtracting 8 from both sides, we get 2x = 58
Dividing both sides by 2, we get x = <<29=29>>29, so Corey has 29 candies.
So the final answer is 29

Input: Freddy is calling his family on New Year's Eve. He calls his dad, who lives in the same city as him, and they talk for 45 minutes. Then he calls his brother, who lives on the other side of the world, and they talk for 31 minutes. Local calls cost 5 cents a minute, while international calls cost 25 cents a minute. How many dollars did Freddy spend calling his family on New Year's Eve?
Output: At 5 cents a minute, calling his father cost Freddy 5* 45 = <<5*45=225>>225 cents.
At 25 cents a minute, calling his brother cost Freddy 25 * 31 = <<25*31=775>>775 cents.
Adding the cost of calling his father and brother, we find that Freddy paid a total of 225 + 775 = <<225+775=1000>>1000 cents.
Since each dollar has 100 cents, Freddy paid 1000 / 100 = <<1000/100=10>>10 dollars
So the final answer is 10

Input: Lawrence worked 8 hours each day on Monday, Tuesday and Friday. He worked 5.5 hours on both Wednesday and Thursday. How many hours would Lawrence work each day if he worked the same number of hours each day?
Output: 8 hours * 3 = <<8*3=24>>24 hours
5.5 * 2 = <<5.5*2=11>>11 hours
24 + 11 = <<24+11=35>>35 hours
35/7 = <<35/7=5>>5 hours
Lawrence would work 5 hours each of the 7 days in a week.
So the final answer is 5

Input: Ali had a stock of 800 books in his Room. He sold 60 on Monday, 10 on Tuesday, 20 on Wednesday, 44 on Thursday and 66 on Friday. How many books were not sold?
Output: We look first for the total number of books that were sold: 60 + 10 + 20 + 44 + 66 = <<60+10+20+44+66=200>>200 books.
So the total number of books that were not sold is: 800 – 200 = <<800-200=600>>600 books.
So the final answer is 600

Input: Michael makes birdhouses to sell at craft shows. He charges $22 for each large birdhouse, $16 for each medium birdhouse, and $7 for each small birdhouse. This week, he sold 2 large birdhouses, 2 medium birdhouses, and 3 small birdhouses. How much money, in dollars, did he make this week?
Output: Michael sold 2 large birdhouses for $22 each, so he made 2*$22= $<<2*22=44>>44 from large birdhouse sales.
Michael also sold 2 medium birdhouses for $16 each, so he made 2*$16= $<<2*16=32>>32 from medium birdhouse sales.
Michael sold 3 small birdhouses for $7 each, so he made 3*7=$<<3*7=21>>21 from small birdhouse sales.
Since Michael made $44 from large birdhouse sales, $32 from medium birdhouse sales, and $21 for small birdhouse sales, he made $44+$32+$21= $<<44+32+21=97>>97 total this week.
So the final answer is 97

Input: Nalani had two female dogs that were expecting and after a month gave birth to 10 puppies each. She then sold 3/4 of the puppies after they came of age, each at $200. Calculate the total amount of money she received from the sale of the puppies.
Output: If the two expectant dogs gave birth to 10 puppies each, the total number of puppies Nalani had is 10+10= <<10+10=20>>20
When they came of age, Nalani sold 3/4 of the dogs, a total of 3/4*20 = <<3/4*20=15>>15 dogs.
If each dog sold for $200, Nalani received 15*200 = $<<15*200=3000>>3000 from the sale of the dogs.
So the final answer is 3000

Input: Boris has 24 books and he donates a fourth of his books to the library. Cameron has 30 books and he donates a third of his books to the library. After donating their books, how many books in total do Boris and Cameron have together?
Output: Boris donates 24 / 4 = <<24/4=6>>6 books
Then Boris has a total of 24 - 6 = <<24-6=18>>18 books
Cameron donates 30 / 3 = <<30/3=10>>10 books
Then Cameron has a total of 30 - 10 = <<30-10=20>>20 books
Altogether, Boris and Cameron have 18 + 20 = <<18+20=38>>38 books
So the final answer is 38

Input: There are 3 boxes of cereal. One box holds 14 ounces of cereal. Another box holds half the amount of the first box and 5 ounces less than the third box. How much cereal is there in all 3 cereal boxes?
Output: First = <<14=14>>14 oz
Second = (1/2) * 14 = <<(1/2)*14=7>>7 oz
Third = 7 + 5 = <<7+5=12>>12 oz
14 + 7 + 12 = <<14+7+12=33>>33 oz
There are 33 ounces of cereal in those 3 boxes.
So the final answer is 33

Input: A jug needs 40 cups of water to be full. A custodian at Truman Elementary School has to fill water jugs for 200 students, who drink 10 cups of water in a day. How many water jugs will the custodian fill with cups of water to provide the students with all the water they need in a day?
Output: Since each student needs 10 cups of water per day and there are 200 students, the custodian has to provide 200*10 = <<200*10=2000>>2000 cups of water.
A jug of water needs 40 cups to be full, so 2000 cups of water will fill 2000/40 = <<2000/40=50>>50 jugs
So the final answer is 50

Input: It takes 1 hour for refrigerated dough to come to room temperature.  It takes 15 minutes to shape the dough and 2 hours to proof.  The bread takes 30 minutes to bake and 15 minutes to cool.  If the bakery opens at 6:00 am, what is the latest time the head baker can make it to the store to start working?
Output: It takes 1 hour to bring the dough to room temp and 2 hours to proof the dough so that's 1+2 = <<1+2=3>>3 hours
It takes 15 minutes to shape the dough, 30 minutes to bake and another 15 minutes to cool for a total of 15+30+15 = <<15+30+15=60>>60 minutes
There are 60 minutes in 1 hour so that takes him 60/60 = <<60/60=1>>1 hour
In total it takes 3+1 = <<3+1=4>>4 hours from start to finish to make the bread
If the bakery opens at 6:00 am and it takes 4 hours to prep then the latest the head baker can show up is 6-4 = <<6-4=2>>2:00 am
So the final answer is 2

Input: Geric had twice as many bills as Kyla who has 2 fewer bills than Jessa.  After giving 3 bills to Geric, Jessa has 7 bills left. How many bills did Geric have at the beginning?
Output: Jessa had 7 + 3 = <<7+3=10>>10 bills before she gave 3 to Geric.
Kyla had 10 - 2 = <<10-2=8>>8 bills.
So, Geric had 8 x 2 = <<8*2=16>>16 bills.
So the final answer is 16

Input: They say the first year of a dog's life equals 15 human years. The second year of a dog's life equals 9 human years and after that, every year of a dog's life equals 5 human years. According to this logic, how many human years has my 10-year-old dog lived?
Output: If your dog is 10 years old then in his first year of life he lived 1*15 = 15 human years
In his second year of life, he lived 1*9 = <<1*9=9>>9 human years
We need to calculate his remaining years or 10-2 = <<10-2=8>>8 years of dog life into human years
If 1 year of dog life after the 2 years equates to 5 human years, then 8 years of dog life equals 8*5 = <<8*5=40>>40 human years
In total, your dog has lived 15 + 9 + 40 = <<15+9+40=64>>64 human years
So the final answer is 64

Input: Company A and Company B merge. Company A receives 60% of the combined profits under the new merger, and company B receives 40% of the profits. If company B gets a total of $60000 in profit, how much does company A get?
Output: Let the total profit of both companies be denoted by x.
Therefore, 40% * x = 0.4 * x = $60000
So x = $60000 / 0.4 = $150000.
If Company B gets 60 % of the profits, it gets 60% of $150000, which is 0.6 * $150000 = $<<0.6*150000=90000>>90000.
So the final answer is 90000

Input: Sam, Sid, and Steve brought popsicle sticks for their group activity in their Art class. Sam has thrice as many as Sid, and Sid has twice as many as Steve. If Steve has 12 popsicle sticks, how many popsicle sticks can they use for their Art class activity?
Output: Sid has 12 x 2 = <<12*2=24>>24 sticks.
Sam has 24 x 3 = <<24*3=72>>72 sticks.
Thus, they have 24 + 72 + 12 = <<24+72+12=108>>108 popsicle sticks.
So the final answer is 108

Input: The price of an iPhone fell 10% in a particular month and another 20% in the second month.  If the initial price was $1000, calculate the price after the second month.
Output: In the first month, the price of the iPhone fell by 10/100*$1000=$<<10/100*1000=100>>100
The new price after the first month is $1000-$100 = $<<1000-100=900>>900
In the second month, the price fell 20% further by 20/100*$900 = $<<20/100*900=180>>180
The new price after the second month is $900-$180 = $<<900-180=720>>720
So the final answer is 720

Input: John uses a 75-watt electric fan for 8 hours a day. How much kWh of electric energy does he consume per month (30 days) for using the electric fan?
Output: In a day, John uses 75x 8 = <<75*8=600>>600 watts.
So in a month, he uses 600 x 30 = 18 000 watts.
Since there are 1000 watts in 1 kilowatt, then John uses 18000/1000 = <<18000/1000=18>>18.
So the final answer is 18

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o16-tgsm8k-s20-rTrue-m4096
                                                                                   Evaluating commonsenseqa :   1%|          | 1/100 [01:33<2:34:58, 93.93s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [03:05<2:31:33, 92.79s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [03:13<2:38:00, 96.74s/it]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 97, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 94, in main
    inference_main(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 90, in inference_main
    query_ids, response_ids, answers, indices = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 65, in run_model
    gen_out = model.generate(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 1454, in generate
    return self.sample(
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 2568, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3586073) of binary: /home/ylu130/.conda/envs/ood/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.conda/envs/ood/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-07_06:33:15
  host      : icgpu06.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3586073)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o17-tgsm8k-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 17 16 17 18 19 20 True 4096 10
[2023-09-07 06:33:19,644] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o17-tgsm8k-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 17
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o17-tgsm8k-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 281892.93it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.20s/it]
 > number of parameters: 6738415616
[2023-09-07 06:33:29,956] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 06:33:30,161] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 06:33:30,163] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 06:33:30,163] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 06:33:30,163] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 06:33:30,163] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 06:33:30,163] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3be1f0>
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 06:33:30,164] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 06:33:30,165] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 06:33:30,165] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Tapanga and Corey have 66 candies together. However, Tapanga has 8 more candies than Corey. How many candies does Corey have?
Output: Let x = the total number of candies Corey has.
x + 8 = the total number of candies Tapanga has.
The equation for the total number of candies is x + (x + 8) = 66
Combining like terms, we get 2x + 8 = 66
Subtracting 8 from both sides, we get 2x = 58
Dividing both sides by 2, we get x = <<29=29>>29, so Corey has 29 candies.
So the final answer is 29

Input: Freddy is calling his family on New Year's Eve. He calls his dad, who lives in the same city as him, and they talk for 45 minutes. Then he calls his brother, who lives on the other side of the world, and they talk for 31 minutes. Local calls cost 5 cents a minute, while international calls cost 25 cents a minute. How many dollars did Freddy spend calling his family on New Year's Eve?
Output: At 5 cents a minute, calling his father cost Freddy 5* 45 = <<5*45=225>>225 cents.
At 25 cents a minute, calling his brother cost Freddy 25 * 31 = <<25*31=775>>775 cents.
Adding the cost of calling his father and brother, we find that Freddy paid a total of 225 + 775 = <<225+775=1000>>1000 cents.
Since each dollar has 100 cents, Freddy paid 1000 / 100 = <<1000/100=10>>10 dollars
So the final answer is 10

Input: Lawrence worked 8 hours each day on Monday, Tuesday and Friday. He worked 5.5 hours on both Wednesday and Thursday. How many hours would Lawrence work each day if he worked the same number of hours each day?
Output: 8 hours * 3 = <<8*3=24>>24 hours
5.5 * 2 = <<5.5*2=11>>11 hours
24 + 11 = <<24+11=35>>35 hours
35/7 = <<35/7=5>>5 hours
Lawrence would work 5 hours each of the 7 days in a week.
So the final answer is 5

Input: Ali had a stock of 800 books in his Room. He sold 60 on Monday, 10 on Tuesday, 20 on Wednesday, 44 on Thursday and 66 on Friday. How many books were not sold?
Output: We look first for the total number of books that were sold: 60 + 10 + 20 + 44 + 66 = <<60+10+20+44+66=200>>200 books.
So the total number of books that were not sold is: 800 – 200 = <<800-200=600>>600 books.
So the final answer is 600

Input: Michael makes birdhouses to sell at craft shows. He charges $22 for each large birdhouse, $16 for each medium birdhouse, and $7 for each small birdhouse. This week, he sold 2 large birdhouses, 2 medium birdhouses, and 3 small birdhouses. How much money, in dollars, did he make this week?
Output: Michael sold 2 large birdhouses for $22 each, so he made 2*$22= $<<2*22=44>>44 from large birdhouse sales.
Michael also sold 2 medium birdhouses for $16 each, so he made 2*$16= $<<2*16=32>>32 from medium birdhouse sales.
Michael sold 3 small birdhouses for $7 each, so he made 3*7=$<<3*7=21>>21 from small birdhouse sales.
Since Michael made $44 from large birdhouse sales, $32 from medium birdhouse sales, and $21 for small birdhouse sales, he made $44+$32+$21= $<<44+32+21=97>>97 total this week.
So the final answer is 97

Input: Nalani had two female dogs that were expecting and after a month gave birth to 10 puppies each. She then sold 3/4 of the puppies after they came of age, each at $200. Calculate the total amount of money she received from the sale of the puppies.
Output: If the two expectant dogs gave birth to 10 puppies each, the total number of puppies Nalani had is 10+10= <<10+10=20>>20
When they came of age, Nalani sold 3/4 of the dogs, a total of 3/4*20 = <<3/4*20=15>>15 dogs.
If each dog sold for $200, Nalani received 15*200 = $<<15*200=3000>>3000 from the sale of the dogs.
So the final answer is 3000

Input: Boris has 24 books and he donates a fourth of his books to the library. Cameron has 30 books and he donates a third of his books to the library. After donating their books, how many books in total do Boris and Cameron have together?
Output: Boris donates 24 / 4 = <<24/4=6>>6 books
Then Boris has a total of 24 - 6 = <<24-6=18>>18 books
Cameron donates 30 / 3 = <<30/3=10>>10 books
Then Cameron has a total of 30 - 10 = <<30-10=20>>20 books
Altogether, Boris and Cameron have 18 + 20 = <<18+20=38>>38 books
So the final answer is 38

Input: There are 3 boxes of cereal. One box holds 14 ounces of cereal. Another box holds half the amount of the first box and 5 ounces less than the third box. How much cereal is there in all 3 cereal boxes?
Output: First = <<14=14>>14 oz
Second = (1/2) * 14 = <<(1/2)*14=7>>7 oz
Third = 7 + 5 = <<7+5=12>>12 oz
14 + 7 + 12 = <<14+7+12=33>>33 oz
There are 33 ounces of cereal in those 3 boxes.
So the final answer is 33

Input: A jug needs 40 cups of water to be full. A custodian at Truman Elementary School has to fill water jugs for 200 students, who drink 10 cups of water in a day. How many water jugs will the custodian fill with cups of water to provide the students with all the water they need in a day?
Output: Since each student needs 10 cups of water per day and there are 200 students, the custodian has to provide 200*10 = <<200*10=2000>>2000 cups of water.
A jug of water needs 40 cups to be full, so 2000 cups of water will fill 2000/40 = <<2000/40=50>>50 jugs
So the final answer is 50

Input: It takes 1 hour for refrigerated dough to come to room temperature.  It takes 15 minutes to shape the dough and 2 hours to proof.  The bread takes 30 minutes to bake and 15 minutes to cool.  If the bakery opens at 6:00 am, what is the latest time the head baker can make it to the store to start working?
Output: It takes 1 hour to bring the dough to room temp and 2 hours to proof the dough so that's 1+2 = <<1+2=3>>3 hours
It takes 15 minutes to shape the dough, 30 minutes to bake and another 15 minutes to cool for a total of 15+30+15 = <<15+30+15=60>>60 minutes
There are 60 minutes in 1 hour so that takes him 60/60 = <<60/60=1>>1 hour
In total it takes 3+1 = <<3+1=4>>4 hours from start to finish to make the bread
If the bakery opens at 6:00 am and it takes 4 hours to prep then the latest the head baker can show up is 6-4 = <<6-4=2>>2:00 am
So the final answer is 2

Input: Geric had twice as many bills as Kyla who has 2 fewer bills than Jessa.  After giving 3 bills to Geric, Jessa has 7 bills left. How many bills did Geric have at the beginning?
Output: Jessa had 7 + 3 = <<7+3=10>>10 bills before she gave 3 to Geric.
Kyla had 10 - 2 = <<10-2=8>>8 bills.
So, Geric had 8 x 2 = <<8*2=16>>16 bills.
So the final answer is 16

Input: They say the first year of a dog's life equals 15 human years. The second year of a dog's life equals 9 human years and after that, every year of a dog's life equals 5 human years. According to this logic, how many human years has my 10-year-old dog lived?
Output: If your dog is 10 years old then in his first year of life he lived 1*15 = 15 human years
In his second year of life, he lived 1*9 = <<1*9=9>>9 human years
We need to calculate his remaining years or 10-2 = <<10-2=8>>8 years of dog life into human years
If 1 year of dog life after the 2 years equates to 5 human years, then 8 years of dog life equals 8*5 = <<8*5=40>>40 human years
In total, your dog has lived 15 + 9 + 40 = <<15+9+40=64>>64 human years
So the final answer is 64

Input: Company A and Company B merge. Company A receives 60% of the combined profits under the new merger, and company B receives 40% of the profits. If company B gets a total of $60000 in profit, how much does company A get?
Output: Let the total profit of both companies be denoted by x.
Therefore, 40% * x = 0.4 * x = $60000
So x = $60000 / 0.4 = $150000.
If Company B gets 60 % of the profits, it gets 60% of $150000, which is 0.6 * $150000 = $<<0.6*150000=90000>>90000.
So the final answer is 90000

Input: Sam, Sid, and Steve brought popsicle sticks for their group activity in their Art class. Sam has thrice as many as Sid, and Sid has twice as many as Steve. If Steve has 12 popsicle sticks, how many popsicle sticks can they use for their Art class activity?
Output: Sid has 12 x 2 = <<12*2=24>>24 sticks.
Sam has 24 x 3 = <<24*3=72>>72 sticks.
Thus, they have 24 + 72 + 12 = <<24+72+12=108>>108 popsicle sticks.
So the final answer is 108

Input: The price of an iPhone fell 10% in a particular month and another 20% in the second month.  If the initial price was $1000, calculate the price after the second month.
Output: In the first month, the price of the iPhone fell by 10/100*$1000=$<<10/100*1000=100>>100
The new price after the first month is $1000-$100 = $<<1000-100=900>>900
In the second month, the price fell 20% further by 20/100*$900 = $<<20/100*900=180>>180
The new price after the second month is $900-$180 = $<<900-180=720>>720
So the final answer is 720

Input: John uses a 75-watt electric fan for 8 hours a day. How much kWh of electric energy does he consume per month (30 days) for using the electric fan?
Output: In a day, John uses 75x 8 = <<75*8=600>>600 watts.
So in a month, he uses 600 x 30 = 18 000 watts.
Since there are 1000 watts in 1 kilowatt, then John uses 18000/1000 = <<18000/1000=18>>18.
So the final answer is 18

Input: Barbara Blackburn can type 212 words per minute.  Due to Carpal tunnel syndrome, Barbara cannot use her left hand for a while so her typing speed is now 40 words less per minute. If she is supposed to type a document with 3440 words, how many minutes will it take her to finish typing the document?
Output: Due to carpal tunnel syndrome, Barbara can only type 212 - 40= <<212-40=172>>172 words per minute.
So, she will be able to finish typing the document in 3440/172 = <<3440/172=20>>20 minutes.
So the final answer is 20

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o17-tgsm8k-s20-rTrue-m4096
                                                                                   Evaluating commonsenseqa :   1%|          | 1/100 [01:31<2:30:21, 91.12s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [03:01<2:27:41, 90.43s/it]    Evaluating commonsenseqa :   3%|▎         | 3/100 [04:29<2:24:56, 89.65s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [05:58<2:22:46, 89.23s/it]    Evaluating commonsenseqa :   5%|▌         | 5/100 [07:27<2:21:16, 89.22s/it]    Evaluating commonsenseqa :   6%|▌         | 6/100 [08:56<2:19:46, 89.22s/it]    Evaluating commonsenseqa :   7%|▋         | 7/100 [10:25<2:18:08, 89.12s/it]    Evaluating commonsenseqa :   8%|▊         | 8/100 [11:54<2:16:25, 88.97s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [13:23<2:15:02, 89.04s/it]    Evaluating commonsenseqa :  10%|█         | 10/100 [14:52<2:13:32, 89.03s/it]     Evaluating commonsenseqa :  11%|█         | 11/100 [16:22<2:12:17, 89.18s/it]     Evaluating commonsenseqa :  12%|█▏        | 12/100 [17:51<2:10:49, 89.19s/it]   Evaluating commonsenseqa :  13%|█▎        | 13/100 [19:20<2:09:28, 89.30s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [20:49<2:07:55, 89.25s/it]   Evaluating commonsenseqa :  15%|█▌        | 15/100 [22:18<2:06:14, 89.11s/it]   Evaluating commonsenseqa :  16%|█▌        | 16/100 [23:47<2:04:42, 89.08s/it]   Evaluating commonsenseqa :  17%|█▋        | 17/100 [25:17<2:03:26, 89.24s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [26:46<2:01:49, 89.14s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [28:15<2:00:10, 89.02s/it]   Evaluating commonsenseqa :  20%|██        | 20/100 [29:44<1:58:45, 89.07s/it]   Evaluating commonsenseqa :  21%|██        | 21/100 [31:13<1:57:18, 89.09s/it]   Evaluating commonsenseqa :  22%|██▏       | 22/100 [32:42<1:55:43, 89.02s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [34:10<1:54:02, 88.87s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [35:39<1:52:38, 88.93s/it]     Evaluating commonsenseqa :  25%|██▌       | 25/100 [37:08<1:51:12, 88.97s/it]     Evaluating commonsenseqa :  26%|██▌       | 26/100 [38:38<1:49:56, 89.14s/it]     Evaluating commonsenseqa :  27%|██▋       | 27/100 [40:07<1:48:15, 88.98s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [41:35<1:46:42, 88.93s/it]     Evaluating commonsenseqa :  29%|██▉       | 29/100 [43:05<1:45:21, 89.04s/it]     Evaluating commonsenseqa :  30%|███       | 30/100 [44:33<1:43:48, 88.97s/it]     Evaluating commonsenseqa :  31%|███       | 31/100 [46:02<1:42:10, 88.85s/it]     Evaluating commonsenseqa :  32%|███▏      | 32/100 [47:31<1:40:37, 88.79s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [49:00<1:39:15, 88.88s/it]   Evaluating commonsenseqa :  34%|███▍      | 34/100 [50:28<1:37:32, 88.67s/it]   Evaluating commonsenseqa :  35%|███▌      | 35/100 [51:57<1:36:09, 88.76s/it]     Evaluating commonsenseqa :  36%|███▌      | 36/100 [53:25<1:34:28, 88.58s/it]     Evaluating commonsenseqa :  37%|███▋      | 37/100 [54:54<1:33:01, 88.60s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [56:22<1:31:34, 88.62s/it]     Evaluating commonsenseqa :  39%|███▉      | 39/100 [57:51<1:30:13, 88.74s/it]     Evaluating commonsenseqa :  40%|████      | 40/100 [59:20<1:28:40, 88.68s/it]     Evaluating commonsenseqa :  41%|████      | 41/100 [1:00:48<1:27:01, 88.49s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:02:17<1:25:38, 88.59s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:03:46<1:24:13, 88.66s/it] Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:05:14<1:22:46, 88.68s/it] Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:06:43<1:21:19, 88.71s/it] Evaluating commonsenseqa :  46%|████▌     | 46/100 [1:08:12<1:19:54, 88.79s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:09:41<1:18:21, 88.70s/it] Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:11:09<1:16:45, 88.57s/it]   Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:12:38<1:15:19, 88.63s/it]   Evaluating commonsenseqa :  50%|█████     | 50/100 [1:14:07<1:14:05, 88.91s/it]   Evaluating commonsenseqa :  51%|█████     | 51/100 [1:15:37<1:12:45, 89.09s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:17:06<1:11:12, 89.01s/it] Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:18:35<1:09:47, 89.10s/it] Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:20:04<1:08:13, 88.99s/it] Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:21:33<1:06:44, 88.99s/it] Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:23:01<1:05:08, 88.82s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:24:30<1:03:37, 88.77s/it] Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:25:59<1:02:11, 88.84s/it] Evaluating commonsenseqa :  59%|█████▉    | 59/100 [1:27:28<1:00:44, 88.88s/it] Evaluating commonsenseqa :  60%|██████    | 60/100 [1:28:57<59:16, 88.90s/it]     Evaluating commonsenseqa :  61%|██████    | 61/100 [1:30:25<57:43, 88.80s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [1:31:54<56:09, 88.68s/it]   Evaluating commonsenseqa :  63%|██████▎   | 63/100 [1:33:22<54:41, 88.69s/it]   Evaluating commonsenseqa :  64%|██████▍   | 64/100 [1:34:51<53:15, 88.75s/it]   Evaluating commonsenseqa :  65%|██████▌   | 65/100 [1:36:20<51:45, 88.72s/it]   Evaluating commonsenseqa :  66%|██████▌   | 66/100 [1:37:49<50:22, 88.91s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [1:39:17<48:47, 88.72s/it]   Evaluating commonsenseqa :  68%|██████▊   | 68/100 [1:40:46<47:14, 88.57s/it]   Evaluating commonsenseqa :  69%|██████▉   | 69/100 [1:42:14<45:46, 88.60s/it] Evaluating commonsenseqa :  70%|███████   | 70/100 [1:43:43<44:15, 88.53s/it] Evaluating commonsenseqa :  71%|███████   | 71/100 [1:45:12<42:51, 88.67s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [1:46:40<41:18, 88.51s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [1:48:09<39:53, 88.66s/it] Evaluating commonsenseqa :  74%|███████▍  | 74/100 [1:49:38<38:26, 88.73s/it] Evaluating commonsenseqa :  75%|███████▌  | 75/100 [1:51:07<37:00, 88.81s/it] Evaluating commonsenseqa :  76%|███████▌  | 76/100 [1:52:36<35:31, 88.81s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [1:54:05<34:07, 89.01s/it] Evaluating commonsenseqa :  78%|███████▊  | 78/100 [1:55:34<32:35, 88.87s/it] Evaluating commonsenseqa :  79%|███████▉  | 79/100 [1:57:03<31:08, 88.96s/it] Evaluating commonsenseqa :  80%|████████  | 80/100 [1:58:31<29:36, 88.81s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [2:00:00<28:08, 88.88s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [2:01:29<26:38, 88.80s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [2:02:58<25:09, 88.77s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [2:04:26<23:40, 88.77s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [2:05:56<22:13, 88.93s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [2:07:24<20:42, 88.74s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [2:08:53<19:13, 88.70s/it] Evaluating commonsenseqa :  88%|████████▊ | 88/100 [2:10:22<17:46, 88.89s/it] Evaluating commonsenseqa :  89%|████████▉ | 89/100 [2:11:50<16:16, 88.76s/it] Evaluating commonsenseqa :  90%|█████████ | 90/100 [2:13:19<14:47, 88.73s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [2:14:48<13:19, 88.80s/it] Evaluating commonsenseqa :  92%|█████████▏| 92/100 [2:16:17<11:50, 88.79s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [2:17:46<10:21, 88.84s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [2:19:14<08:52, 88.71s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [2:20:43<07:23, 88.72s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [2:22:12<05:55, 88.95s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [2:23:41<04:26, 88.96s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [2:25:10<02:57, 88.90s/it] Evaluating commonsenseqa :  99%|█████████▉| 99/100 [2:26:39<01:28, 88.94s/it] Evaluating commonsenseqa : 100%|██████████| 100/100 [2:28:08<00:00, 88.90s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:28:08<00:00, 88.88s/it]
name: commonsenseqa | avg. gen lenth: 404.14 | time: 8891.527032375336s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o18-tgsm8k-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 18 16 17 18 19 20 True 4096 10
[2023-09-07 09:01:50,282] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o18-tgsm8k-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 18
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o18-tgsm8k-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 272703.53it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
 > number of parameters: 6738415616
[2023-09-07 09:01:59,821] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 09:02:00,024] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 09:02:00,026] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0280>
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 09:02:00,026] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 09:02:00,027] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 09:02:00,028] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 09:02:00,028] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 09:02:00,028] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 09:02:00,028] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 09:02:00,028] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Tapanga and Corey have 66 candies together. However, Tapanga has 8 more candies than Corey. How many candies does Corey have?
Output: Let x = the total number of candies Corey has.
x + 8 = the total number of candies Tapanga has.
The equation for the total number of candies is x + (x + 8) = 66
Combining like terms, we get 2x + 8 = 66
Subtracting 8 from both sides, we get 2x = 58
Dividing both sides by 2, we get x = <<29=29>>29, so Corey has 29 candies.
So the final answer is 29

Input: Freddy is calling his family on New Year's Eve. He calls his dad, who lives in the same city as him, and they talk for 45 minutes. Then he calls his brother, who lives on the other side of the world, and they talk for 31 minutes. Local calls cost 5 cents a minute, while international calls cost 25 cents a minute. How many dollars did Freddy spend calling his family on New Year's Eve?
Output: At 5 cents a minute, calling his father cost Freddy 5* 45 = <<5*45=225>>225 cents.
At 25 cents a minute, calling his brother cost Freddy 25 * 31 = <<25*31=775>>775 cents.
Adding the cost of calling his father and brother, we find that Freddy paid a total of 225 + 775 = <<225+775=1000>>1000 cents.
Since each dollar has 100 cents, Freddy paid 1000 / 100 = <<1000/100=10>>10 dollars
So the final answer is 10

Input: Lawrence worked 8 hours each day on Monday, Tuesday and Friday. He worked 5.5 hours on both Wednesday and Thursday. How many hours would Lawrence work each day if he worked the same number of hours each day?
Output: 8 hours * 3 = <<8*3=24>>24 hours
5.5 * 2 = <<5.5*2=11>>11 hours
24 + 11 = <<24+11=35>>35 hours
35/7 = <<35/7=5>>5 hours
Lawrence would work 5 hours each of the 7 days in a week.
So the final answer is 5

Input: Ali had a stock of 800 books in his Room. He sold 60 on Monday, 10 on Tuesday, 20 on Wednesday, 44 on Thursday and 66 on Friday. How many books were not sold?
Output: We look first for the total number of books that were sold: 60 + 10 + 20 + 44 + 66 = <<60+10+20+44+66=200>>200 books.
So the total number of books that were not sold is: 800 – 200 = <<800-200=600>>600 books.
So the final answer is 600

Input: Michael makes birdhouses to sell at craft shows. He charges $22 for each large birdhouse, $16 for each medium birdhouse, and $7 for each small birdhouse. This week, he sold 2 large birdhouses, 2 medium birdhouses, and 3 small birdhouses. How much money, in dollars, did he make this week?
Output: Michael sold 2 large birdhouses for $22 each, so he made 2*$22= $<<2*22=44>>44 from large birdhouse sales.
Michael also sold 2 medium birdhouses for $16 each, so he made 2*$16= $<<2*16=32>>32 from medium birdhouse sales.
Michael sold 3 small birdhouses for $7 each, so he made 3*7=$<<3*7=21>>21 from small birdhouse sales.
Since Michael made $44 from large birdhouse sales, $32 from medium birdhouse sales, and $21 for small birdhouse sales, he made $44+$32+$21= $<<44+32+21=97>>97 total this week.
So the final answer is 97

Input: Nalani had two female dogs that were expecting and after a month gave birth to 10 puppies each. She then sold 3/4 of the puppies after they came of age, each at $200. Calculate the total amount of money she received from the sale of the puppies.
Output: If the two expectant dogs gave birth to 10 puppies each, the total number of puppies Nalani had is 10+10= <<10+10=20>>20
When they came of age, Nalani sold 3/4 of the dogs, a total of 3/4*20 = <<3/4*20=15>>15 dogs.
If each dog sold for $200, Nalani received 15*200 = $<<15*200=3000>>3000 from the sale of the dogs.
So the final answer is 3000

Input: Boris has 24 books and he donates a fourth of his books to the library. Cameron has 30 books and he donates a third of his books to the library. After donating their books, how many books in total do Boris and Cameron have together?
Output: Boris donates 24 / 4 = <<24/4=6>>6 books
Then Boris has a total of 24 - 6 = <<24-6=18>>18 books
Cameron donates 30 / 3 = <<30/3=10>>10 books
Then Cameron has a total of 30 - 10 = <<30-10=20>>20 books
Altogether, Boris and Cameron have 18 + 20 = <<18+20=38>>38 books
So the final answer is 38

Input: There are 3 boxes of cereal. One box holds 14 ounces of cereal. Another box holds half the amount of the first box and 5 ounces less than the third box. How much cereal is there in all 3 cereal boxes?
Output: First = <<14=14>>14 oz
Second = (1/2) * 14 = <<(1/2)*14=7>>7 oz
Third = 7 + 5 = <<7+5=12>>12 oz
14 + 7 + 12 = <<14+7+12=33>>33 oz
There are 33 ounces of cereal in those 3 boxes.
So the final answer is 33

Input: A jug needs 40 cups of water to be full. A custodian at Truman Elementary School has to fill water jugs for 200 students, who drink 10 cups of water in a day. How many water jugs will the custodian fill with cups of water to provide the students with all the water they need in a day?
Output: Since each student needs 10 cups of water per day and there are 200 students, the custodian has to provide 200*10 = <<200*10=2000>>2000 cups of water.
A jug of water needs 40 cups to be full, so 2000 cups of water will fill 2000/40 = <<2000/40=50>>50 jugs
So the final answer is 50

Input: It takes 1 hour for refrigerated dough to come to room temperature.  It takes 15 minutes to shape the dough and 2 hours to proof.  The bread takes 30 minutes to bake and 15 minutes to cool.  If the bakery opens at 6:00 am, what is the latest time the head baker can make it to the store to start working?
Output: It takes 1 hour to bring the dough to room temp and 2 hours to proof the dough so that's 1+2 = <<1+2=3>>3 hours
It takes 15 minutes to shape the dough, 30 minutes to bake and another 15 minutes to cool for a total of 15+30+15 = <<15+30+15=60>>60 minutes
There are 60 minutes in 1 hour so that takes him 60/60 = <<60/60=1>>1 hour
In total it takes 3+1 = <<3+1=4>>4 hours from start to finish to make the bread
If the bakery opens at 6:00 am and it takes 4 hours to prep then the latest the head baker can show up is 6-4 = <<6-4=2>>2:00 am
So the final answer is 2

Input: Geric had twice as many bills as Kyla who has 2 fewer bills than Jessa.  After giving 3 bills to Geric, Jessa has 7 bills left. How many bills did Geric have at the beginning?
Output: Jessa had 7 + 3 = <<7+3=10>>10 bills before she gave 3 to Geric.
Kyla had 10 - 2 = <<10-2=8>>8 bills.
So, Geric had 8 x 2 = <<8*2=16>>16 bills.
So the final answer is 16

Input: They say the first year of a dog's life equals 15 human years. The second year of a dog's life equals 9 human years and after that, every year of a dog's life equals 5 human years. According to this logic, how many human years has my 10-year-old dog lived?
Output: If your dog is 10 years old then in his first year of life he lived 1*15 = 15 human years
In his second year of life, he lived 1*9 = <<1*9=9>>9 human years
We need to calculate his remaining years or 10-2 = <<10-2=8>>8 years of dog life into human years
If 1 year of dog life after the 2 years equates to 5 human years, then 8 years of dog life equals 8*5 = <<8*5=40>>40 human years
In total, your dog has lived 15 + 9 + 40 = <<15+9+40=64>>64 human years
So the final answer is 64

Input: Company A and Company B merge. Company A receives 60% of the combined profits under the new merger, and company B receives 40% of the profits. If company B gets a total of $60000 in profit, how much does company A get?
Output: Let the total profit of both companies be denoted by x.
Therefore, 40% * x = 0.4 * x = $60000
So x = $60000 / 0.4 = $150000.
If Company B gets 60 % of the profits, it gets 60% of $150000, which is 0.6 * $150000 = $<<0.6*150000=90000>>90000.
So the final answer is 90000

Input: Sam, Sid, and Steve brought popsicle sticks for their group activity in their Art class. Sam has thrice as many as Sid, and Sid has twice as many as Steve. If Steve has 12 popsicle sticks, how many popsicle sticks can they use for their Art class activity?
Output: Sid has 12 x 2 = <<12*2=24>>24 sticks.
Sam has 24 x 3 = <<24*3=72>>72 sticks.
Thus, they have 24 + 72 + 12 = <<24+72+12=108>>108 popsicle sticks.
So the final answer is 108

Input: The price of an iPhone fell 10% in a particular month and another 20% in the second month.  If the initial price was $1000, calculate the price after the second month.
Output: In the first month, the price of the iPhone fell by 10/100*$1000=$<<10/100*1000=100>>100
The new price after the first month is $1000-$100 = $<<1000-100=900>>900
In the second month, the price fell 20% further by 20/100*$900 = $<<20/100*900=180>>180
The new price after the second month is $900-$180 = $<<900-180=720>>720
So the final answer is 720

Input: John uses a 75-watt electric fan for 8 hours a day. How much kWh of electric energy does he consume per month (30 days) for using the electric fan?
Output: In a day, John uses 75x 8 = <<75*8=600>>600 watts.
So in a month, he uses 600 x 30 = 18 000 watts.
Since there are 1000 watts in 1 kilowatt, then John uses 18000/1000 = <<18000/1000=18>>18.
So the final answer is 18

Input: Barbara Blackburn can type 212 words per minute.  Due to Carpal tunnel syndrome, Barbara cannot use her left hand for a while so her typing speed is now 40 words less per minute. If she is supposed to type a document with 3440 words, how many minutes will it take her to finish typing the document?
Output: Due to carpal tunnel syndrome, Barbara can only type 212 - 40= <<212-40=172>>172 words per minute.
So, she will be able to finish typing the document in 3440/172 = <<3440/172=20>>20 minutes.
So the final answer is 20

Input: Max fills up water balloons for 30 minutes at a rate of 2 water balloons every minute. Max’s friend Zach fills up water balloons for 40 minutes at a rate of 3 water balloons every minute. In the process, 10 of the water balloons pop on the ground. How many filled water balloons do Max and Zach have in total?
Output: Max fills 30 * 2 = <<30*2=60>>60 water balloons
Zach fills 40 * 3 = <<40*3=120>>120 water balloons
Max and Zach have a total of 60 + 120 - 10 = <<60+120-10=170>>170 water balloons
So the final answer is 170

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o18-tgsm8k-s20-rTrue-m4096
                                                                                                   Evaluating commonsenseqa :   1%|          | 1/100 [01:29<2:27:18, 89.27s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [02:57<2:24:40, 88.58s/it]                    Evaluating commonsenseqa :   3%|▎         | 3/100 [04:24<2:22:28, 88.13s/it]                    Evaluating commonsenseqa :   4%|▍         | 4/100 [05:52<2:20:35, 87.87s/it]                    Evaluating commonsenseqa :   5%|▌         | 5/100 [07:19<2:18:55, 87.74s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [08:47<2:17:25, 87.72s/it]                    Evaluating commonsenseqa :   7%|▋         | 7/100 [10:14<2:15:46, 87.59s/it]                    Evaluating commonsenseqa :   8%|▊         | 8/100 [11:42<2:14:04, 87.44s/it]50<00:00, 111.76s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [3:06:50<00:00, 112.10s/it]
name: commonsenseqa | avg. gen lenth: 381.167 | time: 11212.656490802765s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu02 --master_port 19674 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tgsm8k-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 13 11 12 13 14 15 True 4096 10
[2023-09-07 09:13:50,925] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tgsm8k-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 13
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tgsm8k-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 327847.76it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.52s/it]
 > number of parameters: 6738415616
[2023-09-07 09:14:03,772] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 09:14:03,977] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 09:14:03,979] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 09:14:03,979] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 09:14:03,979] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 09:14:03,979] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 09:14:03,979] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0250>
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 09:14:03,980] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 09:14:03,981] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 09:14:03,981] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Tapanga and Corey have 66 candies together. However, Tapanga has 8 more candies than Corey. How many candies does Corey have?
Output: Let x = the total number of candies Corey has.
x + 8 = the total number of candies Tapanga has.
The equation for the total number of candies is x + (x + 8) = 66
Combining like terms, we get 2x + 8 = 66
Subtracting 8 from both sides, we get 2x = 58
Dividing both sides by 2, we get x = <<29=29>>29, so Corey has 29 candies.
So the final answer is 29

Input: Freddy is calling his family on New Year's Eve. He calls his dad, who lives in the same city as him, and they talk for 45 minutes. Then he calls his brother, who lives on the other side of the world, and they talk for 31 minutes. Local calls cost 5 cents a minute, while international calls cost 25 cents a minute. How many dollars did Freddy spend calling his family on New Year's Eve?
Output: At 5 cents a minute, calling his father cost Freddy 5* 45 = <<5*45=225>>225 cents.
At 25 cents a minute, calling his brother cost Freddy 25 * 31 = <<25*31=775>>775 cents.
Adding the cost of calling his father and brother, we find that Freddy paid a total of 225 + 775 = <<225+775=1000>>1000 cents.
Since each dollar has 100 cents, Freddy paid 1000 / 100 = <<1000/100=10>>10 dollars
So the final answer is 10

Input: Lawrence worked 8 hours each day on Monday, Tuesday and Friday. He worked 5.5 hours on both Wednesday and Thursday. How many hours would Lawrence work each day if he worked the same number of hours each day?
Output: 8 hours * 3 = <<8*3=24>>24 hours
5.5 * 2 = <<5.5*2=11>>11 hours
24 + 11 = <<24+11=35>>35 hours
35/7 = <<35/7=5>>5 hours
Lawrence would work 5 hours each of the 7 days in a week.
So the final answer is 5

Input: Ali had a stock of 800 books in his Room. He sold 60 on Monday, 10 on Tuesday, 20 on Wednesday, 44 on Thursday and 66 on Friday. How many books were not sold?
Output: We look first for the total number of books that were sold: 60 + 10 + 20 + 44 + 66 = <<60+10+20+44+66=200>>200 books.
So the total number of books that were not sold is: 800 – 200 = <<800-200=600>>600 books.
So the final answer is 600

Input: Michael makes birdhouses to sell at craft shows. He charges $22 for each large birdhouse, $16 for each medium birdhouse, and $7 for each small birdhouse. This week, he sold 2 large birdhouses, 2 medium birdhouses, and 3 small birdhouses. How much money, in dollars, did he make this week?
Output: Michael sold 2 large birdhouses for $22 each, so he made 2*$22= $<<2*22=44>>44 from large birdhouse sales.
Michael also sold 2 medium birdhouses for $16 each, so he made 2*$16= $<<2*16=32>>32 from medium birdhouse sales.
Michael sold 3 small birdhouses for $7 each, so he made 3*7=$<<3*7=21>>21 from small birdhouse sales.
Since Michael made $44 from large birdhouse sales, $32 from medium birdhouse sales, and $21 for small birdhouse sales, he made $44+$32+$21= $<<44+32+21=97>>97 total this week.
So the final answer is 97

Input: Nalani had two female dogs that were expecting and after a month gave birth to 10 puppies each. She then sold 3/4 of the puppies after they came of age, each at $200. Calculate the total amount of money she received from the sale of the puppies.
Output: If the two expectant dogs gave birth to 10 puppies each, the total number of puppies Nalani had is 10+10= <<10+10=20>>20
When they came of age, Nalani sold 3/4 of the dogs, a total of 3/4*20 = <<3/4*20=15>>15 dogs.
If each dog sold for $200, Nalani received 15*200 = $<<15*200=3000>>3000 from the sale of the dogs.
So the final answer is 3000

Input: Boris has 24 books and he donates a fourth of his books to the library. Cameron has 30 books and he donates a third of his books to the library. After donating their books, how many books in total do Boris and Cameron have together?
Output: Boris donates 24 / 4 = <<24/4=6>>6 books
Then Boris has a total of 24 - 6 = <<24-6=18>>18 books
Cameron donates 30 / 3 = <<30/3=10>>10 books
Then Cameron has a total of 30 - 10 = <<30-10=20>>20 books
Altogether, Boris and Cameron have 18 + 20 = <<18+20=38>>38 books
So the final answer is 38

Input: There are 3 boxes of cereal. One box holds 14 ounces of cereal. Another box holds half the amount of the first box and 5 ounces less than the third box. How much cereal is there in all 3 cereal boxes?
Output: First = <<14=14>>14 oz
Second = (1/2) * 14 = <<(1/2)*14=7>>7 oz
Third = 7 + 5 = <<7+5=12>>12 oz
14 + 7 + 12 = <<14+7+12=33>>33 oz
There are 33 ounces of cereal in those 3 boxes.
So the final answer is 33

Input: A jug needs 40 cups of water to be full. A custodian at Truman Elementary School has to fill water jugs for 200 students, who drink 10 cups of water in a day. How many water jugs will the custodian fill with cups of water to provide the students with all the water they need in a day?
Output: Since each student needs 10 cups of water per day and there are 200 students, the custodian has to provide 200*10 = <<200*10=2000>>2000 cups of water.
A jug of water needs 40 cups to be full, so 2000 cups of water will fill 2000/40 = <<2000/40=50>>50 jugs
So the final answer is 50

Input: It takes 1 hour for refrigerated dough to come to room temperature.  It takes 15 minutes to shape the dough and 2 hours to proof.  The bread takes 30 minutes to bake and 15 minutes to cool.  If the bakery opens at 6:00 am, what is the latest time the head baker can make it to the store to start working?
Output: It takes 1 hour to bring the dough to room temp and 2 hours to proof the dough so that's 1+2 = <<1+2=3>>3 hours
It takes 15 minutes to shape the dough, 30 minutes to bake and another 15 minutes to cool for a total of 15+30+15 = <<15+30+15=60>>60 minutes
There are 60 minutes in 1 hour so that takes him 60/60 = <<60/60=1>>1 hour
In total it takes 3+1 = <<3+1=4>>4 hours from start to finish to make the bread
If the bakery opens at 6:00 am and it takes 4 hours to prep then the latest the head baker can show up is 6-4 = <<6-4=2>>2:00 am
So the final answer is 2

Input: Geric had twice as many bills as Kyla who has 2 fewer bills than Jessa.  After giving 3 bills to Geric, Jessa has 7 bills left. How many bills did Geric have at the beginning?
Output: Jessa had 7 + 3 = <<7+3=10>>10 bills before she gave 3 to Geric.
Kyla had 10 - 2 = <<10-2=8>>8 bills.
So, Geric had 8 x 2 = <<8*2=16>>16 bills.
So the final answer is 16

Input: They say the first year of a dog's life equals 15 human years. The second year of a dog's life equals 9 human years and after that, every year of a dog's life equals 5 human years. According to this logic, how many human years has my 10-year-old dog lived?
Output: If your dog is 10 years old then in his first year of life he lived 1*15 = 15 human years
In his second year of life, he lived 1*9 = <<1*9=9>>9 human years
We need to calculate his remaining years or 10-2 = <<10-2=8>>8 years of dog life into human years
If 1 year of dog life after the 2 years equates to 5 human years, then 8 years of dog life equals 8*5 = <<8*5=40>>40 human years
In total, your dog has lived 15 + 9 + 40 = <<15+9+40=64>>64 human years
So the final answer is 64

Input: Company A and Company B merge. Company A receives 60% of the combined profits under the new merger, and company B receives 40% of the profits. If company B gets a total of $60000 in profit, how much does company A get?
Output: Let the total profit of both companies be denoted by x.
Therefore, 40% * x = 0.4 * x = $60000
So x = $60000 / 0.4 = $150000.
If Company B gets 60 % of the profits, it gets 60% of $150000, which is 0.6 * $150000 = $<<0.6*150000=90000>>90000.
So the final answer is 90000

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tgsm8k-s20-rTrue-m4096
                                                                               Evaluating commonsenseqa :   1%|          | 1/100 [01:46<2:55:06, 106.13s/it]  Evaluating commonsenseqa :   2%|▏         | 2/100 [03:30<2:51:56, 105.27s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [05:15<2:49:31, 104.86s/it]  Evaluating commonsenseqa :   4%|▍         | 4/100 [07:00<2:48:05, 105.06s/it]  Evaluating commonsenseqa :   5%|▌         | 5/100 [08:45<2:46:17, 105.02s/it]                                                                                    Evaluating commonsenseqa :   6%|▌         | 6/100 [10:30<2:44:19, 104.89s/it]  Evaluating commonsenseqa :   7%|▋         | 7/100 [12:13<2:41:48, 104.39s/it]  Evaluating commonsenseqa :   8%|▊         | 8/100 [13:57<2:39:59, 104.35s/it]  Evaluating commonsenseqa :   9%|▉         | 9/100 [15:42<2:38:16, 104.36s/it]  Evaluating commonsenseqa :  10%|█         | 10/100 [17:25<2:36:16, 104.18s/it]                                                                                   Evaluating commonsenseqa :  11%|█         | 11/100 [19:10<2:34:37, 104.24s/it]   Evaluating commonsenseqa :  12%|█▏        | 12/100 [20:56<2:33:33, 104.70s/it] Evaluating commonsenseqa :  13%|█▎        | 13/100 [22:41<2:32:13, 104.98s/it] Evaluating commonsenseqa :  14%|█▍        | 14/100 [24:26<2:30:20, 104.89s/it] Evaluating commonsenseqa :  15%|█▌        | 15/100 [26:10<2:28:17, 104.67s/it]                                                                                     Evaluating commonsenseqa :  16%|█▌        | 16/100 [27:54<2:26:12, 104.43s/it] Evaluating commonsenseqa :  17%|█▋        | 17/100 [29:39<2:24:53, 104.74s/it] Evaluating commonsenseqa :  18%|█▊        | 18/100 [31:24<2:22:54, 104.57s/it] Evaluating commonsenseqa :  19%|█▉        | 19/100 [33:08<2:21:13, 104.61s/it] Evaluating commonsenseqa :  20%|██        | 20/100 [34:54<2:20:02, 105.04s/it]                                                                                         Evaluating commonsenseqa :  21%|██        | 21/100 [36:39<2:18:19, 105.06s/it]   Evaluating commonsenseqa :  22%|██▏       | 22/100 [38:24<2:16:17, 104.83s/it] Evaluating commonsenseqa :  23%|██▎       | 23/100 [40:08<2:14:30, 104.81s/it] Evaluating commonsenseqa :  24%|██▍       | 24/100 [41:54<2:13:13, 105.17s/it] Evaluating commonsenseqa :  25%|██▌       | 25/100 [43:39<2:11:19, 105.05s/it]                                                                                       Evaluating commonsenseqa :  26%|██▌       | 26/100 [45:24<2:09:32, 105.04s/it] Evaluating commonsenseqa :  41%|████      | 41/100 [59:49<1:26:16, 87.73s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:01:17<1:24:56, 87.87s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:02:45<1:23:22, 87.77s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:04:12<1:21:52, 87.72s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:05:40<1:20:18, 87.60s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [1:07:07<1:18:46, 87.53s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:08:35<1:17:21, 87.58s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:10:02<1:15:55, 87.61s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:11:30<1:14:32, 87.70s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [1:12:58<1:13:01, 87.63s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [1:14:26<1:11:36, 87.68s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:15:53<1:10:07, 87.67s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:17:21<1:08:35, 87.57s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:18:48<1:07:04, 87.48s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:20:15<1:05:38, 87.51s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:21:43<1:04:14, 87.61s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:23:11<1:02:45, 87.58s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:24:38<1:01:18, 87.58s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [1:26:06<59:52, 87.63s/it]  Evaluating commonsenseqa :  60%|██████    | 60/100 [1:27:33<58:19, 87.49s/it] Evaluating commonsenseqa :  61%|██████    | 61/100 [1:29:01<56:50, 87.45s/it] Evaluating commonsenseqa :  62%|██████▏   | 62/100 [1:30:29<55:30, 87.65s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [1:31:56<54:03, 87.65s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [1:33:24<52:32, 87.56s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [1:34:52<51:06, 87.61s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [1:36:19<49:41, 87.70s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [1:37:47<48:14, 87.72s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [1:39:14<46:42, 87.58s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [1:40:42<45:11, 87.48s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [1:42:09<43:43, 87.44s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [1:43:36<42:14, 87.40s/it] Evaluating commonsenseqa :  72%|███████▏  | 72/100 [1:45:04<40:47, 87.39s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [1:46:31<39:21, 87.48s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [1:47:59<37:57, 87.58s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [1:49:27<36:28, 87.53s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [1:50:55<35:03, 87.64s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [1:52:22<33:35, 87.63s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [1:53:49<32:05, 87.53s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [1:55:17<30:37, 87.51s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [1:56:44<29:10, 87.51s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [1:58:12<27:42, 87.48s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [1:59:40<26:16, 87.59s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [2:01:07<24:49, 87.65s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [2:02:35<23:21, 87.57s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [2:04:02<21:50, 87.39s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [2:05:30<20:25, 87.53s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [2:06:57<18:58, 87.58s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [2:08:25<17:31, 87.65s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [2:09:53<16:04, 87.67s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [2:11:21<14:38, 87.80s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [2:12:48<13:09, 87.70s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [2:14:16<11:42, 87.76s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [2:15:44<10:13, 87.61s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [2:17:11<08:45, 87.51s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [2:18:39<07:17, 87.57s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [2:20:06<05:50, 87.62s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [2:21:34<04:23, 87.74s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [2:23:02<02:55, 87.84s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [2:24:30<01:27, 87.73s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:25:58<00:00, 87.77s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:25:58<00:00, 87.58s/it]
name: commonsenseqa | avg. gen lenth: 379.752 | time: 8761.538417816162s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o19-tgsm8k-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 19 16 17 18 19 20 True 4096 10
[2023-09-07 11:28:06,408] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o19-tgsm8k-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 19
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o19-tgsm8k-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 262337.57it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.20s/it]
 > number of parameters: 6738415616
[2023-09-07 11:28:22,661] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 11:28:22,865] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 11:28:22,867] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 11:28:22,868] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 11:28:22,868] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 11:28:22,868] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 11:28:22,868] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 11:28:22,868] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 11:28:22,868] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 11:28:22,868] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 11:28:22,868] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 11:28:22,868] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 11:28:22,868] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c01f0>
[2023-09-07 11:28:22,868] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 11:28:22,868] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 11:28:22,868] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 11:28:22,868] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 11:28:22,869] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 11:28:22,870] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 11:28:22,870] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 11:28:22,870] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 11:28:22,870] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 11:28:22,870] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 11:28:22,870] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 11:28:22,870] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 11:28:22,870] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 11:28:22,870] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 11:28:22,870] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 11:28:22,870] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 11:28:22,870] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 11:28:22,870] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 11:28:22,870] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 11:28:22,870] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 11:28:22,870] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Tapanga and Corey have 66 candies together. However, Tapanga has 8 more candies than Corey. How many candies does Corey have?
Output: Let x = the total number of candies Corey has.
x + 8 = the total number of candies Tapanga has.
The equation for the total number of candies is x + (x + 8) = 66
Combining like terms, we get 2x + 8 = 66
Subtracting 8 from both sides, we get 2x = 58
Dividing both sides by 2, we get x = <<29=29>>29, so Corey has 29 candies.
So the final answer is 29

Input: Freddy is calling his family on New Year's Eve. He calls his dad, who lives in the same city as him, and they talk for 45 minutes. Then he calls his brother, who lives on the other side of the world, and they talk for 31 minutes. Local calls cost 5 cents a minute, while international calls cost 25 cents a minute. How many dollars did Freddy spend calling his family on New Year's Eve?
Output: At 5 cents a minute, calling his father cost Freddy 5* 45 = <<5*45=225>>225 cents.
At 25 cents a minute, calling his brother cost Freddy 25 * 31 = <<25*31=775>>775 cents.
Adding the cost of calling his father and brother, we find that Freddy paid a total of 225 + 775 = <<225+775=1000>>1000 cents.
Since each dollar has 100 cents, Freddy paid 1000 / 100 = <<1000/100=10>>10 dollars
So the final answer is 10

Input: Lawrence worked 8 hours each day on Monday, Tuesday and Friday. He worked 5.5 hours on both Wednesday and Thursday. How many hours would Lawrence work each day if he worked the same number of hours each day?
Output: 8 hours * 3 = <<8*3=24>>24 hours
5.5 * 2 = <<5.5*2=11>>11 hours
24 + 11 = <<24+11=35>>35 hours
35/7 = <<35/7=5>>5 hours
Lawrence would work 5 hours each of the 7 days in a week.
So the final answer is 5

Input: Ali had a stock of 800 books in his Room. He sold 60 on Monday, 10 on Tuesday, 20 on Wednesday, 44 on Thursday and 66 on Friday. How many books were not sold?
Output: We look first for the total number of books that were sold: 60 + 10 + 20 + 44 + 66 = <<60+10+20+44+66=200>>200 books.
So the total number of books that were not sold is: 800 – 200 = <<800-200=600>>600 books.
So the final answer is 600

Input: Michael makes birdhouses to sell at craft shows. He charges $22 for each large birdhouse, $16 for each medium birdhouse, and $7 for each small birdhouse. This week, he sold 2 large birdhouses, 2 medium birdhouses, and 3 small birdhouses. How much money, in dollars, did he make this week?
Output: Michael sold 2 large birdhouses for $22 each, so he made 2*$22= $<<2*22=44>>44 from large birdhouse sales.
Michael also sold 2 medium birdhouses for $16 each, so he made 2*$16= $<<2*16=32>>32 from medium birdhouse sales.
Michael sold 3 small birdhouses for $7 each, so he made 3*7=$<<3*7=21>>21 from small birdhouse sales.
Since Michael made $44 from large birdhouse sales, $32 from medium birdhouse sales, and $21 for small birdhouse sales, he made $44+$32+$21= $<<44+32+21=97>>97 total this week.
So the final answer is 97

Input: Nalani had two female dogs that were expecting and after a month gave birth to 10 puppies each. She then sold 3/4 of the puppies after they came of age, each at $200. Calculate the total amount of money she received from the sale of the puppies.
Output: If the two expectant dogs gave birth to 10 puppies each, the total number of puppies Nalani had is 10+10= <<10+10=20>>20
When they came of age, Nalani sold 3/4 of the dogs, a total of 3/4*20 = <<3/4*20=15>>15 dogs.
If each dog sold for $200, Nalani received 15*200 = $<<15*200=3000>>3000 from the sale of the dogs.
So the final answer is 3000

Input: Boris has 24 books and he donates a fourth of his books to the library. Cameron has 30 books and he donates a third of his books to the library. After donating their books, how many books in total do Boris and Cameron have together?
Output: Boris donates 24 / 4 = <<24/4=6>>6 books
Then Boris has a total of 24 - 6 = <<24-6=18>>18 books
Cameron donates 30 / 3 = <<30/3=10>>10 books
Then Cameron has a total of 30 - 10 = <<30-10=20>>20 books
Altogether, Boris and Cameron have 18 + 20 = <<18+20=38>>38 books
So the final answer is 38

Input: There are 3 boxes of cereal. One box holds 14 ounces of cereal. Another box holds half the amount of the first box and 5 ounces less than the third box. How much cereal is there in all 3 cereal boxes?
Output: First = <<14=14>>14 oz
Second = (1/2) * 14 = <<(1/2)*14=7>>7 oz
Third = 7 + 5 = <<7+5=12>>12 oz
14 + 7 + 12 = <<14+7+12=33>>33 oz
There are 33 ounces of cereal in those 3 boxes.
So the final answer is 33

Input: A jug needs 40 cups of water to be full. A custodian at Truman Elementary School has to fill water jugs for 200 students, who drink 10 cups of water in a day. How many water jugs will the custodian fill with cups of water to provide the students with all the water they need in a day?
Output: Since each student needs 10 cups of water per day and there are 200 students, the custodian has to provide 200*10 = <<200*10=2000>>2000 cups of water.
A jug of water needs 40 cups to be full, so 2000 cups of water will fill 2000/40 = <<2000/40=50>>50 jugs
So the final answer is 50

Input: It takes 1 hour for refrigerated dough to come to room temperature.  It takes 15 minutes to shape the dough and 2 hours to proof.  The bread takes 30 minutes to bake and 15 minutes to cool.  If the bakery opens at 6:00 am, what is the latest time the head baker can make it to the store to start working?
Output: It takes 1 hour to bring the dough to room temp and 2 hours to proof the dough so that's 1+2 = <<1+2=3>>3 hours
It takes 15 minutes to shape the dough, 30 minutes to bake and another 15 minutes to cool for a total of 15+30+15 = <<15+30+15=60>>60 minutes
There are 60 minutes in 1 hour so that takes him 60/60 = <<60/60=1>>1 hour
In total it takes 3+1 = <<3+1=4>>4 hours from start to finish to make the bread
If the bakery opens at 6:00 am and it takes 4 hours to prep then the latest the head baker can show up is 6-4 = <<6-4=2>>2:00 am
So the final answer is 2

Input: Geric had twice as many bills as Kyla who has 2 fewer bills than Jessa.  After giving 3 bills to Geric, Jessa has 7 bills left. How many bills did Geric have at the beginning?
Output: Jessa had 7 + 3 = <<7+3=10>>10 bills before she gave 3 to Geric.
Kyla had 10 - 2 = <<10-2=8>>8 bills.
So, Geric had 8 x 2 = <<8*2=16>>16 bills.
So the final answer is 16

Input: They say the first year of a dog's life equals 15 human years. The second year of a dog's life equals 9 human years and after that, every year of a dog's life equals 5 human years. According to this logic, how many human years has my 10-year-old dog lived?
Output: If your dog is 10 years old then in his first year of life he lived 1*15 = 15 human years
In his second year of life, he lived 1*9 = <<1*9=9>>9 human years
We need to calculate his remaining years or 10-2 = <<10-2=8>>8 years of dog life into human years
If 1 year of dog life after the 2 years equates to 5 human years, then 8 years of dog life equals 8*5 = <<8*5=40>>40 human years
In total, your dog has lived 15 + 9 + 40 = <<15+9+40=64>>64 human years
So the final answer is 64

Input: Company A and Company B merge. Company A receives 60% of the combined profits under the new merger, and company B receives 40% of the profits. If company B gets a total of $60000 in profit, how much does company A get?
Output: Let the total profit of both companies be denoted by x.
Therefore, 40% * x = 0.4 * x = $60000
So x = $60000 / 0.4 = $150000.
If Company B gets 60 % of the profits, it gets 60% of $150000, which is 0.6 * $150000 = $<<0.6*150000=90000>>90000.
So the final answer is 90000

Input: Sam, Sid, and Steve brought popsicle sticks for their group activity in their Art class. Sam has thrice as many as Sid, and Sid has twice as many as Steve. If Steve has 12 popsicle sticks, how many popsicle sticks can they use for their Art class activity?
Output: Sid has 12 x 2 = <<12*2=24>>24 sticks.
Sam has 24 x 3 = <<24*3=72>>72 sticks.
Thus, they have 24 + 72 + 12 = <<24+72+12=108>>108 popsicle sticks.
So the final answer is 108

Input: The price of an iPhone fell 10% in a particular month and another 20% in the second month.  If the initial price was $1000, calculate the price after the second month.
Output: In the first month, the price of the iPhone fell by 10/100*$1000=$<<10/100*1000=100>>100
The new price after the first month is $1000-$100 = $<<1000-100=900>>900
In the second month, the price fell 20% further by 20/100*$900 = $<<20/100*900=180>>180
The new price after the second month is $900-$180 = $<<900-180=720>>720
So the final answer is 720

Input: John uses a 75-watt electric fan for 8 hours a day. How much kWh of electric energy does he consume per month (30 days) for using the electric fan?
Output: In a day, John uses 75x 8 = <<75*8=600>>600 watts.
So in a month, he uses 600 x 30 = 18 000 watts.
Since there are 1000 watts in 1 kilowatt, then John uses 18000/1000 = <<18000/1000=18>>18.
So the final answer is 18

Input: Barbara Blackburn can type 212 words per minute.  Due to Carpal tunnel syndrome, Barbara cannot use her left hand for a while so her typing speed is now 40 words less per minute. If she is supposed to type a document with 3440 words, how many minutes will it take her to finish typing the document?
Output: Due to carpal tunnel syndrome, Barbara can only type 212 - 40= <<212-40=172>>172 words per minute.
So, she will be able to finish typing the document in 3440/172 = <<3440/172=20>>20 minutes.
So the final answer is 20

Input: Max fills up water balloons for 30 minutes at a rate of 2 water balloons every minute. Max’s friend Zach fills up water balloons for 40 minutes at a rate of 3 water balloons every minute. In the process, 10 of the water balloons pop on the ground. How many filled water balloons do Max and Zach have in total?
Output: Max fills 30 * 2 = <<30*2=60>>60 water balloons
Zach fills 40 * 3 = <<40*3=120>>120 water balloons
Max and Zach have a total of 60 + 120 - 10 = <<60+120-10=170>>170 water balloons
So the final answer is 170

Input: John has 54 pieces of gum, Cole has 45 pieces of gum and Aubrey has no pieces of gum. They decide to share the gum equally between the 3 of them. How many pieces of gum will each one get?
Output: Total pieces of gum are 54 pieces + 45 pieces + 0 pieces = <<54+45=99>>99 pieces
When the pieces are shared equally amongst 3 people, each one gets 99 pieces / 3 people = <<99/3=33>>33 pieces/person
So the final answer is 33

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o19-tgsm8k-s20-rTrue-m4096
                                                                                               Evaluating commonsenseqa :   1%|          | 1/100 [01:29<2:27:41, 89.51s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [02:57<2:24:35, 88.53s/it]                Evaluating commonsenseqa :   3%|▎         | 3/100 [04:23<2:21:43, 87.66s/it]                Evaluating commonsenseqa :   4%|▍         | 4/100 [05:50<2:19:47, 87.37s/it]                Evaluating commonsenseqa :   5%|▌         | 5/100 [07:17<2:18:01, 87.17s/it]                Evaluating commonsenseqa :   6%|▌         | 6/100 [08:44<2:16:27, 87.10s/it]                  Evaluating commonsenseqa :   7%|▋         | 7/100 [10:11<2:14:50, 86.99s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [11:38<2:13:24, 87.00s/it]                  Evaluating commonsenseqa :   9%|▉         | 9/100 [13:05<2:11:51, 86.93s/it]                  Evaluating commonsenseqa :  10%|█         | 10/100 [14:32<2:10:22, 86.92s/it]                 Evaluating commonsenseqa :  11%|█         | 11/100 [15:58<2:08:50, 86.86s/it]                 Evaluating commonsenseqa :  12%|█▏        | 12/100 [17:25<2:07:29, 86.93s/it]               Evaluating commonsenseqa :  13%|█▎        | 13/100 [18:52<2:06:00, 86.90s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [20:19<2:04:32, 86.89s/it]               Evaluating commonsenseqa :  15%|█▌        | 15/100 [21:46<2:03:06, 86.90s/it]               Evaluating commonsenseqa :  16%|█▌        | 16/100 [23:13<2:01:46, 86.98s/it]               Evaluating commonsenseqa :  17%|█▋        | 17/100 [24:40<2:00:18, 86.97s/it]               Evaluating commonsenseqa :  18%|█▊        | 18/100 [26:07<1:58:50, 86.96s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [27:34<1:57:19, 86.90s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [29:01<1:55:54, 86.93s/it]                 Evaluating commonsenseqa :  21%|██        | 21/100 [30:28<1:54:27, 86.93s/it]                 Evaluating commonsenseqa :  22%|██▏       | 22/100 [31:55<1:53:00, 86.93s/it]               Evaluating commonsenseqa :  23%|██▎       | 23/100 [33:22<1:51:33, 86.92s/it]               Evaluating commonsenseqa :  24%|██▍       | 24/100 [34:49<1:50:07, 86.94s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [36:16<1:48:46, 87.03s/it]               Evaluating commonsenseqa :  26%|██▌       | 26/100 [37:43<1:47:19, 87.02s/it]               Evaluating commonsenseqa :  27%|██▋       | 27/100 [39:10<1:45:49, 86.99s/it]               Evaluating commonsenseqa :  28%|██▊       | 28/100 [40:37<1:44:21, 86.97s/it]:00, 104.89s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:54:55<00:00, 104.95s/it]
name: commonsenseqa | avg. gen lenth: 406.082 | time: 10498.272512197495s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu02 --master_port 19674 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o14-tgsm8k-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 14 11 12 13 14 15 True 4096 10
[2023-09-07 12:09:08,558] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o14-tgsm8k-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 14
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o14-tgsm8k-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 320663.63it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.66s/it]
 > number of parameters: 6738415616
[2023-09-07 12:09:21,551] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 12:09:21,748] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 12:09:21,749] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 12:09:21,749] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 12:09:21,749] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 12:09:21,749] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 12:09:21,749] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c21c0>
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 12:09:21,750] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 12:09:21,751] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 12:09:21,751] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Tapanga and Corey have 66 candies together. However, Tapanga has 8 more candies than Corey. How many candies does Corey have?
Output: Let x = the total number of candies Corey has.
x + 8 = the total number of candies Tapanga has.
The equation for the total number of candies is x + (x + 8) = 66
Combining like terms, we get 2x + 8 = 66
Subtracting 8 from both sides, we get 2x = 58
Dividing both sides by 2, we get x = <<29=29>>29, so Corey has 29 candies.
So the final answer is 29

Input: Freddy is calling his family on New Year's Eve. He calls his dad, who lives in the same city as him, and they talk for 45 minutes. Then he calls his brother, who lives on the other side of the world, and they talk for 31 minutes. Local calls cost 5 cents a minute, while international calls cost 25 cents a minute. How many dollars did Freddy spend calling his family on New Year's Eve?
Output: At 5 cents a minute, calling his father cost Freddy 5* 45 = <<5*45=225>>225 cents.
At 25 cents a minute, calling his brother cost Freddy 25 * 31 = <<25*31=775>>775 cents.
Adding the cost of calling his father and brother, we find that Freddy paid a total of 225 + 775 = <<225+775=1000>>1000 cents.
Since each dollar has 100 cents, Freddy paid 1000 / 100 = <<1000/100=10>>10 dollars
So the final answer is 10

Input: Lawrence worked 8 hours each day on Monday, Tuesday and Friday. He worked 5.5 hours on both Wednesday and Thursday. How many hours would Lawrence work each day if he worked the same number of hours each day?
Output: 8 hours * 3 = <<8*3=24>>24 hours
5.5 * 2 = <<5.5*2=11>>11 hours
24 + 11 = <<24+11=35>>35 hours
35/7 = <<35/7=5>>5 hours
Lawrence would work 5 hours each of the 7 days in a week.
So the final answer is 5

Input: Ali had a stock of 800 books in his Room. He sold 60 on Monday, 10 on Tuesday, 20 on Wednesday, 44 on Thursday and 66 on Friday. How many books were not sold?
Output: We look first for the total number of books that were sold: 60 + 10 + 20 + 44 + 66 = <<60+10+20+44+66=200>>200 books.
So the total number of books that were not sold is: 800 – 200 = <<800-200=600>>600 books.
So the final answer is 600

Input: Michael makes birdhouses to sell at craft shows. He charges $22 for each large birdhouse, $16 for each medium birdhouse, and $7 for each small birdhouse. This week, he sold 2 large birdhouses, 2 medium birdhouses, and 3 small birdhouses. How much money, in dollars, did he make this week?
Output: Michael sold 2 large birdhouses for $22 each, so he made 2*$22= $<<2*22=44>>44 from large birdhouse sales.
Michael also sold 2 medium birdhouses for $16 each, so he made 2*$16= $<<2*16=32>>32 from medium birdhouse sales.
Michael sold 3 small birdhouses for $7 each, so he made 3*7=$<<3*7=21>>21 from small birdhouse sales.
Since Michael made $44 from large birdhouse sales, $32 from medium birdhouse sales, and $21 for small birdhouse sales, he made $44+$32+$21= $<<44+32+21=97>>97 total this week.
So the final answer is 97

Input: Nalani had two female dogs that were expecting and after a month gave birth to 10 puppies each. She then sold 3/4 of the puppies after they came of age, each at $200. Calculate the total amount of money she received from the sale of the puppies.
Output: If the two expectant dogs gave birth to 10 puppies each, the total number of puppies Nalani had is 10+10= <<10+10=20>>20
When they came of age, Nalani sold 3/4 of the dogs, a total of 3/4*20 = <<3/4*20=15>>15 dogs.
If each dog sold for $200, Nalani received 15*200 = $<<15*200=3000>>3000 from the sale of the dogs.
So the final answer is 3000

Input: Boris has 24 books and he donates a fourth of his books to the library. Cameron has 30 books and he donates a third of his books to the library. After donating their books, how many books in total do Boris and Cameron have together?
Output: Boris donates 24 / 4 = <<24/4=6>>6 books
Then Boris has a total of 24 - 6 = <<24-6=18>>18 books
Cameron donates 30 / 3 = <<30/3=10>>10 books
Then Cameron has a total of 30 - 10 = <<30-10=20>>20 books
Altogether, Boris and Cameron have 18 + 20 = <<18+20=38>>38 books
So the final answer is 38

Input: There are 3 boxes of cereal. One box holds 14 ounces of cereal. Another box holds half the amount of the first box and 5 ounces less than the third box. How much cereal is there in all 3 cereal boxes?
Output: First = <<14=14>>14 oz
Second = (1/2) * 14 = <<(1/2)*14=7>>7 oz
Third = 7 + 5 = <<7+5=12>>12 oz
14 + 7 + 12 = <<14+7+12=33>>33 oz
There are 33 ounces of cereal in those 3 boxes.
So the final answer is 33

Input: A jug needs 40 cups of water to be full. A custodian at Truman Elementary School has to fill water jugs for 200 students, who drink 10 cups of water in a day. How many water jugs will the custodian fill with cups of water to provide the students with all the water they need in a day?
Output: Since each student needs 10 cups of water per day and there are 200 students, the custodian has to provide 200*10 = <<200*10=2000>>2000 cups of water.
A jug of water needs 40 cups to be full, so 2000 cups of water will fill 2000/40 = <<2000/40=50>>50 jugs
So the final answer is 50

Input: It takes 1 hour for refrigerated dough to come to room temperature.  It takes 15 minutes to shape the dough and 2 hours to proof.  The bread takes 30 minutes to bake and 15 minutes to cool.  If the bakery opens at 6:00 am, what is the latest time the head baker can make it to the store to start working?
Output: It takes 1 hour to bring the dough to room temp and 2 hours to proof the dough so that's 1+2 = <<1+2=3>>3 hours
It takes 15 minutes to shape the dough, 30 minutes to bake and another 15 minutes to cool for a total of 15+30+15 = <<15+30+15=60>>60 minutes
There are 60 minutes in 1 hour so that takes him 60/60 = <<60/60=1>>1 hour
In total it takes 3+1 = <<3+1=4>>4 hours from start to finish to make the bread
If the bakery opens at 6:00 am and it takes 4 hours to prep then the latest the head baker can show up is 6-4 = <<6-4=2>>2:00 am
So the final answer is 2

Input: Geric had twice as many bills as Kyla who has 2 fewer bills than Jessa.  After giving 3 bills to Geric, Jessa has 7 bills left. How many bills did Geric have at the beginning?
Output: Jessa had 7 + 3 = <<7+3=10>>10 bills before she gave 3 to Geric.
Kyla had 10 - 2 = <<10-2=8>>8 bills.
So, Geric had 8 x 2 = <<8*2=16>>16 bills.
So the final answer is 16

Input: They say the first year of a dog's life equals 15 human years. The second year of a dog's life equals 9 human years and after that, every year of a dog's life equals 5 human years. According to this logic, how many human years has my 10-year-old dog lived?
Output: If your dog is 10 years old then in his first year of life he lived 1*15 = 15 human years
In his second year of life, he lived 1*9 = <<1*9=9>>9 human years
We need to calculate his remaining years or 10-2 = <<10-2=8>>8 years of dog life into human years
If 1 year of dog life after the 2 years equates to 5 human years, then 8 years of dog life equals 8*5 = <<8*5=40>>40 human years
In total, your dog has lived 15 + 9 + 40 = <<15+9+40=64>>64 human years
So the final answer is 64

Input: Company A and Company B merge. Company A receives 60% of the combined profits under the new merger, and company B receives 40% of the profits. If company B gets a total of $60000 in profit, how much does company A get?
Output: Let the total profit of both companies be denoted by x.
Therefore, 40% * x = 0.4 * x = $60000
So x = $60000 / 0.4 = $150000.
If Company B gets 60 % of the profits, it gets 60% of $150000, which is 0.6 * $150000 = $<<0.6*150000=90000>>90000.
So the final answer is 90000

Input: Sam, Sid, and Steve brought popsicle sticks for their group activity in their Art class. Sam has thrice as many as Sid, and Sid has twice as many as Steve. If Steve has 12 popsicle sticks, how many popsicle sticks can they use for their Art class activity?
Output: Sid has 12 x 2 = <<12*2=24>>24 sticks.
Sam has 24 x 3 = <<24*3=72>>72 sticks.
Thus, they have 24 + 72 + 12 = <<24+72+12=108>>108 popsicle sticks.
So the final answer is 108

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o14-tgsm8k-s20-rTrue-m4096
                                                                                    Evaluating commonsenseqa :   1%|          | 1/100 [01:41<2:46:44, 101.06s/it]      Evaluating commonsenseqa :   2%|▏         | 2/100 [03:20<2:43:22, 100.02s/it]    Evaluating commonsenseqa :   3%|▎         | 3/100 [05:00<2:41:47, 100.08s/it]      Evaluating commonsenseqa :   4%|▍         | 4/100 [06:39<2:39:09, 99.48s/it]       Evaluating commonsenseqa :   5%|▌         | 5/100 [08:17<2:37:07, 99.23s/it]       Evaluating commonsenseqa :   6%|▌         | 6/100 [09:58<2:36:05, 99.63s/it]       Evaluating commonsenseqa :   7%|▋         | 7/100 [11:36<2:33:55, 99.30s/it]                                                                                             Evaluating commonsenseqa :   8%|▊         | 8/100 [13:17<2:32:53, 99.71s/it]       Evaluating commonsenseqa :   9%|▉         | 9/100 [14:55<2:30:25, 99.18s/it]       Evaluating commonsenseqa :  10%|█         | 10/100 [16:34<2:28:48, 99.21s/it]      Evaluating commonsenseqa :  11%|█         | 11/100 [18:14<2:27:11, 99.23s/it]      Evaluating commonsenseqa :  12%|█▏        | 12/100 [19:54<2:26:13, 99.70s/it]        Evaluating commonsenseqa :  13%|█▎        | 13/100 [21:35<2:24:59, 100.00s/it]       Evaluating commonsenseqa :  14%|█▍        | 14/100 [23:15<2:23:10, 99.89s/it]                                                                                                  Evaluating commonsenseqa :  15%|█▌        | 15/100 [24:54<2:21:10, 99.66s/it]        Evaluating commonsenseqa :  16%|█▌        | 16/100 [26:33<2:19:14, 99.45s/it]        Evaluating commonsenseqa :  17%|█▋        | 17/100 [28:12<2:17:29, 99.39s/it]        Evaluating commonsenseqa :  18%|█▊        | 18/100 [29:53<2:16:19, 99.75s/it]        Evaluating commonsenseqa :  19%|█▉        | 19/100 [31:31<2:14:12, 99.41s/it]        Evaluating commonsenseqa :  20%|██        | 20/100 [33:11<2:12:48, 99.61s/it]        Evaluating commonsenseqa :  21%|██        | 21/100 [34:53<2:12:11, 100.40s/it]                                                                                                     Evaluating commonsenseqa :  22%|██▏       | 22/100 [36:33<2:09:59, 99.99s/it]        Evaluating commonsenseqa :  23%|██▎       | 23/100 [38:12<2:08:11, 99.89s/it]        Evaluating commonsenseqa :  24%|██▍       | 24/100 [39:53<2:06:49, 100.13s/it]       Evaluating commonsenseqa :  25%|██▌       | 25/100 [41:34<2:05:35, 100.47s/it]       Evaluating commonsenseqa :  26%|██▌       | 26/100 [43:13<2:03:14, 99.92s/it]        Evaluating commonsenseqa :  27%|██▋       | 27/100 [44:53<2:01:37, 99.96s/it]                                                                                                  Evaluating commonsenseqa :  28%|██▊       | 28/100 [46:33<1:59:54, 99.92s/it]      Evaluating commonsenseqa :  29%|██▉       | 29/100 [48:13<1:58:17, 99.96s/it]        Evaluating commonsenseqa :  30%|███       | 30/100 [49:54<1:56:57, 100.25s/it]       Evaluating commonsenseqa :  31%|███       | 31/100 [51:33<1:54:58, 99.97s/it]        Evaluating commonsenseqa :  32%|███▏      | 32/100 [53:12<1:52:54, 99.63s/it]      Evaluating commonsenseqa :  33%|███▎      | 33/100 [54:52<1:51:26, 99.80s/it]      Evaluating commonsenseqa :  34%|███▍      | 34/100 [56:32<1:49:43, 99.75s/it]                                                                                                  Evaluating commonsenseqa :  35%|███▌      | 35/100 [58:12<1:48:06, 99.79s/it]      Evaluating commonsenseqa :  70%|███████   | 70/100 [1:41:31<43:32, 87.09s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [1:42:58<42:04, 87.06s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [1:44:25<40:38, 87.08s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [1:45:52<39:13, 87.15s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [1:47:20<37:46, 87.19s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [1:48:47<36:19, 87.20s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [1:50:14<34:51, 87.14s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [1:51:41<33:22, 87.07s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [1:53:08<31:56, 87.13s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [1:54:35<30:29, 87.13s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [1:56:02<29:02, 87.15s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [1:57:29<27:35, 87.14s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [1:58:57<26:11, 87.29s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [2:00:24<24:43, 87.27s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [2:01:51<23:15, 87.20s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [2:03:18<21:47, 87.16s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [2:04:45<20:20, 87.15s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [2:06:12<18:51, 87.07s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [2:07:40<17:26, 87.18s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [2:09:07<15:58, 87.12s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [2:10:34<14:31, 87.11s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [2:12:01<13:04, 87.18s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [2:13:29<11:37, 87.23s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [2:14:56<10:10, 87.19s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [2:16:22<08:42, 87.07s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [2:17:50<07:15, 87.11s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [2:19:17<05:48, 87.19s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [2:20:44<04:21, 87.18s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [2:22:11<02:54, 87.10s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [2:23:38<01:27, 87.05s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:25:05<00:00, 87.10s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:25:05<00:00, 87.06s/it]
name: commonsenseqa | avg. gen lenth: 357.478 | time: 8708.905460357666s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o20-tgsm8k-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 20 16 17 18 19 20 True 4096 10
[2023-09-07 13:53:37,249] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o20-tgsm8k-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 20
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o20-tgsm8k-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 257311.65it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.86s/it]
 > number of parameters: 6738415616
[2023-09-07 13:53:55,012] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 13:53:55,217] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 13:53:55,219] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 13:53:55,219] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 13:53:55,219] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 13:53:55,219] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 13:53:55,219] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 13:53:55,219] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c01c0>
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 13:53:55,220] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 13:53:55,221] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 13:53:55,221] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Tapanga and Corey have 66 candies together. However, Tapanga has 8 more candies than Corey. How many candies does Corey have?
Output: Let x = the total number of candies Corey has.
x + 8 = the total number of candies Tapanga has.
The equation for the total number of candies is x + (x + 8) = 66
Combining like terms, we get 2x + 8 = 66
Subtracting 8 from both sides, we get 2x = 58
Dividing both sides by 2, we get x = <<29=29>>29, so Corey has 29 candies.
So the final answer is 29

Input: Freddy is calling his family on New Year's Eve. He calls his dad, who lives in the same city as him, and they talk for 45 minutes. Then he calls his brother, who lives on the other side of the world, and they talk for 31 minutes. Local calls cost 5 cents a minute, while international calls cost 25 cents a minute. How many dollars did Freddy spend calling his family on New Year's Eve?
Output: At 5 cents a minute, calling his father cost Freddy 5* 45 = <<5*45=225>>225 cents.
At 25 cents a minute, calling his brother cost Freddy 25 * 31 = <<25*31=775>>775 cents.
Adding the cost of calling his father and brother, we find that Freddy paid a total of 225 + 775 = <<225+775=1000>>1000 cents.
Since each dollar has 100 cents, Freddy paid 1000 / 100 = <<1000/100=10>>10 dollars
So the final answer is 10

Input: Lawrence worked 8 hours each day on Monday, Tuesday and Friday. He worked 5.5 hours on both Wednesday and Thursday. How many hours would Lawrence work each day if he worked the same number of hours each day?
Output: 8 hours * 3 = <<8*3=24>>24 hours
5.5 * 2 = <<5.5*2=11>>11 hours
24 + 11 = <<24+11=35>>35 hours
35/7 = <<35/7=5>>5 hours
Lawrence would work 5 hours each of the 7 days in a week.
So the final answer is 5

Input: Ali had a stock of 800 books in his Room. He sold 60 on Monday, 10 on Tuesday, 20 on Wednesday, 44 on Thursday and 66 on Friday. How many books were not sold?
Output: We look first for the total number of books that were sold: 60 + 10 + 20 + 44 + 66 = <<60+10+20+44+66=200>>200 books.
So the total number of books that were not sold is: 800 – 200 = <<800-200=600>>600 books.
So the final answer is 600

Input: Michael makes birdhouses to sell at craft shows. He charges $22 for each large birdhouse, $16 for each medium birdhouse, and $7 for each small birdhouse. This week, he sold 2 large birdhouses, 2 medium birdhouses, and 3 small birdhouses. How much money, in dollars, did he make this week?
Output: Michael sold 2 large birdhouses for $22 each, so he made 2*$22= $<<2*22=44>>44 from large birdhouse sales.
Michael also sold 2 medium birdhouses for $16 each, so he made 2*$16= $<<2*16=32>>32 from medium birdhouse sales.
Michael sold 3 small birdhouses for $7 each, so he made 3*7=$<<3*7=21>>21 from small birdhouse sales.
Since Michael made $44 from large birdhouse sales, $32 from medium birdhouse sales, and $21 for small birdhouse sales, he made $44+$32+$21= $<<44+32+21=97>>97 total this week.
So the final answer is 97

Input: Nalani had two female dogs that were expecting and after a month gave birth to 10 puppies each. She then sold 3/4 of the puppies after they came of age, each at $200. Calculate the total amount of money she received from the sale of the puppies.
Output: If the two expectant dogs gave birth to 10 puppies each, the total number of puppies Nalani had is 10+10= <<10+10=20>>20
When they came of age, Nalani sold 3/4 of the dogs, a total of 3/4*20 = <<3/4*20=15>>15 dogs.
If each dog sold for $200, Nalani received 15*200 = $<<15*200=3000>>3000 from the sale of the dogs.
So the final answer is 3000

Input: Boris has 24 books and he donates a fourth of his books to the library. Cameron has 30 books and he donates a third of his books to the library. After donating their books, how many books in total do Boris and Cameron have together?
Output: Boris donates 24 / 4 = <<24/4=6>>6 books
Then Boris has a total of 24 - 6 = <<24-6=18>>18 books
Cameron donates 30 / 3 = <<30/3=10>>10 books
Then Cameron has a total of 30 - 10 = <<30-10=20>>20 books
Altogether, Boris and Cameron have 18 + 20 = <<18+20=38>>38 books
So the final answer is 38

Input: There are 3 boxes of cereal. One box holds 14 ounces of cereal. Another box holds half the amount of the first box and 5 ounces less than the third box. How much cereal is there in all 3 cereal boxes?
Output: First = <<14=14>>14 oz
Second = (1/2) * 14 = <<(1/2)*14=7>>7 oz
Third = 7 + 5 = <<7+5=12>>12 oz
14 + 7 + 12 = <<14+7+12=33>>33 oz
There are 33 ounces of cereal in those 3 boxes.
So the final answer is 33

Input: A jug needs 40 cups of water to be full. A custodian at Truman Elementary School has to fill water jugs for 200 students, who drink 10 cups of water in a day. How many water jugs will the custodian fill with cups of water to provide the students with all the water they need in a day?
Output: Since each student needs 10 cups of water per day and there are 200 students, the custodian has to provide 200*10 = <<200*10=2000>>2000 cups of water.
A jug of water needs 40 cups to be full, so 2000 cups of water will fill 2000/40 = <<2000/40=50>>50 jugs
So the final answer is 50

Input: It takes 1 hour for refrigerated dough to come to room temperature.  It takes 15 minutes to shape the dough and 2 hours to proof.  The bread takes 30 minutes to bake and 15 minutes to cool.  If the bakery opens at 6:00 am, what is the latest time the head baker can make it to the store to start working?
Output: It takes 1 hour to bring the dough to room temp and 2 hours to proof the dough so that's 1+2 = <<1+2=3>>3 hours
It takes 15 minutes to shape the dough, 30 minutes to bake and another 15 minutes to cool for a total of 15+30+15 = <<15+30+15=60>>60 minutes
There are 60 minutes in 1 hour so that takes him 60/60 = <<60/60=1>>1 hour
In total it takes 3+1 = <<3+1=4>>4 hours from start to finish to make the bread
If the bakery opens at 6:00 am and it takes 4 hours to prep then the latest the head baker can show up is 6-4 = <<6-4=2>>2:00 am
So the final answer is 2

Input: Geric had twice as many bills as Kyla who has 2 fewer bills than Jessa.  After giving 3 bills to Geric, Jessa has 7 bills left. How many bills did Geric have at the beginning?
Output: Jessa had 7 + 3 = <<7+3=10>>10 bills before she gave 3 to Geric.
Kyla had 10 - 2 = <<10-2=8>>8 bills.
So, Geric had 8 x 2 = <<8*2=16>>16 bills.
So the final answer is 16

Input: They say the first year of a dog's life equals 15 human years. The second year of a dog's life equals 9 human years and after that, every year of a dog's life equals 5 human years. According to this logic, how many human years has my 10-year-old dog lived?
Output: If your dog is 10 years old then in his first year of life he lived 1*15 = 15 human years
In his second year of life, he lived 1*9 = <<1*9=9>>9 human years
We need to calculate his remaining years or 10-2 = <<10-2=8>>8 years of dog life into human years
If 1 year of dog life after the 2 years equates to 5 human years, then 8 years of dog life equals 8*5 = <<8*5=40>>40 human years
In total, your dog has lived 15 + 9 + 40 = <<15+9+40=64>>64 human years
So the final answer is 64

Input: Company A and Company B merge. Company A receives 60% of the combined profits under the new merger, and company B receives 40% of the profits. If company B gets a total of $60000 in profit, how much does company A get?
Output: Let the total profit of both companies be denoted by x.
Therefore, 40% * x = 0.4 * x = $60000
So x = $60000 / 0.4 = $150000.
If Company B gets 60 % of the profits, it gets 60% of $150000, which is 0.6 * $150000 = $<<0.6*150000=90000>>90000.
So the final answer is 90000

Input: Sam, Sid, and Steve brought popsicle sticks for their group activity in their Art class. Sam has thrice as many as Sid, and Sid has twice as many as Steve. If Steve has 12 popsicle sticks, how many popsicle sticks can they use for their Art class activity?
Output: Sid has 12 x 2 = <<12*2=24>>24 sticks.
Sam has 24 x 3 = <<24*3=72>>72 sticks.
Thus, they have 24 + 72 + 12 = <<24+72+12=108>>108 popsicle sticks.
So the final answer is 108

Input: The price of an iPhone fell 10% in a particular month and another 20% in the second month.  If the initial price was $1000, calculate the price after the second month.
Output: In the first month, the price of the iPhone fell by 10/100*$1000=$<<10/100*1000=100>>100
The new price after the first month is $1000-$100 = $<<1000-100=900>>900
In the second month, the price fell 20% further by 20/100*$900 = $<<20/100*900=180>>180
The new price after the second month is $900-$180 = $<<900-180=720>>720
So the final answer is 720

Input: John uses a 75-watt electric fan for 8 hours a day. How much kWh of electric energy does he consume per month (30 days) for using the electric fan?
Output: In a day, John uses 75x 8 = <<75*8=600>>600 watts.
So in a month, he uses 600 x 30 = 18 000 watts.
Since there are 1000 watts in 1 kilowatt, then John uses 18000/1000 = <<18000/1000=18>>18.
So the final answer is 18

Input: Barbara Blackburn can type 212 words per minute.  Due to Carpal tunnel syndrome, Barbara cannot use her left hand for a while so her typing speed is now 40 words less per minute. If she is supposed to type a document with 3440 words, how many minutes will it take her to finish typing the document?
Output: Due to carpal tunnel syndrome, Barbara can only type 212 - 40= <<212-40=172>>172 words per minute.
So, she will be able to finish typing the document in 3440/172 = <<3440/172=20>>20 minutes.
So the final answer is 20

Input: Max fills up water balloons for 30 minutes at a rate of 2 water balloons every minute. Max’s friend Zach fills up water balloons for 40 minutes at a rate of 3 water balloons every minute. In the process, 10 of the water balloons pop on the ground. How many filled water balloons do Max and Zach have in total?
Output: Max fills 30 * 2 = <<30*2=60>>60 water balloons
Zach fills 40 * 3 = <<40*3=120>>120 water balloons
Max and Zach have a total of 60 + 120 - 10 = <<60+120-10=170>>170 water balloons
So the final answer is 170

Input: John has 54 pieces of gum, Cole has 45 pieces of gum and Aubrey has no pieces of gum. They decide to share the gum equally between the 3 of them. How many pieces of gum will each one get?
Output: Total pieces of gum are 54 pieces + 45 pieces + 0 pieces = <<54+45=99>>99 pieces
When the pieces are shared equally amongst 3 people, each one gets 99 pieces / 3 people = <<99/3=33>>33 pieces/person
So the final answer is 33

Input: Mira jogs every morning. She jogs 5 miles per hour. If she jogs for 2 hours every morning, how many miles can she jog for five days?
Output: Mira jogs 5 x 2 = <<5*2=10>>10 miles per day.
Therefore she can jog 10 x 5 = <<10*5=50>>50 miles for 5 days.
So the final answer is 50

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o20-tgsm8k-s20-rTrue-m4096
                                                                                              Evaluating commonsenseqa :   1%|          | 1/100 [01:29<2:27:37, 89.47s/it]                  Evaluating commonsenseqa :   2%|▏         | 2/100 [02:57<2:24:40, 88.57s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [04:24<2:22:12, 87.97s/it]                Evaluating commonsenseqa :   4%|▍         | 4/100 [05:51<2:20:12, 87.64s/it]              Evaluating commonsenseqa :   5%|▌         | 5/100 [07:18<2:18:29, 87.47s/it]             Evaluating commonsenseqa :   6%|▌         | 6/100 [08:46<2:16:49, 87.34s/it]             Evaluating commonsenseqa :   7%|▋         | 7/100 [10:13<2:15:17, 87.28s/it]             Evaluating commonsenseqa :   8%|▊         | 8/100 [11:40<2:13:45, 87.23s/it]             Evaluating commonsenseqa :   9%|▉         | 9/100 [13:07<2:12:17, 87.22s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [14:34<2:10:44, 87.16s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [16:01<2:09:17, 87.16s/it]               Evaluating commonsenseqa :  12%|█▏        | 12/100 [17:29<2:08:04, 87.33s/it]             Evaluating commonsenseqa :  13%|█▎        | 13/100 [18:56<2:06:31, 87.26s/it]            Evaluating commonsenseqa :  14%|█▍        | 14/100 [20:23<2:05:00, 87.21s/it]            Evaluating commonsenseqa :  15%|█▌        | 15/100 [21:50<2:03:29, 87.17s/it]             Evaluating commonsenseqa :  16%|█▌        | 16/100 [23:17<2:02:02, 87.17s/it]             Evaluating commonsenseqa :  17%|█▋        | 17/100 [24:45<2:00:36, 87.19s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [26:12<1:59:08, 87.18s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [27:39<1:57:42, 87.19s/it]            Evaluating commonsenseqa :  20%|██        | 20/100 [29:06<1:56:12, 87.16s/it]            Evaluating commonsenseqa :  21%|██        | 21/100 [30:33<1:54:49, 87.21s/it]            Evaluating commonsenseqa :  22%|██▏       | 22/100 [32:01<1:53:22, 87.21s/it]            Evaluating commonsenseqa :  23%|██▎       | 23/100 [33:28<1:51:55, 87.21s/it]            Evaluating commonsenseqa :  24%|██▍       | 24/100 [34:55<1:50:29, 87.24s/it]            Evaluating commonsenseqa :  25%|██▌       | 25/100 [36:22<1:49:03, 87.25s/it]             Evaluating commonsenseqa :  26%|██▌       | 26/100 [37:50<1:47:37, 87.26s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [39:17<1:46:07, 87.23s/it]             Evaluating commonsenseqa :  28%|██▊       | 28/100 [40:44<1:44:42, 87.26s/it]             Evaluating commonsenseqa :  29%|██▉       | 29/100 [42:11<1:43:15, 87.26s/it]            Evaluating commonsenseqa :  30%|███       | 30/100 [43:39<1:41:48, 87.27s/it]            Evaluating commonsenseqa :  31%|███       | 31/100 [45:06<1:40:18, 87.23s/it]             Evaluating commonsenseqa :  32%|███▏      | 32/100 [46:33<1:38:52, 87.24s/it]           Evaluating commonsenseqa :  33%|███▎      | 33/100 [48:00<1:37:27, 87.27s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [49:28<1:36:01, 87.30s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [50:55<1:34:34, 87.30s/it]             Evaluating commonsenseqa :  36%|███▌      | 36/100 [52:22<1:33:06, 87.29s/it]             Evaluating commonsenseqa :  37%|███▋      | 37/100 [53:50<1:31:36, 87.25s/it]             Evaluating commonsenseqa :  38%|███▊      | 38/100 [55:17<1:30:09, 87.25s/it]             Evaluating commonsenseqa :  39%|███▉      | 39/100 [56:44<1:28:40, 87.22s/it]             Evaluating commonsenseqa :  40%|████      | 40/100 [58:11<1:27:14, 87.25s/it]             Evaluating commonsenseqa :  41%|████      | 41/100 [59:39<1:25:48, 87.26s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:01:06<1:24:22, 87.29s/it]         Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:02:33<1:22:55, 87.29s/it]9.90s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:46:26<00:00, 99.86s/it]
name: commonsenseqa | avg. gen lenth: 394.092 | time: 9989.283538341522s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu02 --master_port 19674 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o15-tgsm8k-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 15 11 12 13 14 15 True 4096 10
[2023-09-07 14:55:55,692] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o15-tgsm8k-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 15
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o15-tgsm8k-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 302187.93it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.66s/it]
 > number of parameters: 6738415616
[2023-09-07 14:56:08,838] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 14:56:09,039] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 14:56:09,041] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0250>
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 14:56:09,041] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 14:56:09,042] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 14:56:09,043] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 14:56:09,043] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 14:56:09,043] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 14:56:09,043] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Tapanga and Corey have 66 candies together. However, Tapanga has 8 more candies than Corey. How many candies does Corey have?
Output: Let x = the total number of candies Corey has.
x + 8 = the total number of candies Tapanga has.
The equation for the total number of candies is x + (x + 8) = 66
Combining like terms, we get 2x + 8 = 66
Subtracting 8 from both sides, we get 2x = 58
Dividing both sides by 2, we get x = <<29=29>>29, so Corey has 29 candies.
So the final answer is 29

Input: Freddy is calling his family on New Year's Eve. He calls his dad, who lives in the same city as him, and they talk for 45 minutes. Then he calls his brother, who lives on the other side of the world, and they talk for 31 minutes. Local calls cost 5 cents a minute, while international calls cost 25 cents a minute. How many dollars did Freddy spend calling his family on New Year's Eve?
Output: At 5 cents a minute, calling his father cost Freddy 5* 45 = <<5*45=225>>225 cents.
At 25 cents a minute, calling his brother cost Freddy 25 * 31 = <<25*31=775>>775 cents.
Adding the cost of calling his father and brother, we find that Freddy paid a total of 225 + 775 = <<225+775=1000>>1000 cents.
Since each dollar has 100 cents, Freddy paid 1000 / 100 = <<1000/100=10>>10 dollars
So the final answer is 10

Input: Lawrence worked 8 hours each day on Monday, Tuesday and Friday. He worked 5.5 hours on both Wednesday and Thursday. How many hours would Lawrence work each day if he worked the same number of hours each day?
Output: 8 hours * 3 = <<8*3=24>>24 hours
5.5 * 2 = <<5.5*2=11>>11 hours
24 + 11 = <<24+11=35>>35 hours
35/7 = <<35/7=5>>5 hours
Lawrence would work 5 hours each of the 7 days in a week.
So the final answer is 5

Input: Ali had a stock of 800 books in his Room. He sold 60 on Monday, 10 on Tuesday, 20 on Wednesday, 44 on Thursday and 66 on Friday. How many books were not sold?
Output: We look first for the total number of books that were sold: 60 + 10 + 20 + 44 + 66 = <<60+10+20+44+66=200>>200 books.
So the total number of books that were not sold is: 800 – 200 = <<800-200=600>>600 books.
So the final answer is 600

Input: Michael makes birdhouses to sell at craft shows. He charges $22 for each large birdhouse, $16 for each medium birdhouse, and $7 for each small birdhouse. This week, he sold 2 large birdhouses, 2 medium birdhouses, and 3 small birdhouses. How much money, in dollars, did he make this week?
Output: Michael sold 2 large birdhouses for $22 each, so he made 2*$22= $<<2*22=44>>44 from large birdhouse sales.
Michael also sold 2 medium birdhouses for $16 each, so he made 2*$16= $<<2*16=32>>32 from medium birdhouse sales.
Michael sold 3 small birdhouses for $7 each, so he made 3*7=$<<3*7=21>>21 from small birdhouse sales.
Since Michael made $44 from large birdhouse sales, $32 from medium birdhouse sales, and $21 for small birdhouse sales, he made $44+$32+$21= $<<44+32+21=97>>97 total this week.
So the final answer is 97

Input: Nalani had two female dogs that were expecting and after a month gave birth to 10 puppies each. She then sold 3/4 of the puppies after they came of age, each at $200. Calculate the total amount of money she received from the sale of the puppies.
Output: If the two expectant dogs gave birth to 10 puppies each, the total number of puppies Nalani had is 10+10= <<10+10=20>>20
When they came of age, Nalani sold 3/4 of the dogs, a total of 3/4*20 = <<3/4*20=15>>15 dogs.
If each dog sold for $200, Nalani received 15*200 = $<<15*200=3000>>3000 from the sale of the dogs.
So the final answer is 3000

Input: Boris has 24 books and he donates a fourth of his books to the library. Cameron has 30 books and he donates a third of his books to the library. After donating their books, how many books in total do Boris and Cameron have together?
Output: Boris donates 24 / 4 = <<24/4=6>>6 books
Then Boris has a total of 24 - 6 = <<24-6=18>>18 books
Cameron donates 30 / 3 = <<30/3=10>>10 books
Then Cameron has a total of 30 - 10 = <<30-10=20>>20 books
Altogether, Boris and Cameron have 18 + 20 = <<18+20=38>>38 books
So the final answer is 38

Input: There are 3 boxes of cereal. One box holds 14 ounces of cereal. Another box holds half the amount of the first box and 5 ounces less than the third box. How much cereal is there in all 3 cereal boxes?
Output: First = <<14=14>>14 oz
Second = (1/2) * 14 = <<(1/2)*14=7>>7 oz
Third = 7 + 5 = <<7+5=12>>12 oz
14 + 7 + 12 = <<14+7+12=33>>33 oz
There are 33 ounces of cereal in those 3 boxes.
So the final answer is 33

Input: A jug needs 40 cups of water to be full. A custodian at Truman Elementary School has to fill water jugs for 200 students, who drink 10 cups of water in a day. How many water jugs will the custodian fill with cups of water to provide the students with all the water they need in a day?
Output: Since each student needs 10 cups of water per day and there are 200 students, the custodian has to provide 200*10 = <<200*10=2000>>2000 cups of water.
A jug of water needs 40 cups to be full, so 2000 cups of water will fill 2000/40 = <<2000/40=50>>50 jugs
So the final answer is 50

Input: It takes 1 hour for refrigerated dough to come to room temperature.  It takes 15 minutes to shape the dough and 2 hours to proof.  The bread takes 30 minutes to bake and 15 minutes to cool.  If the bakery opens at 6:00 am, what is the latest time the head baker can make it to the store to start working?
Output: It takes 1 hour to bring the dough to room temp and 2 hours to proof the dough so that's 1+2 = <<1+2=3>>3 hours
It takes 15 minutes to shape the dough, 30 minutes to bake and another 15 minutes to cool for a total of 15+30+15 = <<15+30+15=60>>60 minutes
There are 60 minutes in 1 hour so that takes him 60/60 = <<60/60=1>>1 hour
In total it takes 3+1 = <<3+1=4>>4 hours from start to finish to make the bread
If the bakery opens at 6:00 am and it takes 4 hours to prep then the latest the head baker can show up is 6-4 = <<6-4=2>>2:00 am
So the final answer is 2

Input: Geric had twice as many bills as Kyla who has 2 fewer bills than Jessa.  After giving 3 bills to Geric, Jessa has 7 bills left. How many bills did Geric have at the beginning?
Output: Jessa had 7 + 3 = <<7+3=10>>10 bills before she gave 3 to Geric.
Kyla had 10 - 2 = <<10-2=8>>8 bills.
So, Geric had 8 x 2 = <<8*2=16>>16 bills.
So the final answer is 16

Input: They say the first year of a dog's life equals 15 human years. The second year of a dog's life equals 9 human years and after that, every year of a dog's life equals 5 human years. According to this logic, how many human years has my 10-year-old dog lived?
Output: If your dog is 10 years old then in his first year of life he lived 1*15 = 15 human years
In his second year of life, he lived 1*9 = <<1*9=9>>9 human years
We need to calculate his remaining years or 10-2 = <<10-2=8>>8 years of dog life into human years
If 1 year of dog life after the 2 years equates to 5 human years, then 8 years of dog life equals 8*5 = <<8*5=40>>40 human years
In total, your dog has lived 15 + 9 + 40 = <<15+9+40=64>>64 human years
So the final answer is 64

Input: Company A and Company B merge. Company A receives 60% of the combined profits under the new merger, and company B receives 40% of the profits. If company B gets a total of $60000 in profit, how much does company A get?
Output: Let the total profit of both companies be denoted by x.
Therefore, 40% * x = 0.4 * x = $60000
So x = $60000 / 0.4 = $150000.
If Company B gets 60 % of the profits, it gets 60% of $150000, which is 0.6 * $150000 = $<<0.6*150000=90000>>90000.
So the final answer is 90000

Input: Sam, Sid, and Steve brought popsicle sticks for their group activity in their Art class. Sam has thrice as many as Sid, and Sid has twice as many as Steve. If Steve has 12 popsicle sticks, how many popsicle sticks can they use for their Art class activity?
Output: Sid has 12 x 2 = <<12*2=24>>24 sticks.
Sam has 24 x 3 = <<24*3=72>>72 sticks.
Thus, they have 24 + 72 + 12 = <<24+72+12=108>>108 popsicle sticks.
So the final answer is 108

Input: The price of an iPhone fell 10% in a particular month and another 20% in the second month.  If the initial price was $1000, calculate the price after the second month.
Output: In the first month, the price of the iPhone fell by 10/100*$1000=$<<10/100*1000=100>>100
The new price after the first month is $1000-$100 = $<<1000-100=900>>900
In the second month, the price fell 20% further by 20/100*$900 = $<<20/100*900=180>>180
The new price after the second month is $900-$180 = $<<900-180=720>>720
So the final answer is 720

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o15-tgsm8k-s20-rTrue-m4096
Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:04:00<1:21:27, 87.28s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:05:28<1:20:00, 87.28s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [1:06:55<1:18:36, 87.35s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:08:23<1:17:07, 87.32s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:09:50<1:15:41, 87.33s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:11:17<1:14:11, 87.29s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [1:12:44<1:12:44, 87.28s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [1:14:11<1:11:13, 87.22s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:15:38<1:09:43, 87.16s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:17:06<1:08:16, 87.16s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:18:33<1:06:47, 87.12s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:20:00<1:05:20, 87.11s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:21:27<1:03:53, 87.13s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:22:54<1:02:23, 87.05s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:24:21<1:00:57, 87.08s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [1:25:48<59:29, 87.05s/it]  Evaluating commonsenseqa :  60%|██████    | 60/100 [1:27:15<58:02, 87.05s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [1:28:42<56:34, 87.03s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [1:30:09<55:07, 87.05s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [1:31:36<53:40, 87.03s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [1:33:03<52:13, 87.03s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [1:34:30<50:45, 87.01s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [1:35:57<49:18, 87.02s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [1:37:24<47:50, 86.98s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [1:38:51<46:22, 86.94s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [1:40:17<44:52, 86.86s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [1:41:44<43:23, 86.80s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [1:43:11<41:56, 86.76s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [1:44:38<40:29, 86.76s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [1:46:04<39:02, 86.74s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [1:47:31<37:34, 86.70s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [1:48:58<36:07, 86.72s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [1:50:24<34:40, 86.69s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [1:51:51<33:13, 86.66s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [1:53:18<31:46, 86.67s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [1:54:44<30:20, 86.69s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [1:56:11<28:53, 86.67s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [1:57:23<26:04, 82.34s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [1:58:50<25:05, 83.65s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [2:00:17<23:57, 84.57s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [2:01:43<22:43, 85.20s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [2:03:10<21:25, 85.71s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [2:04:37<20:04, 86.05s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [2:06:04<18:40, 86.22s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [2:07:30<17:16, 86.39s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [2:08:57<15:51, 86.48s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [2:10:24<14:25, 86.57s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [2:11:51<12:59, 86.62s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [2:13:17<11:33, 86.65s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [2:14:44<10:06, 86.66s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [2:16:11<08:40, 86.75s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [2:17:38<07:13, 86.73s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [2:19:04<05:46, 86.72s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [2:20:31<04:20, 86.72s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [2:21:58<02:53, 86.73s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [2:23:25<01:26, 86.75s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:24:51<00:00, 86.77s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:24:51<00:00, 86.92s/it]
name: commonsenseqa | avg. gen lenth: 323.87 | time: 8695.096986055374s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o16-tgsm8k-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 16 16 17 18 19 20 True 4096 10
[2023-09-07 16:18:58,158] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o16-tgsm8k-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 16
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o16-tgsm8k-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 327377.53it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.37s/it]
 > number of parameters: 6738415616
[2023-09-07 16:19:14,595] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 16:19:14,802] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 16:19:14,803] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0250>
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 16:19:14,804] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 16:19:14,805] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 16:19:14,805] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The car-rental agency charges $30/day for a car, or $190 for the first week for a rental that lasts an entire week or longer. Jennie rented a car for 11 days. How much, in dollars, did she pay for the rental?
Output: The first 7 days were $190.
There were 11-7=<<11-7=4>>4 days left.
The additional 4 days were 4*30=<<4*30=120>>120.
And 190+120=<<190+120=310>>310.
So the final answer is 310

Input: A hurricane is approaching the southern coast of Texas, and a rancher is planning to move 400 head of cattle 60 miles to higher ground to protect them from possible inland flooding that might occur.  His animal transport truck holds 20 head of cattle.  Traveling at 60 miles per hour, what is the total driving time, in hours, it will take to transport all of his cattle to higher ground?
Output: Given the limited capacity of his transport vehicle (20 head of cattle), the 400 head of cattle will require 400/20=<<400/20=20>>20 trips using his transport vehicle.
Traveling to the site at 60 mph for 60 miles it will take 60/60=<<60/60=1>>1 hour to travel one-way.
Since each trip requires driving to and returning from the relocation site, each complete round trip will take 2*1=<<2*1=2>>2 hours.
Thus, 20 complete trips will take 20*2=<<20*2=40>>40 hours of driving time.
So the final answer is 40

Input: Jason has a carriage house that he rents out.  He’s charging $50.00 per day or $500.00 for 14 days.  Eric wants to rent the house for 20 days.  How much will it cost him?
Output: He wants to rent for 20 days and there is a deal if you rent for 14 days so that leaves 20-14 = <<20-14=6>>6 individual days
Each individual day is $50.00 and he will have 6 individual days for a total of 50*6 = $<<50*6=300.00>>300.00
14 days costs $500.00 and 6 days costs $300.00 for a total of 500+300 = $800.00
So the final answer is 800

Input: Melissa works on a poultry farm. She drives to town twice each month to buy supplies. If it takes her 3 hours to drive to town and back, how many hours does Melissa spend driving in a year?
Output: Melissa spends 2x3=<<2*3=6>>6 hours driving each month.
Since there are 12 months in a year, she spends 6x12=<<6*12=72>>72 hours driving each year.
So the final answer is 72

Input: The ratio of boys to girls in a family is 5:7. The total number of children in the family is 180. If the boys are given $3900 to share, how much money does each boy receive?
Output: The total ratio representing the number of children in the family is 5+7 = <<5+7=12>>12
From the total ratio of children in the family, 5/12 represent the number of boys, meaning that the number of boys in the family is 5/12*180 = <<5/12*180=75>>75
If the boys are given $3900 to share, each boy receives $3900/75 = $<<3900/75=52>>52
So the final answer is 52

Input: Josephine receives a bill from the hospital for 5000$.  50 percent of the bill is for medication.  25 percent of the remaining bill is for overnight stays, and 175$ is for food.  The rest of the bill is for the ambulance ride.  How much did the ambulance ride cost?
Output: Medication:5000(.50)=2500
Overnight Stays:2500(.25)=625
2500-625=<<2500-625=1875>>1875
Food:1875-175=<<1875-175=1700>>1700$
Ambulance ride:1700$
So the final answer is 1700

Input: It was time for Kelly to harvest her carrots that she had planted in three different beds.  In the first bed she pulled out 55 carrots.  In the second bed she pulled out 101 carrots and in the third bed she pulled out 78 carrots.  She found that 6 carrots weighed one pound.  How many pounds of carrots did Kelly harvest?
Output: She pulled out 55 + 101 + 78 = <<55+101+78=234>>234
6 carrots weigh one pound so 234/6 = <<234/6=39>>39 pounds of carrots
So the final answer is 39

Input: Iris’ family is planning a surprise birthday party for her. The party will include her 3 uncles and 4 aunts who have a son and daughter each as well as her brother and mother. In total, how many people are coming to Iris’ birthday party?
Output: Each of her aunts and uncles have a family unit of 1 son + 1 daughter + 1 aunt/uncle = <<1+1+1=3>>3 people.
Iris has a total of 3 uncles + 4 aunts = <<3+4=7>>7 aunts or uncles in these family units.
So among her aunts, uncles, and cousins, there will be 7 family units * 3 people in each family unit = <<7*3=21>>21 people.
Including her mother and brother, there will be a total of 21 people + 1 mother + 1 brother = <<21+1+1=23>>23 people coming to her party.
So the final answer is 23

Input: Lyra bought a pair of shoes at a 20% discount.  If she paid $480, how much was the original price of the pair of shoes?
Output: Lyra only paid $480 for the pair of shoes which is only 100% - 20% = 80% of the original price.
So let x be the original price.
Then 0.8x = $480
Thus x = $480 / 0.8 = $<<480/0.8=600>>600
So the final answer is 600

Input: Rhett has been late on two of his monthly rent payments, but his landlord does not charge late fees and so he will be able to pay their total cost with 3/5 of his next month's salary after taxes. If he is currently paid $5000 per month and has to pay 10% tax, calculate his rent expense per month?
Output: If Rhett is currently paid $5000 per month, he pays 10/100*$5000 = $<<10/100*5000=500>>500 in taxes.
Rhett has been late on two of his monthly rent payments and plans to pay them with 3/5*$4500=$<<3/5*4500=2700>>2700 from his salary after taxes.
If he is to pay $2700 for two late monthly rent payments, his monthly rent expense is $2700/2=$<<2700/2=1350>>1350
So the final answer is 1350

Input: Ten friends decide to get an end-of-year gift for their teacher. They plan to split the cost of the gift equally. But four of the group drop out. The remaining friends split the cost equally among themselves. If each share is now $8 more, how much does the gift cost, in dollars?
Output: Let N be the original price each friend was going to pay.
10N=6(N+8)
10N=6N+48
4N=48
N=<<12=12>>12
Then the present costs 10*12=<<10*12=120>>120.
So the final answer is 120

Input: At the feline sanctuary, there were 12 lions, 14 tigers, and several cougars.  If there were half as many cougars as lions and tigers combined, then what was the total number of big cats at the feline sanctuary?
Output: Half as many cougars as lions and tigers combined is (12+14)/2=13.
Then the total number of big cats at the feline sanctuary is 12+14+13=<<12+14+13=39>>39.
So the final answer is 39

Input: A pea patch is twice as big as a radish patch. If one sixth of the pea patch is 5 square feet.  How much is a whole radish patch in square feet?
Output: A whole pea patch is 5*6 = <<5*6=30>>30 square feet.
A radish patch is 30/2 = <<30/2=15>>15 square feet.
So the final answer is 15

Input: Phoebe has two pizzas to share with her and three friends. One has pepperoni and the other has cheese. They both have 16 slices. They all eat the same amount. One friend eats only pepperoni, while the rest have an equal number of slices of each. At the end, there is one slice of pepperoni left and 7 slices of cheese, how many slices does each person eat?
Output: They are 9 slices of cheese because 16 - 7 = <<16-7=9>>9
They ate three slices of cheese each because 9 / 3 = <<9/3=3>>3
They are 6 slices each in total because 3 x 2 = <<3*2=6>>6
So the final answer is 6

Input: Jeremy buys 30 watermelons. He eats 3 watermelons per week. Each week he gives 2 to his dad. How many weeks will the watermelons last?
Output: Jeremey goes through 3+2=<<3+2=5>>5 watermelons per week.
Therefore, the watermelons will last him to 30/5=<<30/5=6>>6 weeks.
So the final answer is 6

Input: Jackson has 5 times more money than Williams. Together, they have $150. How much money, in dollars, does Jackson have?
Output: Let x be the amount of money Williams has.
Jackson has 5*x dollars.
5*x+x=150
6*x=150
x=<<25=25>>25
Jackson has 25*5=<<25*5=125>>125 dollars.
So the final answer is 125

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o16-tgsm8k-s30-rTrue-m4096
Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:23:18<1:14:44, 95.42s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:24:54<1:13:13, 95.51s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:26:29<1:11:33, 95.41s/it]                                                                                Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:28:05<1:10:02, 95.50s/it]                                                                                Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:29:40<1:08:24, 95.46s/it]                                                                                Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:31:16<1:06:54, 95.59s/it]                                                                                Evaluating commonsenseqa :  59%|█████▉    | 59/100 [1:32:52<1:05:20, 95.63s/it]                                                                                Evaluating commonsenseqa :  60%|██████    | 60/100 [1:34:27<1:03:41, 95.55s/it]                                                                                Evaluating commonsenseqa :  61%|██████    | 61/100 [1:36:03<1:02:09, 95.62s/it]                                                                                Evaluating commonsenseqa :  62%|██████▏   | 62/100 [1:37:38<1:00:33, 95.61s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [1:39:14<58:55, 95.55s/it]  Evaluating commonsenseqa :  64%|██████▍   | 64/100 [1:40:48<57:08, 95.23s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [1:42:24<55:41, 95.48s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [1:44:01<54:21, 95.93s/it]                                                                                   Evaluating commonsenseqa :  67%|██████▋   | 67/100 [1:45:38<52:49, 96.03s/it]                                                                                   Evaluating commonsenseqa :  68%|██████▊   | 68/100 [1:47:13<51:08, 95.88s/it]                                                                                   Evaluating commonsenseqa :  69%|██████▉   | 69/100 [1:48:49<49:35, 95.99s/it]                                                                                   Evaluating commonsenseqa :  70%|███████   | 70/100 [1:50:24<47:48, 95.63s/it]                                                                                   Evaluating commonsenseqa :  71%|███████   | 71/100 [1:52:00<46:11, 95.57s/it]                                                                                   Evaluating commonsenseqa :  72%|███████▏  | 72/100 [1:53:35<44:34, 95.51s/it]                                                                                   Evaluating commonsenseqa :  73%|███████▎  | 73/100 [1:55:11<42:59, 95.53s/it]                                                                                   Evaluating commonsenseqa :  74%|███████▍  | 74/100 [1:56:47<41:28, 95.69s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [1:58:23<39:54, 95.76s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [1:59:58<38:16, 95.68s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [2:01:34<36:43, 95.78s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [2:03:10<35:09, 95.87s/it]                                                                                     Evaluating commonsenseqa :  79%|███████▉  | 79/100 [2:04:46<33:32, 95.85s/it]                                                                                     Evaluating commonsenseqa :  80%|████████  | 80/100 [2:06:21<31:51, 95.59s/it]                                                                                     Evaluating commonsenseqa :  81%|████████  | 81/100 [2:07:57<30:17, 95.66s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [45:30<2:09:55, 105.34s/it]                                                                                                Evaluating commonsenseqa :  27%|██▋       | 27/100 [47:15<2:08:11, 105.37s/it]                                                                                                Evaluating commonsenseqa :  28%|██▊       | 28/100 [48:59<2:05:57, 104.96s/it]                                                                                                Evaluating commonsenseqa :  29%|██▉       | 29/100 [50:44<2:03:58, 104.76s/it]           Evaluating commonsenseqa :  30%|███       | 30/100 [52:29<2:02:37, 105.10s/it]           Evaluating commonsenseqa :  31%|███       | 31/100 [54:14<2:00:45, 105.01s/it]           Evaluating commonsenseqa :  32%|███▏      | 32/100 [56:00<1:59:21, 105.31s/it]                                                                                                         Evaluating commonsenseqa :  33%|███▎      | 33/100 [57:46<1:57:46, 105.47s/it]                                                                                                Evaluating commonsenseqa :  34%|███▍      | 34/100 [59:32<1:56:03, 105.51s/it]                                                                                                Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:01:15<1:53:44, 104.99s/it]                                                                                                  Evaluating commonsenseqa :  36%|███▌      | 36/100 [1:02:59<1:51:35, 104.61s/it]                                                                                                  Evaluating commonsenseqa :  37%|███▋      | 37/100 [1:04:44<1:49:49, 104.60s/it]                                                                                                  Evaluating commonsenseqa :  38%|███▊      | 38/100 [1:06:29<1:48:14, 104.75s/it]                                                                                                  Evaluating commonsenseqa :  39%|███▉      | 39/100 [1:08:15<1:47:00, 105.25s/it]         Evaluating commonsenseqa :  40%|████      | 40/100 [1:10:01<1:45:17, 105.29s/it]         Evaluating commonsenseqa :  41%|████      | 41/100 [1:11:46<1:43:27, 105.21s/it]         Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:13:32<1:42:07, 105.65s/it]52s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:38:08<00:00, 94.49s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:38:08<00:00, 94.88s/it]
name: commonsenseqa | avg. gen lenth: 401.426 | time: 9490.995200872421s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu02 --master_port 19674 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o11-tgsm8k-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 11 11 12 13 14 15 True 4096 10
[2023-09-07 17:34:27,261] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o11-tgsm8k-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 11
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o11-tgsm8k-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 402719.66it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:15:18<1:40:28, 105.76s/it] checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.81s/it]
 > number of parameters: 6738415616
[2023-09-07 17:34:42,570] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 17:34:42,773] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 17:34:42,775] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 17:34:42,776] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 17:34:42,776] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 17:34:42,776] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 17:34:42,776] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 17:34:42,776] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 17:34:42,776] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 17:34:42,776] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 17:34:42,776] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 17:34:42,776] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 17:34:42,776] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0250>
[2023-09-07 17:34:42,776] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 17:34:42,776] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 17:34:42,776] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 17:34:42,776] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 17:34:42,776] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 17:34:42,777] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 17:34:42,778] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 17:34:42,778] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The car-rental agency charges $30/day for a car, or $190 for the first week for a rental that lasts an entire week or longer. Jennie rented a car for 11 days. How much, in dollars, did she pay for the rental?
Output: The first 7 days were $190.
There were 11-7=<<11-7=4>>4 days left.
The additional 4 days were 4*30=<<4*30=120>>120.
And 190+120=<<190+120=310>>310.
So the final answer is 310

Input: A hurricane is approaching the southern coast of Texas, and a rancher is planning to move 400 head of cattle 60 miles to higher ground to protect them from possible inland flooding that might occur.  His animal transport truck holds 20 head of cattle.  Traveling at 60 miles per hour, what is the total driving time, in hours, it will take to transport all of his cattle to higher ground?
Output: Given the limited capacity of his transport vehicle (20 head of cattle), the 400 head of cattle will require 400/20=<<400/20=20>>20 trips using his transport vehicle.
Traveling to the site at 60 mph for 60 miles it will take 60/60=<<60/60=1>>1 hour to travel one-way.
Since each trip requires driving to and returning from the relocation site, each complete round trip will take 2*1=<<2*1=2>>2 hours.
Thus, 20 complete trips will take 20*2=<<20*2=40>>40 hours of driving time.
So the final answer is 40

Input: Jason has a carriage house that he rents out.  He’s charging $50.00 per day or $500.00 for 14 days.  Eric wants to rent the house for 20 days.  How much will it cost him?
Output: He wants to rent for 20 days and there is a deal if you rent for 14 days so that leaves 20-14 = <<20-14=6>>6 individual days
Each individual day is $50.00 and he will have 6 individual days for a total of 50*6 = $<<50*6=300.00>>300.00
14 days costs $500.00 and 6 days costs $300.00 for a total of 500+300 = $800.00
So the final answer is 800

Input: Melissa works on a poultry farm. She drives to town twice each month to buy supplies. If it takes her 3 hours to drive to town and back, how many hours does Melissa spend driving in a year?
Output: Melissa spends 2x3=<<2*3=6>>6 hours driving each month.
Since there are 12 months in a year, she spends 6x12=<<6*12=72>>72 hours driving each year.
So the final answer is 72

Input: The ratio of boys to girls in a family is 5:7. The total number of children in the family is 180. If the boys are given $3900 to share, how much money does each boy receive?
Output: The total ratio representing the number of children in the family is 5+7 = <<5+7=12>>12
From the total ratio of children in the family, 5/12 represent the number of boys, meaning that the number of boys in the family is 5/12*180 = <<5/12*180=75>>75
If the boys are given $3900 to share, each boy receives $3900/75 = $<<3900/75=52>>52
So the final answer is 52

Input: Josephine receives a bill from the hospital for 5000$.  50 percent of the bill is for medication.  25 percent of the remaining bill is for overnight stays, and 175$ is for food.  The rest of the bill is for the ambulance ride.  How much did the ambulance ride cost?
Output: Medication:5000(.50)=2500
Overnight Stays:2500(.25)=625
2500-625=<<2500-625=1875>>1875
Food:1875-175=<<1875-175=1700>>1700$
Ambulance ride:1700$
So the final answer is 1700

Input: It was time for Kelly to harvest her carrots that she had planted in three different beds.  In the first bed she pulled out 55 carrots.  In the second bed she pulled out 101 carrots and in the third bed she pulled out 78 carrots.  She found that 6 carrots weighed one pound.  How many pounds of carrots did Kelly harvest?
Output: She pulled out 55 + 101 + 78 = <<55+101+78=234>>234
6 carrots weigh one pound so 234/6 = <<234/6=39>>39 pounds of carrots
So the final answer is 39

Input: Iris’ family is planning a surprise birthday party for her. The party will include her 3 uncles and 4 aunts who have a son and daughter each as well as her brother and mother. In total, how many people are coming to Iris’ birthday party?
Output: Each of her aunts and uncles have a family unit of 1 son + 1 daughter + 1 aunt/uncle = <<1+1+1=3>>3 people.
Iris has a total of 3 uncles + 4 aunts = <<3+4=7>>7 aunts or uncles in these family units.
So among her aunts, uncles, and cousins, there will be 7 family units * 3 people in each family unit = <<7*3=21>>21 people.
Including her mother and brother, there will be a total of 21 people + 1 mother + 1 brother = <<21+1+1=23>>23 people coming to her party.
So the final answer is 23

Input: Lyra bought a pair of shoes at a 20% discount.  If she paid $480, how much was the original price of the pair of shoes?
Output: Lyra only paid $480 for the pair of shoes which is only 100% - 20% = 80% of the original price.
So let x be the original price.
Then 0.8x = $480
Thus x = $480 / 0.8 = $<<480/0.8=600>>600
So the final answer is 600

Input: Rhett has been late on two of his monthly rent payments, but his landlord does not charge late fees and so he will be able to pay their total cost with 3/5 of his next month's salary after taxes. If he is currently paid $5000 per month and has to pay 10% tax, calculate his rent expense per month?
Output: If Rhett is currently paid $5000 per month, he pays 10/100*$5000 = $<<10/100*5000=500>>500 in taxes.
Rhett has been late on two of his monthly rent payments and plans to pay them with 3/5*$4500=$<<3/5*4500=2700>>2700 from his salary after taxes.
If he is to pay $2700 for two late monthly rent payments, his monthly rent expense is $2700/2=$<<2700/2=1350>>1350
So the final answer is 1350

Input: Ten friends decide to get an end-of-year gift for their teacher. They plan to split the cost of the gift equally. But four of the group drop out. The remaining friends split the cost equally among themselves. If each share is now $8 more, how much does the gift cost, in dollars?
Output: Let N be the original price each friend was going to pay.
10N=6(N+8)
10N=6N+48
4N=48
N=<<12=12>>12
Then the present costs 10*12=<<10*12=120>>120.
So the final answer is 120

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o11-tgsm8k-s30-rTrue-m4096
                                                                                           Evaluating commonsenseqa :   1%|          | 1/100 [02:13<3:40:07, 133.41s/it]                                                                                           Evaluating commonsenseqa :   2%|▏         | 2/100 [04:26<3:37:19, 133.06s/it]                                                                                           Evaluating commonsenseqa :   3%|▎         | 3/100 [06:42<3:37:21, 134.45s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:22:16<1:32:20, 104.54s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:24:00<1:30:35, 104.52s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:25:45<1:28:50, 104.52s/it]                                                                                Evaluating commonsenseqa :  50%|█████     | 50/100 [1:27:30<1:27:14, 104.69s/it]                                                                                Evaluating commonsenseqa :  51%|█████     | 51/100 [1:29:16<1:25:46, 105.03s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:31:00<1:23:55, 104.91s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:32:46<1:22:24, 105.21s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:34:30<1:20:27, 104.94s/it]                                                                                Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:36:15<1:18:44, 104.98s/it]                                                                                 Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:38:01<1:17:12, 105.28s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:39:49<1:15:50, 105.82s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:41:34<1:14:04, 105.82s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [1:43:20<1:12:22, 105.91s/it]                                                                                   Evaluating commonsenseqa :  60%|██████    | 60/100 [1:45:05<1:10:21, 105.53s/it]                                                                                   Evaluating commonsenseqa :  61%|██████    | 61/100 [1:46:49<1:08:21, 105.16s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [1:48:33<1:06:21, 104.78s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [1:50:19<1:04:43, 104.97s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [1:52:04<1:03:01, 105.03s/it]                                                                                   Evaluating commonsenseqa :  65%|██████▌   | 65/100 [1:53:50<1:01:26, 105.32s/it]                                                                                   Evaluating commonsenseqa :  66%|██████▌   | 66/100 [1:55:34<59:29, 104.97s/it]  Evaluating commonsenseqa :  67%|██████▋   | 67/100 [1:57:22<58:13, 105.86s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [1:59:06<56:09, 105.28s/it]                                                                                   Evaluating commonsenseqa :  69%|██████▉   | 69/100 [2:00:52<54:27, 105.40s/it]                                                                                   Evaluating commonsenseqa :  70%|███████   | 70/100 [2:02:36<52:34, 105.13s/it]                                                                                     Evaluating commonsenseqa :  71%|███████   | 71/100 [2:04:21<50:50, 105.17s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [2:06:07<49:12, 105.44s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [2:07:51<47:10, 104.85s/it]                                                                                     Evaluating commonsenseqa :  74%|███████▍  | 74/100 [2:09:37<45:33, 105.13s/it]                                                                                     Evaluating commonsenseqa :  75%|███████▌  | 75/100 [2:11:21<43:44, 104.97s/it]                                                                                     Evaluating commonsenseqa :  76%|███████▌  | 76/100 [2:13:08<42:09, 105.38s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [2:14:53<40:24, 105.41s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [2:16:38<38:38, 105.38s/it]                                                                                       Evaluating commonsenseqa :  79%|███████▉  | 79/100 [2:18:24<36:52, 105.34s/it]                                                                                       Evaluating commonsenseqa :  80%|████████  | 80/100 [2:20:07<34:56, 104.85s/it]                                                                                       Evaluating commonsenseqa :  81%|████████  | 81/100 [2:21:53<33:19, 105.22s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [2:23:38<31:33, 105.17s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [2:25:25<29:54, 105.56s/it]                                                                                         Evaluating commonsenseqa :  84%|████████▍ | 84/100 [2:27:13<28:18, 106.18s/it]                                                                                         Evaluating commonsenseqa :  85%|████████▌ | 85/100 [2:28:57<26:24, 105.61s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [2:30:43<24:38, 105.64s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [2:32:28<22:53, 105.66s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [2:34:13<21:04, 105.35s/it]                                                                                         Evaluating commonsenseqa :  89%|████████▉ | 89/100 [2:35:57<19:15, 105.03s/it]                                                                                         Evaluating commonsenseqa :  90%|█████████ | 90/100 [2:37:41<17:27, 104.76s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [2:39:27<15:44, 104.93s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [2:41:12<13:59, 104.93s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [2:42:58<12:16, 105.28s/it]                                                                                         Evaluating commonsenseqa :  94%|█████████▍| 94/100 [2:44:42<10:30, 105.04s/it]                                                                                         Evaluating commonsenseqa :  95%|█████████▌| 95/100 [2:46:27<08:45, 105.10s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [2:48:13<07:00, 105.13s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [2:49:59<05:16, 105.37s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [2:51:43<03:30, 105.01s/it]                                                                                           Evaluating commonsenseqa :  99%|█████████▉| 99/100 [2:53:31<01:45, 105.96s/it]                                                                                           Evaluating commonsenseqa : 100%|██████████| 100/100 [2:55:17<00:00, 105.91s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:55:17<00:00, 105.17s/it]
name: commonsenseqa | avg. gen lenth: 381.45 | time: 10519.937495708466s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o17-tgsm8k-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 17 16 17 18 19 20 True 4096 10
[2023-09-07 19:14:40,825] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o17-tgsm8k-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 17
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o17-tgsm8k-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 309813.95it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.28s/it]
 > number of parameters: 6738415616
[2023-09-07 19:14:55,083] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 19:14:55,287] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 19:14:55,289] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 19:14:55,289] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 19:14:55,289] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 19:14:55,289] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 19:14:55,289] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c01f0>
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 19:14:55,290] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 19:14:55,291] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 19:14:55,291] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The car-rental agency charges $30/day for a car, or $190 for the first week for a rental that lasts an entire week or longer. Jennie rented a car for 11 days. How much, in dollars, did she pay for the rental?
Output: The first 7 days were $190.
There were 11-7=<<11-7=4>>4 days left.
The additional 4 days were 4*30=<<4*30=120>>120.
And 190+120=<<190+120=310>>310.
So the final answer is 310

Input: A hurricane is approaching the southern coast of Texas, and a rancher is planning to move 400 head of cattle 60 miles to higher ground to protect them from possible inland flooding that might occur.  His animal transport truck holds 20 head of cattle.  Traveling at 60 miles per hour, what is the total driving time, in hours, it will take to transport all of his cattle to higher ground?
Output: Given the limited capacity of his transport vehicle (20 head of cattle), the 400 head of cattle will require 400/20=<<400/20=20>>20 trips using his transport vehicle.
Traveling to the site at 60 mph for 60 miles it will take 60/60=<<60/60=1>>1 hour to travel one-way.
Since each trip requires driving to and returning from the relocation site, each complete round trip will take 2*1=<<2*1=2>>2 hours.
Thus, 20 complete trips will take 20*2=<<20*2=40>>40 hours of driving time.
So the final answer is 40

Input: Jason has a carriage house that he rents out.  He’s charging $50.00 per day or $500.00 for 14 days.  Eric wants to rent the house for 20 days.  How much will it cost him?
Output: He wants to rent for 20 days and there is a deal if you rent for 14 days so that leaves 20-14 = <<20-14=6>>6 individual days
Each individual day is $50.00 and he will have 6 individual days for a total of 50*6 = $<<50*6=300.00>>300.00
14 days costs $500.00 and 6 days costs $300.00 for a total of 500+300 = $800.00
So the final answer is 800

Input: Melissa works on a poultry farm. She drives to town twice each month to buy supplies. If it takes her 3 hours to drive to town and back, how many hours does Melissa spend driving in a year?
Output: Melissa spends 2x3=<<2*3=6>>6 hours driving each month.
Since there are 12 months in a year, she spends 6x12=<<6*12=72>>72 hours driving each year.
So the final answer is 72

Input: The ratio of boys to girls in a family is 5:7. The total number of children in the family is 180. If the boys are given $3900 to share, how much money does each boy receive?
Output: The total ratio representing the number of children in the family is 5+7 = <<5+7=12>>12
From the total ratio of children in the family, 5/12 represent the number of boys, meaning that the number of boys in the family is 5/12*180 = <<5/12*180=75>>75
If the boys are given $3900 to share, each boy receives $3900/75 = $<<3900/75=52>>52
So the final answer is 52

Input: Josephine receives a bill from the hospital for 5000$.  50 percent of the bill is for medication.  25 percent of the remaining bill is for overnight stays, and 175$ is for food.  The rest of the bill is for the ambulance ride.  How much did the ambulance ride cost?
Output: Medication:5000(.50)=2500
Overnight Stays:2500(.25)=625
2500-625=<<2500-625=1875>>1875
Food:1875-175=<<1875-175=1700>>1700$
Ambulance ride:1700$
So the final answer is 1700

Input: It was time for Kelly to harvest her carrots that she had planted in three different beds.  In the first bed she pulled out 55 carrots.  In the second bed she pulled out 101 carrots and in the third bed she pulled out 78 carrots.  She found that 6 carrots weighed one pound.  How many pounds of carrots did Kelly harvest?
Output: She pulled out 55 + 101 + 78 = <<55+101+78=234>>234
6 carrots weigh one pound so 234/6 = <<234/6=39>>39 pounds of carrots
So the final answer is 39

Input: Iris’ family is planning a surprise birthday party for her. The party will include her 3 uncles and 4 aunts who have a son and daughter each as well as her brother and mother. In total, how many people are coming to Iris’ birthday party?
Output: Each of her aunts and uncles have a family unit of 1 son + 1 daughter + 1 aunt/uncle = <<1+1+1=3>>3 people.
Iris has a total of 3 uncles + 4 aunts = <<3+4=7>>7 aunts or uncles in these family units.
So among her aunts, uncles, and cousins, there will be 7 family units * 3 people in each family unit = <<7*3=21>>21 people.
Including her mother and brother, there will be a total of 21 people + 1 mother + 1 brother = <<21+1+1=23>>23 people coming to her party.
So the final answer is 23

Input: Lyra bought a pair of shoes at a 20% discount.  If she paid $480, how much was the original price of the pair of shoes?
Output: Lyra only paid $480 for the pair of shoes which is only 100% - 20% = 80% of the original price.
So let x be the original price.
Then 0.8x = $480
Thus x = $480 / 0.8 = $<<480/0.8=600>>600
So the final answer is 600

Input: Rhett has been late on two of his monthly rent payments, but his landlord does not charge late fees and so he will be able to pay their total cost with 3/5 of his next month's salary after taxes. If he is currently paid $5000 per month and has to pay 10% tax, calculate his rent expense per month?
Output: If Rhett is currently paid $5000 per month, he pays 10/100*$5000 = $<<10/100*5000=500>>500 in taxes.
Rhett has been late on two of his monthly rent payments and plans to pay them with 3/5*$4500=$<<3/5*4500=2700>>2700 from his salary after taxes.
If he is to pay $2700 for two late monthly rent payments, his monthly rent expense is $2700/2=$<<2700/2=1350>>1350
So the final answer is 1350

Input: Ten friends decide to get an end-of-year gift for their teacher. They plan to split the cost of the gift equally. But four of the group drop out. The remaining friends split the cost equally among themselves. If each share is now $8 more, how much does the gift cost, in dollars?
Output: Let N be the original price each friend was going to pay.
10N=6(N+8)
10N=6N+48
4N=48
N=<<12=12>>12
Then the present costs 10*12=<<10*12=120>>120.
So the final answer is 120

Input: At the feline sanctuary, there were 12 lions, 14 tigers, and several cougars.  If there were half as many cougars as lions and tigers combined, then what was the total number of big cats at the feline sanctuary?
Output: Half as many cougars as lions and tigers combined is (12+14)/2=13.
Then the total number of big cats at the feline sanctuary is 12+14+13=<<12+14+13=39>>39.
So the final answer is 39

Input: A pea patch is twice as big as a radish patch. If one sixth of the pea patch is 5 square feet.  How much is a whole radish patch in square feet?
Output: A whole pea patch is 5*6 = <<5*6=30>>30 square feet.
A radish patch is 30/2 = <<30/2=15>>15 square feet.
So the final answer is 15

Input: Phoebe has two pizzas to share with her and three friends. One has pepperoni and the other has cheese. They both have 16 slices. They all eat the same amount. One friend eats only pepperoni, while the rest have an equal number of slices of each. At the end, there is one slice of pepperoni left and 7 slices of cheese, how many slices does each person eat?
Output: They are 9 slices of cheese because 16 - 7 = <<16-7=9>>9
They ate three slices of cheese each because 9 / 3 = <<9/3=3>>3
They are 6 slices each in total because 3 x 2 = <<3*2=6>>6
So the final answer is 6

Input: Jeremy buys 30 watermelons. He eats 3 watermelons per week. Each week he gives 2 to his dad. How many weeks will the watermelons last?
Output: Jeremey goes through 3+2=<<3+2=5>>5 watermelons per week.
Therefore, the watermelons will last him to 30/5=<<30/5=6>>6 weeks.
So the final answer is 6

Input: Jackson has 5 times more money than Williams. Together, they have $150. How much money, in dollars, does Jackson have?
Output: Let x be the amount of money Williams has.
Jackson has 5*x dollars.
5*x+x=150
6*x=150
x=<<25=25>>25
Jackson has 25*5=<<25*5=125>>125 dollars.
So the final answer is 125

Input: Janet counts 30 crows on the powerlines and 60% more hawks than crows. How many birds does she count total?
Output: First find how many more hawks than crows there are: 60% * 30 crows = <<60*.01*30=18>>18 crows
Then add that number to the number of crows to find the total number of hawks: 18 crows + 30 crows = <<18+30=48>>48 crows
Then add the number of crows to the number of hawks to find the total number of birds: 48 crows + 30 crows = <<48+30=78>>78 crows
So the final answer is 78

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o17-tgsm8k-s30-rTrue-m4096
                                                                                           Evaluating commonsenseqa :   1%|          | 1/100 [01:40<2:45:47, 100.48s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [03:19<2:42:54, 99.74s/it]            Evaluating commonsenseqa :   3%|▎         | 3/100 [04:57<2:39:54, 98.92s/it]                                                                                           Evaluating commonsenseqa :   4%|▍         | 4/100 [06:37<2:38:58, 99.36s/it]                                                                                           Evaluating commonsenseqa :   5%|▌         | 5/100 [08:15<2:36:41, 98.97s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [09:55<2:35:31, 99.27s/it]            Evaluating commonsenseqa :   7%|▋         | 7/100 [11:34<2:33:36, 99.10s/it]                                                                                           Evaluating commonsenseqa :   8%|▊         | 8/100 [13:13<2:31:57, 99.10s/it]                                                                                             Evaluating commonsenseqa :   9%|▉         | 9/100 [14:53<2:30:27, 99.20s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [16:32<2:28:52, 99.25s/it]             Evaluating commonsenseqa :  11%|█         | 11/100 [18:11<2:27:07, 99.19s/it]                                                                                             Evaluating commonsenseqa :  12%|█▏        | 12/100 [19:50<2:25:31, 99.22s/it]                                                                                             Evaluating commonsenseqa :  13%|█▎        | 13/100 [21:29<2:23:49, 99.19s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [23:08<2:21:58, 99.05s/it]           Evaluating commonsenseqa :  15%|█▌        | 15/100 [24:47<2:20:21, 99.08s/it]                                                                                             Evaluating commonsenseqa :  16%|█▌        | 16/100 [26:27<2:19:03, 99.33s/it]                                                                                             Evaluating commonsenseqa :  17%|█▋        | 17/100 [28:07<2:17:29, 99.39s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [29:45<2:15:29, 99.14s/it]           Evaluating commonsenseqa :  19%|█▉        | 19/100 [31:25<2:14:03, 99.30s/it]                                                                                             Evaluating commonsenseqa :  20%|██        | 20/100 [33:04<2:12:20, 99.26s/it]                                                                                             Evaluating commonsenseqa :  21%|██        | 21/100 [34:43<2:10:28, 99.10s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [36:23<2:09:03, 99.27s/it]           Evaluating commonsenseqa :  23%|██▎       | 23/100 [38:03<2:07:50, 99.61s/it]                                                                                               Evaluating commonsenseqa :  24%|██▍       | 24/100 [39:44<2:06:35, 99.94s/it]                                                                                               Evaluating commonsenseqa :  25%|██▌       | 25/100 [41:22<2:04:14, 99.39s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [43:01<2:02:37, 99.42s/it]           Evaluating commonsenseqa :  27%|██▋       | 27/100 [44:41<2:01:08, 99.57s/it]                                                                                               Evaluating commonsenseqa :  28%|██▊       | 28/100 [46:21<1:59:34, 99.64s/it]                                                                                               Evaluating commonsenseqa :  29%|██▉       | 29/100 [47:59<1:57:29, 99.29s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [49:39<1:55:59, 99.42s/it]           Evaluating commonsenseqa :  31%|███       | 31/100 [51:18<1:54:13, 99.32s/it]                                                                                               Evaluating commonsenseqa :  32%|███▏      | 32/100 [52:57<1:52:19, 99.11s/it]                                                                                               Evaluating commonsenseqa :  33%|███▎      | 33/100 [54:36<1:50:31, 98.97s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [56:14<1:48:47, 98.90s/it]         Evaluating commonsenseqa :  35%|███▌      | 35/100 [57:53<1:47:05, 98.86s/it]                                                                                                 Evaluating commonsenseqa :  36%|███▌      | 36/100 [59:32<1:45:24, 98.82s/it]                                                                                                 Evaluating commonsenseqa :  37%|███▋      | 37/100 [1:01:12<1:44:15, 99.30s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [1:02:52<1:42:52, 99.56s/it]         Evaluating commonsenseqa :  39%|███▉      | 39/100 [1:04:32<1:41:18, 99.65s/it]                                                                                               Evaluating commonsenseqa :  40%|████      | 40/100 [1:06:13<1:39:56, 99.95s/it]                                                                                               Evaluating commonsenseqa :  41%|████      | 41/100 [1:07:52<1:38:07, 99.79s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:09:31<1:36:10, 99.49s/it]     Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:11:10<1:34:18, 99.27s/it]                                                                                               Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:12:49<1:32:40, 99.30s/it]                                                                                               Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:14:29<1:31:14, 99.53s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [1:16:09<1:29:45, 99.74s/it]     Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:17:49<1:28:07, 99.77s/it]                                                                                               Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:19:28<1:26:13, 99.49s/it]                                                                                                 Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:21:07<1:24:17, 99.17s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [1:22:47<1:22:54, 99.49s/it]       Evaluating commonsenseqa :  51%|█████     | 51/100 [1:24:26<1:21:04, 99.27s/it]                                                                                                 Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:26:06<1:19:40, 99.60s/it]                                                                                                 Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:27:45<1:17:49, 99.35s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:29:25<1:16:19, 99.56s/it]     Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:31:05<1:14:51, 99.81s/it]                                                                                                 Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:32:46<1:13:23, 100.09s/it]                                                                                                 Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:34:26<1:11:40, 100.02s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:36:04<1:09:45, 99.64s/it]     Evaluating commonsenseqa :  59%|█████▉    | 59/100 [1:37:44<1:07:58, 99.49s/it]                                                                                                 Evaluating commonsenseqa :  60%|██████    | 60/100 [1:39:23<1:06:14, 99.37s/it]                                                                                                 Evaluating commonsenseqa :  61%|██████    | 61/100 [1:41:03<1:04:45, 99.64s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [1:42:42<1:02:56, 99.37s/it]     Evaluating commonsenseqa :  63%|██████▎   | 63/100 [1:44:23<1:01:33, 99.83s/it]                                                                                                   Evaluating commonsenseqa :  64%|██████▍   | 64/100 [1:46:02<59:44, 99.57s/it]                                                                                                     Evaluating commonsenseqa :  65%|██████▌   | 65/100 [1:47:41<58:00, 99.45s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [1:49:20<56:14, 99.24s/it]       Evaluating commonsenseqa :  67%|██████▋   | 67/100 [1:50:59<54:33, 99.21s/it]                                                                                                   Evaluating commonsenseqa :  68%|██████▊   | 68/100 [1:52:38<53:00, 99.39s/it]                                                                                                   Evaluating commonsenseqa :  69%|██████▉   | 69/100 [1:54:18<51:24, 99.49s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [1:55:58<49:47, 99.59s/it]       Evaluating commonsenseqa :  71%|███████   | 71/100 [1:57:38<48:14, 99.81s/it]                                                                                                   Evaluating commonsenseqa :  72%|███████▏  | 72/100 [1:59:19<46:43, 100.11s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [3:41:01<00:00, 133.19s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [3:41:01<00:00, 132.61s/it]
name: commonsenseqa | avg. gen lenth: 367.357 | time: 13263.514818191528s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu02 --master_port 19674 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o12-tgsm8k-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 12 11 12 13 14 15 True 4096 10
Evaluating commonsenseqa :  73%|███████▎  | 73/100 [2:00:58<44:49, 99.62s/it] or to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o12-tgsm8k-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 12
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o12-tgsm8k-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 380055.40it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.97s/it]
 > number of parameters: 6738415616
[2023-09-07 21:16:08,480] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 21:16:08,686] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 21:16:08,688] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 21:16:08,688] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 21:16:08,688] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 21:16:08,688] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 21:16:08,688] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c01f0>
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 21:16:08,689] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 21:16:08,690] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 21:16:08,690] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The car-rental agency charges $30/day for a car, or $190 for the first week for a rental that lasts an entire week or longer. Jennie rented a car for 11 days. How much, in dollars, did she pay for the rental?
Output: The first 7 days were $190.
There were 11-7=<<11-7=4>>4 days left.
The additional 4 days were 4*30=<<4*30=120>>120.
And 190+120=<<190+120=310>>310.
So the final answer is 310

Input: A hurricane is approaching the southern coast of Texas, and a rancher is planning to move 400 head of cattle 60 miles to higher ground to protect them from possible inland flooding that might occur.  His animal transport truck holds 20 head of cattle.  Traveling at 60 miles per hour, what is the total driving time, in hours, it will take to transport all of his cattle to higher ground?
Output: Given the limited capacity of his transport vehicle (20 head of cattle), the 400 head of cattle will require 400/20=<<400/20=20>>20 trips using his transport vehicle.
Traveling to the site at 60 mph for 60 miles it will take 60/60=<<60/60=1>>1 hour to travel one-way.
Since each trip requires driving to and returning from the relocation site, each complete round trip will take 2*1=<<2*1=2>>2 hours.
Thus, 20 complete trips will take 20*2=<<20*2=40>>40 hours of driving time.
So the final answer is 40

Input: Jason has a carriage house that he rents out.  He’s charging $50.00 per day or $500.00 for 14 days.  Eric wants to rent the house for 20 days.  How much will it cost him?
Output: He wants to rent for 20 days and there is a deal if you rent for 14 days so that leaves 20-14 = <<20-14=6>>6 individual days
Each individual day is $50.00 and he will have 6 individual days for a total of 50*6 = $<<50*6=300.00>>300.00
14 days costs $500.00 and 6 days costs $300.00 for a total of 500+300 = $800.00
So the final answer is 800

Input: Melissa works on a poultry farm. She drives to town twice each month to buy supplies. If it takes her 3 hours to drive to town and back, how many hours does Melissa spend driving in a year?
Output: Melissa spends 2x3=<<2*3=6>>6 hours driving each month.
Since there are 12 months in a year, she spends 6x12=<<6*12=72>>72 hours driving each year.
So the final answer is 72

Input: The ratio of boys to girls in a family is 5:7. The total number of children in the family is 180. If the boys are given $3900 to share, how much money does each boy receive?
Output: The total ratio representing the number of children in the family is 5+7 = <<5+7=12>>12
From the total ratio of children in the family, 5/12 represent the number of boys, meaning that the number of boys in the family is 5/12*180 = <<5/12*180=75>>75
If the boys are given $3900 to share, each boy receives $3900/75 = $<<3900/75=52>>52
So the final answer is 52

Input: Josephine receives a bill from the hospital for 5000$.  50 percent of the bill is for medication.  25 percent of the remaining bill is for overnight stays, and 175$ is for food.  The rest of the bill is for the ambulance ride.  How much did the ambulance ride cost?
Output: Medication:5000(.50)=2500
Overnight Stays:2500(.25)=625
2500-625=<<2500-625=1875>>1875
Food:1875-175=<<1875-175=1700>>1700$
Ambulance ride:1700$
So the final answer is 1700

Input: It was time for Kelly to harvest her carrots that she had planted in three different beds.  In the first bed she pulled out 55 carrots.  In the second bed she pulled out 101 carrots and in the third bed she pulled out 78 carrots.  She found that 6 carrots weighed one pound.  How many pounds of carrots did Kelly harvest?
Output: She pulled out 55 + 101 + 78 = <<55+101+78=234>>234
6 carrots weigh one pound so 234/6 = <<234/6=39>>39 pounds of carrots
So the final answer is 39

Input: Iris’ family is planning a surprise birthday party for her. The party will include her 3 uncles and 4 aunts who have a son and daughter each as well as her brother and mother. In total, how many people are coming to Iris’ birthday party?
Output: Each of her aunts and uncles have a family unit of 1 son + 1 daughter + 1 aunt/uncle = <<1+1+1=3>>3 people.
Iris has a total of 3 uncles + 4 aunts = <<3+4=7>>7 aunts or uncles in these family units.
So among her aunts, uncles, and cousins, there will be 7 family units * 3 people in each family unit = <<7*3=21>>21 people.
Including her mother and brother, there will be a total of 21 people + 1 mother + 1 brother = <<21+1+1=23>>23 people coming to her party.
So the final answer is 23

Input: Lyra bought a pair of shoes at a 20% discount.  If she paid $480, how much was the original price of the pair of shoes?
Output: Lyra only paid $480 for the pair of shoes which is only 100% - 20% = 80% of the original price.
So let x be the original price.
Then 0.8x = $480
Thus x = $480 / 0.8 = $<<480/0.8=600>>600
So the final answer is 600

Input: Rhett has been late on two of his monthly rent payments, but his landlord does not charge late fees and so he will be able to pay their total cost with 3/5 of his next month's salary after taxes. If he is currently paid $5000 per month and has to pay 10% tax, calculate his rent expense per month?
Output: If Rhett is currently paid $5000 per month, he pays 10/100*$5000 = $<<10/100*5000=500>>500 in taxes.
Rhett has been late on two of his monthly rent payments and plans to pay them with 3/5*$4500=$<<3/5*4500=2700>>2700 from his salary after taxes.
If he is to pay $2700 for two late monthly rent payments, his monthly rent expense is $2700/2=$<<2700/2=1350>>1350
So the final answer is 1350

Input: Ten friends decide to get an end-of-year gift for their teacher. They plan to split the cost of the gift equally. But four of the group drop out. The remaining friends split the cost equally among themselves. If each share is now $8 more, how much does the gift cost, in dollars?
Output: Let N be the original price each friend was going to pay.
10N=6(N+8)
10N=6N+48
4N=48
N=<<12=12>>12
Then the present costs 10*12=<<10*12=120>>120.
So the final answer is 120

Input: At the feline sanctuary, there were 12 lions, 14 tigers, and several cougars.  If there were half as many cougars as lions and tigers combined, then what was the total number of big cats at the feline sanctuary?
Output: Half as many cougars as lions and tigers combined is (12+14)/2=13.
Then the total number of big cats at the feline sanctuary is 12+14+13=<<12+14+13=39>>39.
So the final answer is 39

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o12-tgsm8k-s30-rTrue-m4096
                                                                                              Evaluating commonsenseqa :   1%|          | 1/100 [02:08<3:31:39, 128.28s/it]                                                                                              Evaluating commonsenseqa :   2%|▏         | 2/100 [04:12<3:25:55, 126.08s/it]                                                                                              Evaluating commonsenseqa :   3%|▎         | 3/100 [06:16<3:22:20, 125.16s/it]                                                                                              Evaluating commonsenseqa :   4%|▍         | 4/100 [08:20<3:19:30, 124.70s/it]                                                                                                            Evaluating commonsenseqa :   5%|▌         | 5/100 [10:25<3:17:22, 124.66s/it]                                                                                              Evaluating commonsenseqa :   6%|▌         | 6/100 [12:30<3:15:44, 124.94s/it]                                                                                              Evaluating commonsenseqa :   7%|▋         | 7/100 [14:37<3:14:15, 125.33s/it]                                                                                                Evaluating commonsenseqa :   8%|▊         | 8/100 [16:41<3:11:43, 125.04s/it]                                                                                                                Evaluating commonsenseqa :   9%|▉         | 9/100 [18:50<3:11:38, 126.36s/it]                                                                                                Evaluating commonsenseqa :  10%|█         | 10/100 [20:56<3:09:19, 126.22s/it]                                                                                                Evaluating commonsenseqa :  11%|█         | 11/100 [23:00<3:06:16, 125.58s/it]                Evaluating commonsenseqa :  12%|█▏        | 12/100 [25:04<3:03:09, 124.88s/it]                                                                                                              Evaluating commonsenseqa :  13%|█▎        | 13/100 [27:10<3:01:41, 125.30s/it]                                                                                                Evaluating commonsenseqa :  14%|█▍        | 14/100 [29:15<2:59:27, 125.20s/it]                                                                                                Evaluating commonsenseqa :  15%|█▌        | 15/100 [31:18<2:56:24, 124.52s/it]               Evaluating commonsenseqa :  93%|█████████▎| 93/100 [2:34:12<11:39, 99.99s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [2:35:51<09:58, 99.67s/it]                                                                                   Evaluating commonsenseqa :  95%|█████████▌| 95/100 [2:37:31<08:19, 99.85s/it]                                                                                   Evaluating commonsenseqa :  96%|█████████▌| 96/100 [2:39:11<06:39, 99.92s/it]                                                                                   Evaluating commonsenseqa :  97%|█████████▋| 97/100 [2:40:52<05:00, 100.12s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [2:42:32<03:20, 100.19s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [2:44:11<01:39, 99.94s/it]                                                                                    Evaluating commonsenseqa : 100%|██████████| 100/100 [2:45:51<00:00, 99.88s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:45:51<00:00, 99.52s/it]
name: commonsenseqa | avg. gen lenth: 399.944 | time: 9954.589261770248s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o18-tgsm8k-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 18 16 17 18 19 20 True 4096 10
[2023-09-07 22:00:57,945] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o18-tgsm8k-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 18
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o18-tgsm8k-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 299925.23it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.81s/it]
 > number of parameters: 6738415616
[2023-09-07 22:01:15,284] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 22:01:15,487] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 22:01:15,489] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 22:01:15,489] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 22:01:15,489] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 22:01:15,489] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 22:01:15,489] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 22:01:15,489] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 22:01:15,489] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 22:01:15,489] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 22:01:15,489] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c01f0>
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 22:01:15,490] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 22:01:15,491] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 22:01:15,491] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The car-rental agency charges $30/day for a car, or $190 for the first week for a rental that lasts an entire week or longer. Jennie rented a car for 11 days. How much, in dollars, did she pay for the rental?
Output: The first 7 days were $190.
There were 11-7=<<11-7=4>>4 days left.
The additional 4 days were 4*30=<<4*30=120>>120.
And 190+120=<<190+120=310>>310.
So the final answer is 310

Input: A hurricane is approaching the southern coast of Texas, and a rancher is planning to move 400 head of cattle 60 miles to higher ground to protect them from possible inland flooding that might occur.  His animal transport truck holds 20 head of cattle.  Traveling at 60 miles per hour, what is the total driving time, in hours, it will take to transport all of his cattle to higher ground?
Output: Given the limited capacity of his transport vehicle (20 head of cattle), the 400 head of cattle will require 400/20=<<400/20=20>>20 trips using his transport vehicle.
Traveling to the site at 60 mph for 60 miles it will take 60/60=<<60/60=1>>1 hour to travel one-way.
Since each trip requires driving to and returning from the relocation site, each complete round trip will take 2*1=<<2*1=2>>2 hours.
Thus, 20 complete trips will take 20*2=<<20*2=40>>40 hours of driving time.
So the final answer is 40

Input: Jason has a carriage house that he rents out.  He’s charging $50.00 per day or $500.00 for 14 days.  Eric wants to rent the house for 20 days.  How much will it cost him?
Output: He wants to rent for 20 days and there is a deal if you rent for 14 days so that leaves 20-14 = <<20-14=6>>6 individual days
Each individual day is $50.00 and he will have 6 individual days for a total of 50*6 = $<<50*6=300.00>>300.00
14 days costs $500.00 and 6 days costs $300.00 for a total of 500+300 = $800.00
So the final answer is 800

Input: Melissa works on a poultry farm. She drives to town twice each month to buy supplies. If it takes her 3 hours to drive to town and back, how many hours does Melissa spend driving in a year?
Output: Melissa spends 2x3=<<2*3=6>>6 hours driving each month.
Since there are 12 months in a year, she spends 6x12=<<6*12=72>>72 hours driving each year.
So the final answer is 72

Input: The ratio of boys to girls in a family is 5:7. The total number of children in the family is 180. If the boys are given $3900 to share, how much money does each boy receive?
Output: The total ratio representing the number of children in the family is 5+7 = <<5+7=12>>12
From the total ratio of children in the family, 5/12 represent the number of boys, meaning that the number of boys in the family is 5/12*180 = <<5/12*180=75>>75
If the boys are given $3900 to share, each boy receives $3900/75 = $<<3900/75=52>>52
So the final answer is 52

Input: Josephine receives a bill from the hospital for 5000$.  50 percent of the bill is for medication.  25 percent of the remaining bill is for overnight stays, and 175$ is for food.  The rest of the bill is for the ambulance ride.  How much did the ambulance ride cost?
Output: Medication:5000(.50)=2500
Overnight Stays:2500(.25)=625
2500-625=<<2500-625=1875>>1875
Food:1875-175=<<1875-175=1700>>1700$
Ambulance ride:1700$
So the final answer is 1700

Input: It was time for Kelly to harvest her carrots that she had planted in three different beds.  In the first bed she pulled out 55 carrots.  In the second bed she pulled out 101 carrots and in the third bed she pulled out 78 carrots.  She found that 6 carrots weighed one pound.  How many pounds of carrots did Kelly harvest?
Output: She pulled out 55 + 101 + 78 = <<55+101+78=234>>234
6 carrots weigh one pound so 234/6 = <<234/6=39>>39 pounds of carrots
So the final answer is 39

Input: Iris’ family is planning a surprise birthday party for her. The party will include her 3 uncles and 4 aunts who have a son and daughter each as well as her brother and mother. In total, how many people are coming to Iris’ birthday party?
Output: Each of her aunts and uncles have a family unit of 1 son + 1 daughter + 1 aunt/uncle = <<1+1+1=3>>3 people.
Iris has a total of 3 uncles + 4 aunts = <<3+4=7>>7 aunts or uncles in these family units.
So among her aunts, uncles, and cousins, there will be 7 family units * 3 people in each family unit = <<7*3=21>>21 people.
Including her mother and brother, there will be a total of 21 people + 1 mother + 1 brother = <<21+1+1=23>>23 people coming to her party.
So the final answer is 23

Input: Lyra bought a pair of shoes at a 20% discount.  If she paid $480, how much was the original price of the pair of shoes?
Output: Lyra only paid $480 for the pair of shoes which is only 100% - 20% = 80% of the original price.
So let x be the original price.
Then 0.8x = $480
Thus x = $480 / 0.8 = $<<480/0.8=600>>600
So the final answer is 600

Input: Rhett has been late on two of his monthly rent payments, but his landlord does not charge late fees and so he will be able to pay their total cost with 3/5 of his next month's salary after taxes. If he is currently paid $5000 per month and has to pay 10% tax, calculate his rent expense per month?
Output: If Rhett is currently paid $5000 per month, he pays 10/100*$5000 = $<<10/100*5000=500>>500 in taxes.
Rhett has been late on two of his monthly rent payments and plans to pay them with 3/5*$4500=$<<3/5*4500=2700>>2700 from his salary after taxes.
If he is to pay $2700 for two late monthly rent payments, his monthly rent expense is $2700/2=$<<2700/2=1350>>1350
So the final answer is 1350

Input: Ten friends decide to get an end-of-year gift for their teacher. They plan to split the cost of the gift equally. But four of the group drop out. The remaining friends split the cost equally among themselves. If each share is now $8 more, how much does the gift cost, in dollars?
Output: Let N be the original price each friend was going to pay.
10N=6(N+8)
10N=6N+48
4N=48
N=<<12=12>>12
Then the present costs 10*12=<<10*12=120>>120.
So the final answer is 120

Input: At the feline sanctuary, there were 12 lions, 14 tigers, and several cougars.  If there were half as many cougars as lions and tigers combined, then what was the total number of big cats at the feline sanctuary?
Output: Half as many cougars as lions and tigers combined is (12+14)/2=13.
Then the total number of big cats at the feline sanctuary is 12+14+13=<<12+14+13=39>>39.
So the final answer is 39

Input: A pea patch is twice as big as a radish patch. If one sixth of the pea patch is 5 square feet.  How much is a whole radish patch in square feet?
Output: A whole pea patch is 5*6 = <<5*6=30>>30 square feet.
A radish patch is 30/2 = <<30/2=15>>15 square feet.
So the final answer is 15

Input: Phoebe has two pizzas to share with her and three friends. One has pepperoni and the other has cheese. They both have 16 slices. They all eat the same amount. One friend eats only pepperoni, while the rest have an equal number of slices of each. At the end, there is one slice of pepperoni left and 7 slices of cheese, how many slices does each person eat?
Output: They are 9 slices of cheese because 16 - 7 = <<16-7=9>>9
They ate three slices of cheese each because 9 / 3 = <<9/3=3>>3
They are 6 slices each in total because 3 x 2 = <<3*2=6>>6
So the final answer is 6

Input: Jeremy buys 30 watermelons. He eats 3 watermelons per week. Each week he gives 2 to his dad. How many weeks will the watermelons last?
Output: Jeremey goes through 3+2=<<3+2=5>>5 watermelons per week.
Therefore, the watermelons will last him to 30/5=<<30/5=6>>6 weeks.
So the final answer is 6

Input: Jackson has 5 times more money than Williams. Together, they have $150. How much money, in dollars, does Jackson have?
Output: Let x be the amount of money Williams has.
Jackson has 5*x dollars.
5*x+x=150
6*x=150
x=<<25=25>>25
Jackson has 25*5=<<25*5=125>>125 dollars.
So the final answer is 125

Input: Janet counts 30 crows on the powerlines and 60% more hawks than crows. How many birds does she count total?
Output: First find how many more hawks than crows there are: 60% * 30 crows = <<60*.01*30=18>>18 crows
Then add that number to the number of crows to find the total number of hawks: 18 crows + 30 crows = <<18+30=48>>48 crows
Then add the number of crows to the number of hawks to find the total number of birds: 48 crows + 30 crows = <<48+30=78>>78 crows
So the final answer is 78

Input: There are 290 liters of oil in 24 cans. If 10 of the cans are holding 8 liters each, how much oil is each of the remaining cans holding?
Output: 10 cans are holding 8 liters each for a total of 10 * 8 = <<10*8=80>>80 liters
There are 290 - 80 = <<290-80=210>>210 litres left
There are 24 - 10 =<<24-10=14>>14 cans left
Each of the remaining cans is holding 210 / 14 = <<210/14=15>>15 liters each
So the final answer is 15

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o18-tgsm8k-s30-rTrue-m4096
Evaluating commonsenseqa :   0%|          | 0/100 [00:07<?, ?it/s]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 97, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 94, in main
    inference_main(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 90, in inference_main
    query_ids, response_ids, answers, indices = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 65, in run_model
    gen_out = model.generate(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 1454, in generate
    return self.sample(
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 2568, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3671428) of binary: /home/ylu130/.conda/envs/ood/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.conda/envs/ood/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-07_22:01:25
  host      : icgpu06.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3671428)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o19-tgsm8k-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 19 16 17 18 19 20 True 4096 10
[2023-09-07 22:01:29,687] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o19-tgsm8k-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 19
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o19-tgsm8k-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 286820.47it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.08s/it]
 > number of parameters: 6738415616
[2023-09-07 22:01:39,595] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 22:01:39,806] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 22:01:39,807] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 22:01:39,807] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 22:01:39,807] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 22:01:39,807] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 22:01:39,807] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c2250>
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 22:01:39,808] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 22:01:39,809] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 22:01:39,809] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The car-rental agency charges $30/day for a car, or $190 for the first week for a rental that lasts an entire week or longer. Jennie rented a car for 11 days. How much, in dollars, did she pay for the rental?
Output: The first 7 days were $190.
There were 11-7=<<11-7=4>>4 days left.
The additional 4 days were 4*30=<<4*30=120>>120.
And 190+120=<<190+120=310>>310.
So the final answer is 310

Input: A hurricane is approaching the southern coast of Texas, and a rancher is planning to move 400 head of cattle 60 miles to higher ground to protect them from possible inland flooding that might occur.  His animal transport truck holds 20 head of cattle.  Traveling at 60 miles per hour, what is the total driving time, in hours, it will take to transport all of his cattle to higher ground?
Output: Given the limited capacity of his transport vehicle (20 head of cattle), the 400 head of cattle will require 400/20=<<400/20=20>>20 trips using his transport vehicle.
Traveling to the site at 60 mph for 60 miles it will take 60/60=<<60/60=1>>1 hour to travel one-way.
Since each trip requires driving to and returning from the relocation site, each complete round trip will take 2*1=<<2*1=2>>2 hours.
Thus, 20 complete trips will take 20*2=<<20*2=40>>40 hours of driving time.
So the final answer is 40

Input: Jason has a carriage house that he rents out.  He’s charging $50.00 per day or $500.00 for 14 days.  Eric wants to rent the house for 20 days.  How much will it cost him?
Output: He wants to rent for 20 days and there is a deal if you rent for 14 days so that leaves 20-14 = <<20-14=6>>6 individual days
Each individual day is $50.00 and he will have 6 individual days for a total of 50*6 = $<<50*6=300.00>>300.00
14 days costs $500.00 and 6 days costs $300.00 for a total of 500+300 = $800.00
So the final answer is 800

Input: Melissa works on a poultry farm. She drives to town twice each month to buy supplies. If it takes her 3 hours to drive to town and back, how many hours does Melissa spend driving in a year?
Output: Melissa spends 2x3=<<2*3=6>>6 hours driving each month.
Since there are 12 months in a year, she spends 6x12=<<6*12=72>>72 hours driving each year.
So the final answer is 72

Input: The ratio of boys to girls in a family is 5:7. The total number of children in the family is 180. If the boys are given $3900 to share, how much money does each boy receive?
Output: The total ratio representing the number of children in the family is 5+7 = <<5+7=12>>12
From the total ratio of children in the family, 5/12 represent the number of boys, meaning that the number of boys in the family is 5/12*180 = <<5/12*180=75>>75
If the boys are given $3900 to share, each boy receives $3900/75 = $<<3900/75=52>>52
So the final answer is 52

Input: Josephine receives a bill from the hospital for 5000$.  50 percent of the bill is for medication.  25 percent of the remaining bill is for overnight stays, and 175$ is for food.  The rest of the bill is for the ambulance ride.  How much did the ambulance ride cost?
Output: Medication:5000(.50)=2500
Overnight Stays:2500(.25)=625
2500-625=<<2500-625=1875>>1875
Food:1875-175=<<1875-175=1700>>1700$
Ambulance ride:1700$
So the final answer is 1700

Input: It was time for Kelly to harvest her carrots that she had planted in three different beds.  In the first bed she pulled out 55 carrots.  In the second bed she pulled out 101 carrots and in the third bed she pulled out 78 carrots.  She found that 6 carrots weighed one pound.  How many pounds of carrots did Kelly harvest?
Output: She pulled out 55 + 101 + 78 = <<55+101+78=234>>234
6 carrots weigh one pound so 234/6 = <<234/6=39>>39 pounds of carrots
So the final answer is 39

Input: Iris’ family is planning a surprise birthday party for her. The party will include her 3 uncles and 4 aunts who have a son and daughter each as well as her brother and mother. In total, how many people are coming to Iris’ birthday party?
Output: Each of her aunts and uncles have a family unit of 1 son + 1 daughter + 1 aunt/uncle = <<1+1+1=3>>3 people.
Iris has a total of 3 uncles + 4 aunts = <<3+4=7>>7 aunts or uncles in these family units.
So among her aunts, uncles, and cousins, there will be 7 family units * 3 people in each family unit = <<7*3=21>>21 people.
Including her mother and brother, there will be a total of 21 people + 1 mother + 1 brother = <<21+1+1=23>>23 people coming to her party.
So the final answer is 23

Input: Lyra bought a pair of shoes at a 20% discount.  If she paid $480, how much was the original price of the pair of shoes?
Output: Lyra only paid $480 for the pair of shoes which is only 100% - 20% = 80% of the original price.
So let x be the original price.
Then 0.8x = $480
Thus x = $480 / 0.8 = $<<480/0.8=600>>600
So the final answer is 600

Input: Rhett has been late on two of his monthly rent payments, but his landlord does not charge late fees and so he will be able to pay their total cost with 3/5 of his next month's salary after taxes. If he is currently paid $5000 per month and has to pay 10% tax, calculate his rent expense per month?
Output: If Rhett is currently paid $5000 per month, he pays 10/100*$5000 = $<<10/100*5000=500>>500 in taxes.
Rhett has been late on two of his monthly rent payments and plans to pay them with 3/5*$4500=$<<3/5*4500=2700>>2700 from his salary after taxes.
If he is to pay $2700 for two late monthly rent payments, his monthly rent expense is $2700/2=$<<2700/2=1350>>1350
So the final answer is 1350

Input: Ten friends decide to get an end-of-year gift for their teacher. They plan to split the cost of the gift equally. But four of the group drop out. The remaining friends split the cost equally among themselves. If each share is now $8 more, how much does the gift cost, in dollars?
Output: Let N be the original price each friend was going to pay.
10N=6(N+8)
10N=6N+48
4N=48
N=<<12=12>>12
Then the present costs 10*12=<<10*12=120>>120.
So the final answer is 120

Input: At the feline sanctuary, there were 12 lions, 14 tigers, and several cougars.  If there were half as many cougars as lions and tigers combined, then what was the total number of big cats at the feline sanctuary?
Output: Half as many cougars as lions and tigers combined is (12+14)/2=13.
Then the total number of big cats at the feline sanctuary is 12+14+13=<<12+14+13=39>>39.
So the final answer is 39

Input: A pea patch is twice as big as a radish patch. If one sixth of the pea patch is 5 square feet.  How much is a whole radish patch in square feet?
Output: A whole pea patch is 5*6 = <<5*6=30>>30 square feet.
A radish patch is 30/2 = <<30/2=15>>15 square feet.
So the final answer is 15

Input: Phoebe has two pizzas to share with her and three friends. One has pepperoni and the other has cheese. They both have 16 slices. They all eat the same amount. One friend eats only pepperoni, while the rest have an equal number of slices of each. At the end, there is one slice of pepperoni left and 7 slices of cheese, how many slices does each person eat?
Output: They are 9 slices of cheese because 16 - 7 = <<16-7=9>>9
They ate three slices of cheese each because 9 / 3 = <<9/3=3>>3
They are 6 slices each in total because 3 x 2 = <<3*2=6>>6
So the final answer is 6

Input: Jeremy buys 30 watermelons. He eats 3 watermelons per week. Each week he gives 2 to his dad. How many weeks will the watermelons last?
Output: Jeremey goes through 3+2=<<3+2=5>>5 watermelons per week.
Therefore, the watermelons will last him to 30/5=<<30/5=6>>6 weeks.
So the final answer is 6

Input: Jackson has 5 times more money than Williams. Together, they have $150. How much money, in dollars, does Jackson have?
Output: Let x be the amount of money Williams has.
Jackson has 5*x dollars.
5*x+x=150
6*x=150
x=<<25=25>>25
Jackson has 25*5=<<25*5=125>>125 dollars.
So the final answer is 125

Input: Janet counts 30 crows on the powerlines and 60% more hawks than crows. How many birds does she count total?
Output: First find how many more hawks than crows there are: 60% * 30 crows = <<60*.01*30=18>>18 crows
Then add that number to the number of crows to find the total number of hawks: 18 crows + 30 crows = <<18+30=48>>48 crows
Then add the number of crows to the number of hawks to find the total number of birds: 48 crows + 30 crows = <<48+30=78>>78 crows
So the final answer is 78

Input: There are 290 liters of oil in 24 cans. If 10 of the cans are holding 8 liters each, how much oil is each of the remaining cans holding?
Output: 10 cans are holding 8 liters each for a total of 10 * 8 = <<10*8=80>>80 liters
There are 290 - 80 = <<290-80=210>>210 litres left
There are 24 - 10 =<<24-10=14>>14 cans left
Each of the remaining cans is holding 210 / 14 = <<210/14=15>>15 liters each
So the final answer is 15

Input: Bill had to finish a project from work that was to take him 4 days. If he took 6 seven-hour naps in the four days, how long did he spend working on the project?
Output: Since a day has 24 hours, the project was to take 24*4 = <<24*4=96>>96 hours.
Bill took 6 seven-hour naps during the four days, a total of 6*7 = <<6*7=42>>42 hours
The time he spent working on the project is 96-42 = <<96-42=54>>54 hours.
So the final answer is 54

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o19-tgsm8k-s30-rTrue-m4096
Evaluating commonsenseqa :   0%|          | 0/100 [00:07<?, ?it/s]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 97, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 94, in main
    inference_main(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 90, in inference_main
    query_ids, response_ids, answers, indices = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 65, in run_model
    gen_out = model.generate(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 1454, in generate
    return self.sample(
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 2568, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3671612) of binary: /home/ylu130/.conda/envs/ood/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.conda/envs/ood/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.conda/envs/ood/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-07_22:01:52
  host      : icgpu06.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3671612)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu06 --master_port 19658 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o20-tgsm8k-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 20 16 17 18 19 20 True 4096 10
[2023-09-07 22:01:56,597] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o20-tgsm8k-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 20
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o20-tgsm8k-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 274307.40it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.30s/it]
 > number of parameters: 6738415616
[2023-09-07 22:02:04,797] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-07 22:02:04,998] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-07 22:02:04,999] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0280>
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-07 22:02:05,000] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-07 22:02:05,001] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-07 22:02:05,001] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The car-rental agency charges $30/day for a car, or $190 for the first week for a rental that lasts an entire week or longer. Jennie rented a car for 11 days. How much, in dollars, did she pay for the rental?
Output: The first 7 days were $190.
There were 11-7=<<11-7=4>>4 days left.
The additional 4 days were 4*30=<<4*30=120>>120.
And 190+120=<<190+120=310>>310.
So the final answer is 310

Input: A hurricane is approaching the southern coast of Texas, and a rancher is planning to move 400 head of cattle 60 miles to higher ground to protect them from possible inland flooding that might occur.  His animal transport truck holds 20 head of cattle.  Traveling at 60 miles per hour, what is the total driving time, in hours, it will take to transport all of his cattle to higher ground?
Output: Given the limited capacity of his transport vehicle (20 head of cattle), the 400 head of cattle will require 400/20=<<400/20=20>>20 trips using his transport vehicle.
Traveling to the site at 60 mph for 60 miles it will take 60/60=<<60/60=1>>1 hour to travel one-way.
Since each trip requires driving to and returning from the relocation site, each complete round trip will take 2*1=<<2*1=2>>2 hours.
Thus, 20 complete trips will take 20*2=<<20*2=40>>40 hours of driving time.
So the final answer is 40

Input: Jason has a carriage house that he rents out.  He’s charging $50.00 per day or $500.00 for 14 days.  Eric wants to rent the house for 20 days.  How much will it cost him?
Output: He wants to rent for 20 days and there is a deal if you rent for 14 days so that leaves 20-14 = <<20-14=6>>6 individual days
Each individual day is $50.00 and he will have 6 individual days for a total of 50*6 = $<<50*6=300.00>>300.00
14 days costs $500.00 and 6 days costs $300.00 for a total of 500+300 = $800.00
So the final answer is 800

Input: Melissa works on a poultry farm. She drives to town twice each month to buy supplies. If it takes her 3 hours to drive to town and back, how many hours does Melissa spend driving in a year?
Output: Melissa spends 2x3=<<2*3=6>>6 hours driving each month.
Since there are 12 months in a year, she spends 6x12=<<6*12=72>>72 hours driving each year.
So the final answer is 72

Input: The ratio of boys to girls in a family is 5:7. The total number of children in the family is 180. If the boys are given $3900 to share, how much money does each boy receive?
Output: The total ratio representing the number of children in the family is 5+7 = <<5+7=12>>12
From the total ratio of children in the family, 5/12 represent the number of boys, meaning that the number of boys in the family is 5/12*180 = <<5/12*180=75>>75
If the boys are given $3900 to share, each boy receives $3900/75 = $<<3900/75=52>>52
So the final answer is 52

Input: Josephine receives a bill from the hospital for 5000$.  50 percent of the bill is for medication.  25 percent of the remaining bill is for overnight stays, and 175$ is for food.  The rest of the bill is for the ambulance ride.  How much did the ambulance ride cost?
Output: Medication:5000(.50)=2500
Overnight Stays:2500(.25)=625
2500-625=<<2500-625=1875>>1875
Food:1875-175=<<1875-175=1700>>1700$
Ambulance ride:1700$
So the final answer is 1700

Input: It was time for Kelly to harvest her carrots that she had planted in three different beds.  In the first bed she pulled out 55 carrots.  In the second bed she pulled out 101 carrots and in the third bed she pulled out 78 carrots.  She found that 6 carrots weighed one pound.  How many pounds of carrots did Kelly harvest?
Output: She pulled out 55 + 101 + 78 = <<55+101+78=234>>234
6 carrots weigh one pound so 234/6 = <<234/6=39>>39 pounds of carrots
So the final answer is 39

Input: Iris’ family is planning a surprise birthday party for her. The party will include her 3 uncles and 4 aunts who have a son and daughter each as well as her brother and mother. In total, how many people are coming to Iris’ birthday party?
Output: Each of her aunts and uncles have a family unit of 1 son + 1 daughter + 1 aunt/uncle = <<1+1+1=3>>3 people.
Iris has a total of 3 uncles + 4 aunts = <<3+4=7>>7 aunts or uncles in these family units.
So among her aunts, uncles, and cousins, there will be 7 family units * 3 people in each family unit = <<7*3=21>>21 people.
Including her mother and brother, there will be a total of 21 people + 1 mother + 1 brother = <<21+1+1=23>>23 people coming to her party.
So the final answer is 23

Input: Lyra bought a pair of shoes at a 20% discount.  If she paid $480, how much was the original price of the pair of shoes?
Output: Lyra only paid $480 for the pair of shoes which is only 100% - 20% = 80% of the original price.
So let x be the original price.
Then 0.8x = $480
Thus x = $480 / 0.8 = $<<480/0.8=600>>600
So the final answer is 600

Input: Rhett has been late on two of his monthly rent payments, but his landlord does not charge late fees and so he will be able to pay their total cost with 3/5 of his next month's salary after taxes. If he is currently paid $5000 per month and has to pay 10% tax, calculate his rent expense per month?
Output: If Rhett is currently paid $5000 per month, he pays 10/100*$5000 = $<<10/100*5000=500>>500 in taxes.
Rhett has been late on two of his monthly rent payments and plans to pay them with 3/5*$4500=$<<3/5*4500=2700>>2700 from his salary after taxes.
If he is to pay $2700 for two late monthly rent payments, his monthly rent expense is $2700/2=$<<2700/2=1350>>1350
So the final answer is 1350

Input: Ten friends decide to get an end-of-year gift for their teacher. They plan to split the cost of the gift equally. But four of the group drop out. The remaining friends split the cost equally among themselves. If each share is now $8 more, how much does the gift cost, in dollars?
Output: Let N be the original price each friend was going to pay.
10N=6(N+8)
10N=6N+48
4N=48
N=<<12=12>>12
Then the present costs 10*12=<<10*12=120>>120.
So the final answer is 120

Input: At the feline sanctuary, there were 12 lions, 14 tigers, and several cougars.  If there were half as many cougars as lions and tigers combined, then what was the total number of big cats at the feline sanctuary?
Output: Half as many cougars as lions and tigers combined is (12+14)/2=13.
Then the total number of big cats at the feline sanctuary is 12+14+13=<<12+14+13=39>>39.
So the final answer is 39

Input: A pea patch is twice as big as a radish patch. If one sixth of the pea patch is 5 square feet.  How much is a whole radish patch in square feet?
Output: A whole pea patch is 5*6 = <<5*6=30>>30 square feet.
A radish patch is 30/2 = <<30/2=15>>15 square feet.
So the final answer is 15

Input: Phoebe has two pizzas to share with her and three friends. One has pepperoni and the other has cheese. They both have 16 slices. They all eat the same amount. One friend eats only pepperoni, while the rest have an equal number of slices of each. At the end, there is one slice of pepperoni left and 7 slices of cheese, how many slices does each person eat?
Output: They are 9 slices of cheese because 16 - 7 = <<16-7=9>>9
They ate three slices of cheese each because 9 / 3 = <<9/3=3>>3
They are 6 slices each in total because 3 x 2 = <<3*2=6>>6
So the final answer is 6

Input: Jeremy buys 30 watermelons. He eats 3 watermelons per week. Each week he gives 2 to his dad. How many weeks will the watermelons last?
Output: Jeremey goes through 3+2=<<3+2=5>>5 watermelons per week.
Therefore, the watermelons will last him to 30/5=<<30/5=6>>6 weeks.
So the final answer is 6

Input: Jackson has 5 times more money than Williams. Together, they have $150. How much money, in dollars, does Jackson have?
Output: Let x be the amount of money Williams has.
Jackson has 5*x dollars.
5*x+x=150
6*x=150
x=<<25=25>>25
Jackson has 25*5=<<25*5=125>>125 dollars.
So the final answer is 125

Input: Janet counts 30 crows on the powerlines and 60% more hawks than crows. How many birds does she count total?
Output: First find how many more hawks than crows there are: 60% * 30 crows = <<60*.01*30=18>>18 crows
Then add that number to the number of crows to find the total number of hawks: 18 crows + 30 crows = <<18+30=48>>48 crows
Then add the number of crows to the number of hawks to find the total number of birds: 48 crows + 30 crows = <<48+30=78>>78 crows
So the final answer is 78

Input: There are 290 liters of oil in 24 cans. If 10 of the cans are holding 8 liters each, how much oil is each of the remaining cans holding?
Output: 10 cans are holding 8 liters each for a total of 10 * 8 = <<10*8=80>>80 liters
There are 290 - 80 = <<290-80=210>>210 litres left
There are 24 - 10 =<<24-10=14>>14 cans left
Each of the remaining cans is holding 210 / 14 = <<210/14=15>>15 liters each
So the final answer is 15

Input: Bill had to finish a project from work that was to take him 4 days. If he took 6 seven-hour naps in the four days, how long did he spend working on the project?
Output: Since a day has 24 hours, the project was to take 24*4 = <<24*4=96>>96 hours.
Bill took 6 seven-hour naps during the four days, a total of 6*7 = <<6*7=42>>42 hours
The time he spent working on the project is 96-42 = <<96-42=54>>54 hours.
So the final answer is 54

Input: There are 120 cards in a box. If 2/5 of the cards are red, exactly 5/9 of the remainder are black, and the rest are green, calculate the number of green cards in the box?
Output: The number of red cards in the box, 2/5 of the total number, is 2/5*120 = <<2/5*120=48>>48.
The number of cards that are not red is 120-48 = <<120-48=72>>72.
If 5/9 of the remaining cards are black, then there are 5/9*72 = <<5/9*72=40>>40 black cards in the box.
If the remaining cards are green, then there are 72-40 = <<72-40=32>>32 green cards in the box.
So the final answer is 32

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o20-tgsm8k-s30-rTrue-m4096
Evaluating commonsenseqa :   1%|          | 1/100 [01:30<2:30:08, 90.99s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [03:01<2:27:45, 90.46s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [04:30<2:25:17, 89.87s/it]      Evaluating commonsenseqa :   4%|▍         | 4/100 [05:59<2:23:40, 89.80s/it]      Evaluating commonsenseqa :   5%|▌         | 5/100 [07:29<2:21:46, 89.54s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [08:58<2:19:59, 89.36s/it]      Evaluating commonsenseqa :   7%|▋         | 7/100 [10:27<2:18:36, 89.42s/it]      Evaluating commonsenseqa :   8%|▊         | 8/100 [11:57<2:17:08, 89.44s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [13:26<2:15:28, 89.33s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [14:55<2:13:56, 89.29s/it]       Evaluating commonsenseqa :  11%|█         | 11/100 [16:25<2:12:37, 89.41s/it]       Evaluating commonsenseqa :  12%|█▏        | 12/100 [17:54<2:11:10, 89.44s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [19:23<2:09:39, 89.42s/it]     Evaluating commonsenseqa :  14%|█▍        | 14/100 [20:52<2:07:53, 89.23s/it]       Evaluating commonsenseqa :  15%|█▌        | 15/100 [22:22<2:06:26, 89.25s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [23:50<2:04:46, 89.12s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [25:19<2:03:08, 89.02s/it]       Evaluating commonsenseqa :  18%|█▊        | 18/100 [26:48<2:01:38, 89.01s/it]       Evaluating commonsenseqa :  19%|█▉        | 19/100 [28:17<1:59:58, 88.87s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [29:46<1:58:38, 88.98s/it]       Evaluating commonsenseqa :  21%|██        | 21/100 [31:15<1:57:11, 89.01s/it]       Evaluating commonsenseqa :  22%|██▏       | 22/100 [32:45<1:55:56, 89.19s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [34:14<1:54:26, 89.18s/it]     Evaluating commonsenseqa :  24%|██▍       | 24/100 [35:43<1:53:08, 89.33s/it]     Evaluating commonsenseqa :  25%|██▌       | 25/100 [37:12<1:51:19, 89.06s/it]     Evaluating commonsenseqa :  26%|██▌       | 26/100 [38:41<1:49:48, 89.03s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [40:10<1:48:23, 89.09s/it]     Evaluating commonsenseqa :  28%|██▊       | 28/100 [41:39<1:47:01, 89.19s/it]       Evaluating commonsenseqa :  29%|██▉       | 29/100 [43:08<1:45:25, 89.09s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [44:37<1:43:55, 89.09s/it]       Evaluating commonsenseqa :  31%|███       | 31/100 [46:06<1:42:21, 89.01s/it]       Evaluating commonsenseqa :  32%|███▏      | 32/100 [47:35<1:40:51, 88.99s/it]     Evaluating commonsenseqa :  33%|███▎      | 33/100 [49:04<1:39:26, 89.05s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [50:34<1:38:01, 89.11s/it]     Evaluating commonsenseqa :  35%|███▌      | 35/100 [52:02<1:36:17, 88.88s/it]     Evaluating commonsenseqa :  36%|███▌      | 36/100 [53:30<1:34:40, 88.76s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [54:59<1:33:06, 88.68s/it]     Evaluating commonsenseqa :  38%|███▊      | 38/100 [56:28<1:31:38, 88.69s/it]     Evaluating commonsenseqa :  39%|███▉      | 39/100 [57:56<1:30:10, 88.69s/it]     Evaluating commonsenseqa :  40%|████      | 40/100 [59:25<1:28:47, 88.79s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [1:00:54<1:27:09, 88.64s/it]   Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:02:22<1:25:31, 88.48s/it]   Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:03:50<1:24:06, 88.54s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:05:19<1:22:43, 88.64s/it]   Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:06:48<1:21:11, 88.57s/it]   Evaluating commonsenseqa :  46%|████▌     | 46/100 [1:08:16<1:19:44, 88.61s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:09:45<1:18:17, 88.64s/it]   Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:11:14<1:16:53, 88.71s/it]   Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:12:43<1:15:30, 88.83s/it]   Evaluating commonsenseqa :  50%|█████     | 50/100 [1:14:12<1:14:07, 88.95s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [1:15:40<1:12:26, 88.71s/it]   Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:17:09<1:10:56, 88.68s/it] Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:18:37<1:09:21, 88.53s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:20:06<1:08:00, 88.70s/it] Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:21:36<1:06:40, 88.90s/it] Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:23:05<1:05:10, 88.87s/it]   Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:24:34<1:03:46, 88.99s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:26:02<1:02:10, 88.82s/it]   Evaluating commonsenseqa :  59%|█████▉    | 59/100 [1:27:31<1:00:43, 88.86s/it]   Evaluating commonsenseqa :  60%|██████    | 60/100 [1:29:01<59:25, 89.14s/it]  Evaluating commonsenseqa :  61%|██████    | 61/100 [1:30:30<58:00, 89.23s/it]     Evaluating commonsenseqa :  62%|██████▏   | 62/100 [1:31:59<56:26, 89.12s/it]   Evaluating commonsenseqa :  63%|██████▎   | 63/100 [1:33:28<54:52, 88.99s/it]   Evaluating commonsenseqa :  64%|██████▍   | 64/100 [1:34:57<53:27, 89.09s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [1:36:26<51:52, 88.92s/it]   Evaluating commonsenseqa :  66%|██████▌   | 66/100 [1:37:55<50:24, 88.96s/it]   Evaluating commonsenseqa :  67%|██████▋   | 67/100 [1:39:23<48:48, 88.74s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [1:40:52<47:21, 88.79s/it]   Evaluating commonsenseqa :  69%|██████▉   | 69/100 [1:42:21<45:54, 88.85s/it]   Evaluating commonsenseqa :  70%|███████   | 70/100 [1:43:50<44:28, 88.95s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [1:45:19<42:59, 88.94s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [1:46:48<41:33, 89.04s/it] Evaluating commonsenseqa :  73%|███████▎  | 73/100 [1:48:17<40:01, 88.93s/it] Evaluating commonsenseqa :  74%|███████▍  | 74/100 [1:49:46<38:29, 88.82s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [1:51:14<37:00, 88.83s/it] Evaluating commonsenseqa :  76%|███████▌  | 76/100 [1:52:43<35:33, 88.89s/it] Evaluating commonsenseqa :  77%|███████▋  | 77/100 [1:54:12<34:03, 88.83s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [1:55:42<32:37, 88.99s/it] Evaluating commonsenseqa :  79%|███████▉  | 79/100 [1:57:10<31:08, 88.96s/it] Evaluating commonsenseqa :  80%|████████  | 80/100 [1:58:39<29:35, 88.77s/it] Evaluating commonsenseqa :  81%|████████  | 81/100 [2:00:08<28:06, 88.78s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [2:01:36<26:36, 88.72s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [2:03:05<25:09, 88.82s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [2:04:35<23:44, 89.05s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [2:06:04<22:15, 89.01s/it] Evaluating commonsenseqa :  86%|████████▌ | 86/100 [2:07:33<20:46, 89.05s/it] Evaluating commonsenseqa :  87%|████████▋ | 87/100 [2:09:02<19:17, 89.02s/it] Evaluating commonsenseqa :  88%|████████▊ | 88/100 [2:10:31<17:49, 89.08s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [2:12:00<16:20, 89.10s/it] Evaluating commonsenseqa :  90%|█████████ | 90/100 [2:13:30<14:52, 89.25s/it] Evaluating commonsenseqa :  91%|█████████ | 91/100 [2:14:59<13:22, 89.12s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [2:16:27<11:51, 88.93s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [2:17:56<10:22, 88.91s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [2:19:24<08:52, 88.78s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [2:20:54<07:24, 88.91s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [2:22:22<05:55, 88.86s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [2:23:52<04:26, 88.98s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [2:25:20<02:57, 88.91s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [2:26:49<01:28, 88.90s/it] Evaluating commonsenseqa : 100%|██████████| 100/100 [2:28:18<00:00, 88.85s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:28:18<00:00, 88.98s/it]
name: commonsenseqa | avg. gen lenth: 387.886 | time: 8901.569969177246s
Evaluating commonsenseqa :  94%|█████████▍| 94/100 [3:16:06<12:27, 124.61s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [3:18:13<10:27, 125.41s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [3:20:20<08:23, 125.79s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [3:22:28<06:19, 126.39s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [3:24:31<04:11, 125.52s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [3:26:35<02:05, 125.16s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [3:28:40<00:00, 124.98s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [3:28:40<00:00, 125.21s/it]
name: commonsenseqa | avg. gen lenth: 389.201 | time: 12523.174822568893s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu02 --master_port 19674 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tgsm8k-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 13 11 12 13 14 15 True 4096 10
[2023-09-08 00:44:57,406] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tgsm8k-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 13
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tgsm8k-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 365271.52it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  6.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.57s/it]
 > number of parameters: 6738415616
[2023-09-08 00:45:14,397] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-08 00:45:14,604] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-08 00:45:14,606] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-08 00:45:14,606] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0250>
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-08 00:45:14,607] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-08 00:45:14,608] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-08 00:45:14,609] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-08 00:45:14,609] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The car-rental agency charges $30/day for a car, or $190 for the first week for a rental that lasts an entire week or longer. Jennie rented a car for 11 days. How much, in dollars, did she pay for the rental?
Output: The first 7 days were $190.
There were 11-7=<<11-7=4>>4 days left.
The additional 4 days were 4*30=<<4*30=120>>120.
And 190+120=<<190+120=310>>310.
So the final answer is 310

Input: A hurricane is approaching the southern coast of Texas, and a rancher is planning to move 400 head of cattle 60 miles to higher ground to protect them from possible inland flooding that might occur.  His animal transport truck holds 20 head of cattle.  Traveling at 60 miles per hour, what is the total driving time, in hours, it will take to transport all of his cattle to higher ground?
Output: Given the limited capacity of his transport vehicle (20 head of cattle), the 400 head of cattle will require 400/20=<<400/20=20>>20 trips using his transport vehicle.
Traveling to the site at 60 mph for 60 miles it will take 60/60=<<60/60=1>>1 hour to travel one-way.
Since each trip requires driving to and returning from the relocation site, each complete round trip will take 2*1=<<2*1=2>>2 hours.
Thus, 20 complete trips will take 20*2=<<20*2=40>>40 hours of driving time.
So the final answer is 40

Input: Jason has a carriage house that he rents out.  He’s charging $50.00 per day or $500.00 for 14 days.  Eric wants to rent the house for 20 days.  How much will it cost him?
Output: He wants to rent for 20 days and there is a deal if you rent for 14 days so that leaves 20-14 = <<20-14=6>>6 individual days
Each individual day is $50.00 and he will have 6 individual days for a total of 50*6 = $<<50*6=300.00>>300.00
14 days costs $500.00 and 6 days costs $300.00 for a total of 500+300 = $800.00
So the final answer is 800

Input: Melissa works on a poultry farm. She drives to town twice each month to buy supplies. If it takes her 3 hours to drive to town and back, how many hours does Melissa spend driving in a year?
Output: Melissa spends 2x3=<<2*3=6>>6 hours driving each month.
Since there are 12 months in a year, she spends 6x12=<<6*12=72>>72 hours driving each year.
So the final answer is 72

Input: The ratio of boys to girls in a family is 5:7. The total number of children in the family is 180. If the boys are given $3900 to share, how much money does each boy receive?
Output: The total ratio representing the number of children in the family is 5+7 = <<5+7=12>>12
From the total ratio of children in the family, 5/12 represent the number of boys, meaning that the number of boys in the family is 5/12*180 = <<5/12*180=75>>75
If the boys are given $3900 to share, each boy receives $3900/75 = $<<3900/75=52>>52
So the final answer is 52

Input: Josephine receives a bill from the hospital for 5000$.  50 percent of the bill is for medication.  25 percent of the remaining bill is for overnight stays, and 175$ is for food.  The rest of the bill is for the ambulance ride.  How much did the ambulance ride cost?
Output: Medication:5000(.50)=2500
Overnight Stays:2500(.25)=625
2500-625=<<2500-625=1875>>1875
Food:1875-175=<<1875-175=1700>>1700$
Ambulance ride:1700$
So the final answer is 1700

Input: It was time for Kelly to harvest her carrots that she had planted in three different beds.  In the first bed she pulled out 55 carrots.  In the second bed she pulled out 101 carrots and in the third bed she pulled out 78 carrots.  She found that 6 carrots weighed one pound.  How many pounds of carrots did Kelly harvest?
Output: She pulled out 55 + 101 + 78 = <<55+101+78=234>>234
6 carrots weigh one pound so 234/6 = <<234/6=39>>39 pounds of carrots
So the final answer is 39

Input: Iris’ family is planning a surprise birthday party for her. The party will include her 3 uncles and 4 aunts who have a son and daughter each as well as her brother and mother. In total, how many people are coming to Iris’ birthday party?
Output: Each of her aunts and uncles have a family unit of 1 son + 1 daughter + 1 aunt/uncle = <<1+1+1=3>>3 people.
Iris has a total of 3 uncles + 4 aunts = <<3+4=7>>7 aunts or uncles in these family units.
So among her aunts, uncles, and cousins, there will be 7 family units * 3 people in each family unit = <<7*3=21>>21 people.
Including her mother and brother, there will be a total of 21 people + 1 mother + 1 brother = <<21+1+1=23>>23 people coming to her party.
So the final answer is 23

Input: Lyra bought a pair of shoes at a 20% discount.  If she paid $480, how much was the original price of the pair of shoes?
Output: Lyra only paid $480 for the pair of shoes which is only 100% - 20% = 80% of the original price.
So let x be the original price.
Then 0.8x = $480
Thus x = $480 / 0.8 = $<<480/0.8=600>>600
So the final answer is 600

Input: Rhett has been late on two of his monthly rent payments, but his landlord does not charge late fees and so he will be able to pay their total cost with 3/5 of his next month's salary after taxes. If he is currently paid $5000 per month and has to pay 10% tax, calculate his rent expense per month?
Output: If Rhett is currently paid $5000 per month, he pays 10/100*$5000 = $<<10/100*5000=500>>500 in taxes.
Rhett has been late on two of his monthly rent payments and plans to pay them with 3/5*$4500=$<<3/5*4500=2700>>2700 from his salary after taxes.
If he is to pay $2700 for two late monthly rent payments, his monthly rent expense is $2700/2=$<<2700/2=1350>>1350
So the final answer is 1350

Input: Ten friends decide to get an end-of-year gift for their teacher. They plan to split the cost of the gift equally. But four of the group drop out. The remaining friends split the cost equally among themselves. If each share is now $8 more, how much does the gift cost, in dollars?
Output: Let N be the original price each friend was going to pay.
10N=6(N+8)
10N=6N+48
4N=48
N=<<12=12>>12
Then the present costs 10*12=<<10*12=120>>120.
So the final answer is 120

Input: At the feline sanctuary, there were 12 lions, 14 tigers, and several cougars.  If there were half as many cougars as lions and tigers combined, then what was the total number of big cats at the feline sanctuary?
Output: Half as many cougars as lions and tigers combined is (12+14)/2=13.
Then the total number of big cats at the feline sanctuary is 12+14+13=<<12+14+13=39>>39.
So the final answer is 39

Input: A pea patch is twice as big as a radish patch. If one sixth of the pea patch is 5 square feet.  How much is a whole radish patch in square feet?
Output: A whole pea patch is 5*6 = <<5*6=30>>30 square feet.
A radish patch is 30/2 = <<30/2=15>>15 square feet.
So the final answer is 15

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tgsm8k-s30-rTrue-m4096
Evaluating commonsenseqa :   1%|          | 1/100 [02:01<3:20:56, 121.79s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [04:01<3:16:38, 120.39s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [06:00<3:13:27, 119.67s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [07:59<3:11:15, 119.54s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [09:59<3:09:42, 119.81s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [11:59<3:07:30, 119.69s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [13:58<3:05:17, 119.55s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [15:58<3:03:28, 119.66s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [17:58<3:01:58, 119.98s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [19:59<3:00:16, 120.18s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [22:00<2:58:47, 120.53s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [24:01<2:56:59, 120.68s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [26:02<2:55:06, 120.77s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [28:02<2:52:38, 120.45s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [30:01<2:50:10, 120.12s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [32:03<2:48:40, 120.48s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [34:01<2:45:49, 119.87s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [36:00<2:43:24, 119.56s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [38:00<2:41:35, 119.69s/it]