torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i0-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 0
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:26:57,991] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:26:57,996] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:26:58,010] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:26:58,025] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:04,347] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:04,363] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:04,369] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:27:04,919] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:11,188] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:11,314] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:11,314] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:27:11,942] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:18,248] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:18,743] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:18,756] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:18,793] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:25,539] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:25,557] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:26,053] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:26,054] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:32,406] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:32,407] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:32,912] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:32,913] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:39,331] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:39,343] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:27:39,806] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:39,932] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:46,257] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:46,281] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:27:46,799] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:46,835] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:53,168] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:53,169] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:53,182] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:27:53,692] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:00,111] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:28:00,629] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:00,630] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:00,632] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:07,052] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:07,053] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:07,053] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:28:07,581] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:13,988] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:14,506] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:14,507] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:14,514] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:20,914] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:20,952] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:21,415] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:21,450] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:27,858] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:27,867] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:28,371] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:28,377] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:34,779] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:35,316] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:35,318] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:35,342] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:41,708] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:41,708] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:42,228] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:42,228] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:48,522] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:48,652] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:28:49,082] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:49,175] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:55,482] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:55,607] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:55,607] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:55,607] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:02,535] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:29:03,044] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:03,066] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:03,067] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:09,406] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:09,516] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:09,516] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:29:10,038] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:16,475] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:16,476] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:29:16,978] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:16,998] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:23,382] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:23,382] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:23,401] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:23,953] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:30,226] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:30,335] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:30,335] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:29:30,848] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:37,271] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:37,802] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:37,804] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:37,810] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:44,214] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:44,231] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:44,710] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:44,723] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:51,173] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:51,173] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:51,678] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:51,695] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:58,136] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:58,145] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:29:58,657] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:58,658] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:05,042] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:05,064] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:05,083] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:05,681] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:11,969] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:12,485] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:12,485] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:12,494] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:18,897] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:18,918] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:30:19,415] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:19,425] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:25,869] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:25,869] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:25,881] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:26,388] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:32,807] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:32,808] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:33,251] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:33,334] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:39,738] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:39,738] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:39,739] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:30:40,263] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...Answers already exist, exiting...

Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:46,689] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:46,705] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:30:47,212] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:47,224] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:53,540] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:30:54,186] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:54,186] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:54,186] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:31:00,581] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:31:00,582] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:31:00,582] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:31:01,112] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s10-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 8
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s10-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9733 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████████████████████████████████████████| 9733/9733 [00:00<00:00, 864384.70it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.22s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.33s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.78s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.77s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.14s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.61s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.20s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.67s/it]
[2023-08-30 00:31:11,486] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 00:31:11,597] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.35s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.86s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.36s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.87s/it]
[2023-08-30 00:31:12,005] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-30 00:31:12,018] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 00:31:12,580] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 00:31:12,581] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 00:31:12,581] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 00:31:12,581] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 00:31:12,581] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 00:31:12,581] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd3fbec52d0>
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 00:31:12,583] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: B: tailor shop

Input: Where are you if your reading magazines while waiting for a vehicle on rails? Choices:  A: vegetables B: market C: doctor D: train station E: bookstore
Output: D: train station

Input: What would need oil to be used? Choices:  A: ground B: human  body C: repair shop D: combustion engines E: service station
Output: D: combustion engines

Input: What is person probably feeling that plans on stopping being married to their spouse? Choices:  A: detachment B: bankruptcy C: sad D: fights E: wrong
Output: A: detachment

Input: What could you use to store a clock? Choices:  A: shelf B: own bedroom C: desk D: wall E: car
Output: A: shelf

Input: The person put on lotion, what did they want? Choices:  A: fresh smell B: good credit C: smooth skin D: fresh produce E: headache
Output: C: smooth skin

Input: They burned the record, they were trying to do what to history? Choices:  A: compact disc B: tape C: rewrite D: play music E: erase
Output: E: erase

Input: She sure didn't have a green thumb, every time she thought she was making grow something it would what? Choices:  A: growth B: flowering C: ground D: die E: plants
Output: D: die

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s10-rFalse-m4096
Evaluating commonsenseqa :   2%|█▍                                                                    | 1/50 [02:08<1:44:44, 128.25s/it]Evaluating commonsenseqa :   4%|██▊                                                                   | 2/50 [04:12<1:40:56, 126.18s/it]Evaluating commonsenseqa :   6%|████▎                                                                  | 3/50 [05:08<1:13:25, 93.74s/it]Evaluating commonsenseqa :   8%|█████▌                                                                | 4/50 [07:13<1:21:33, 106.39s/it]Evaluating commonsenseqa :  10%|███████                                                               | 5/50 [09:19<1:24:56, 113.25s/it]Evaluating commonsenseqa :  12%|████████▍                                                             | 6/50 [11:26<1:26:24, 117.84s/it]Evaluating commonsenseqa :  14%|█████████▊                                                            | 7/50 [12:35<1:13:08, 102.05s/it]Evaluating commonsenseqa :  16%|███████████▏                                                          | 8/50 [14:43<1:17:17, 110.42s/it]Evaluating commonsenseqa :  18%|████████████▌                                                         | 9/50 [16:51<1:19:05, 115.75s/it]Evaluating commonsenseqa :  20%|█████████████▊                                                       | 10/50 [18:58<1:19:29, 119.24s/it]Evaluating commonsenseqa :  22%|███████████████▏                                                     | 11/50 [21:03<1:18:42, 121.09s/it]Evaluating commonsenseqa :  24%|████████████████▌                                                    | 12/50 [23:09<1:17:40, 122.64s/it]Evaluating commonsenseqa :  26%|█████████████████▉                                                   | 13/50 [25:15<1:16:08, 123.47s/it]Evaluating commonsenseqa :  28%|███████████████████▎                                                 | 14/50 [27:24<1:15:04, 125.12s/it]Evaluating commonsenseqa :  30%|████████████████████▋                                                | 15/50 [29:30<1:13:08, 125.38s/it]Evaluating commonsenseqa :  32%|██████████████████████▋                                                | 16/50 [30:20<58:15, 102.81s/it]Evaluating commonsenseqa :  34%|███████████████████████▍                                             | 17/50 [32:29<1:00:48, 110.57s/it]Evaluating commonsenseqa :  36%|████████████████████████▊                                            | 18/50 [34:34<1:01:16, 114.89s/it]Evaluating commonsenseqa :  38%|██████████████████████████▏                                          | 19/50 [36:39<1:01:03, 118.17s/it]Evaluating commonsenseqa :  40%|████████████████████████████▍                                          | 20/50 [38:05<54:15, 108.51s/it]Evaluating commonsenseqa :  42%|█████████████████████████████▊                                         | 21/50 [40:11<54:51, 113.49s/it]Evaluating commonsenseqa :  44%|███████████████████████████████▏                                       | 22/50 [42:15<54:32, 116.89s/it]Evaluating commonsenseqa :  46%|████████████████████████████████▋                                      | 23/50 [44:24<54:08, 120.30s/it]Evaluating commonsenseqa :  48%|██████████████████████████████████                                     | 24/50 [46:31<53:03, 122.45s/it]Evaluating commonsenseqa :  50%|███████████████████████████████████▌                                   | 25/50 [48:39<51:41, 124.05s/it]Evaluating commonsenseqa :  52%|████████████████████████████████████▉                                  | 26/50 [50:46<49:56, 124.84s/it]Evaluating commonsenseqa :  54%|██████████████████████████████████████▎                                | 27/50 [52:52<48:01, 125.27s/it]Evaluating commonsenseqa :  56%|███████████████████████████████████████▊                               | 28/50 [55:00<46:12, 126.00s/it]Evaluating commonsenseqa :  58%|█████████████████████████████████████████▏                             | 29/50 [57:06<44:10, 126.21s/it]Evaluating commonsenseqa :  60%|██████████████████████████████████████████▌                            | 30/50 [59:16<42:24, 127.23s/it]Evaluating commonsenseqa :  62%|██████████████████████████████████████████▊                          | 31/50 [1:01:22<40:13, 127.03s/it]Evaluating commonsenseqa :  64%|████████████████████████████████████████████▏                        | 32/50 [1:03:26<37:46, 125.93s/it]Evaluating commonsenseqa :  66%|█████████████████████████████████████████████▌                       | 33/50 [1:05:31<35:35, 125.60s/it]Evaluating commonsenseqa :  68%|██████████████████████████████████████████████▉                      | 34/50 [1:07:36<33:28, 125.55s/it]Evaluating commonsenseqa :  70%|████████████████████████████████████████████████▎                    | 35/50 [1:09:45<31:36, 126.43s/it]Evaluating commonsenseqa :  72%|█████████████████████████████████████████████████▋                   | 36/50 [1:11:51<29:30, 126.47s/it]Evaluating commonsenseqa :  74%|███████████████████████████████████████████████████                  | 37/50 [1:13:58<27:26, 126.67s/it]Evaluating commonsenseqa :  76%|████████████████████████████████████████████████████▍                | 38/50 [1:16:06<25:23, 126.95s/it]Evaluating commonsenseqa :  78%|█████████████████████████████████████████████████████▊               | 39/50 [1:18:13<23:18, 127.14s/it]Evaluating commonsenseqa :  80%|███████████████████████████████████████████████████████▏             | 40/50 [1:20:22<21:14, 127.50s/it]Evaluating commonsenseqa :  82%|████████████████████████████████████████████████████████▌            | 41/50 [1:22:30<19:10, 127.82s/it]Evaluating commonsenseqa :  84%|█████████████████████████████████████████████████████████▉           | 42/50 [1:24:39<17:05, 128.17s/it]Evaluating commonsenseqa :  86%|███████████████████████████████████████████████████████████▎         | 43/50 [1:26:10<13:38, 116.87s/it]Evaluating commonsenseqa :  88%|████████████████████████████████████████████████████████████▋        | 44/50 [1:28:18<12:01, 120.33s/it]Evaluating commonsenseqa :  90%|██████████████████████████████████████████████████████████████       | 45/50 [1:30:23<10:08, 121.76s/it]Evaluating commonsenseqa :  92%|███████████████████████████████████████████████████████████████▍     | 46/50 [1:32:33<08:16, 124.07s/it]Evaluating commonsenseqa :  94%|████████████████████████████████████████████████████████████████▊    | 47/50 [1:34:35<06:10, 123.54s/it]Evaluating commonsenseqa :  96%|██████████████████████████████████████████████████████████████████▏  | 48/50 [1:36:40<04:08, 124.06s/it]Evaluating commonsenseqa :  98%|███████████████████████████████████████████████████████████████████▌ | 49/50 [1:38:15<01:55, 115.32s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:40:24<00:00, 119.36s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:40:24<00:00, 120.49s/it]
name: commonsenseqa | avg. gen lenth: 272.64 | time: 6024.883975982666s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 02:11:44,346] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 02:11:44,523] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 02:11:44,608] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 02:11:44,730] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s10-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 9
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i9-s10-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9732 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████████████████████████████████████████| 9732/9732 [00:00<00:00, 817638.49it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.15s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.26s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.33s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.40s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.24s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.68s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.29s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.74s/it]
[2023-08-30 02:11:55,401] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.32s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.77s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.32s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.78s/it]
[2023-08-30 02:11:55,523] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 02:11:55,548] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-30 02:11:55,575] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 02:11:56,150] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 02:11:56,152] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0ca2ab92d0>
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 02:11:56,154] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: B: tailor shop

Input: Where are you if your reading magazines while waiting for a vehicle on rails? Choices:  A: vegetables B: market C: doctor D: train station E: bookstore
Output: D: train station

Input: What would need oil to be used? Choices:  A: ground B: human  body C: repair shop D: combustion engines E: service station
Output: D: combustion engines

Input: What is person probably feeling that plans on stopping being married to their spouse? Choices:  A: detachment B: bankruptcy C: sad D: fights E: wrong
Output: A: detachment

Input: What could you use to store a clock? Choices:  A: shelf B: own bedroom C: desk D: wall E: car
Output: A: shelf

Input: The person put on lotion, what did they want? Choices:  A: fresh smell B: good credit C: smooth skin D: fresh produce E: headache
Output: C: smooth skin

Input: They burned the record, they were trying to do what to history? Choices:  A: compact disc B: tape C: rewrite D: play music E: erase
Output: E: erase

Input: She sure didn't have a green thumb, every time she thought she was making grow something it would what? Choices:  A: growth B: flowering C: ground D: die E: plants
Output: D: die

Input: After a long night out the drunken man lost consciousness, he showed a sign of sickness right before passing out, what was it? Choices:  A: dream B: vomiting C: panic D: cancer E: blurred vision
Output: B: vomiting

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i9-s10-rFalse-m4096
Evaluating commonsenseqa :   2%|█▍                                                                       | 1/50 [00:46<37:44, 46.21s/it]Evaluating commonsenseqa :   4%|██▊                                                                    | 2/50 [02:50<1:13:38, 92.06s/it]Evaluating commonsenseqa :   6%|████▎                                                                  | 3/50 [04:20<1:11:24, 91.17s/it]Evaluating commonsenseqa :   8%|█████▌                                                                | 4/50 [06:30<1:21:32, 106.36s/it]Evaluating commonsenseqa :  10%|███████                                                                | 5/50 [07:58<1:14:52, 99.84s/it]Evaluating commonsenseqa :  12%|████████▍                                                             | 6/50 [10:02<1:19:20, 108.18s/it]Evaluating commonsenseqa :  14%|█████████▊                                                            | 7/50 [12:05<1:20:59, 113.02s/it]Evaluating commonsenseqa :  16%|███████████▏                                                          | 8/50 [14:12<1:22:15, 117.52s/it]Evaluating commonsenseqa :  18%|████████████▌                                                         | 9/50 [16:19<1:22:10, 120.25s/it]Evaluating commonsenseqa :  20%|█████████████▊                                                       | 10/50 [18:23<1:21:01, 121.53s/it]Evaluating commonsenseqa :  22%|███████████████▏                                                     | 11/50 [20:30<1:20:02, 123.14s/it]Evaluating commonsenseqa :  24%|████████████████▌                                                    | 12/50 [22:37<1:18:46, 124.39s/it]Evaluating commonsenseqa :  26%|█████████████████▉                                                   | 13/50 [24:44<1:17:13, 125.23s/it]Evaluating commonsenseqa :  28%|███████████████████▎                                                 | 14/50 [26:49<1:15:03, 125.08s/it]Evaluating commonsenseqa :  30%|████████████████████▋                                                | 15/50 [28:58<1:13:34, 126.14s/it]Evaluating commonsenseqa :  32%|██████████████████████                                               | 16/50 [31:04<1:11:35, 126.34s/it]Evaluating commonsenseqa :  34%|███████████████████████▍                                             | 17/50 [33:10<1:09:23, 126.15s/it]Evaluating commonsenseqa :  36%|████████████████████████▊                                            | 18/50 [35:16<1:07:11, 125.98s/it]Evaluating commonsenseqa :  38%|██████████████████████████▏                                          | 19/50 [37:22<1:05:05, 125.97s/it]Evaluating commonsenseqa :  40%|███████████████████████████▌                                         | 20/50 [39:28<1:03:05, 126.18s/it]Evaluating commonsenseqa :  42%|████████████████████████████▉                                        | 21/50 [41:34<1:00:57, 126.13s/it]Evaluating commonsenseqa :  44%|███████████████████████████████▏                                       | 22/50 [43:41<58:54, 126.23s/it]Evaluating commonsenseqa :  46%|████████████████████████████████▋                                      | 23/50 [45:47<56:49, 126.29s/it]Evaluating commonsenseqa :  48%|██████████████████████████████████                                     | 24/50 [47:23<50:48, 117.27s/it]Evaluating commonsenseqa :  50%|███████████████████████████████████▌                                   | 25/50 [49:28<49:46, 119.47s/it]Evaluating commonsenseqa :  52%|████████████████████████████████████▉                                  | 26/50 [51:34<48:35, 121.50s/it]Evaluating commonsenseqa :  54%|██████████████████████████████████████▎                                | 27/50 [53:38<46:50, 122.19s/it]Evaluating commonsenseqa :  56%|███████████████████████████████████████▊                               | 28/50 [55:44<45:14, 123.40s/it]Evaluating commonsenseqa :  58%|█████████████████████████████████████████▏                             | 29/50 [57:49<43:21, 123.88s/it]Evaluating commonsenseqa :  60%|██████████████████████████████████████████▌                            | 30/50 [59:53<41:19, 123.95s/it]Evaluating commonsenseqa :  62%|██████████████████████████████████████████▊                          | 31/50 [1:01:58<39:18, 124.13s/it]Evaluating commonsenseqa :  64%|████████████████████████████████████████████▏                        | 32/50 [1:04:03<37:20, 124.45s/it]Evaluating commonsenseqa :  66%|█████████████████████████████████████████████▌                       | 33/50 [1:06:07<35:09, 124.12s/it]Evaluating commonsenseqa :  68%|██████████████████████████████████████████████▉                      | 34/50 [1:08:14<33:21, 125.09s/it]Evaluating commonsenseqa :  70%|████████████████████████████████████████████████▎                    | 35/50 [1:10:21<31:24, 125.61s/it]Evaluating commonsenseqa :  72%|█████████████████████████████████████████████████▋                   | 36/50 [1:12:22<29:00, 124.30s/it]Evaluating commonsenseqa :  74%|███████████████████████████████████████████████████                  | 37/50 [1:14:26<26:55, 124.31s/it]Evaluating commonsenseqa :  76%|████████████████████████████████████████████████████▍                | 38/50 [1:16:29<24:46, 123.89s/it]Evaluating commonsenseqa :  78%|█████████████████████████████████████████████████████▊               | 39/50 [1:18:36<22:51, 124.67s/it]Evaluating commonsenseqa :  80%|███████████████████████████████████████████████████████▏             | 40/50 [1:20:43<20:56, 125.62s/it]Evaluating commonsenseqa :  82%|████████████████████████████████████████████████████████▌            | 41/50 [1:22:46<18:41, 124.58s/it]Evaluating commonsenseqa :  84%|█████████████████████████████████████████████████████████▉           | 42/50 [1:24:49<16:34, 124.30s/it]Evaluating commonsenseqa :  86%|███████████████████████████████████████████████████████████▎         | 43/50 [1:26:57<14:36, 125.25s/it]Evaluating commonsenseqa :  88%|████████████████████████████████████████████████████████████▋        | 44/50 [1:29:01<12:30, 125.01s/it]Evaluating commonsenseqa :  90%|██████████████████████████████████████████████████████████████       | 45/50 [1:30:18<09:12, 110.42s/it]Evaluating commonsenseqa :  92%|███████████████████████████████████████████████████████████████▍     | 46/50 [1:32:25<07:41, 115.41s/it]Evaluating commonsenseqa :  94%|████████████████████████████████████████████████████████████████▊    | 47/50 [1:34:30<05:55, 118.43s/it]Evaluating commonsenseqa :  96%|██████████████████████████████████████████████████████████████████▏  | 48/50 [1:36:36<04:01, 120.56s/it]Evaluating commonsenseqa :  98%|███████████████████████████████████████████████████████████████████▌ | 49/50 [1:38:43<02:02, 122.74s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:40:50<00:00, 123.75s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:40:50<00:00, 121.00s/it]
name: commonsenseqa | avg. gen lenth: 266.952 | time: 6050.371901988983s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 03:52:53,240] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 03:52:53,292] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 03:52:53,307] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 03:52:53,312] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 1
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9740 [00:00<?, ?it/s]Loading data: 100%|████████████████████████████████████████████████████████████████████████████| 9740/9740 [00:00<00:00, 1244896.42it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:06<00:06,  6.95s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.05s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.21s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.64s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.12s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.56s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.18s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.60s/it]
[2023-08-30 03:53:03,677] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-30 03:53:03,723] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.25s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.69s/it]
[2023-08-30 03:53:03,978] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.51s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.98s/it]
[2023-08-30 03:53:04,611] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 03:53:05,162] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 03:53:05,163] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 03:53:05,163] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f01bd7c1300>
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 03:53:05,165] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. Let's analyze each answer choice in the context of the sentence.
2. The sentence says that the machine was'very intricate'. It means the machine has complex and detailed features.
3. First, see Option A: 'box'. A box is a simple container and it does not convey the intricate mechanisms or complexity of a machine.
4. Option D: 'wash dishes' does not fit the context as it's related to a very specific function and does not effectively describe the complexity of a machine.
5. Option C: 'appliance'. This usually refers to a device, like a refrigerator or a toaster, doing practical work, but the level of intricacy or complexity in the usual sense of the term 'appliance' is not as high as the context demands.
6. Option E: 'implement'. An implement is a tool or instrument, which might be intricate, but this word is typically used for hand-held things and wouldn't usually describe a complex machine.
7. Option B: 'apparatus'. An apparatus is often used to refer to a complex instrument or machine with a particular purpose. 
8. Therefore, considering all the option meanings and their contextual relevance, we can conclude that the best choice to complete the sentence is B: 'apparatus' as it signifies the necessary complexity and intricacy the sentence demands.
So the final answer is B: apparatus

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|█▍                                                                    | 1/50 [02:07<1:44:29, 127.95s/it]Evaluating commonsenseqa :   4%|██▊                                                                   | 2/50 [04:15<1:42:23, 128.00s/it]Evaluating commonsenseqa :   6%|████▏                                                                 | 3/50 [06:22<1:39:53, 127.53s/it]Evaluating commonsenseqa :   8%|█████▌                                                                | 4/50 [08:30<1:37:44, 127.50s/it]Evaluating commonsenseqa :  10%|███████                                                               | 5/50 [10:35<1:35:05, 126.78s/it]Evaluating commonsenseqa :  12%|████████▍                                                             | 6/50 [12:41<1:32:47, 126.53s/it]Evaluating commonsenseqa :  14%|█████████▊                                                            | 7/50 [14:48<1:30:35, 126.41s/it]Evaluating commonsenseqa :  16%|███████████▏                                                          | 8/50 [16:56<1:28:51, 126.93s/it]Evaluating commonsenseqa :  18%|████████████▌                                                         | 9/50 [19:02<1:26:36, 126.75s/it]Evaluating commonsenseqa :  20%|█████████████▊                                                       | 10/50 [21:10<1:24:40, 127.01s/it]Evaluating commonsenseqa :  22%|███████████████▏                                                     | 11/50 [23:16<1:22:28, 126.89s/it]Evaluating commonsenseqa :  24%|████████████████▌                                                    | 12/50 [25:22<1:20:14, 126.70s/it]Evaluating commonsenseqa :  26%|█████████████████▉                                                   | 13/50 [27:32<1:18:42, 127.62s/it]Evaluating commonsenseqa :  28%|███████████████████▎                                                 | 14/50 [29:38<1:16:17, 127.14s/it]Evaluating commonsenseqa :  30%|████████████████████▋                                                | 15/50 [31:44<1:13:58, 126.80s/it]Evaluating commonsenseqa :  32%|██████████████████████                                               | 16/50 [33:51<1:11:47, 126.68s/it]Evaluating commonsenseqa :  34%|███████████████████████▍                                             | 17/50 [35:57<1:09:32, 126.44s/it]Evaluating commonsenseqa :  36%|████████████████████████▊                                            | 18/50 [38:04<1:07:31, 126.60s/it]Evaluating commonsenseqa :  38%|██████████████████████████▏                                          | 19/50 [40:02<1:04:07, 124.12s/it]Evaluating commonsenseqa :  40%|███████████████████████████▌                                         | 20/50 [42:07<1:02:17, 124.57s/it]Evaluating commonsenseqa :  42%|████████████████████████████▉                                        | 21/50 [44:13<1:00:17, 124.75s/it]Evaluating commonsenseqa :  44%|███████████████████████████████▏                                       | 22/50 [46:19<58:26, 125.24s/it]Evaluating commonsenseqa :  46%|████████████████████████████████▋                                      | 23/50 [48:25<56:28, 125.49s/it]Evaluating commonsenseqa :  48%|██████████████████████████████████                                     | 24/50 [50:32<54:33, 125.91s/it]Evaluating commonsenseqa :  50%|███████████████████████████████████▌                                   | 25/50 [52:37<52:23, 125.75s/it]Evaluating commonsenseqa :  52%|████████████████████████████████████▉                                  | 26/50 [54:46<50:39, 126.66s/it]Evaluating commonsenseqa :  54%|██████████████████████████████████████▎                                | 27/50 [56:51<48:22, 126.21s/it]Evaluating commonsenseqa :  56%|███████████████████████████████████████▊                               | 28/50 [58:57<46:11, 125.95s/it]Evaluating commonsenseqa :  58%|████████████████████████████████████████                             | 29/50 [1:01:04<44:15, 126.48s/it]Evaluating commonsenseqa :  60%|█████████████████████████████████████████▍                           | 30/50 [1:03:11<42:09, 126.50s/it]Evaluating commonsenseqa :  62%|██████████████████████████████████████████▊                          | 31/50 [1:05:19<40:13, 127.02s/it]Evaluating commonsenseqa :  64%|████████████████████████████████████████████▏                        | 32/50 [1:07:26<38:03, 126.85s/it]Evaluating commonsenseqa :  66%|█████████████████████████████████████████████▌                       | 33/50 [1:09:32<35:54, 126.73s/it]Evaluating commonsenseqa :  68%|██████████████████████████████████████████████▉                      | 34/50 [1:11:38<33:46, 126.63s/it]Evaluating commonsenseqa :  70%|████████████████████████████████████████████████▎                    | 35/50 [1:13:44<31:34, 126.30s/it]Evaluating commonsenseqa :  72%|█████████████████████████████████████████████████▋                   | 36/50 [1:15:40<28:44, 123.19s/it]Evaluating commonsenseqa :  74%|███████████████████████████████████████████████████                  | 37/50 [1:17:49<27:05, 125.00s/it]Evaluating commonsenseqa :  76%|████████████████████████████████████████████████████▍                | 38/50 [1:19:56<25:05, 125.46s/it]Evaluating commonsenseqa :  78%|█████████████████████████████████████████████████████▊               | 39/50 [1:22:02<23:04, 125.86s/it]Evaluating commonsenseqa :  80%|███████████████████████████████████████████████████████▏             | 40/50 [1:24:13<21:13, 127.38s/it]Evaluating commonsenseqa :  82%|████████████████████████████████████████████████████████▌            | 41/50 [1:26:21<19:06, 127.42s/it]Evaluating commonsenseqa :  84%|█████████████████████████████████████████████████████████▉           | 42/50 [1:28:27<16:56, 127.11s/it]Evaluating commonsenseqa :  86%|███████████████████████████████████████████████████████████▎         | 43/50 [1:30:35<14:51, 127.40s/it]Evaluating commonsenseqa :  88%|████████████████████████████████████████████████████████████▋        | 44/50 [1:32:42<12:42, 127.13s/it]Evaluating commonsenseqa :  90%|██████████████████████████████████████████████████████████████       | 45/50 [1:34:48<10:33, 126.78s/it]Evaluating commonsenseqa :  92%|███████████████████████████████████████████████████████████████▍     | 46/50 [1:36:56<08:28, 127.05s/it]Evaluating commonsenseqa :  94%|████████████████████████████████████████████████████████████████▊    | 47/50 [1:39:03<06:21, 127.06s/it]Evaluating commonsenseqa :  96%|██████████████████████████████████████████████████████████████████▏  | 48/50 [1:41:08<04:13, 126.70s/it]Evaluating commonsenseqa :  98%|███████████████████████████████████████████████████████████████████▌ | 49/50 [1:43:14<02:06, 126.36s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:45:22<00:00, 126.73s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:45:22<00:00, 126.44s/it]
name: commonsenseqa | avg. gen lenth: 390.652 | time: 6322.509834051132s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 05:38:37,184] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 05:38:37,187] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 05:38:37,190] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 05:38:37,205] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 2
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9739 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████████████████████████████████████████| 9739/9739 [00:00<00:00, 667849.17it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:06<00:06,  6.75s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.36s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.38s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.39s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  3.99s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.40s/it]
[2023-08-30 05:38:47,477] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.27s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.73s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.29s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.75s/it]
 > number of parameters: 6738415616
[2023-08-30 05:38:48,078] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 05:38:48,140] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.40s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.85s/it]
[2023-08-30 05:38:48,329] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 05:38:48,955] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 05:38:48,957] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 05:38:48,957] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 05:38:48,957] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 05:38:48,957] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 05:38:48,957] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f86c33bd2a0>
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 05:38:48,960] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. The key to answering this question correctly is understanding the context and meaning of the question. The sentence is describing a machine that is said to be very intricate.
2. "Intricate" means that something is complicated or detailed. So, the word that follows should reasonably describe a detailed or complex machine.
3. We can now evaluate each of the multiple-choice options in light of this reasoning. In this case, the options are A: box, B: apparatus, C: appliance, D: wash dishes, and E: implement.
4. Option A, box, is unlikely because a box is a simple, generally not intricate item.
5. Option C, appliance, generally refers to devices for particular uses in the home, like a dishwasher. However, appliances aren't necessarily intricate or complex.
6. Option D, wash dishes, is not correct because it is not a noun but a verb phrase and doesn't follow grammatically after "an."
7. Option E, implement, usually refers to a tool or piece of equipment, but doesn't inherently suggest something intricate.
8. This leaves us with option B: apparatus, which is a technical term for a complex device or piece of equipment. It fits best with the description of the machine being intricate.

Therefore, the correct answer is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: Step 1: Identify the question and understand what it's asking. The question is "Where do you get petrol?"
Step 2: Consider the context of the question. It's asking for a location, so we can exclude any answers that are not places.
Step 3: Eliminate answer choice A (burn hot) because it isn't a place where you can get petrol
Step 4: Eliminate answer choice B (fuel tank) because while petrol can be stored there, it's not the place where you obtain it from.
Step 5: Eliminate answer choice C (burn hot) again because it isn't a place where you can get petrol.
Step 6: Eliminate answer choice D (car) because similar to a fuel tank, a car may store petrol but it's not the place where you get it from.
Step 7: By process of elimination, you're left with answer E (gas station) which is indeed a place where you can get petrol. 
Step 8: So, the answer is E: gas station.
So the final answer is E: gas station

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|█▍                                                                    | 1/50 [01:58<1:36:26, 118.08s/it]Evaluating commonsenseqa :   4%|██▊                                                                   | 2/50 [03:55<1:34:00, 117.51s/it]Evaluating commonsenseqa :   6%|████▏                                                                 | 3/50 [05:51<1:31:38, 116.99s/it]Evaluating commonsenseqa :   8%|█████▌                                                                | 4/50 [07:47<1:29:18, 116.50s/it]Evaluating commonsenseqa :  10%|███████                                                               | 5/50 [09:44<1:27:30, 116.67s/it]Evaluating commonsenseqa :  12%|████████▍                                                             | 6/50 [11:42<1:25:49, 117.03s/it]Evaluating commonsenseqa :  14%|█████████▊                                                            | 7/50 [13:39<1:23:56, 117.12s/it]Evaluating commonsenseqa :  16%|███████████▏                                                          | 8/50 [15:28<1:20:09, 114.51s/it]Evaluating commonsenseqa :  18%|████████████▌                                                         | 9/50 [17:24<1:18:43, 115.20s/it]Evaluating commonsenseqa :  20%|█████████████▊                                                       | 10/50 [19:23<1:17:25, 116.14s/it]Evaluating commonsenseqa :  22%|███████████████▏                                                     | 11/50 [21:23<1:16:16, 117.35s/it]Evaluating commonsenseqa :  24%|████████████████▌                                                    | 12/50 [23:19<1:14:09, 117.08s/it]Evaluating commonsenseqa :  26%|█████████████████▉                                                   | 13/50 [25:00<1:09:03, 111.98s/it]Evaluating commonsenseqa :  28%|███████████████████▎                                                 | 14/50 [26:57<1:08:15, 113.76s/it]Evaluating commonsenseqa :  30%|████████████████████▋                                                | 15/50 [28:56<1:07:10, 115.16s/it]Evaluating commonsenseqa :  32%|██████████████████████                                               | 16/50 [30:52<1:05:22, 115.36s/it]Evaluating commonsenseqa :  34%|███████████████████████▍                                             | 17/50 [32:47<1:03:30, 115.48s/it]Evaluating commonsenseqa :  36%|████████████████████████▊                                            | 18/50 [34:43<1:01:38, 115.59s/it]Evaluating commonsenseqa :  38%|██████████████████████████▉                                            | 19/50 [36:38<59:37, 115.41s/it]Evaluating commonsenseqa :  40%|████████████████████████████▍                                          | 20/50 [38:35<57:52, 115.74s/it]Evaluating commonsenseqa :  42%|█████████████████████████████▊                                         | 21/50 [40:32<56:10, 116.21s/it]Evaluating commonsenseqa :  44%|███████████████████████████████▏                                       | 22/50 [42:28<54:11, 116.11s/it]Evaluating commonsenseqa :  46%|████████████████████████████████▋                                      | 23/50 [44:24<52:16, 116.15s/it]Evaluating commonsenseqa :  48%|██████████████████████████████████                                     | 24/50 [46:21<50:25, 116.36s/it]Evaluating commonsenseqa :  50%|███████████████████████████████████▌                                   | 25/50 [48:00<46:15, 111.02s/it]Evaluating commonsenseqa :  52%|████████████████████████████████████▉                                  | 26/50 [49:55<44:56, 112.35s/it]Evaluating commonsenseqa :  54%|██████████████████████████████████████▎                                | 27/50 [51:51<43:27, 113.35s/it]Evaluating commonsenseqa :  56%|███████████████████████████████████████▊                               | 28/50 [53:48<42:01, 114.64s/it]Evaluating commonsenseqa :  58%|█████████████████████████████████████████▏                             | 29/50 [55:47<40:34, 115.93s/it]Evaluating commonsenseqa :  60%|██████████████████████████████████████████▌                            | 30/50 [57:46<38:56, 116.80s/it]Evaluating commonsenseqa :  62%|████████████████████████████████████████████                           | 31/50 [59:42<36:52, 116.42s/it]Evaluating commonsenseqa :  64%|████████████████████████████████████████████▏                        | 32/50 [1:01:38<34:53, 116.28s/it]Evaluating commonsenseqa :  66%|█████████████████████████████████████████████▌                       | 33/50 [1:03:35<33:03, 116.69s/it]Evaluating commonsenseqa :  68%|██████████████████████████████████████████████▉                      | 34/50 [1:05:34<31:15, 117.25s/it]Evaluating commonsenseqa :  70%|████████████████████████████████████████████████▎                    | 35/50 [1:07:32<29:22, 117.53s/it]Evaluating commonsenseqa :  72%|█████████████████████████████████████████████████▋                   | 36/50 [1:09:30<27:25, 117.55s/it]Evaluating commonsenseqa :  74%|███████████████████████████████████████████████████                  | 37/50 [1:11:28<25:30, 117.72s/it]Evaluating commonsenseqa :  76%|████████████████████████████████████████████████████▍                | 38/50 [1:13:25<23:30, 117.51s/it]Evaluating commonsenseqa :  78%|█████████████████████████████████████████████████████▊               | 39/50 [1:15:21<21:29, 117.27s/it]Evaluating commonsenseqa :  80%|███████████████████████████████████████████████████████▏             | 40/50 [1:17:17<19:28, 116.85s/it]Evaluating commonsenseqa :  82%|████████████████████████████████████████████████████████▌            | 41/50 [1:19:15<17:32, 116.99s/it]Evaluating commonsenseqa :  84%|█████████████████████████████████████████████████████████▉           | 42/50 [1:21:11<15:33, 116.69s/it]Evaluating commonsenseqa :  86%|███████████████████████████████████████████████████████████▎         | 43/50 [1:23:05<13:31, 115.88s/it]Evaluating commonsenseqa :  88%|████████████████████████████████████████████████████████████▋        | 44/50 [1:25:02<11:37, 116.29s/it]Evaluating commonsenseqa :  90%|██████████████████████████████████████████████████████████████       | 45/50 [1:26:57<09:40, 116.03s/it]Evaluating commonsenseqa :  92%|███████████████████████████████████████████████████████████████▍     | 46/50 [1:28:55<07:45, 116.41s/it]Evaluating commonsenseqa :  94%|████████████████████████████████████████████████████████████████▊    | 47/50 [1:30:50<05:48, 116.21s/it]Evaluating commonsenseqa :  96%|██████████████████████████████████████████████████████████████████▏  | 48/50 [1:32:48<03:53, 116.56s/it]Evaluating commonsenseqa :  98%|███████████████████████████████████████████████████████████████████▌ | 49/50 [1:34:44<01:56, 116.36s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:36:40<00:00, 116.34s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:36:40<00:00, 116.01s/it]
name: commonsenseqa | avg. gen lenth: 361.584 | time: 5800.766259670258s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 07:15:45,595] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 07:15:45,595] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 07:15:45,599] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 07:15:45,612] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 3
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i3-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9738 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████████████████████████████████████████| 9738/9738 [00:00<00:00, 590301.37it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:06<00:06,  6.83s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:06<00:06,  6.89s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:06<00:06,  6.92s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.15s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.05s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.46s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.08s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.50s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.11s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.53s/it]
[2023-08-30 07:15:55,936] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 07:15:56,015] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.15s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.60s/it]
 > number of parameters: 6738415616
[2023-08-30 07:15:56,083] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 07:15:56,215] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 07:15:56,823] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 07:15:56,825] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f92b9bc52d0>
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 07:15:56,826] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. To answer this question, it is necessary to understand the context of a complex, detailed machine. 
2. Equally important is to know the meanings of the provided choices, which are: a box, an apparatus, an appliance, washing dishes, and an implement.
3. Let's go through the choices by their definition:
   - A box is simply a container, which doesn't describe something intricate.
   - An apparatus encompasses a set of machines or equipment designed for a particular function, which could be intricate.
   - An appliance refers to a device or piece of equipment designed to perform a specific task, typically a household task. The definition does not imply intricacy.
   - "Wash dishes" is an action, not a descriptor for a complex machine.
   - An implement is a tool, utensil, or other piece of equipment, which usually refers to simpler tools, not intricate machines.
4. Keeping the given context and definitions in mind, it is clear that the word 'apparatus' best fits the description of a machine that is'very intricate'.
5. Therefore, the correct answer is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: 1. The question asks for the location where you typically obtain petrol.
2. Petrol is a type of fuel used to run cars and other automobiles.
3. Choices A and C, "burn hot," are incorrect because they are not locations nor related to obtaining petrol.
4. Choice B, "fuel tank," is the part of the car where petrol is stored and not the place where it is procured.
5. Choice D, "car," is also incorrect because we don't get petrol from the car itself; instead, we add petrol to the car.
6. Therefore, the correct answer is choice E, "gas station," because this is the location where you can buy and get petrol for your car.
So the final answer is E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: 1. First, we consider the fact that a television is a piece of technology that needs to be plugged into an electricity source to be used.
2. A television is also typically used indoors due to the need to protect it from weather and theft.
3. With this in mind, we can eliminate option A: 'cabinet' because it is unlikely to be used there, and 'woods' because a television is typically not used outdoors.
4. We are left with 'house', 'apartment' and 'bedroom'. All these places can have electricity and are inside a building.
5. However, considering the word 'night' in the question suggests the location is likely to be a place used during the night.
6. While 'house' and 'apartment' could be correct, they are not specific enough  as they consist of many rooms.
7. 'Bedroom' is the most likely answer because it is common to watch television in a bedroom at night. Hence, the answer is D: bedroom.
So the final answer is D: bedroom

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i3-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|█▍                                                                    | 1/50 [01:53<1:32:44, 113.57s/it]Evaluating commonsenseqa :   4%|██▊                                                                   | 2/50 [03:44<1:29:23, 111.74s/it]Evaluating commonsenseqa :   6%|████▏                                                                 | 3/50 [05:35<1:27:15, 111.40s/it]Evaluating commonsenseqa :   8%|█████▌                                                                | 4/50 [07:26<1:25:29, 111.51s/it]Evaluating commonsenseqa :  10%|███████                                                               | 5/50 [08:55<1:17:23, 103.18s/it]Evaluating commonsenseqa :  12%|████████▍                                                             | 6/50 [10:44<1:17:13, 105.31s/it]Evaluating commonsenseqa :  14%|█████████▊                                                            | 7/50 [12:34<1:16:34, 106.84s/it]Evaluating commonsenseqa :  16%|███████████▏                                                          | 8/50 [14:24<1:15:25, 107.76s/it]Evaluating commonsenseqa :  18%|████████████▌                                                         | 9/50 [16:13<1:14:01, 108.34s/it]Evaluating commonsenseqa :  20%|█████████████▊                                                       | 10/50 [18:03<1:12:34, 108.87s/it]Evaluating commonsenseqa :  22%|███████████████▏                                                     | 11/50 [19:54<1:11:06, 109.40s/it]Evaluating commonsenseqa :  24%|████████████████▌                                                    | 12/50 [21:46<1:09:41, 110.04s/it]Evaluating commonsenseqa :  26%|█████████████████▉                                                   | 13/50 [23:36<1:07:54, 110.11s/it]Evaluating commonsenseqa :  28%|███████████████████▎                                                 | 14/50 [25:25<1:05:53, 109.82s/it]Evaluating commonsenseqa :  30%|████████████████████▋                                                | 15/50 [27:16<1:04:20, 110.29s/it]Evaluating commonsenseqa :  32%|██████████████████████                                               | 16/50 [29:05<1:02:16, 109.89s/it]Evaluating commonsenseqa :  34%|███████████████████████▍                                             | 17/50 [30:55<1:00:27, 109.91s/it]Evaluating commonsenseqa :  36%|█████████████████████████▌                                             | 18/50 [32:44<58:26, 109.59s/it]Evaluating commonsenseqa :  38%|██████████████████████████▉                                            | 19/50 [34:36<56:54, 110.14s/it]Evaluating commonsenseqa :  40%|████████████████████████████▍                                          | 20/50 [36:26<55:02, 110.09s/it]Evaluating commonsenseqa :  42%|█████████████████████████████▊                                         | 21/50 [38:16<53:15, 110.17s/it]Evaluating commonsenseqa :  44%|███████████████████████████████▏                                       | 22/50 [40:06<51:28, 110.29s/it]Evaluating commonsenseqa :  46%|████████████████████████████████▋                                      | 23/50 [41:56<49:29, 109.99s/it]Evaluating commonsenseqa :  48%|██████████████████████████████████                                     | 24/50 [43:45<47:36, 109.88s/it]Evaluating commonsenseqa :  50%|███████████████████████████████████▌                                   | 25/50 [45:35<45:47, 109.91s/it]Evaluating commonsenseqa :  52%|████████████████████████████████████▉                                  | 26/50 [47:25<43:54, 109.76s/it]Evaluating commonsenseqa :  54%|██████████████████████████████████████▎                                | 27/50 [49:16<42:17, 110.31s/it]Evaluating commonsenseqa :  56%|███████████████████████████████████████▊                               | 28/50 [51:07<40:31, 110.54s/it]Evaluating commonsenseqa :  58%|█████████████████████████████████████████▏                             | 29/50 [52:57<38:37, 110.36s/it]Evaluating commonsenseqa :  60%|██████████████████████████████████████████▌                            | 30/50 [54:47<36:40, 110.02s/it]Evaluating commonsenseqa :  62%|████████████████████████████████████████████                           | 31/50 [56:37<34:51, 110.08s/it]Evaluating commonsenseqa :  64%|█████████████████████████████████████████████▍                         | 32/50 [58:27<33:00, 110.02s/it]Evaluating commonsenseqa :  66%|█████████████████████████████████████████████▌                       | 33/50 [1:00:19<31:19, 110.57s/it]Evaluating commonsenseqa :  68%|██████████████████████████████████████████████▉                      | 34/50 [1:02:08<29:22, 110.17s/it]Evaluating commonsenseqa :  70%|████████████████████████████████████████████████▎                    | 35/50 [1:03:59<27:36, 110.41s/it]Evaluating commonsenseqa :  72%|█████████████████████████████████████████████████▋                   | 36/50 [1:05:47<25:37, 109.80s/it]Evaluating commonsenseqa :  74%|███████████████████████████████████████████████████                  | 37/50 [1:07:38<23:53, 110.26s/it]Evaluating commonsenseqa :  76%|████████████████████████████████████████████████████▍                | 38/50 [1:09:29<22:02, 110.24s/it]Evaluating commonsenseqa :  78%|█████████████████████████████████████████████████████▊               | 39/50 [1:11:19<20:11, 110.16s/it]Evaluating commonsenseqa :  80%|███████████████████████████████████████████████████████▏             | 40/50 [1:13:09<18:21, 110.17s/it]Evaluating commonsenseqa :  82%|████████████████████████████████████████████████████████▌            | 41/50 [1:14:58<16:27, 109.73s/it]Evaluating commonsenseqa :  84%|█████████████████████████████████████████████████████████▉           | 42/50 [1:16:49<14:42, 110.27s/it]Evaluating commonsenseqa :  86%|███████████████████████████████████████████████████████████▎         | 43/50 [1:18:38<12:49, 109.99s/it]Evaluating commonsenseqa :  88%|████████████████████████████████████████████████████████████▋        | 44/50 [1:20:28<11:00, 110.02s/it]Evaluating commonsenseqa :  90%|██████████████████████████████████████████████████████████████       | 45/50 [1:22:19<09:10, 110.08s/it]Evaluating commonsenseqa :  92%|███████████████████████████████████████████████████████████████▍     | 46/50 [1:24:10<07:21, 110.47s/it]Evaluating commonsenseqa :  94%|████████████████████████████████████████████████████████████████▊    | 47/50 [1:26:00<05:30, 110.19s/it]Evaluating commonsenseqa :  96%|██████████████████████████████████████████████████████████████████▏  | 48/50 [1:27:51<03:40, 110.48s/it]Evaluating commonsenseqa :  98%|███████████████████████████████████████████████████████████████████▌ | 49/50 [1:29:41<01:50, 110.40s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:31:31<00:00, 110.22s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:31:31<00:00, 109.83s/it]
name: commonsenseqa | avg. gen lenth: 400.16 | time: 5491.700274467468s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 08:47:58,210] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 08:47:58,220] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 08:47:58,296] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 08:47:58,312] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 4
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i4-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9737 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████████████████████████████████████████| 9737/9737 [00:00<00:00, 477096.51it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:06<00:06,  6.93s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.31s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.25s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.35s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.21s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.62s/it]
[2023-08-30 08:48:08,905] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.30s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.74s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.39s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.82s/it]
[2023-08-30 08:48:09,235] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.42s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.86s/it]
[2023-08-30 08:48:09,354] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-30 08:48:09,521] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 08:48:10,075] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 08:48:10,076] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb473ecd2d0>
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 08:48:10,078] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. In this question, you need to identify the best word to complete the sentence.
2. The sentence is talking about an intricate machine, which means it is complex or complicated.
3. Now, look at the options given - A: box, B: apparatus, C: appliance, D: wash dishes, E: implement. 
4. The word "box" usually refers to a simple container, not something intricate or complex.
5. "Apparatus" refers to a complex machine or device, which fits with the description in the question.
6. "Appliance" refers to a device or piece of equipment designed to perform a specific task, usually a domestic one. The context of the sentence does not suggest a domestic usage.
7. "Wash dishes" is a verb phrase, not a noun, therefore it can't be the answer because we are describing a noun. 
8. "Implement" would imply a tool or instrument for a specific task, which does not necessarily need to be intricate or complex.
9. Therefore, the best fit from the given options to complete the sentence is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: 1. The question is asking about the location where one can obtain petrol. 
2. Petrol is a type of fuel that is used to run engines.
3. By going over the options given, we can eliminate options that do not fit the context of the question.
4. Options A and C, "burn hot," are incorrect because this response suggests a state of matter, not a location.
5. Option B, "fuel tank", while related to petrol, is where the petrol is stored in a car, not where it is originally obtained.
6. Option D, "car", is incorrect because cars use petrol, they do not provide it.
7. This leaves us with the last option, E, "gas station".
8. A gas station is a location where fuel, including petrol, is sold and distributed to customers for their vehicles. 
9. Therefore, the answer is E: gas station.
So the final answer is E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: 1. A television is usually used in places where people live or work, as this tool is typically used for entertainment or informative purposes.
2. The options provided in the question are cabinet, house, apartment, bedroom, and woods. 
3. A cabinet is not a suitable location for using a television as it is a storage unit and does not usually provide enough space to watch television comfortably.
4. The woods is also not a good option as it typically lacks the necessary resources such as electricity and proper protective shielding for the television.
5. The remaining options are house, apartment, and bedroom. The question indicates that the television is used at night, suggesting the place should be somewhere people often reside at night.
6. Although people reside both in houses and apartments, they are too broad and do not specify a particular area within them where a television might be used at night. A television can be in many rooms within a house or an apartment.
7. Considering all this, the most suitable answer among the provided options would be a bedroom as it specifies a location within a residential structure where people commonly use televisions at night. Hence, the answer is D: bedroom.
So the final answer is D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: 1. The question asks about the location of getting knowledge which is described as being "expensive."
2. If we consider the options, gaining knowledge from a book, field, meeting, or class doesn't typically have a high financial cost associated with it.
3. However, attending a university typically involves substantial financial expenses in terms of tuition, living expenses, etc.
4. The term "expensive" is a key term in this question because it differentiates university from the other options.
5. So, by considering the distinctive "expensive" feature, we can conclude that the location is the university. Hence, the answer is A: university.
So the final answer is A: university

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i4-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|█▍                                                                    | 1/50 [01:44<1:25:10, 104.30s/it]Evaluating commonsenseqa :   4%|██▊                                                                   | 2/50 [03:26<1:22:40, 103.34s/it]Evaluating commonsenseqa :   6%|████▏                                                                 | 3/50 [05:09<1:20:36, 102.91s/it]Evaluating commonsenseqa :   8%|█████▌                                                                | 4/50 [06:51<1:18:35, 102.51s/it]Evaluating commonsenseqa :  10%|███████                                                               | 5/50 [08:33<1:16:48, 102.42s/it]Evaluating commonsenseqa :  12%|████████▍                                                             | 6/50 [10:16<1:15:15, 102.63s/it]Evaluating commonsenseqa :  14%|█████████▊                                                            | 7/50 [11:59<1:13:40, 102.80s/it]Evaluating commonsenseqa :  16%|███████████▏                                                          | 8/50 [13:42<1:12:03, 102.93s/it]Evaluating commonsenseqa :  18%|████████████▌                                                         | 9/50 [15:25<1:10:20, 102.93s/it]Evaluating commonsenseqa :  20%|█████████████▊                                                       | 10/50 [17:07<1:08:22, 102.57s/it]Evaluating commonsenseqa :  22%|███████████████▏                                                     | 11/50 [18:50<1:06:38, 102.54s/it]Evaluating commonsenseqa :  24%|████████████████▌                                                    | 12/50 [20:32<1:04:53, 102.47s/it]Evaluating commonsenseqa :  26%|█████████████████▉                                                   | 13/50 [22:14<1:03:02, 102.22s/it]Evaluating commonsenseqa :  28%|███████████████████▎                                                 | 14/50 [23:56<1:01:18, 102.18s/it]Evaluating commonsenseqa :  30%|█████████████████████▎                                                 | 15/50 [25:37<59:32, 102.07s/it]Evaluating commonsenseqa :  32%|██████████████████████▋                                                | 16/50 [27:20<57:56, 102.26s/it]Evaluating commonsenseqa :  34%|████████████████████████▏                                              | 17/50 [29:02<56:09, 102.12s/it]Evaluating commonsenseqa :  36%|█████████████████████████▌                                             | 18/50 [30:45<54:34, 102.33s/it]Evaluating commonsenseqa :  38%|██████████████████████████▉                                            | 19/50 [32:27<52:54, 102.40s/it]Evaluating commonsenseqa :  40%|████████████████████████████▍                                          | 20/50 [34:10<51:14, 102.50s/it]Evaluating commonsenseqa :  42%|█████████████████████████████▊                                         | 21/50 [35:52<49:24, 102.21s/it]Evaluating commonsenseqa :  44%|███████████████████████████████▏                                       | 22/50 [37:35<47:53, 102.62s/it]Evaluating commonsenseqa :  46%|████████████████████████████████▋                                      | 23/50 [39:19<46:17, 102.87s/it]Evaluating commonsenseqa :  48%|██████████████████████████████████                                     | 24/50 [41:01<44:31, 102.74s/it]Evaluating commonsenseqa :  50%|███████████████████████████████████▌                                   | 25/50 [42:39<42:11, 101.25s/it]Evaluating commonsenseqa :  52%|████████████████████████████████████▉                                  | 26/50 [44:21<40:33, 101.41s/it]Evaluating commonsenseqa :  54%|██████████████████████████████████████▎                                | 27/50 [46:04<39:05, 101.97s/it]Evaluating commonsenseqa :  56%|████████████████████████████████████████▎                               | 28/50 [47:24<35:00, 95.49s/it]Evaluating commonsenseqa :  58%|█████████████████████████████████████████▊                              | 29/50 [49:07<34:10, 97.66s/it]Evaluating commonsenseqa :  60%|███████████████████████████████████████████▏                            | 30/50 [50:50<33:06, 99.31s/it]Evaluating commonsenseqa :  62%|████████████████████████████████████████████                           | 31/50 [52:32<31:42, 100.11s/it]Evaluating commonsenseqa :  64%|█████████████████████████████████████████████▍                         | 32/50 [54:14<30:11, 100.66s/it]Evaluating commonsenseqa :  66%|██████████████████████████████████████████████▊                        | 33/50 [55:59<28:50, 101.81s/it]Evaluating commonsenseqa :  68%|████████████████████████████████████████████████▎                      | 34/50 [57:42<27:14, 102.16s/it]Evaluating commonsenseqa :  70%|█████████████████████████████████████████████████▋                     | 35/50 [59:23<25:31, 102.08s/it]Evaluating commonsenseqa :  72%|█████████████████████████████████████████████████▋                   | 36/50 [1:01:07<23:56, 102.60s/it]Evaluating commonsenseqa :  74%|███████████████████████████████████████████████████                  | 37/50 [1:02:49<22:12, 102.49s/it]Evaluating commonsenseqa :  76%|████████████████████████████████████████████████████▍                | 38/50 [1:04:32<20:31, 102.59s/it]Evaluating commonsenseqa :  78%|█████████████████████████████████████████████████████▊               | 39/50 [1:06:17<18:54, 103.11s/it]Evaluating commonsenseqa :  80%|███████████████████████████████████████████████████████▏             | 40/50 [1:07:59<17:09, 102.98s/it]Evaluating commonsenseqa :  82%|████████████████████████████████████████████████████████▌            | 41/50 [1:09:41<15:23, 102.62s/it]Evaluating commonsenseqa :  84%|█████████████████████████████████████████████████████████▉           | 42/50 [1:11:24<13:42, 102.77s/it]Evaluating commonsenseqa :  86%|███████████████████████████████████████████████████████████▎         | 43/50 [1:13:06<11:57, 102.50s/it]Evaluating commonsenseqa :  88%|█████████████████████████████████████████████████████████████▌        | 44/50 [1:14:37<09:54, 99.11s/it]Evaluating commonsenseqa :  90%|██████████████████████████████████████████████████████████████       | 45/50 [1:16:20<08:21, 100.30s/it]Evaluating commonsenseqa :  92%|███████████████████████████████████████████████████████████████▍     | 46/50 [1:18:03<06:43, 100.95s/it]Evaluating commonsenseqa :  94%|████████████████████████████████████████████████████████████████▊    | 47/50 [1:19:45<05:03, 101.26s/it]Evaluating commonsenseqa :  96%|██████████████████████████████████████████████████████████████████▏  | 48/50 [1:21:28<03:23, 101.91s/it]Evaluating commonsenseqa :  98%|███████████████████████████████████████████████████████████████████▌ | 49/50 [1:23:11<01:42, 102.21s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:24:53<00:00, 102.25s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████████████████████████████████████████| 50/50 [1:24:53<00:00, 101.88s/it]
name: commonsenseqa | avg. gen lenth: 398.152 | time: 5094.448546171188s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 10:14:31,018] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 10:14:31,031] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 10:14:31,049] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 10:14:31,068] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 5
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i5-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9736 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████████████████████████████████████████| 9736/9736 [00:00<00:00, 392251.59it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:06<00:06,  6.82s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:06<00:06,  6.85s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:06<00:06,  6.90s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.11s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.43s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.06s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.48s/it]
[2023-08-30 10:14:41,454] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.11s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.52s/it]
 > number of parameters: 6738415616
[2023-08-30 10:14:41,556] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.12s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.57s/it]
[2023-08-30 10:14:41,625] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 10:14:41,676] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 10:14:42,261] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 10:14:42,262] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 10:14:42,262] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 10:14:42,262] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 10:14:42,262] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 10:14:42,262] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe9befc12d0>
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 10:14:42,264] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. The sentence talks about a machine being very intricate.
2. The missing word would need to be something that can be described as being complex or intricate. 
3. An 'intricate box' doesn't make much sense as a box is often simple in its design. Hence option A is incorrect.
4. Similarly, 'intricate wash dishes' does not fit the given context. Hence option D is incorrect.
5. Option E, 'implement' often refers to tools or equipment for specific purposes, which can be intricate but it is less often used to describe machinery. Hence it might not be the best fit.
6. Although appliances can be intricate, they are generally things like a kitchen or household devices, which may not be the best fit for the general term of'machine'. Therefore, option C might not be the best fit.
7. That leaves 'apparatus'. An apparatus is a technical device or machinery, and can indeed be intricate, fitting the sentence context well.
8. Therefore, the answer is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: 1. The question is asking where petrol is obtained from or where we can usually get petrol. 
2. Option A: burn hot, does not pertain to the question since it does not have a location where petrol can be obtained. It seems more related to the process of burning or combustion.
3. Option B: fuel tank, while it is a place where petrol is stored, it is not typically where individuals go to obtain petrol. Therefore, this option is skipped.
4. Option C: burn hot, as it is the same as option A and it does not provide an answer related to where petrol is typically obtained. 
5. Option D: car; petrol is used by a car and is stored in its fuel tank for operation, but it is not typically the place where individuals go to obtain or purchase petrol. 
6. Option E: gas station; this is the correct answer as this is the typical place where individuals go to obtain or purchase petrol for their vehicles.
So the final answer is E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: Step 1: Consider the options. The question asks where a television, used at night, might be.
Step 2: Option A: Cabinet is not a valid option since televisions are typically stored in cabinets when not in use, not when they are being watched. 
Step 3: Option B: House is a valid option as televisions are typically found in houses. But it's quite broad and does not specify a particular location within a house where a television would be used at night.
Step 4: Option C: Apartment is also a valid option like a house. But the same reasoning as option B applies here - it does not specify a particular place where a television would usually be used at night.
Step 5: Option D: Bedroom is a valid answer. People often watch TV in their bedrooms before going to sleep at night, so this choice is more specific than just a house or an apartment.
Step 6: Option E: Woods is not a valid option. Televisions require a power source and strong signal connection to broadcast, which are typically not available in the woods.
Step 7: Among all the options, bedroom is the most specific one which answers the question of where a television might be used at night. Therefore, the answer is D: bedroom.
So the final answer is D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: 1. The question asks about where an expensive knowledge acquisition process takes place.
2. We start by examining each of the choices.
3. Choice B: Knowledge acquired from books can be expensive, but not necessarily as it can also be freely accessible through libraries or online sources. So, it might not be the correct answer.
4. Choice C: Getting knowledge through field or practical experience is not necessarily expensive, it depends more on the nature of the job or project. So, it's less likely to be the right answer.
5. Choice D: Obtaining knowledge via a meeting is not usually thought of as an expensive process, as meetings can often be attended for free or at a low cost. Therefore, it rules out the choice.
6. Choice E: While knowledge can be expensive to gain in classes, this could be any type of class, not necessarily one that comes with a high cost.
7. Choice A: Universities are commonly referred to as places where one gains knowledge, and it is typically an expensive process due to tuition fees, living expenses, etc. Thus, it fits best with the descriptor 'expensive.'
8. Therefore, the answer is A: University.
So the final answer is A: university

Input: Where are small grapes likely to be found? Choices:  A: lunch box B: food store C: wine country D: kitchen E: raisins
Output: 1. This question is asking for a place where small grapes are typically found.
2. If we look at choice A: lunch box, we can eliminate this option because any type of fruit, not specifically small grapes, could be in a lunch box.  
3. Choice B: food store is a broader option as it could contain all kinds of food, not specifically small grapes.
4. Choice D: kitchen, a place where all types of food are prepared and not solely devoted for small grapes.
5. Choice E: raisins. While grapes are indeed used to make raisins, the question asks for a place where one would find small grapes, not their byproducts.
6. The answer is therefore choice C: wine country. This is because small grapes are often used in making certain types of wine, and vineyards in wine countries are known to cultivate and harvest small grapes specifically for this purpose.
So the final answer is C: wine country

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i5-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|█▍                                                                     | 1/50 [01:32<1:15:29, 92.44s/it]Evaluating commonsenseqa :   4%|██▊                                                                    | 2/50 [03:03<1:13:10, 91.47s/it]Evaluating commonsenseqa :   6%|████▎                                                                  | 3/50 [04:34<1:11:37, 91.44s/it]Evaluating commonsenseqa :   8%|█████▋                                                                 | 4/50 [06:06<1:10:05, 91.43s/it]Evaluating commonsenseqa :  10%|███████                                                                | 5/50 [07:37<1:08:33, 91.40s/it]Evaluating commonsenseqa :  12%|████████▌                                                              | 6/50 [09:08<1:06:56, 91.29s/it]Evaluating commonsenseqa :  14%|█████████▉                                                             | 7/50 [10:40<1:05:42, 91.68s/it]Evaluating commonsenseqa :  16%|███████████▎                                                           | 8/50 [12:13<1:04:21, 91.94s/it]Evaluating commonsenseqa :  18%|████████████▊                                                          | 9/50 [13:45<1:02:44, 91.82s/it]Evaluating commonsenseqa :  20%|██████████████                                                        | 10/50 [15:16<1:01:03, 91.59s/it]Evaluating commonsenseqa :  22%|███████████████▊                                                        | 11/50 [16:46<59:21, 91.32s/it]Evaluating commonsenseqa :  24%|█████████████████▎                                                      | 12/50 [18:18<57:49, 91.29s/it]Evaluating commonsenseqa :  26%|██████████████████▋                                                     | 13/50 [19:48<56:12, 91.15s/it]Evaluating commonsenseqa :  28%|████████████████████▏                                                   | 14/50 [21:20<54:48, 91.34s/it]Evaluating commonsenseqa :  30%|█████████████████████▌                                                  | 15/50 [22:52<53:26, 91.61s/it]Evaluating commonsenseqa :  32%|███████████████████████                                                 | 16/50 [24:24<51:56, 91.67s/it]Evaluating commonsenseqa :  34%|████████████████████████▍                                               | 17/50 [25:55<50:16, 91.40s/it]Evaluating commonsenseqa :  36%|█████████████████████████▉                                              | 18/50 [27:26<48:41, 91.28s/it]Evaluating commonsenseqa :  38%|███████████████████████████▎                                            | 19/50 [28:57<47:04, 91.12s/it]Evaluating commonsenseqa :  40%|████████████████████████████▊                                           | 20/50 [30:27<45:30, 91.01s/it]Evaluating commonsenseqa :  42%|██████████████████████████████▏                                         | 21/50 [31:58<43:51, 90.75s/it]Evaluating commonsenseqa :  44%|███████████████████████████████▋                                        | 22/50 [33:30<42:31, 91.11s/it]Evaluating commonsenseqa :  46%|█████████████████████████████████                                       | 23/50 [35:00<40:58, 91.04s/it]Evaluating commonsenseqa :  48%|██████████████████████████████████▌                                     | 24/50 [36:31<39:26, 91.03s/it]Evaluating commonsenseqa :  50%|████████████████████████████████████                                    | 25/50 [38:02<37:56, 91.04s/it]Evaluating commonsenseqa :  52%|█████████████████████████████████████▍                                  | 26/50 [39:34<36:26, 91.10s/it]Evaluating commonsenseqa :  54%|██████████████████████████████████████▉                                 | 27/50 [41:05<34:56, 91.16s/it]Evaluating commonsenseqa :  56%|████████████████████████████████████████▎                               | 28/50 [42:37<33:33, 91.51s/it]Evaluating commonsenseqa :  58%|█████████████████████████████████████████▊                              | 29/50 [44:08<31:58, 91.36s/it]Evaluating commonsenseqa :  60%|███████████████████████████████████████████▏                            | 30/50 [45:39<30:23, 91.15s/it]Evaluating commonsenseqa :  62%|████████████████████████████████████████████▋                           | 31/50 [47:10<28:50, 91.10s/it]Evaluating commonsenseqa :  64%|██████████████████████████████████████████████                          | 32/50 [48:41<27:19, 91.10s/it]Evaluating commonsenseqa :  66%|███████████████████████████████████████████████▌                        | 33/50 [50:13<25:50, 91.22s/it]Evaluating commonsenseqa :  68%|████████████████████████████████████████████████▉                       | 34/50 [51:45<24:24, 91.51s/it]Evaluating commonsenseqa :  70%|██████████████████████████████████████████████████▍                     | 35/50 [53:16<22:52, 91.50s/it]Evaluating commonsenseqa :  72%|███████████████████████████████████████████████████▊                    | 36/50 [54:47<21:18, 91.31s/it]Evaluating commonsenseqa :  74%|█████████████████████████████████████████████████████▎                  | 37/50 [56:18<19:45, 91.16s/it]Evaluating commonsenseqa :  76%|██████████████████████████████████████████████████████▋                 | 38/50 [57:49<18:11, 90.98s/it]Evaluating commonsenseqa :  78%|████████████████████████████████████████████████████████▏               | 39/50 [59:20<16:41, 91.00s/it]Evaluating commonsenseqa :  80%|████████████████████████████████████████████████████████              | 40/50 [1:00:50<15:08, 90.86s/it]Evaluating commonsenseqa :  82%|█████████████████████████████████████████████████████████▍            | 41/50 [1:02:22<13:41, 91.31s/it]Evaluating commonsenseqa :  84%|██████████████████████████████████████████████████████████▊           | 42/50 [1:03:54<12:11, 91.45s/it]Evaluating commonsenseqa :  86%|████████████████████████████████████████████████████████████▏         | 43/50 [1:05:26<10:40, 91.47s/it]Evaluating commonsenseqa :  88%|█████████████████████████████████████████████████████████████▌        | 44/50 [1:06:57<09:08, 91.46s/it]Evaluating commonsenseqa :  90%|███████████████████████████████████████████████████████████████       | 45/50 [1:08:28<07:36, 91.34s/it]Evaluating commonsenseqa :  92%|████████████████████████████████████████████████████████████████▍     | 46/50 [1:10:00<06:05, 91.36s/it]Evaluating commonsenseqa :  94%|█████████████████████████████████████████████████████████████████▊    | 47/50 [1:11:31<04:34, 91.50s/it]Evaluating commonsenseqa :  96%|███████████████████████████████████████████████████████████████████▏  | 48/50 [1:13:03<03:02, 91.46s/it]Evaluating commonsenseqa :  98%|████████████████████████████████████████████████████████████████████▌ | 49/50 [1:14:34<01:31, 91.43s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:16:06<00:00, 91.67s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:16:06<00:00, 91.34s/it]
name: commonsenseqa | avg. gen lenth: 425.5 | time: 4567.483436584473s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 11:31:02,906] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 11:31:02,930] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 11:31:02,937] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 11:31:02,980] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 6
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i6-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9735 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████████████████████████████████████████| 9735/9735 [00:00<00:00, 419581.25it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:06<00:06,  6.78s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.09s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.14s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.31s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  3.99s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.41s/it]
[2023-08-30 11:31:13,213] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.17s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.61s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.18s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.62s/it]
[2023-08-30 11:31:13,572] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 11:31:13,623] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.37s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.81s/it]
 > number of parameters: 6738415616
[2023-08-30 11:31:14,062] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 11:31:14,726] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 11:31:14,728] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6c24dbd2d0>
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 11:31:14,729] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. We need to fill in the blank using one of the provided choices. 
2. The context implies that the machine is very intricate or complex. Therefore, we need to find a word among the choices that adequately represents a complex machine.
3. The words "box" and "wash dishes" don't fit because they don't convey the complexity of a machine.
4. Although "appliance" and "implement" might make sense of complexity, they're too specific. These words are used for machines designed for a specific purpose. Without knowing the specific function of the machine, these words might misrepresent it.
5. The word "apparatus" is a general term for a complex machine, irrespective of what specific function it performs, fitting the context of sentence.
6. Therefore, the answer is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: 1. First, eliminate irrelevant choices. In this context, the items "burn hot" and "car" can be eliminated as they do not specifically relate to the source of petrol.
2. Among the remaining options, "fuel tank" can be eliminated as a petrol source. Although petrol is stored in a fuel tank, it does not originate from there.
3. Therefore, the correct answer is "gas station", where petrol is usually purchased or obtained.
So the final answer is E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: 1. The question asks where a television used at night might be.
2. A television is typically an indoor item, so both the woods and a cabinet wouldn't provide the appropriate setting or the purpose of watching television. So options A and E can be eliminated.
3. Remaining choices are the house, the apartment, and the bedroom.
4. While a television can be located in both a house and an apartment, these choices lack the specificity needed to determine the exact location within those places, hence may not be suitable responses.
5. This leaves the bedroom where it's common for people to have a TV and use at night before sleeping. 
6. Thus, the answer is D: bedroom.
So the final answer is D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: 1. The question is asking for a place where gaining knowledge is considered expensive. 
2. Although knowledge can be gained in all of the options given (university, book, field, meeting, class), not all of them are notably expensive. 
3. From our everyday understanding, we know that buying a book is considerably cheaper than paying for a university or class. Similarly, learning in a field or a meeting does not necessarily imply a high cost. 
4. Therefore, amongst the given choices, getting a university education usually involves significant costs, making it the most expensive medium to gain knowledge. 
5. Thus, the answer is "A: university".
So the final answer is A: university

Input: Where are small grapes likely to be found? Choices:  A: lunch box B: food store C: wine country D: kitchen E: raisins
Output: 1. The question is asking where you are likely to find small grapes. 
2. Grapes can be found in all the options provided. However, the question specifically mentions "small" grapes. 
3. In a lunch box, kitchen, or food store, the size of the grapes might vary and are not necessarily small.
4. Raisins are dried grapes, so they wouldn't be considered as small grapes. 
5. Small grapes are generally used in wine production because they have a better skin to fruit ratio, which is ideal for wine-making.
6. Wine is predominantly made in the wine country.
7. Therefore, small grapes are most likely to be found in the wine country. Hence, the answer is C: wine country.
So the final answer is C: wine country

Input: The painter explained how he never achieved a flawless portrait, he said this was because all people are what? Choices:  A: imperfect B: disfigured C: damaged D: flawed E: defective
Output: 1. Begin with understanding the question: we're being asked to figure out what the painter associates with why he is unable to create a flawless portrait.
2. Consider all the choices given: imperfect, disfigured, damaged, flawed, defective. 
3. Evaluate all the presented choices. The painter's inability to create a perfect portrait does not necessarily mean that all people are disfigured, damaged, flawed, or defective. These are strong words and talk about physical or mental damage or deformities, which is not implied in the question.
4. The word 'flawless' is the clue it's synonymous with 'perfect'. So the reason why the painter never achieved a flawless portrait is because he believes all people are imperfect.
5. Therefore the answer is A: imperfect. 
6. Conclude that the reasoning behind this answer is that 'imperfect' is a less harsh term and is synonymous with 'flawless', making it the most fitting word choice in this context.
So the final answer is A: imperfect

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i6-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|█▍                                                                     | 1/50 [01:36<1:18:47, 96.47s/it]Evaluating commonsenseqa :   4%|██▊                                                                    | 2/50 [03:11<1:16:32, 95.68s/it]Evaluating commonsenseqa :   6%|████▎                                                                  | 3/50 [04:46<1:14:45, 95.43s/it]Evaluating commonsenseqa :   8%|█████▋                                                                 | 4/50 [06:23<1:13:33, 95.95s/it]Evaluating commonsenseqa :  10%|███████                                                                | 5/50 [07:58<1:11:42, 95.61s/it]Evaluating commonsenseqa :  12%|████████▌                                                              | 6/50 [09:33<1:09:57, 95.39s/it]Evaluating commonsenseqa :  14%|█████████▉                                                             | 7/50 [11:08<1:08:10, 95.12s/it]Evaluating commonsenseqa :  16%|███████████▎                                                           | 8/50 [12:43<1:06:43, 95.33s/it]Evaluating commonsenseqa :  18%|████████████▊                                                          | 9/50 [14:18<1:05:02, 95.18s/it]Evaluating commonsenseqa :  20%|██████████████                                                        | 10/50 [15:53<1:03:28, 95.22s/it]Evaluating commonsenseqa :  22%|███████████████▍                                                      | 11/50 [17:28<1:01:45, 95.01s/it]Evaluating commonsenseqa :  24%|████████████████▊                                                     | 12/50 [19:02<1:00:00, 94.74s/it]Evaluating commonsenseqa :  26%|██████████████████▋                                                     | 13/50 [20:36<58:16, 94.49s/it]Evaluating commonsenseqa :  28%|████████████████████▏                                                   | 14/50 [22:12<57:00, 95.02s/it]Evaluating commonsenseqa :  30%|█████████████████████▌                                                  | 15/50 [23:49<55:43, 95.53s/it]Evaluating commonsenseqa :  32%|███████████████████████                                                 | 16/50 [25:24<53:59, 95.29s/it]Evaluating commonsenseqa :  34%|████████████████████████▍                                               | 17/50 [26:59<52:20, 95.18s/it]Evaluating commonsenseqa :  36%|█████████████████████████▉                                              | 18/50 [28:34<50:44, 95.13s/it]Evaluating commonsenseqa :  38%|███████████████████████████▎                                            | 19/50 [30:09<49:08, 95.10s/it]Evaluating commonsenseqa :  40%|████████████████████████████▊                                           | 20/50 [31:43<47:28, 94.96s/it]Evaluating commonsenseqa :  42%|██████████████████████████████▏                                         | 21/50 [33:17<45:45, 94.66s/it]Evaluating commonsenseqa :  44%|███████████████████████████████▋                                        | 22/50 [34:52<44:09, 94.64s/it]Evaluating commonsenseqa :  46%|█████████████████████████████████                                       | 23/50 [36:27<42:39, 94.78s/it]Evaluating commonsenseqa :  48%|██████████████████████████████████▌                                     | 24/50 [38:02<41:06, 94.86s/it]Evaluating commonsenseqa :  50%|████████████████████████████████████                                    | 25/50 [39:36<39:26, 94.65s/it]Evaluating commonsenseqa :  52%|█████████████████████████████████████▍                                  | 26/50 [41:11<37:50, 94.59s/it]Evaluating commonsenseqa :  54%|██████████████████████████████████████▉                                 | 27/50 [42:45<36:14, 94.55s/it]Evaluating commonsenseqa :  56%|████████████████████████████████████████▎                               | 28/50 [44:22<34:54, 95.22s/it]Evaluating commonsenseqa :  58%|█████████████████████████████████████████▊                              | 29/50 [45:57<33:20, 95.28s/it]Evaluating commonsenseqa :  60%|███████████████████████████████████████████▏                            | 30/50 [47:32<31:42, 95.15s/it]Evaluating commonsenseqa :  62%|████████████████████████████████████████████▋                           | 31/50 [49:06<30:02, 94.88s/it]Evaluating commonsenseqa :  64%|██████████████████████████████████████████████                          | 32/50 [50:42<28:31, 95.08s/it]Evaluating commonsenseqa :  66%|███████████████████████████████████████████████▌                        | 33/50 [52:17<26:54, 95.00s/it]Evaluating commonsenseqa :  68%|████████████████████████████████████████████████▉                       | 34/50 [53:51<25:17, 94.86s/it]Evaluating commonsenseqa :  70%|██████████████████████████████████████████████████▍                     | 35/50 [55:27<23:45, 95.04s/it]Evaluating commonsenseqa :  72%|███████████████████████████████████████████████████▊                    | 36/50 [57:03<22:17, 95.53s/it]Evaluating commonsenseqa :  74%|█████████████████████████████████████████████████████▎                  | 37/50 [58:38<20:36, 95.11s/it]Evaluating commonsenseqa :  76%|█████████████████████████████████████████████████████▏                | 38/50 [1:00:12<18:59, 95.00s/it]Evaluating commonsenseqa :  78%|██████████████████████████████████████████████████████▌               | 39/50 [1:01:48<17:28, 95.30s/it]Evaluating commonsenseqa :  80%|████████████████████████████████████████████████████████              | 40/50 [1:03:23<15:51, 95.17s/it]Evaluating commonsenseqa :  82%|█████████████████████████████████████████████████████████▍            | 41/50 [1:04:58<14:15, 95.03s/it]Evaluating commonsenseqa :  84%|██████████████████████████████████████████████████████████▊           | 42/50 [1:06:34<12:42, 95.36s/it]Evaluating commonsenseqa :  86%|████████████████████████████████████████████████████████████▏         | 43/50 [1:08:09<11:05, 95.13s/it]Evaluating commonsenseqa :  88%|█████████████████████████████████████████████████████████████▌        | 44/50 [1:09:44<09:31, 95.25s/it]Evaluating commonsenseqa :  90%|███████████████████████████████████████████████████████████████       | 45/50 [1:11:19<07:55, 95.11s/it]Evaluating commonsenseqa :  92%|████████████████████████████████████████████████████████████████▍     | 46/50 [1:12:54<06:19, 94.97s/it]Evaluating commonsenseqa :  94%|█████████████████████████████████████████████████████████████████▊    | 47/50 [1:14:29<04:45, 95.21s/it]Evaluating commonsenseqa :  96%|███████████████████████████████████████████████████████████████████▏  | 48/50 [1:16:04<03:10, 95.10s/it]Evaluating commonsenseqa :  98%|████████████████████████████████████████████████████████████████████▌ | 49/50 [1:17:41<01:35, 95.57s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:19:16<00:00, 95.45s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:19:16<00:00, 95.13s/it]
name: commonsenseqa | avg. gen lenth: 416.216 | time: 4756.975300788879s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 12:50:50,032] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 12:50:50,123] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 12:50:50,140] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 12:50:50,149] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 7
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i7-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9734 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████████████████████████████████████████| 9734/9734 [00:00<00:00, 336296.10it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:06<00:06,  6.32s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:06<00:06,  6.71s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.11s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:07<00:07,  7.14s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  3.80s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.18s/it]
[2023-08-30 12:51:00,192] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.41s/it]
 > number of parameters: 6738415616
[2023-08-30 12:51:00,448] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.31s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.73s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.30s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.73s/it]
[2023-08-30 12:51:01,119] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 12:51:01,164] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 12:51:01,771] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 12:51:01,772] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 12:51:01,772] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 12:51:01,772] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 12:51:01,772] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 12:51:01,772] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f23af1b9300>
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 12:51:01,774] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. The sentence in the question is describing a complicated machine. The right word must fit well into this context.
2. The choices are: A: box B: apparatus C: appliance D: wash dishes E: implement.
3. Looking up each choice, an 'apparatus' can denote a complex machine or device, 'appliance' stands for a piece of equipment designed to perform a specific task, typically a domestic one, 'implement' is a tool or utensil helping to do something(not complex enough), and 'box' is a container or 'wash dishes' obviously does not fit in the context.
4. Therefore, the appropriate word is apparatus as it is complex and it fits the context of the sentence well. 
5. So, the answer is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: 1. The question asks where we can obtain petrol.
2. Petrol, also known as gasoline, is a type of fuel commonly used in internal combustion engines.
3. Looking at the choices:
    - Option A and C suggest "burn hot". However, this is a description of what can happen when petrol is used, not a place where it can be obtained, so both options A and C can be eliminated.
    - Option B suggests a "fuel tank." While a fuel tank is where petrol is stored once obtained, it's not a place where one can go to get petrol, hence option B is not correct.
    - Option D suggests a "car." While it's true that petrol is used in cars, cars are not a place where you can get petrol. They are where you use petrol. So, option D can also be eliminated.
    - Option E suggests a "gas station." Gas stations are exactly where one would go to obtain petrol to fuel their cars or other engines.
4. Therefore, the answer is E: Gas station, which is the place where petrol can be obtained.
So the final answer is E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: 1. A television used at night most likely implies it is used for personal entertainment during leisure time, especially before sleep time.
2. If we go through the answer options: a cabinet is usually for storing items, so it's not likely to be the correct choice.
3. A house or an apartment could be a right answer, as they do have televisions. However, these options are pretty broad because they contain many other rooms/areas where people may or may not watch television.
4. Woods is an unlikely answer as it is open and not resided in, making it unfeasible for using a television, especially during the night.
5. That leaves us the option of a bedroom. People often watch TV in their bedrooms at night as a form of relaxation or to fall asleep. 
6. Therefore, the most accurate answer would be D. bedroom, because it's the most likely place for a television to be used at night.
So the final answer is D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: 1. The phrase "The knowledge was expensive to get" implies a place where extensive and high-level learning occurs, often requiring significant financial investment.
2. A book (choice B), could contain expensive knowledge, but the cost of a book is generally not high compared to other means of obtaining knowledge.
3. A field (choice C) typically refers to "field of study" or on the job training, this could be seen as costly in terms of time and commitment, but it's not inherently expensive.
4. Meetings (choice D) could present costly information if it's a high-end conference or seminar, but again meetings in general do not require a significant financial investment.
5. A class (choice E) might be expensive depending on the course or program, but a class is typically part of a larger educational institution or system.
6. Out of all the choices, a university (choice A) is the most likely to require a substantial financial investment. This is due to high tuition costs, cost of books, housing fees, and other related expenses.
7. Therefore, the answer is A: University.
So the final answer is A: university

Input: Where are small grapes likely to be found? Choices:  A: lunch box B: food store C: wine country D: kitchen E: raisins
Output: 1. The question is asking for a location where small grapes are likely found.
2. Looking at the answer choices, we have a lunch box, a food store, wine country, a kitchen and raisins.
3. Think about the context in which small grapes specifically can be seen.
4. Small grapes could exist in a lunch box, a food store, or a kitchen, but there's no specification in these places for the grapes to be small.
5. But in the case of wine country, wine grapes are typically smaller in size than the grapes typically eaten as fruit.
6. Raisins are not a location but are dried grapes. 
7. Therefore, the most likely answer would be wine country (C) where small grapes are specifically cultivated for wine production.
So the final answer is C: wine country

Input: The painter explained how he never achieved a flawless portrait, he said this was because all people are what? Choices:  A: imperfect B: disfigured C: damaged D: flawed E: defective
Output: 1. The question is asking for a word from the choices that could describe the reason why the painter never achieved a flawless portrait.
2. As per the painter's explanation, the reason is related to the inherent quality of people.
3. The choices offer various negative characteristics that could potentially describe people, which include being imperfect, disfigured, damaged, flawed, or defective.
4. The words 'disfigured', 'damaged', 'defective' could all be considered to mean 'broken' or 'wounded'. It's not in line with the painter's statement which intended to imply a natural perceived imperfection rather than a severe condition.
5. The word 'flawed' could also apply but it is more often used to describe faults or mistakes.
6. The word 'imperfect' best fits the explanation as it refers to the natural shortcomings or limitations that all people inherently have.
7. Therefore, the answer is A: imperfect.
So the final answer is A: imperfect

Input: If you're betting with a shark, where are you likely playing? Choices:  A: aquarium B: mediterranean sea C: south pacific D: pool hall E: pacific ocean
Output: 1. The term 'betting with a shark' is a slang term. 
2. This term does not refer to the animal shark, but to an experienced gambler who wins consistently, often in pool or billiards games.
3. Thus, it's necessary to identify which of the options provided is a place where this type of activity could happen. 
4. The options aquarium, mediterranean sea, south pacific and pacific ocean - while they could contain the animal species'shark' - do not relate to the context of gambling or pool games. 
5. The only option left and that fits the context of gambling specifically is 'pool hall', which is commonly associated with pool or billiards games. 
6. Therefore, the answer is D: pool hall.
So the final answer is D: pool hall

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i7-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|█▍                                                                     | 1/50 [01:23<1:08:03, 83.34s/it]Evaluating commonsenseqa :   4%|██▊                                                                    | 2/50 [02:45<1:06:00, 82.50s/it]Evaluating commonsenseqa :   6%|████▎                                                                  | 3/50 [04:07<1:04:37, 82.50s/it]Evaluating commonsenseqa :   8%|█████▋                                                                 | 4/50 [05:29<1:03:08, 82.37s/it]Evaluating commonsenseqa :  10%|███████                                                                | 5/50 [06:52<1:01:47, 82.38s/it]Evaluating commonsenseqa :  12%|████████▌                                                              | 6/50 [08:15<1:00:35, 82.63s/it]Evaluating commonsenseqa :  14%|██████████▏                                                              | 7/50 [09:37<59:11, 82.59s/it]Evaluating commonsenseqa :  16%|███████████▋                                                             | 8/50 [10:59<57:38, 82.35s/it]Evaluating commonsenseqa :  18%|█████████████▏                                                           | 9/50 [12:22<56:21, 82.48s/it]Evaluating commonsenseqa :  20%|██████████████▍                                                         | 10/50 [13:44<54:50, 82.25s/it]Evaluating commonsenseqa :  22%|███████████████▊                                                        | 11/50 [15:06<53:33, 82.39s/it]Evaluating commonsenseqa :  24%|█████████████████▎                                                      | 12/50 [16:29<52:08, 82.32s/it]Evaluating commonsenseqa :  26%|██████████████████▋                                                     | 13/50 [17:52<51:01, 82.74s/it]Evaluating commonsenseqa :  28%|████████████████████▏                                                   | 14/50 [19:16<49:45, 82.94s/it]Evaluating commonsenseqa :  30%|█████████████████████▌                                                  | 15/50 [20:38<48:15, 82.74s/it]Evaluating commonsenseqa :  32%|███████████████████████                                                 | 16/50 [22:00<46:41, 82.39s/it]Evaluating commonsenseqa :  34%|████████████████████████▍                                               | 17/50 [23:22<45:15, 82.27s/it]Evaluating commonsenseqa :  36%|█████████████████████████▉                                              | 18/50 [24:44<43:50, 82.21s/it]Evaluating commonsenseqa :  38%|███████████████████████████▎                                            | 19/50 [26:06<42:28, 82.21s/it]Evaluating commonsenseqa :  40%|████████████████████████████▊                                           | 20/50 [27:29<41:13, 82.45s/it]Evaluating commonsenseqa :  42%|██████████████████████████████▏                                         | 21/50 [28:51<39:44, 82.23s/it]Evaluating commonsenseqa :  44%|███████████████████████████████▋                                        | 22/50 [30:13<38:21, 82.19s/it]Evaluating commonsenseqa :  46%|█████████████████████████████████                                       | 23/50 [31:35<36:55, 82.07s/it]Evaluating commonsenseqa :  48%|██████████████████████████████████▌                                     | 24/50 [32:58<35:42, 82.42s/it]Evaluating commonsenseqa :  50%|████████████████████████████████████                                    | 25/50 [34:19<34:13, 82.16s/it]Evaluating commonsenseqa :  52%|█████████████████████████████████████▍                                  | 26/50 [35:41<32:51, 82.15s/it]Evaluating commonsenseqa :  54%|██████████████████████████████████████▉                                 | 27/50 [37:05<31:37, 82.50s/it]Evaluating commonsenseqa :  56%|████████████████████████████████████████▎                               | 28/50 [38:27<30:15, 82.53s/it]Evaluating commonsenseqa :  58%|█████████████████████████████████████████▊                              | 29/50 [39:50<28:54, 82.58s/it]Evaluating commonsenseqa :  60%|███████████████████████████████████████████▏                            | 30/50 [41:11<27:23, 82.17s/it]Evaluating commonsenseqa :  62%|████████████████████████████████████████████▋                           | 31/50 [42:34<26:02, 82.24s/it]Evaluating commonsenseqa :  64%|██████████████████████████████████████████████                          | 32/50 [43:56<24:40, 82.26s/it]Evaluating commonsenseqa :  66%|███████████████████████████████████████████████▌                        | 33/50 [45:18<23:17, 82.22s/it]Evaluating commonsenseqa :  68%|████████████████████████████████████████████████▉                       | 34/50 [46:41<21:56, 82.29s/it]Evaluating commonsenseqa :  70%|██████████████████████████████████████████████████▍                     | 35/50 [48:03<20:36, 82.41s/it]Evaluating commonsenseqa :  72%|███████████████████████████████████████████████████▊                    | 36/50 [49:26<19:13, 82.40s/it]Evaluating commonsenseqa :  74%|█████████████████████████████████████████████████████▎                  | 37/50 [50:37<17:08, 79.10s/it]Evaluating commonsenseqa :  76%|██████████████████████████████████████████████████████▋                 | 38/50 [52:00<16:04, 80.33s/it]Evaluating commonsenseqa :  78%|████████████████████████████████████████████████████████▏               | 39/50 [53:23<14:50, 80.92s/it]Evaluating commonsenseqa :  80%|█████████████████████████████████████████████████████████▌              | 40/50 [54:45<13:33, 81.37s/it]Evaluating commonsenseqa :  82%|███████████████████████████████████████████████████████████             | 41/50 [56:08<12:16, 81.84s/it]Evaluating commonsenseqa :  84%|████████████████████████████████████████████████████████████▍           | 42/50 [57:30<10:54, 81.85s/it]Evaluating commonsenseqa :  86%|█████████████████████████████████████████████████████████████▉          | 43/50 [58:53<09:35, 82.19s/it]Evaluating commonsenseqa :  88%|█████████████████████████████████████████████████████████████▌        | 44/50 [1:00:16<08:14, 82.36s/it]Evaluating commonsenseqa :  90%|███████████████████████████████████████████████████████████████       | 45/50 [1:01:38<06:51, 82.37s/it]Evaluating commonsenseqa :  92%|████████████████████████████████████████████████████████████████▍     | 46/50 [1:03:00<05:29, 82.34s/it]Evaluating commonsenseqa :  94%|█████████████████████████████████████████████████████████████████▊    | 47/50 [1:04:23<04:07, 82.57s/it]Evaluating commonsenseqa :  96%|███████████████████████████████████████████████████████████████████▏  | 48/50 [1:05:45<02:44, 82.44s/it]Evaluating commonsenseqa :  98%|████████████████████████████████████████████████████████████████████▌ | 49/50 [1:07:07<01:22, 82.23s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:08:29<00:00, 82.18s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████████████████████████████████████████| 50/50 [1:08:29<00:00, 82.19s/it]
name: commonsenseqa | avg. gen lenth: 418.628 | time: 4110.270145893097s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 13:59:56,694] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 13:59:56,759] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 13:59:56,761] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 13:59:56,791] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 8
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9733 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████████████████████████████████████████| 9733/9733 [00:00<00:00, 300971.42it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:06<00:06,  6.92s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:06<00:06,  6.98s/it]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:06<00:06,  6.98s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.07s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.50s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.11s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.54s/it]
[2023-08-30 14:00:07,394] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.11s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.54s/it]
 > number of parameters: 6738415616
[2023-08-30 14:00:07,467] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 14:00:07,579] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 1/2 [00:10<00:10, 10.94s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:13<00:00,  5.74s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:13<00:00,  6.52s/it]
[2023-08-30 14:00:11,449] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 14:00:12,104] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 14:00:12,106] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3b361bd300>
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 14:00:12,109] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. The sentence implies that the machine was quite complex or complicated. 
2. We need to find a word that could be used to refer to a complex machine.
3. Looking at the choices, "box" would not be correct because it is not typically used to describe something intricate or complex.
4. Similarly, "wash dishes" is incorrect as it's an action, not a descriptor.
5. While "appliance" and "implement" are words used to describe types of machines, they don't specifically describe a machine as being intricate.
6. Only the word "apparatus" fits well, as it can be used to refer to a complex equipment or machinery.
7. Therefore, the answer is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: 1. The question asks where you get petrol, which requires an understanding of what petrol is used for and where it is typically acquired.
2. Petrol is a type of fuel used to power internal combustion engines, which are commonly found in cars and other types of vehicles.
3. Looking at the choices, we can eliminate A and C because "burn hot" does not align with the concept of obtaining petrol.
4. Choice B, "fuel tank", is also not a place where you get petrol but rather where petrol is stored after it is acquired.
5. Choice D, "car", can also be eliminated because although we put petrol in a car, we do not obtain petrol from a car.
6. Through process of elimination, we are left with choice E, "gas station".
7. A gas station is where you generally go to acquire petrol for your car.
8. So the best answer to the question "Where do you get petrol?" would be E: gas station.
So the final answer is E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: Step 1: The question is asking for a location where a television is typically used at night.
Step 2: Let's consider each option individually.
Step 3: Option A is a "cabinet". A cabinet is not a location where a television is typically used at night. Therefore, a cabinet is not the correct answer.
Step 4: Next are options B and C, which are "house" and "apartment". While a television can certainly be used in both a house and an apartment, these are not the most specific answers because they could refer to anywhere within these buildings.
Step 5: We now consider option D, "bedroom". People often use a television in a bedroom at night before sleeping, making this a likely answer.
Step 6: The last option is E, "woods". The woods is not usually a place where people use a television, especially at night.
Step 7: Comparing all the options, the best and most specific answer is D, "bedroom". So, a television used at night might be in a bedroom.
So the final answer is D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: 1. The question is asking about the location knowledge was being gained.
2. The aspect of being "expensive" is a vital hint given in the question.
3. We know knowledge can be attained from various sources - a book, in the field, during meetings or in a class.
4. However, not all options associate with being "expensive". Books can be priced but it's not necessarily considered that high compared with other options.
5. Knowledge gained at a meeting or in the field does not typically require a large financial investment.
6. When we talk about an expensive learning environment, universities come into play as they often requires significant monetary investment in terms of tuition and other fees.
7. Therefore, the correct answer is A: university.
So the final answer is A: university

Input: Where are small grapes likely to be found? Choices:  A: lunch box B: food store C: wine country D: kitchen E: raisins
Output: 1. To solve this, you must examine each of the answer choices in relation to the original question. A lunch box, a food store, a kitchen, and raisins are all places where grapes of any size might be located. However, these options do not specify the size of the grapes.

2. The term'small grapes' can often be associated with the type of grapes used in wine making. These grapes are typically smaller than the type you'd find in a grocery store.

3. The term 'wine country' refers to regions that are famed for their vineyards and wine production. 

4. Thus option C: wine country is the place where small grapes are most likely to be found because it is the only choice that directly connects to the concept of small grapes used in wine making. 

5. Hence, the answer is C: wine country.
So the final answer is C: wine country

Input: The painter explained how he never achieved a flawless portrait, he said this was because all people are what? Choices:  A: imperfect B: disfigured C: damaged D: flawed E: defective
Output: Step 1: The question asks what the painter believes about all people, which is the reason he never achieves a flawless portrait. 
Step 2: To begin the reasoning process, we need to find a term from the choices that aligns with the notion that people are not without flaws. 
Step 3: A look at the choices presents us with the options: imperfect, disfigured, damaged, flawed, and defective. 
Step 4: Disfigured, damaged, and defective are eliminated because they imply physical or serious flaws which may not apply to all people and don't necessarily represent the common understanding of human nature. 
Step 5: Looking at the remaining options (imperfect and flawed), flawed carries a significantly negative connotation compared to imperfect. Since there is no indication from the question that the painter views all people in a negative light, we eliminate flawed. 
Final Step: Therefore, the only suitable choice that aligns with believing all people have some shortcomings but does not denote a negative connotation is "imperfect," making option A: Imperfect the correct answer.
So the final answer is A: imperfect

Input: If you're betting with a shark, where are you likely playing? Choices:  A: aquarium B: mediterranean sea C: south pacific D: pool hall E: pacific ocean
Output: 1. Understand the question and the set of answers given. It may seem that the question refers to a real shark, implying water-based environments like an aquarium, the Mediterranean Sea, South Pacific or Pacific Ocean.
2. Understand the slang term “shark.” In some contexts, especially in the field of gaming and betting, a "shark" is a slang term referring to a person who is an expert or highly skilled professional.
3. Apply the meaning of "shark" to the given options. Considering the "shark" as a skilled gamer, it can be deduced that a shark does not necessarily mean a real marine creature but someone good at games.
4. Analyze the answer choices. Among the options listed, the only place where you are likely to play or bet with a highly skilled professional or expert "shark" in gaming is D: pool hall. 
5. Therefore, the answer is D: Pool Hall. A pool hall is a place where "sharks" or skilled individuals play pool games and sometimes place bets.
So the final answer is D: pool hall

Input: Marmot's can be found in high places.  They have hands that they can use to get to the top of what? Choices:  A: outside B: hill C: jungle D: rocky hillside E: tree
Output: 1. The first step is to understand the main features of the question which talks about Marmot's ability to reach high places due to their hands.
2. As per the question, we should associate their hands with an ability to climb.
3. The next step is to identify the options that involve climbing.
4. Among the given options, 'outside' is too general and does not specify a particular place or object that a marmot could climb.
5. 'Hill' and 'rocky hillside' can be eliminated because marmots cannot use their hands to climb these surfaces due to their steepness and rough texture.
6. 'Jungle' is also eliminated because it does not indicate a specific high place that marmots can climb using their hands.
7. This leaves us with 'tree', which is an object that marmots can climb using their hands.
8. Therefore, the answer is E: Tree.
So the final answer is E: tree

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|█▍                                                                     | 1/50 [01:17<1:03:35, 77.86s/it]Evaluating commonsenseqa :   4%|██▊                                                                    | 2/50 [02:34<1:01:36, 77.02s/it]Evaluating commonsenseqa :   6%|████▎                                                                  | 3/50 [03:51<1:00:17, 76.97s/it]Evaluating commonsenseqa :   8%|█████▊                                                                   | 4/50 [05:07<58:43, 76.59s/it]