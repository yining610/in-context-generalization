PYTHONPATH=/home/ylu130/workspace/in-context-generalization
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i0-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 0
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 14:38:49,290] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:38:49,291] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:38:49,325] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:38:49,364] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 14:38:55,975] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-28 14:38:56,483] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:38:56,496] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:38:56,508] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 14:39:03,039] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:03,051] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:03,555] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:03,565] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 14:39:10,022] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:10,237] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:10,514] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:10,519] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 14:39:17,799] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:17,805] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:17,805] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:17,839] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 14:39:25,119] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:25,141] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:25,184] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:25,212] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 14:39:31,958] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:31,986] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-28 14:39:32,472] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:32,490] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 14:39:39,227] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:39,691] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:39,701] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:39,716] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 14:39:46,436] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:46,871] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:46,955] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:46,955] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 14:39:53,268] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:53,376] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:53,391] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:39:53,894] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 14:40:00,310] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:40:00,310] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:40:00,310] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-28 14:40:00,836] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 14:40:07,662] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:40:07,685] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:40:08,203] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:40:08,204] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 14:40:14,525] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:40:14,540] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:40:14,552] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-28 14:40:15,096] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 14:40:21,551] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:40:21,551] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:40:21,558] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-28 14:40:22,047] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 14:40:28,949] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:40:28,949] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-28 14:40:29,468] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:40:29,481] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 14:40:35,893] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:40:35,929] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:40:36,099] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 14:40:36,460] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 6
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i6-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                    | 0/9735 [00:00<?, ?it/s]Loading data: 100%|█████████████████████| 9735/9735 [00:00<00:00, 927020.60it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████████         | 1/2 [00:09<00:09,  9.29s/it]Loading checkpoint shards:  50%|█████████         | 1/2 [00:09<00:09,  9.51s/it]Loading checkpoint shards:  50%|█████████         | 1/2 [00:09<00:09,  9.88s/it]Loading checkpoint shards:  50%|█████████         | 1/2 [00:10<00:10, 10.55s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.40s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.99s/it]
[2023-08-28 14:40:49,694] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████| 2/2 [00:12<00:00,  5.43s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:12<00:00,  6.04s/it]
Loading checkpoint shards: 100%|██████████████████| 2/2 [00:12<00:00,  5.40s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:12<00:00,  6.07s/it]
[2023-08-28 14:40:49,836] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-28 14:40:49,886] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████| 2/2 [00:13<00:00,  5.79s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:13<00:00,  6.50s/it]
[2023-08-28 14:40:50,818] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 14:40:51,420] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-28 14:40:51,422] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2002c3f520>
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-28 14:40:51,423] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-28 14:40:51,424] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-28 14:40:51,425] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-28 14:40:51,425] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-28 14:40:51,425] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-28 14:40:51,425] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-28 14:40:51,425] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-28 14:40:51,425] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-28 14:40:51,425] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-28 14:40:51,425] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-28 14:40:51,425] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-28 14:40:51,425] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-28 14:40:51,425] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-28 14:40:51,425] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-28 14:40:51,425] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-28 14:40:51,425] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-28 14:40:51,425] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-28 14:40:51,425] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                         | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Where could you find some plumbing that would not be of use to you if you are thirsty? Choices:  A: oil refineries B: wall C: show D: own home E: water fountain
Output: A: oil refineries

Input: When a person is beginning work, what aren't they doing yet? Choices:  A: working B: resting C: tiredness D: accomplishing E: momentum
Output: D: accomplishing

Input: Where might I find pens with a company logo? Choices:  A: office B: on a pencil C: write sentences on paper D: school E: backpack
Output: A: office

Input: Billy called out to John, and listened for what? Choices:  A: silence B: response C: communication D: hanging up E: whisper
Output: B: response

Input: The lizard frightened the hiker, it's movements made what rustle? Choices:  A: garden B: trees C: books D: rocks E: bushes
Output: E: bushes

Input: The man spent big money and time maintaining his lawn, it was part of keeping up with the Joneses where? Choices:  A: front yard B: suburbia C: neighborhood D: back yard E: golf course
Output: B: suburbia

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i6-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▎                | 1/63 [00:44<46:02, 44.55s/it]Evaluating commonsenseqa :   3%|▌                | 2/63 [01:46<55:28, 54.56s/it]Evaluating commonsenseqa :   5%|▋              | 3/63 [03:32<1:18:24, 78.41s/it]Evaluating commonsenseqa :   6%|▉              | 4/63 [05:20<1:28:32, 90.04s/it]Evaluating commonsenseqa :   8%|█▏             | 5/63 [07:06<1:32:28, 95.67s/it]Evaluating commonsenseqa :  10%|█▎            | 6/63 [08:56<1:35:23, 100.41s/it]Evaluating commonsenseqa :  11%|█▌            | 7/63 [10:35<1:33:28, 100.16s/it]Evaluating commonsenseqa :  13%|█▊            | 8/63 [12:23<1:34:01, 102.58s/it]Evaluating commonsenseqa :  14%|██            | 9/63 [14:10<1:33:42, 104.12s/it]Evaluating commonsenseqa :  16%|██           | 10/63 [15:58<1:33:02, 105.33s/it]Evaluating commonsenseqa :  17%|██▍           | 11/63 [16:32<1:12:13, 83.34s/it]Evaluating commonsenseqa :  19%|██▋           | 12/63 [18:19<1:16:51, 90.43s/it]Evaluating commonsenseqa :  21%|██▉           | 13/63 [20:04<1:19:05, 94.91s/it]Evaluating commonsenseqa :  22%|███           | 14/63 [21:48<1:19:46, 97.68s/it]Evaluating commonsenseqa :  24%|███          | 15/63 [23:34<1:20:14, 100.30s/it]Evaluating commonsenseqa :  25%|███▎         | 16/63 [25:21<1:20:09, 102.34s/it]Evaluating commonsenseqa :  27%|███▊          | 17/63 [26:38<1:12:36, 94.71s/it]Evaluating commonsenseqa :  29%|████          | 18/63 [28:07<1:09:45, 93.01s/it]Evaluating commonsenseqa :  30%|████▏         | 19/63 [29:55<1:11:27, 97.44s/it]Evaluating commonsenseqa :  32%|████▏        | 20/63 [31:41<1:11:41, 100.04s/it]Evaluating commonsenseqa :  33%|████▎        | 21/63 [33:28<1:11:32, 102.21s/it]Evaluating commonsenseqa :  35%|████▌        | 22/63 [35:17<1:11:04, 104.00s/it]Evaluating commonsenseqa :  37%|████▋        | 23/63 [37:04<1:09:59, 104.98s/it]Evaluating commonsenseqa :  38%|██████          | 24/63 [37:51<56:59, 87.69s/it]Evaluating commonsenseqa :  40%|██████▎         | 25/63 [39:38<59:03, 93.26s/it]Evaluating commonsenseqa :  41%|██████▌         | 26/63 [41:23<59:45, 96.90s/it]Evaluating commonsenseqa :  43%|██████▊         | 27/63 [43:07<59:28, 99.14s/it]Evaluating commonsenseqa :  44%|██████▋        | 28/63 [44:51<58:37, 100.51s/it]Evaluating commonsenseqa :  46%|██████▉        | 29/63 [46:39<58:11, 102.70s/it]Evaluating commonsenseqa :  48%|███████▏       | 30/63 [48:27<57:19, 104.23s/it]Evaluating commonsenseqa :  49%|███████▍       | 31/63 [50:13<55:52, 104.78s/it]Evaluating commonsenseqa :  51%|███████▌       | 32/63 [51:58<54:14, 104.98s/it]Evaluating commonsenseqa :  52%|███████▊       | 33/63 [53:45<52:45, 105.52s/it]Evaluating commonsenseqa :  54%|████████       | 34/63 [55:31<51:06, 105.73s/it]Evaluating commonsenseqa :  56%|████████▎      | 35/63 [57:15<49:01, 105.05s/it]Evaluating commonsenseqa :  57%|████████▌      | 36/63 [59:00<47:20, 105.21s/it]Evaluating commonsenseqa :  59%|███████▋     | 37/63 [1:00:47<45:49, 105.73s/it]Evaluating commonsenseqa :  60%|████████▍     | 38/63 [1:01:47<38:19, 91.99s/it]Evaluating commonsenseqa :  62%|████████▋     | 39/63 [1:03:32<38:18, 95.76s/it]Evaluating commonsenseqa :  63%|████████▉     | 40/63 [1:05:20<38:09, 99.55s/it]Evaluating commonsenseqa :  65%|████████▍    | 41/63 [1:07:08<37:23, 101.98s/it]Evaluating commonsenseqa :  67%|█████████▎    | 42/63 [1:07:44<28:46, 82.19s/it]Evaluating commonsenseqa :  68%|█████████▌    | 43/63 [1:09:02<26:57, 80.89s/it]Evaluating commonsenseqa :  70%|█████████▊    | 44/63 [1:10:49<28:09, 88.90s/it]Evaluating commonsenseqa :  71%|██████████    | 45/63 [1:12:35<28:10, 93.89s/it]Evaluating commonsenseqa :  73%|██████████▏   | 46/63 [1:14:22<27:43, 97.83s/it]Evaluating commonsenseqa :  75%|██████████▍   | 47/63 [1:15:48<25:09, 94.33s/it]Evaluating commonsenseqa :  76%|██████████▋   | 48/63 [1:17:35<24:33, 98.24s/it]Evaluating commonsenseqa :  78%|██████████   | 49/63 [1:19:21<23:27, 100.52s/it]Evaluating commonsenseqa :  79%|██████████▎  | 50/63 [1:21:07<22:06, 102.05s/it]Evaluating commonsenseqa :  81%|██████████▌  | 51/63 [1:22:53<20:41, 103.48s/it]Evaluating commonsenseqa :  83%|███████████▌  | 52/63 [1:23:48<16:16, 88.80s/it]Evaluating commonsenseqa :  84%|███████████▊  | 53/63 [1:25:34<15:38, 93.87s/it]Evaluating commonsenseqa :  86%|████████████  | 54/63 [1:27:19<14:35, 97.23s/it]Evaluating commonsenseqa :  87%|████████████▏ | 55/63 [1:29:04<13:18, 99.76s/it]Evaluating commonsenseqa :  89%|████████████▍ | 56/63 [1:30:37<11:23, 97.69s/it]Evaluating commonsenseqa :  90%|███████████▊ | 57/63 [1:32:24<10:02, 100.34s/it]Evaluating commonsenseqa :  92%|███████████▉ | 58/63 [1:34:10<08:30, 102.03s/it]Evaluating commonsenseqa :  94%|████████████▏| 59/63 [1:35:57<06:54, 103.69s/it]Evaluating commonsenseqa :  95%|████████████▍| 60/63 [1:37:46<05:15, 105.13s/it]Evaluating commonsenseqa :  97%|████████████▌| 61/63 [1:39:31<03:30, 105.10s/it]Evaluating commonsenseqa :  98%|████████████▊| 62/63 [1:41:16<01:45, 105.20s/it]Evaluating commonsenseqa : 100%|██████████████| 63/63 [1:42:17<00:00, 91.73s/it]Evaluating commonsenseqa : 100%|██████████████| 63/63 [1:42:17<00:00, 97.41s/it]
name: commonsenseqa | avg. gen lenth: 279.912 | time: 6137.40145111084s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 16:25:50,461] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 16:25:50,475] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 16:25:50,484] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 16:25:50,499] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 7
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i7-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                    | 0/9734 [00:00<?, ?it/s]Loading data: 100%|█████████████████████| 9734/9734 [00:00<00:00, 876386.79it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████████         | 1/2 [00:07<00:07,  7.09s/it]Loading checkpoint shards:  50%|█████████         | 1/2 [00:07<00:07,  7.19s/it]Loading checkpoint shards:  50%|█████████         | 1/2 [00:07<00:07,  7.24s/it]Loading checkpoint shards:  50%|█████████         | 1/2 [00:07<00:07,  7.26s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.20s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.64s/it]
Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.23s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.68s/it]
Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.25s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.70s/it]
[2023-08-28 16:26:01,056] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.27s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.71s/it]
[2023-08-28 16:26:01,164] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-28 16:26:01,228] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 16:26:01,368] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 16:26:02,008] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-28 16:26:02,010] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-28 16:26:02,011] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-28 16:26:02,011] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-28 16:26:02,011] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-28 16:26:02,011] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-28 16:26:02,011] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-28 16:26:02,011] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-28 16:26:02,011] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-28 16:26:02,011] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-28 16:26:02,011] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-28 16:26:02,011] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4581c47610>
[2023-08-28 16:26:02,011] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-28 16:26:02,011] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-28 16:26:02,011] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-28 16:26:02,012] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-28 16:26:02,013] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-28 16:26:02,013] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                         | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Where could you find some plumbing that would not be of use to you if you are thirsty? Choices:  A: oil refineries B: wall C: show D: own home E: water fountain
Output: A: oil refineries

Input: When a person is beginning work, what aren't they doing yet? Choices:  A: working B: resting C: tiredness D: accomplishing E: momentum
Output: D: accomplishing

Input: Where might I find pens with a company logo? Choices:  A: office B: on a pencil C: write sentences on paper D: school E: backpack
Output: A: office

Input: Billy called out to John, and listened for what? Choices:  A: silence B: response C: communication D: hanging up E: whisper
Output: B: response

Input: The lizard frightened the hiker, it's movements made what rustle? Choices:  A: garden B: trees C: books D: rocks E: bushes
Output: E: bushes

Input: The man spent big money and time maintaining his lawn, it was part of keeping up with the Joneses where? Choices:  A: front yard B: suburbia C: neighborhood D: back yard E: golf course
Output: B: suburbia

Input: What would a human do if they want to get to a store that he or she can see? Choices:  A: cross road B: see around C: drink coffee D: dream dreams E: think critically
Output: A: cross road

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i7-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▏             | 1/63 [01:47<1:51:15, 107.67s/it]Evaluating commonsenseqa :   3%|▍             | 2/63 [03:32<1:47:48, 106.04s/it]Evaluating commonsenseqa :   5%|▋              | 3/63 [04:54<1:34:59, 94.98s/it]Evaluating commonsenseqa :   6%|▉              | 4/63 [06:38<1:37:07, 98.77s/it]Evaluating commonsenseqa :   8%|█             | 5/63 [08:24<1:37:54, 101.29s/it]Evaluating commonsenseqa :  10%|█▍             | 6/63 [09:30<1:24:52, 89.34s/it]Evaluating commonsenseqa :  11%|█▋             | 7/63 [11:13<1:27:31, 93.78s/it]Evaluating commonsenseqa :  13%|█▉             | 8/63 [12:56<1:28:42, 96.77s/it]Evaluating commonsenseqa :  14%|██▏            | 9/63 [14:39<1:28:38, 98.48s/it]Evaluating commonsenseqa :  16%|██▏           | 10/63 [16:05<1:23:44, 94.81s/it]Evaluating commonsenseqa :  17%|██▍           | 11/63 [17:49<1:24:31, 97.53s/it]Evaluating commonsenseqa :  19%|██▋           | 12/63 [19:32<1:24:14, 99.11s/it]Evaluating commonsenseqa :  21%|██▉           | 13/63 [21:02<1:20:16, 96.32s/it]Evaluating commonsenseqa :  22%|███           | 14/63 [22:45<1:20:30, 98.58s/it]Evaluating commonsenseqa :  24%|███          | 15/63 [24:31<1:20:34, 100.72s/it]Evaluating commonsenseqa :  25%|███▎         | 16/63 [26:13<1:19:08, 101.03s/it]Evaluating commonsenseqa :  27%|███▌         | 17/63 [27:58<1:18:17, 102.12s/it]Evaluating commonsenseqa :  29%|███▋         | 18/63 [29:39<1:16:29, 101.99s/it]Evaluating commonsenseqa :  30%|███▉         | 19/63 [31:24<1:15:26, 102.88s/it]Evaluating commonsenseqa :  32%|████▏        | 20/63 [33:07<1:13:48, 103.00s/it]Evaluating commonsenseqa :  33%|████▎        | 21/63 [34:52<1:12:24, 103.44s/it]Evaluating commonsenseqa :  35%|████▌        | 22/63 [36:36<1:10:43, 103.50s/it]Evaluating commonsenseqa :  37%|████▋        | 23/63 [38:20<1:09:08, 103.70s/it]Evaluating commonsenseqa :  38%|██████          | 24/63 [39:02<55:26, 85.28s/it]Evaluating commonsenseqa :  40%|██████▎         | 25/63 [40:21<52:45, 83.30s/it]Evaluating commonsenseqa :  41%|██████▌         | 26/63 [42:06<55:24, 89.86s/it]Evaluating commonsenseqa :  43%|██████▊         | 27/63 [43:50<56:33, 94.26s/it]Evaluating commonsenseqa :  44%|███████         | 28/63 [45:16<53:25, 91.58s/it]Evaluating commonsenseqa :  46%|███████▎        | 29/63 [46:58<53:38, 94.66s/it]Evaluating commonsenseqa :  48%|███████▌        | 30/63 [48:45<54:11, 98.53s/it]Evaluating commonsenseqa :  49%|███████▊        | 31/63 [49:25<43:11, 81.00s/it]Evaluating commonsenseqa :  51%|████████▏       | 32/63 [51:10<45:28, 88.02s/it]Evaluating commonsenseqa :  52%|████████▍       | 33/63 [52:55<46:39, 93.31s/it]Evaluating commonsenseqa :  54%|████████▋       | 34/63 [54:39<46:37, 96.48s/it]Evaluating commonsenseqa :  56%|████████▉       | 35/63 [56:24<46:08, 98.87s/it]Evaluating commonsenseqa :  57%|████████▌      | 36/63 [58:07<45:08, 100.33s/it]Evaluating commonsenseqa :  59%|████████▊      | 37/63 [59:51<43:57, 101.45s/it]Evaluating commonsenseqa :  60%|███████▊     | 38/63 [1:01:35<42:34, 102.20s/it]Evaluating commonsenseqa :  62%|████████     | 39/63 [1:03:20<41:08, 102.86s/it]Evaluating commonsenseqa :  63%|████████▉     | 40/63 [1:04:17<34:08, 89.05s/it]Evaluating commonsenseqa :  65%|█████████     | 41/63 [1:05:42<32:12, 87.83s/it]Evaluating commonsenseqa :  67%|█████████▎    | 42/63 [1:07:27<32:32, 92.96s/it]Evaluating commonsenseqa :  68%|█████████▌    | 43/63 [1:09:09<31:59, 95.95s/it]Evaluating commonsenseqa :  70%|█████████▊    | 44/63 [1:10:52<31:03, 98.08s/it]Evaluating commonsenseqa :  71%|██████████    | 45/63 [1:12:36<29:57, 99.84s/it]Evaluating commonsenseqa :  73%|█████████▍   | 46/63 [1:14:21<28:40, 101.22s/it]Evaluating commonsenseqa :  75%|█████████▋   | 47/63 [1:16:06<27:17, 102.32s/it]Evaluating commonsenseqa :  76%|██████████▋   | 48/63 [1:17:39<24:54, 99.65s/it]Evaluating commonsenseqa :  78%|██████████   | 49/63 [1:19:25<23:41, 101.51s/it]Evaluating commonsenseqa :  79%|██████████▎  | 50/63 [1:21:09<22:08, 102.20s/it]Evaluating commonsenseqa :  81%|██████████▌  | 51/63 [1:22:51<20:25, 102.13s/it]Evaluating commonsenseqa :  83%|██████████▋  | 52/63 [1:24:35<18:50, 102.82s/it]Evaluating commonsenseqa :  84%|██████████▉  | 53/63 [1:26:19<17:09, 103.00s/it]Evaluating commonsenseqa :  86%|████████████  | 54/63 [1:27:22<13:39, 91.11s/it]Evaluating commonsenseqa :  87%|████████████▏ | 55/63 [1:29:06<12:39, 94.90s/it]Evaluating commonsenseqa :  89%|████████████▍ | 56/63 [1:30:50<11:23, 97.68s/it]Evaluating commonsenseqa :  90%|████████████▋ | 57/63 [1:32:35<09:58, 99.81s/it]Evaluating commonsenseqa :  92%|███████████▉ | 58/63 [1:34:17<08:23, 100.65s/it]Evaluating commonsenseqa :  94%|█████████████ | 59/63 [1:35:12<05:48, 87.02s/it]Evaluating commonsenseqa :  95%|█████████████▎| 60/63 [1:36:22<04:05, 81.74s/it]Evaluating commonsenseqa :  97%|█████████████▌| 61/63 [1:38:06<02:56, 88.43s/it]Evaluating commonsenseqa :  98%|█████████████▊| 62/63 [1:39:51<01:33, 93.28s/it]Evaluating commonsenseqa : 100%|██████████████| 63/63 [1:40:48<00:00, 82.57s/it]Evaluating commonsenseqa : 100%|██████████████| 63/63 [1:40:48<00:00, 96.01s/it]
name: commonsenseqa | avg. gen lenth: 302.812 | time: 6048.930378437042s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 18:12:14,385] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 18:12:14,418] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 18:12:14,428] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 18:12:14,436] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 8
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                    | 0/9733 [00:00<?, ?it/s]Loading data: 100%|█████████████████████| 9733/9733 [00:00<00:00, 848150.10it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████████         | 1/2 [00:06<00:06,  6.78s/it]Loading checkpoint shards:  50%|█████████         | 1/2 [00:06<00:06,  6.87s/it]Loading checkpoint shards:  50%|█████████         | 1/2 [00:07<00:07,  7.27s/it]Loading checkpoint shards:  50%|█████████         | 1/2 [00:08<00:08,  8.82s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:08<00:00,  4.02s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:08<00:00,  4.44s/it]
Loading checkpoint shards: 100%|██████████████████| 2/2 [00:08<00:00,  4.04s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:08<00:00,  4.46s/it]
[2023-08-28 18:12:24,606] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-28 18:12:24,674] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.32s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.76s/it]
[2023-08-28 18:12:25,267] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████| 2/2 [00:10<00:00,  4.77s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:10<00:00,  5.38s/it]
[2023-08-28 18:12:26,500] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 18:12:27,139] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-28 18:12:27,141] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-28 18:12:27,142] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-28 18:12:27,142] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-28 18:12:27,142] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-28 18:12:27,142] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb07743b550>
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-28 18:12:27,143] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-28 18:12:27,144] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-28 18:12:27,145] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-28 18:12:27,145] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-28 18:12:27,145] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-28 18:12:27,145] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-28 18:12:27,145] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-28 18:12:27,145] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-28 18:12:27,145] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-28 18:12:27,145] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-28 18:12:27,145] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-28 18:12:27,145] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-28 18:12:27,145] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                         | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Where could you find some plumbing that would not be of use to you if you are thirsty? Choices:  A: oil refineries B: wall C: show D: own home E: water fountain
Output: A: oil refineries

Input: When a person is beginning work, what aren't they doing yet? Choices:  A: working B: resting C: tiredness D: accomplishing E: momentum
Output: D: accomplishing

Input: Where might I find pens with a company logo? Choices:  A: office B: on a pencil C: write sentences on paper D: school E: backpack
Output: A: office

Input: Billy called out to John, and listened for what? Choices:  A: silence B: response C: communication D: hanging up E: whisper
Output: B: response

Input: The lizard frightened the hiker, it's movements made what rustle? Choices:  A: garden B: trees C: books D: rocks E: bushes
Output: E: bushes

Input: The man spent big money and time maintaining his lawn, it was part of keeping up with the Joneses where? Choices:  A: front yard B: suburbia C: neighborhood D: back yard E: golf course
Output: B: suburbia

Input: What would a human do if they want to get to a store that he or she can see? Choices:  A: cross road B: see around C: drink coffee D: dream dreams E: think critically
Output: A: cross road

Input: Where would you grab an object contained by a doorway? Choices:  A: television B: control panel C: opening doors D: doorknob E: doorway
Output: E: doorway

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▏              | 1/63 [01:07<1:10:12, 67.94s/it]Evaluating commonsenseqa :   3%|▍              | 2/63 [02:52<1:30:47, 89.31s/it]Evaluating commonsenseqa :   5%|▋              | 3/63 [04:37<1:36:40, 96.68s/it]Evaluating commonsenseqa :   6%|▉              | 4/63 [06:19<1:37:12, 98.85s/it]Evaluating commonsenseqa :   8%|█▏             | 5/63 [08:01<1:36:24, 99.74s/it]Evaluating commonsenseqa :  10%|█▍             | 6/63 [08:59<1:21:32, 85.83s/it]Evaluating commonsenseqa :  11%|█▋             | 7/63 [10:38<1:23:50, 89.84s/it]Evaluating commonsenseqa :  13%|█▉             | 8/63 [12:22<1:26:43, 94.62s/it]Evaluating commonsenseqa :  14%|██▏            | 9/63 [14:07<1:27:50, 97.60s/it]Evaluating commonsenseqa :  16%|██▏           | 10/63 [15:49<1:27:24, 98.96s/it]Evaluating commonsenseqa :  17%|██▍           | 11/63 [17:31<1:26:39, 99.99s/it]Evaluating commonsenseqa :  19%|██▋           | 12/63 [18:47<1:18:48, 92.72s/it]Evaluating commonsenseqa :  21%|██▉           | 13/63 [20:29<1:19:33, 95.47s/it]Evaluating commonsenseqa :  22%|███           | 14/63 [22:11<1:19:41, 97.58s/it]Evaluating commonsenseqa :  24%|███▎          | 15/63 [23:54<1:19:13, 99.02s/it]Evaluating commonsenseqa :  25%|███▌          | 16/63 [24:46<1:06:36, 85.03s/it]Evaluating commonsenseqa :  27%|███▊          | 17/63 [26:28<1:09:08, 90.19s/it]Evaluating commonsenseqa :  29%|████          | 18/63 [28:06<1:09:20, 92.45s/it]Evaluating commonsenseqa :  30%|████▏         | 19/63 [29:48<1:09:53, 95.32s/it]Evaluating commonsenseqa :  32%|████▍         | 20/63 [31:32<1:10:10, 97.92s/it]Evaluating commonsenseqa :  33%|████▋         | 21/63 [32:39<1:02:05, 88.71s/it]Evaluating commonsenseqa :  35%|████▉         | 22/63 [34:23<1:03:42, 93.22s/it]Evaluating commonsenseqa :  37%|█████         | 23/63 [36:06<1:04:05, 96.14s/it]Evaluating commonsenseqa :  38%|█████▎        | 24/63 [37:48<1:03:34, 97.81s/it]Evaluating commonsenseqa :  40%|█████▌        | 25/63 [39:28<1:02:22, 98.49s/it]Evaluating commonsenseqa :  41%|█████▊        | 26/63 [41:09<1:01:18, 99.41s/it]Evaluating commonsenseqa :  43%|█████▌       | 27/63 [42:54<1:00:34, 100.95s/it]Evaluating commonsenseqa :  44%|██████▋        | 28/63 [44:36<59:09, 101.42s/it]Evaluating commonsenseqa :  46%|██████▉        | 29/63 [46:21<58:02, 102.43s/it]Evaluating commonsenseqa :  48%|███████▏       | 30/63 [48:03<56:17, 102.34s/it]Evaluating commonsenseqa :  49%|███████▍       | 31/63 [49:45<54:27, 102.12s/it]Evaluating commonsenseqa :  51%|███████▌       | 32/63 [51:29<53:08, 102.85s/it]Evaluating commonsenseqa :  52%|███████▊       | 33/63 [53:13<51:30, 103.03s/it]Evaluating commonsenseqa :  54%|████████       | 34/63 [54:49<48:44, 100.86s/it]Evaluating commonsenseqa :  56%|████████▎      | 35/63 [56:31<47:12, 101.16s/it]Evaluating commonsenseqa :  57%|████████▌      | 36/63 [58:12<45:37, 101.39s/it]Evaluating commonsenseqa :  59%|█████████▍      | 37/63 [59:25<40:09, 92.68s/it]Evaluating commonsenseqa :  60%|████████▍     | 38/63 [1:00:18<33:39, 80.78s/it]Evaluating commonsenseqa :  62%|████████▋     | 39/63 [1:02:00<34:55, 87.31s/it]Evaluating commonsenseqa :  63%|████████▉     | 40/63 [1:03:42<35:04, 91.49s/it]Evaluating commonsenseqa :  65%|█████████     | 41/63 [1:05:24<34:43, 94.69s/it]Evaluating commonsenseqa :  67%|█████████▎    | 42/63 [1:07:08<34:07, 97.49s/it]Evaluating commonsenseqa :  68%|█████████▌    | 43/63 [1:08:09<28:52, 86.64s/it]Evaluating commonsenseqa :  70%|█████████▊    | 44/63 [1:09:53<29:03, 91.78s/it]Evaluating commonsenseqa :  71%|██████████    | 45/63 [1:11:33<28:18, 94.38s/it]Evaluating commonsenseqa :  73%|██████████▏   | 46/63 [1:13:15<27:23, 96.68s/it]Evaluating commonsenseqa :  75%|██████████▍   | 47/63 [1:14:47<25:24, 95.28s/it]Evaluating commonsenseqa :  76%|██████████▋   | 48/63 [1:16:30<24:23, 97.55s/it]Evaluating commonsenseqa :  78%|██████████▉   | 49/63 [1:18:15<23:16, 99.78s/it]Evaluating commonsenseqa :  79%|██████████▎  | 50/63 [1:19:59<21:53, 101.03s/it]Evaluating commonsenseqa :  81%|██████████▌  | 51/63 [1:21:42<20:18, 101.53s/it]Evaluating commonsenseqa :  83%|██████████▋  | 52/63 [1:23:24<18:37, 101.58s/it]Evaluating commonsenseqa :  84%|██████████▉  | 53/63 [1:25:08<17:03, 102.33s/it]Evaluating commonsenseqa :  86%|███████████▏ | 54/63 [1:26:52<15:25, 102.80s/it]Evaluating commonsenseqa :  87%|███████████▎ | 55/63 [1:28:32<13:37, 102.15s/it]Evaluating commonsenseqa :  89%|████████████▍ | 56/63 [1:29:29<10:18, 88.42s/it]Evaluating commonsenseqa :  90%|████████████▋ | 57/63 [1:31:12<09:16, 92.82s/it]Evaluating commonsenseqa :  92%|████████████▉ | 58/63 [1:32:38<07:35, 91.01s/it]Evaluating commonsenseqa :  94%|█████████████ | 59/63 [1:34:22<06:19, 94.84s/it]Evaluating commonsenseqa :  95%|█████████████▎| 60/63 [1:36:09<04:55, 98.37s/it]Evaluating commonsenseqa :  97%|█████████████▌| 61/63 [1:37:51<03:18, 99.42s/it]Evaluating commonsenseqa :  98%|████████████▊| 62/63 [1:39:34<01:40, 100.52s/it]Evaluating commonsenseqa : 100%|██████████████| 63/63 [1:40:30<00:00, 87.24s/it]Evaluating commonsenseqa : 100%|██████████████| 63/63 [1:40:30<00:00, 95.72s/it]
name: commonsenseqa | avg. gen lenth: 279.672 | time: 6030.823568820953s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 19:54:28,028] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 19:54:28,043] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 19:54:28,080] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 19:54:28,085] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 9
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i9-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                    | 0/9732 [00:00<?, ?it/s]Loading data: 100%|█████████████████████| 9732/9732 [00:00<00:00, 795598.30it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████████         | 1/2 [00:07<00:07,  7.07s/it]Loading checkpoint shards:  50%|█████████         | 1/2 [00:07<00:07,  7.97s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.10s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.55s/it]
[2023-08-28 19:54:38,570] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████| 2/2 [00:10<00:00,  4.49s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:10<00:00,  5.01s/it]
[2023-08-28 19:54:39,687] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards:  50%|█████████         | 1/2 [00:12<00:12, 12.08s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:14<00:00,  6.63s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:14<00:00,  7.45s/it]
 > number of parameters: 6738415616
[2023-08-28 19:54:44,517] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards:  50%|█████████         | 1/2 [00:15<00:15, 15.90s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:17<00:00,  7.74s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:17<00:00,  8.96s/it]
[2023-08-28 19:54:47,596] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 19:54:48,300] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-28 19:54:48,302] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-28 19:54:48,302] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-28 19:54:48,302] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-28 19:54:48,302] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-28 19:54:48,302] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff72a263640>
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-28 19:54:48,303] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-28 19:54:48,304] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-28 19:54:48,305] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-28 19:54:48,305] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-28 19:54:48,305] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-28 19:54:48,305] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                         | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Where could you find some plumbing that would not be of use to you if you are thirsty? Choices:  A: oil refineries B: wall C: show D: own home E: water fountain
Output: A: oil refineries

Input: When a person is beginning work, what aren't they doing yet? Choices:  A: working B: resting C: tiredness D: accomplishing E: momentum
Output: D: accomplishing

Input: Where might I find pens with a company logo? Choices:  A: office B: on a pencil C: write sentences on paper D: school E: backpack
Output: A: office

Input: Billy called out to John, and listened for what? Choices:  A: silence B: response C: communication D: hanging up E: whisper
Output: B: response

Input: The lizard frightened the hiker, it's movements made what rustle? Choices:  A: garden B: trees C: books D: rocks E: bushes
Output: E: bushes

Input: The man spent big money and time maintaining his lawn, it was part of keeping up with the Joneses where? Choices:  A: front yard B: suburbia C: neighborhood D: back yard E: golf course
Output: B: suburbia

Input: What would a human do if they want to get to a store that he or she can see? Choices:  A: cross road B: see around C: drink coffee D: dream dreams E: think critically
Output: A: cross road

Input: Where would you grab an object contained by a doorway? Choices:  A: television B: control panel C: opening doors D: doorknob E: doorway
Output: E: doorway

Input: Sarah knew she was committing perjury, so there was a lot of what feeling between her and the prosecutor? Choices:  A: arrest B: tension C: shame D: attraction E: embarrassment
Output: B: tension

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i9-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▏             | 1/63 [01:42<1:46:21, 102.93s/it]Evaluating commonsenseqa :   3%|▍              | 2/63 [02:47<1:21:49, 80.49s/it]Evaluating commonsenseqa :   5%|▋              | 3/63 [04:30<1:30:44, 90.74s/it]Evaluating commonsenseqa :   6%|▉              | 4/63 [05:53<1:26:03, 87.52s/it]Evaluating commonsenseqa :   8%|█▏             | 5/63 [07:34<1:29:17, 92.37s/it]Evaluating commonsenseqa :  10%|█▍             | 6/63 [09:14<1:30:20, 95.10s/it]Evaluating commonsenseqa :  11%|█▋             | 7/63 [10:56<1:30:50, 97.34s/it]Evaluating commonsenseqa :  13%|█▉             | 8/63 [12:36<1:30:04, 98.27s/it]Evaluating commonsenseqa :  14%|██▏            | 9/63 [14:15<1:28:37, 98.47s/it]Evaluating commonsenseqa :  16%|██▏           | 10/63 [15:55<1:27:27, 99.02s/it]Evaluating commonsenseqa :  17%|██▍           | 11/63 [17:36<1:26:08, 99.40s/it]Evaluating commonsenseqa :  19%|██▍          | 12/63 [19:18<1:25:16, 100.33s/it]Evaluating commonsenseqa :  21%|██▉           | 13/63 [20:35<1:17:43, 93.26s/it]Evaluating commonsenseqa :  22%|███           | 14/63 [22:14<1:17:33, 94.98s/it]Evaluating commonsenseqa :  24%|███▎          | 15/63 [23:37<1:13:00, 91.25s/it]Evaluating commonsenseqa :  25%|███▌          | 16/63 [25:15<1:13:11, 93.43s/it]Evaluating commonsenseqa :  27%|███▊          | 17/63 [26:54<1:12:51, 95.03s/it]Evaluating commonsenseqa :  29%|████          | 18/63 [28:34<1:12:21, 96.48s/it]Evaluating commonsenseqa :  30%|████▏         | 19/63 [30:12<1:11:13, 97.13s/it]Evaluating commonsenseqa :  32%|████▍         | 20/63 [31:53<1:10:15, 98.03s/it]Evaluating commonsenseqa :  33%|████▋         | 21/63 [33:31<1:08:45, 98.22s/it]Evaluating commonsenseqa :  35%|████▉         | 22/63 [35:11<1:07:31, 98.82s/it]Evaluating commonsenseqa :  37%|█████         | 23/63 [36:53<1:06:22, 99.56s/it]Evaluating commonsenseqa :  38%|████▉        | 24/63 [38:34<1:05:00, 100.02s/it]Evaluating commonsenseqa :  40%|█████▌        | 25/63 [40:13<1:03:12, 99.79s/it]Evaluating commonsenseqa :  41%|██████▌         | 26/63 [41:31<57:24, 93.09s/it]Evaluating commonsenseqa :  43%|██████▊         | 27/63 [43:13<57:31, 95.88s/it]Evaluating commonsenseqa :  44%|███████         | 28/63 [44:52<56:31, 96.90s/it]Evaluating commonsenseqa :  46%|███████▎        | 29/63 [45:52<48:31, 85.64s/it]Evaluating commonsenseqa :  48%|███████▌        | 30/63 [46:49<42:30, 77.29s/it]Evaluating commonsenseqa :  49%|███████▊        | 31/63 [48:29<44:43, 83.85s/it]Evaluating commonsenseqa :  51%|████████▏       | 32/63 [50:10<46:06, 89.24s/it]Evaluating commonsenseqa :  52%|████████▍       | 33/63 [51:49<46:00, 92.00s/it]Evaluating commonsenseqa :  54%|████████▋       | 34/63 [52:41<38:38, 79.95s/it]Evaluating commonsenseqa :  56%|████████▉       | 35/63 [54:19<39:51, 85.42s/it]Evaluating commonsenseqa :  57%|█████████▏      | 36/63 [55:57<40:13, 89.37s/it]Evaluating commonsenseqa :  59%|█████████▍      | 37/63 [57:38<40:07, 92.61s/it]Evaluating commonsenseqa :  60%|█████████▋      | 38/63 [59:18<39:30, 94.83s/it]Evaluating commonsenseqa :  62%|████████▋     | 39/63 [1:00:57<38:31, 96.31s/it]Evaluating commonsenseqa :  63%|████████▉     | 40/63 [1:01:58<32:51, 85.70s/it]Evaluating commonsenseqa :  65%|█████████     | 41/63 [1:03:38<32:56, 89.86s/it]Evaluating commonsenseqa :  67%|█████████▎    | 42/63 [1:05:17<32:27, 92.74s/it]Evaluating commonsenseqa :  68%|█████████▌    | 43/63 [1:07:00<31:55, 95.78s/it]Evaluating commonsenseqa :  70%|█████████▊    | 44/63 [1:08:40<30:42, 96.96s/it]Evaluating commonsenseqa :  71%|██████████    | 45/63 [1:10:19<29:18, 97.69s/it]Evaluating commonsenseqa :  73%|██████████▏   | 46/63 [1:11:59<27:48, 98.14s/it]Evaluating commonsenseqa :  75%|██████████▍   | 47/63 [1:13:11<24:05, 90.34s/it]Evaluating commonsenseqa :  76%|██████████▋   | 48/63 [1:14:50<23:17, 93.15s/it]Evaluating commonsenseqa :  78%|██████████▉   | 49/63 [1:16:31<22:15, 95.42s/it]Evaluating commonsenseqa :  79%|███████████   | 50/63 [1:17:28<18:12, 84.01s/it]Evaluating commonsenseqa :  81%|███████████▎  | 51/63 [1:19:00<17:16, 86.40s/it]Evaluating commonsenseqa :  83%|███████████▌  | 52/63 [1:20:42<16:39, 90.83s/it]Evaluating commonsenseqa :  84%|███████████▊  | 53/63 [1:22:20<15:30, 93.06s/it]Evaluating commonsenseqa :  86%|████████████  | 54/63 [1:23:59<14:13, 94.82s/it]Evaluating commonsenseqa :  87%|████████████▏ | 55/63 [1:25:38<12:48, 96.00s/it]Evaluating commonsenseqa :  89%|████████████▍ | 56/63 [1:27:19<11:23, 97.57s/it]Evaluating commonsenseqa :  90%|████████████▋ | 57/63 [1:28:57<09:47, 97.88s/it]Evaluating commonsenseqa :  92%|████████████▉ | 58/63 [1:30:41<08:17, 99.47s/it]Evaluating commonsenseqa :  94%|████████████▏| 59/63 [1:32:22<06:40, 100.01s/it]Evaluating commonsenseqa :  95%|█████████████▎| 60/63 [1:33:24<04:26, 88.73s/it]Evaluating commonsenseqa :  97%|█████████████▌| 61/63 [1:35:03<03:03, 91.68s/it]Evaluating commonsenseqa :  98%|█████████████▊| 62/63 [1:36:36<01:31, 92.00s/it]Evaluating commonsenseqa : 100%|██████████████| 63/63 [1:36:58<00:00, 71.20s/it]Evaluating commonsenseqa : 100%|██████████████| 63/63 [1:36:58<00:00, 92.36s/it]
name: commonsenseqa | avg. gen lenth: 331.652 | time: 5819.054495573044s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 21:33:16,573] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 21:33:16,588] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 21:33:16,597] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 21:33:16,597] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 1
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                    | 0/9740 [00:00<?, ?it/s]Loading data: 100%|█████████████████████| 9740/9740 [00:00<00:00, 908560.65it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████████         | 1/2 [00:06<00:06,  6.90s/it]Loading checkpoint shards:  50%|█████████         | 1/2 [00:07<00:07,  7.02s/it]Loading checkpoint shards:  50%|█████████         | 1/2 [00:07<00:07,  7.05s/it]Loading checkpoint shards:  50%|█████████         | 1/2 [00:07<00:07,  7.07s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.12s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.54s/it]
Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.13s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.56s/it]
Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.13s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.57s/it]
[2023-08-28 21:33:26,883] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 21:33:26,936] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.14s/it]Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.58s/it]
[2023-08-28 21:33:26,980] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-28 21:33:27,111] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 21:33:27,751] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-28 21:33:27,754] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-28 21:33:27,755] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-28 21:33:27,755] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-28 21:33:27,755] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-28 21:33:27,755] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f90d208b640>
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-28 21:33:27,756] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-28 21:33:27,757] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-28 21:33:27,758] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-28 21:33:27,758] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-28 21:33:27,758] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-28 21:33:27,758] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-28 21:33:27,758] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-28 21:33:27,758] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-28 21:33:27,758] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-28 21:33:27,758] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-28 21:33:27,758] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-28 21:33:27,758] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-28 21:33:27,758] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-28 21:33:27,758] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-28 21:33:27,758] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-28 21:33:27,758] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-28 21:33:27,758] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-28 21:33:27,758] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-28 21:33:27,758] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                         | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: 1. Analyze the question: The question asks for a type of'seller' where fabric is cut to order.
2. Consider each option in relation to fabric being cut to order. 
3. Option A: Curtains - These are a type of product, not a place where fabric is cut to order.
4. Option B: Tailor shop - This location specializes in custom fitting and modifying garments, which would require cutting fabric to order in order for each piece to be customized to the specifications of the customer.
5. Option C: Clothing store - Most clothing stores sell ready-made clothes and do not typically provide a service to cut fabric to order.
6. Option D: Sewing room - This is not a seller, but rather a location where someone might cut fabric for personal use.
7. Option E: Hardware store - This is a store that typically sells tools and building materials, not fabric or clothing.
8. Therefore, the answer is B: tailor shop as it is the only seller in the provided options that cuts fabric specific to the customer's measurements.
So the final answer is B: tailor shop

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s10-rTrue-m4096
Evaluating commonsenseqa :   2%|▏              | 1/63 [01:26<1:29:41, 86.79s/it]Evaluating commonsenseqa :   3%|▍              | 2/63 [03:13<1:40:05, 98.45s/it]Evaluating commonsenseqa :   5%|▋             | 3/63 [05:00<1:42:29, 102.49s/it]Evaluating commonsenseqa :   6%|▉             | 4/63 [06:41<1:40:10, 101.87s/it]Evaluating commonsenseqa :   8%|█             | 5/63 [08:28<1:40:03, 103.51s/it]Evaluating commonsenseqa :  10%|█▎            | 6/63 [10:13<1:39:00, 104.22s/it]Evaluating commonsenseqa :  11%|█▋             | 7/63 [11:12<1:23:23, 89.35s/it]Evaluating commonsenseqa :  13%|█▉             | 8/63 [12:59<1:27:08, 95.07s/it]Evaluating commonsenseqa :  14%|██▏            | 9/63 [14:46<1:28:51, 98.72s/it]Evaluating commonsenseqa :  16%|██           | 10/63 [16:31<1:28:56, 100.68s/it]Evaluating commonsenseqa :  17%|██▎          | 11/63 [18:16<1:28:19, 101.92s/it]Evaluating commonsenseqa :  19%|██▍          | 12/63 [20:02<1:27:46, 103.26s/it]Evaluating commonsenseqa :  21%|██▋          | 13/63 [21:47<1:26:26, 103.73s/it]Evaluating commonsenseqa :  22%|██▉          | 14/63 [23:32<1:25:02, 104.13s/it]Evaluating commonsenseqa :  24%|███          | 15/63 [25:19<1:24:04, 105.10s/it]Evaluating commonsenseqa :  25%|███▎         | 16/63 [27:07<1:22:55, 105.87s/it]Evaluating commonsenseqa :  27%|███▌         | 17/63 [28:54<1:21:31, 106.33s/it]Evaluating commonsenseqa :  29%|███▋         | 18/63 [30:39<1:19:20, 105.78s/it]Evaluating commonsenseqa :  30%|███▉         | 19/63 [32:25<1:17:35, 105.81s/it]Evaluating commonsenseqa :  32%|████▏        | 20/63 [34:10<1:15:37, 105.53s/it]