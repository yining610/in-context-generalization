torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i0-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 0
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:26:57,991] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:26:57,996] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:26:58,010] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:26:58,025] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:04,347] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:04,363] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:04,369] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:27:04,919] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:11,188] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:11,314] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:11,314] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:27:11,942] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:18,248] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:18,743] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:18,756] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:18,793] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:25,539] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:25,557] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:26,053] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:26,054] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:32,406] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:32,407] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:32,912] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:32,913] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:39,331] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:39,343] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:27:39,806] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:39,932] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:46,257] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:46,281] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:27:46,799] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:46,835] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:53,168] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:53,169] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:53,182] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:27:53,692] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:00,111] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:28:00,629] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:00,630] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:00,632] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:07,052] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:07,053] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:07,053] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:28:07,581] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:13,988] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:14,506] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:14,507] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:14,514] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:20,914] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:20,952] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:21,415] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:21,450] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:27,858] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:27,867] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:28,371] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:28,377] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:34,779] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:35,316] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:35,318] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:35,342] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:41,708] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:41,708] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:42,228] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:42,228] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:48,522] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:48,652] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:28:49,082] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:49,175] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:55,482] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:55,607] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:55,607] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:55,607] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:02,535] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:29:03,044] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:03,066] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:03,067] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:09,406] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:09,516] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:09,516] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:29:10,038] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:16,475] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:16,476] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:29:16,978] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:16,998] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:23,382] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:23,382] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:23,401] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:23,953] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:30,226] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:30,335] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:30,335] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:29:30,848] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:37,271] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:37,802] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:37,804] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:37,810] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:44,214] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:44,231] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:44,710] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:44,723] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:51,173] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:51,173] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:51,678] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:51,695] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:58,136] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:58,145] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:29:58,657] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:58,658] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:05,042] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:05,064] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:05,083] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:05,681] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:11,969] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:12,485] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:12,485] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:12,494] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:18,897] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:18,918] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:30:19,415] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:19,425] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:25,869] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:25,869] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:25,881] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:26,388] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:32,807] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:32,808] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:33,251] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:33,334] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:39,738] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:39,738] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:39,739] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:30:40,263] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...Answers already exist, exiting...

Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:46,689] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:46,705] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:30:47,212] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:47,224] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:53,540] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:30:54,186] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:54,186] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:54,186] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:31:00,581] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:31:00,582] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:31:00,582] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:31:01,112] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s10-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 8
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s10-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9733 [00:00<?, ?it/s]Loading data: 100%|| 9733/9733 [00:00<00:00, 864384.70it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.22s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.33s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.78s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.77s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.14s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.61s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.20s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.67s/it]
[2023-08-30 00:31:11,486] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 00:31:11,597] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.35s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.86s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.36s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.87s/it]
[2023-08-30 00:31:12,005] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-30 00:31:12,018] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 00:31:12,580] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 00:31:12,581] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 00:31:12,581] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 00:31:12,581] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 00:31:12,581] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 00:31:12,581] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd3fbec52d0>
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 00:31:12,583] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: B: tailor shop

Input: Where are you if your reading magazines while waiting for a vehicle on rails? Choices:  A: vegetables B: market C: doctor D: train station E: bookstore
Output: D: train station

Input: What would need oil to be used? Choices:  A: ground B: human  body C: repair shop D: combustion engines E: service station
Output: D: combustion engines

Input: What is person probably feeling that plans on stopping being married to their spouse? Choices:  A: detachment B: bankruptcy C: sad D: fights E: wrong
Output: A: detachment

Input: What could you use to store a clock? Choices:  A: shelf B: own bedroom C: desk D: wall E: car
Output: A: shelf

Input: The person put on lotion, what did they want? Choices:  A: fresh smell B: good credit C: smooth skin D: fresh produce E: headache
Output: C: smooth skin

Input: They burned the record, they were trying to do what to history? Choices:  A: compact disc B: tape C: rewrite D: play music E: erase
Output: E: erase

Input: She sure didn't have a green thumb, every time she thought she was making grow something it would what? Choices:  A: growth B: flowering C: ground D: die E: plants
Output: D: die

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s10-rFalse-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [02:08<1:44:44, 128.25s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [04:12<1:40:56, 126.18s/it]Evaluating commonsenseqa :   6%|                                                                  | 3/50 [05:08<1:13:25, 93.74s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [07:13<1:21:33, 106.39s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [09:19<1:24:56, 113.25s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [11:26<1:26:24, 117.84s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [12:35<1:13:08, 102.05s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [14:43<1:17:17, 110.42s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [16:51<1:19:05, 115.75s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [18:58<1:19:29, 119.24s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [21:03<1:18:42, 121.09s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [23:09<1:17:40, 122.64s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [25:15<1:16:08, 123.47s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [27:24<1:15:04, 125.12s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [29:30<1:13:08, 125.38s/it]Evaluating commonsenseqa :  32%|                                                | 16/50 [30:20<58:15, 102.81s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [32:29<1:00:48, 110.57s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [34:34<1:01:16, 114.89s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [36:39<1:01:03, 118.17s/it]Evaluating commonsenseqa :  40%|                                          | 20/50 [38:05<54:15, 108.51s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [40:11<54:51, 113.49s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [42:15<54:32, 116.89s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [44:24<54:08, 120.30s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [46:31<53:03, 122.45s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [48:39<51:41, 124.05s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [50:46<49:56, 124.84s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [52:52<48:01, 125.27s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [55:00<46:12, 126.00s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [57:06<44:10, 126.21s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [59:16<42:24, 127.23s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:01:22<40:13, 127.03s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:03:26<37:46, 125.93s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:05:31<35:35, 125.60s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:07:36<33:28, 125.55s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:09:45<31:36, 126.43s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:11:51<29:30, 126.47s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:13:58<27:26, 126.67s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:16:06<25:23, 126.95s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:18:13<23:18, 127.14s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:20:22<21:14, 127.50s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:22:30<19:10, 127.82s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:24:39<17:05, 128.17s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:26:10<13:38, 116.87s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:28:18<12:01, 120.33s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:30:23<10:08, 121.76s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:32:33<08:16, 124.07s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:34:35<06:10, 123.54s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:36:40<04:08, 124.06s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:38:15<01:55, 115.32s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:40:24<00:00, 119.36s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:40:24<00:00, 120.49s/it]
name: commonsenseqa | avg. gen lenth: 272.64 | time: 6024.883975982666s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 02:11:44,346] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 02:11:44,523] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 02:11:44,608] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 02:11:44,730] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s10-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 9
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i9-s10-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9732 [00:00<?, ?it/s]Loading data: 100%|| 9732/9732 [00:00<00:00, 817638.49it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.15s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.26s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.33s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.40s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.24s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.68s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.29s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.74s/it]
[2023-08-30 02:11:55,401] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.32s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.77s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.32s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.78s/it]
[2023-08-30 02:11:55,523] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 02:11:55,548] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-30 02:11:55,575] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 02:11:56,150] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 02:11:56,152] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0ca2ab92d0>
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 02:11:56,154] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: B: tailor shop

Input: Where are you if your reading magazines while waiting for a vehicle on rails? Choices:  A: vegetables B: market C: doctor D: train station E: bookstore
Output: D: train station

Input: What would need oil to be used? Choices:  A: ground B: human  body C: repair shop D: combustion engines E: service station
Output: D: combustion engines

Input: What is person probably feeling that plans on stopping being married to their spouse? Choices:  A: detachment B: bankruptcy C: sad D: fights E: wrong
Output: A: detachment

Input: What could you use to store a clock? Choices:  A: shelf B: own bedroom C: desk D: wall E: car
Output: A: shelf

Input: The person put on lotion, what did they want? Choices:  A: fresh smell B: good credit C: smooth skin D: fresh produce E: headache
Output: C: smooth skin

Input: They burned the record, they were trying to do what to history? Choices:  A: compact disc B: tape C: rewrite D: play music E: erase
Output: E: erase

Input: She sure didn't have a green thumb, every time she thought she was making grow something it would what? Choices:  A: growth B: flowering C: ground D: die E: plants
Output: D: die

Input: After a long night out the drunken man lost consciousness, he showed a sign of sickness right before passing out, what was it? Choices:  A: dream B: vomiting C: panic D: cancer E: blurred vision
Output: B: vomiting

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i9-s10-rFalse-m4096
Evaluating commonsenseqa :   2%|                                                                       | 1/50 [00:46<37:44, 46.21s/it]Evaluating commonsenseqa :   4%|                                                                    | 2/50 [02:50<1:13:38, 92.06s/it]Evaluating commonsenseqa :   6%|                                                                  | 3/50 [04:20<1:11:24, 91.17s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [06:30<1:21:32, 106.36s/it]Evaluating commonsenseqa :  10%|                                                                | 5/50 [07:58<1:14:52, 99.84s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [10:02<1:19:20, 108.18s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [12:05<1:20:59, 113.02s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [14:12<1:22:15, 117.52s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [16:19<1:22:10, 120.25s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [18:23<1:21:01, 121.53s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [20:30<1:20:02, 123.14s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [22:37<1:18:46, 124.39s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [24:44<1:17:13, 125.23s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [26:49<1:15:03, 125.08s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [28:58<1:13:34, 126.14s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [31:04<1:11:35, 126.34s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [33:10<1:09:23, 126.15s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [35:16<1:07:11, 125.98s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [37:22<1:05:05, 125.97s/it]Evaluating commonsenseqa :  40%|                                         | 20/50 [39:28<1:03:05, 126.18s/it]Evaluating commonsenseqa :  42%|                                        | 21/50 [41:34<1:00:57, 126.13s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [43:41<58:54, 126.23s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [45:47<56:49, 126.29s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [47:23<50:48, 117.27s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [49:28<49:46, 119.47s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [51:34<48:35, 121.50s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [53:38<46:50, 122.19s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [55:44<45:14, 123.40s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [57:49<43:21, 123.88s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [59:53<41:19, 123.95s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:01:58<39:18, 124.13s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:04:03<37:20, 124.45s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:06:07<35:09, 124.12s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:08:14<33:21, 125.09s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:10:21<31:24, 125.61s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:12:22<29:00, 124.30s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:14:26<26:55, 124.31s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:16:29<24:46, 123.89s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:18:36<22:51, 124.67s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:20:43<20:56, 125.62s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:22:46<18:41, 124.58s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:24:49<16:34, 124.30s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:26:57<14:36, 125.25s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:29:01<12:30, 125.01s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:30:18<09:12, 110.42s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:32:25<07:41, 115.41s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:34:30<05:55, 118.43s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:36:36<04:01, 120.56s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:38:43<02:02, 122.74s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:40:50<00:00, 123.75s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:40:50<00:00, 121.00s/it]
name: commonsenseqa | avg. gen lenth: 266.952 | time: 6050.371901988983s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 03:52:53,240] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 03:52:53,292] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 03:52:53,307] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 03:52:53,312] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 1
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9740 [00:00<?, ?it/s]Loading data: 100%|| 9740/9740 [00:00<00:00, 1244896.42it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.95s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.05s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.21s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.64s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.12s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.56s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.18s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.60s/it]
[2023-08-30 03:53:03,677] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-30 03:53:03,723] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.25s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.69s/it]
[2023-08-30 03:53:03,978] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.51s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.98s/it]
[2023-08-30 03:53:04,611] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 03:53:05,162] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 03:53:05,163] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 03:53:05,163] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f01bd7c1300>
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 03:53:05,165] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. Let's analyze each answer choice in the context of the sentence.
2. The sentence says that the machine was'very intricate'. It means the machine has complex and detailed features.
3. First, see Option A: 'box'. A box is a simple container and it does not convey the intricate mechanisms or complexity of a machine.
4. Option D: 'wash dishes' does not fit the context as it's related to a very specific function and does not effectively describe the complexity of a machine.
5. Option C: 'appliance'. This usually refers to a device, like a refrigerator or a toaster, doing practical work, but the level of intricacy or complexity in the usual sense of the term 'appliance' is not as high as the context demands.
6. Option E: 'implement'. An implement is a tool or instrument, which might be intricate, but this word is typically used for hand-held things and wouldn't usually describe a complex machine.
7. Option B: 'apparatus'. An apparatus is often used to refer to a complex instrument or machine with a particular purpose. 
8. Therefore, considering all the option meanings and their contextual relevance, we can conclude that the best choice to complete the sentence is B: 'apparatus' as it signifies the necessary complexity and intricacy the sentence demands.
So the final answer is B: apparatus

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [02:07<1:44:29, 127.95s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [04:15<1:42:23, 128.00s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [06:22<1:39:53, 127.53s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [08:30<1:37:44, 127.50s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [10:35<1:35:05, 126.78s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [12:41<1:32:47, 126.53s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [14:48<1:30:35, 126.41s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [16:56<1:28:51, 126.93s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [19:02<1:26:36, 126.75s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [21:10<1:24:40, 127.01s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [23:16<1:22:28, 126.89s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [25:22<1:20:14, 126.70s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [27:32<1:18:42, 127.62s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [29:38<1:16:17, 127.14s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [31:44<1:13:58, 126.80s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [33:51<1:11:47, 126.68s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [35:57<1:09:32, 126.44s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [38:04<1:07:31, 126.60s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [40:02<1:04:07, 124.12s/it]Evaluating commonsenseqa :  40%|                                         | 20/50 [42:07<1:02:17, 124.57s/it]Evaluating commonsenseqa :  42%|                                        | 21/50 [44:13<1:00:17, 124.75s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [46:19<58:26, 125.24s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [48:25<56:28, 125.49s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [50:32<54:33, 125.91s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [52:37<52:23, 125.75s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [54:46<50:39, 126.66s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [56:51<48:22, 126.21s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [58:57<46:11, 125.95s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [1:01:04<44:15, 126.48s/it]Evaluating commonsenseqa :  60%|                           | 30/50 [1:03:11<42:09, 126.50s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:05:19<40:13, 127.02s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:07:26<38:03, 126.85s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:09:32<35:54, 126.73s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:11:38<33:46, 126.63s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:13:44<31:34, 126.30s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:15:40<28:44, 123.19s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:17:49<27:05, 125.00s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:19:56<25:05, 125.46s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:22:02<23:04, 125.86s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:24:13<21:13, 127.38s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:26:21<19:06, 127.42s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:28:27<16:56, 127.11s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:30:35<14:51, 127.40s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:32:42<12:42, 127.13s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:34:48<10:33, 126.78s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:36:56<08:28, 127.05s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:39:03<06:21, 127.06s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:41:08<04:13, 126.70s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:43:14<02:06, 126.36s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:45:22<00:00, 126.73s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:45:22<00:00, 126.44s/it]
name: commonsenseqa | avg. gen lenth: 390.652 | time: 6322.509834051132s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 05:38:37,184] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 05:38:37,187] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 05:38:37,190] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 05:38:37,205] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 2
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9739 [00:00<?, ?it/s]Loading data: 100%|| 9739/9739 [00:00<00:00, 667849.17it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.75s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.36s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.38s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.39s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.99s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.40s/it]
[2023-08-30 05:38:47,477] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.27s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.73s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.29s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.75s/it]
 > number of parameters: 6738415616
[2023-08-30 05:38:48,078] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 05:38:48,140] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.40s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.85s/it]
[2023-08-30 05:38:48,329] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 05:38:48,955] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 05:38:48,957] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 05:38:48,957] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 05:38:48,957] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 05:38:48,957] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 05:38:48,957] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f86c33bd2a0>
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 05:38:48,960] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. The key to answering this question correctly is understanding the context and meaning of the question. The sentence is describing a machine that is said to be very intricate.
2. "Intricate" means that something is complicated or detailed. So, the word that follows should reasonably describe a detailed or complex machine.
3. We can now evaluate each of the multiple-choice options in light of this reasoning. In this case, the options are A: box, B: apparatus, C: appliance, D: wash dishes, and E: implement.
4. Option A, box, is unlikely because a box is a simple, generally not intricate item.
5. Option C, appliance, generally refers to devices for particular uses in the home, like a dishwasher. However, appliances aren't necessarily intricate or complex.
6. Option D, wash dishes, is not correct because it is not a noun but a verb phrase and doesn't follow grammatically after "an."
7. Option E, implement, usually refers to a tool or piece of equipment, but doesn't inherently suggest something intricate.
8. This leaves us with option B: apparatus, which is a technical term for a complex device or piece of equipment. It fits best with the description of the machine being intricate.

Therefore, the correct answer is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: Step 1: Identify the question and understand what it's asking. The question is "Where do you get petrol?"
Step 2: Consider the context of the question. It's asking for a location, so we can exclude any answers that are not places.
Step 3: Eliminate answer choice A (burn hot) because it isn't a place where you can get petrol
Step 4: Eliminate answer choice B (fuel tank) because while petrol can be stored there, it's not the place where you obtain it from.
Step 5: Eliminate answer choice C (burn hot) again because it isn't a place where you can get petrol.
Step 6: Eliminate answer choice D (car) because similar to a fuel tank, a car may store petrol but it's not the place where you get it from.
Step 7: By process of elimination, you're left with answer E (gas station) which is indeed a place where you can get petrol. 
Step 8: So, the answer is E: gas station.
So the final answer is E: gas station

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [01:58<1:36:26, 118.08s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [03:55<1:34:00, 117.51s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [05:51<1:31:38, 116.99s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [07:47<1:29:18, 116.50s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [09:44<1:27:30, 116.67s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [11:42<1:25:49, 117.03s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [13:39<1:23:56, 117.12s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [15:28<1:20:09, 114.51s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [17:24<1:18:43, 115.20s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [19:23<1:17:25, 116.14s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [21:23<1:16:16, 117.35s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [23:19<1:14:09, 117.08s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [25:00<1:09:03, 111.98s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [26:57<1:08:15, 113.76s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [28:56<1:07:10, 115.16s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [30:52<1:05:22, 115.36s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [32:47<1:03:30, 115.48s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [34:43<1:01:38, 115.59s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [36:38<59:37, 115.41s/it]Evaluating commonsenseqa :  40%|                                          | 20/50 [38:35<57:52, 115.74s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [40:32<56:10, 116.21s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [42:28<54:11, 116.11s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [44:24<52:16, 116.15s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [46:21<50:25, 116.36s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [48:00<46:15, 111.02s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [49:55<44:56, 112.35s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [51:51<43:27, 113.35s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [53:48<42:01, 114.64s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [55:47<40:34, 115.93s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [57:46<38:56, 116.80s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [59:42<36:52, 116.42s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:01:38<34:53, 116.28s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:03:35<33:03, 116.69s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:05:34<31:15, 117.25s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:07:32<29:22, 117.53s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:09:30<27:25, 117.55s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:11:28<25:30, 117.72s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:13:25<23:30, 117.51s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:15:21<21:29, 117.27s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:17:17<19:28, 116.85s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:19:15<17:32, 116.99s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:21:11<15:33, 116.69s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:23:05<13:31, 115.88s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:25:02<11:37, 116.29s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:26:57<09:40, 116.03s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:28:55<07:45, 116.41s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:30:50<05:48, 116.21s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:32:48<03:53, 116.56s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:34:44<01:56, 116.36s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:36:40<00:00, 116.34s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:36:40<00:00, 116.01s/it]
name: commonsenseqa | avg. gen lenth: 361.584 | time: 5800.766259670258s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 07:15:45,595] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 07:15:45,595] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 07:15:45,599] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 07:15:45,612] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 3
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i3-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9738 [00:00<?, ?it/s]Loading data: 100%|| 9738/9738 [00:00<00:00, 590301.37it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.83s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.89s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.92s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.15s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.05s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.46s/it]
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.08s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.50s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.11s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.53s/it]
[2023-08-30 07:15:55,936] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 07:15:56,015] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.15s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.60s/it]
 > number of parameters: 6738415616
[2023-08-30 07:15:56,083] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 07:15:56,215] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 07:15:56,823] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 07:15:56,825] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f92b9bc52d0>
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 07:15:56,826] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. To answer this question, it is necessary to understand the context of a complex, detailed machine. 
2. Equally important is to know the meanings of the provided choices, which are: a box, an apparatus, an appliance, washing dishes, and an implement.
3. Let's go through the choices by their definition:
   - A box is simply a container, which doesn't describe something intricate.
   - An apparatus encompasses a set of machines or equipment designed for a particular function, which could be intricate.
   - An appliance refers to a device or piece of equipment designed to perform a specific task, typically a household task. The definition does not imply intricacy.
   - "Wash dishes" is an action, not a descriptor for a complex machine.
   - An implement is a tool, utensil, or other piece of equipment, which usually refers to simpler tools, not intricate machines.
4. Keeping the given context and definitions in mind, it is clear that the word 'apparatus' best fits the description of a machine that is'very intricate'.
5. Therefore, the correct answer is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: 1. The question asks for the location where you typically obtain petrol.
2. Petrol is a type of fuel used to run cars and other automobiles.
3. Choices A and C, "burn hot," are incorrect because they are not locations nor related to obtaining petrol.
4. Choice B, "fuel tank," is the part of the car where petrol is stored and not the place where it is procured.
5. Choice D, "car," is also incorrect because we don't get petrol from the car itself; instead, we add petrol to the car.
6. Therefore, the correct answer is choice E, "gas station," because this is the location where you can buy and get petrol for your car.
So the final answer is E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: 1. First, we consider the fact that a television is a piece of technology that needs to be plugged into an electricity source to be used.
2. A television is also typically used indoors due to the need to protect it from weather and theft.
3. With this in mind, we can eliminate option A: 'cabinet' because it is unlikely to be used there, and 'woods' because a television is typically not used outdoors.
4. We are left with 'house', 'apartment' and 'bedroom'. All these places can have electricity and are inside a building.
5. However, considering the word 'night' in the question suggests the location is likely to be a place used during the night.
6. While 'house' and 'apartment' could be correct, they are not specific enough  as they consist of many rooms.
7. 'Bedroom' is the most likely answer because it is common to watch television in a bedroom at night. Hence, the answer is D: bedroom.
So the final answer is D: bedroom

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i3-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [01:53<1:32:44, 113.57s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [03:44<1:29:23, 111.74s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [05:35<1:27:15, 111.40s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [07:26<1:25:29, 111.51s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [08:55<1:17:23, 103.18s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [10:44<1:17:13, 105.31s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [12:34<1:16:34, 106.84s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [14:24<1:15:25, 107.76s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [16:13<1:14:01, 108.34s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [18:03<1:12:34, 108.87s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [19:54<1:11:06, 109.40s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [21:46<1:09:41, 110.04s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [23:36<1:07:54, 110.11s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [25:25<1:05:53, 109.82s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [27:16<1:04:20, 110.29s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [29:05<1:02:16, 109.89s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [30:55<1:00:27, 109.91s/it]Evaluating commonsenseqa :  36%|                                             | 18/50 [32:44<58:26, 109.59s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [34:36<56:54, 110.14s/it]Evaluating commonsenseqa :  40%|                                          | 20/50 [36:26<55:02, 110.09s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [38:16<53:15, 110.17s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [40:06<51:28, 110.29s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [41:56<49:29, 109.99s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [43:45<47:36, 109.88s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [45:35<45:47, 109.91s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [47:25<43:54, 109.76s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [49:16<42:17, 110.31s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [51:07<40:31, 110.54s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [52:57<38:37, 110.36s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [54:47<36:40, 110.02s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [56:37<34:51, 110.08s/it]Evaluating commonsenseqa :  64%|                         | 32/50 [58:27<33:00, 110.02s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:00:19<31:19, 110.57s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:02:08<29:22, 110.17s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:03:59<27:36, 110.41s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:05:47<25:37, 109.80s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:07:38<23:53, 110.26s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:09:29<22:02, 110.24s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:11:19<20:11, 110.16s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:13:09<18:21, 110.17s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:14:58<16:27, 109.73s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:16:49<14:42, 110.27s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:18:38<12:49, 109.99s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:20:28<11:00, 110.02s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:22:19<09:10, 110.08s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:24:10<07:21, 110.47s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:26:00<05:30, 110.19s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:27:51<03:40, 110.48s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:29:41<01:50, 110.40s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:31:31<00:00, 110.22s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:31:31<00:00, 109.83s/it]
name: commonsenseqa | avg. gen lenth: 400.16 | time: 5491.700274467468s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 08:47:58,210] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 08:47:58,220] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 08:47:58,296] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 08:47:58,312] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 4
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i4-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9737 [00:00<?, ?it/s]Loading data: 100%|| 9737/9737 [00:00<00:00, 477096.51it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.93s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.31s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.25s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.35s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.21s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.62s/it]
[2023-08-30 08:48:08,905] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.30s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.74s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.39s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.82s/it]
[2023-08-30 08:48:09,235] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.42s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.86s/it]
[2023-08-30 08:48:09,354] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-30 08:48:09,521] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 08:48:10,075] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 08:48:10,076] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb473ecd2d0>
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 08:48:10,078] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. In this question, you need to identify the best word to complete the sentence.
2. The sentence is talking about an intricate machine, which means it is complex or complicated.
3. Now, look at the options given - A: box, B: apparatus, C: appliance, D: wash dishes, E: implement. 
4. The word "box" usually refers to a simple container, not something intricate or complex.
5. "Apparatus" refers to a complex machine or device, which fits with the description in the question.
6. "Appliance" refers to a device or piece of equipment designed to perform a specific task, usually a domestic one. The context of the sentence does not suggest a domestic usage.
7. "Wash dishes" is a verb phrase, not a noun, therefore it can't be the answer because we are describing a noun. 
8. "Implement" would imply a tool or instrument for a specific task, which does not necessarily need to be intricate or complex.
9. Therefore, the best fit from the given options to complete the sentence is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: 1. The question is asking about the location where one can obtain petrol. 
2. Petrol is a type of fuel that is used to run engines.
3. By going over the options given, we can eliminate options that do not fit the context of the question.
4. Options A and C, "burn hot," are incorrect because this response suggests a state of matter, not a location.
5. Option B, "fuel tank", while related to petrol, is where the petrol is stored in a car, not where it is originally obtained.
6. Option D, "car", is incorrect because cars use petrol, they do not provide it.
7. This leaves us with the last option, E, "gas station".
8. A gas station is a location where fuel, including petrol, is sold and distributed to customers for their vehicles. 
9. Therefore, the answer is E: gas station.
So the final answer is E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: 1. A television is usually used in places where people live or work, as this tool is typically used for entertainment or informative purposes.
2. The options provided in the question are cabinet, house, apartment, bedroom, and woods. 
3. A cabinet is not a suitable location for using a television as it is a storage unit and does not usually provide enough space to watch television comfortably.
4. The woods is also not a good option as it typically lacks the necessary resources such as electricity and proper protective shielding for the television.
5. The remaining options are house, apartment, and bedroom. The question indicates that the television is used at night, suggesting the place should be somewhere people often reside at night.
6. Although people reside both in houses and apartments, they are too broad and do not specify a particular area within them where a television might be used at night. A television can be in many rooms within a house or an apartment.
7. Considering all this, the most suitable answer among the provided options would be a bedroom as it specifies a location within a residential structure where people commonly use televisions at night. Hence, the answer is D: bedroom.
So the final answer is D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: 1. The question asks about the location of getting knowledge which is described as being "expensive."
2. If we consider the options, gaining knowledge from a book, field, meeting, or class doesn't typically have a high financial cost associated with it.
3. However, attending a university typically involves substantial financial expenses in terms of tuition, living expenses, etc.
4. The term "expensive" is a key term in this question because it differentiates university from the other options.
5. So, by considering the distinctive "expensive" feature, we can conclude that the location is the university. Hence, the answer is A: university.
So the final answer is A: university

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i4-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [01:44<1:25:10, 104.30s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [03:26<1:22:40, 103.34s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [05:09<1:20:36, 102.91s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [06:51<1:18:35, 102.51s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [08:33<1:16:48, 102.42s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [10:16<1:15:15, 102.63s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [11:59<1:13:40, 102.80s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [13:42<1:12:03, 102.93s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [15:25<1:10:20, 102.93s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [17:07<1:08:22, 102.57s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [18:50<1:06:38, 102.54s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [20:32<1:04:53, 102.47s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [22:14<1:03:02, 102.22s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [23:56<1:01:18, 102.18s/it]Evaluating commonsenseqa :  30%|                                                 | 15/50 [25:37<59:32, 102.07s/it]Evaluating commonsenseqa :  32%|                                                | 16/50 [27:20<57:56, 102.26s/it]Evaluating commonsenseqa :  34%|                                              | 17/50 [29:02<56:09, 102.12s/it]Evaluating commonsenseqa :  36%|                                             | 18/50 [30:45<54:34, 102.33s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [32:27<52:54, 102.40s/it]Evaluating commonsenseqa :  40%|                                          | 20/50 [34:10<51:14, 102.50s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [35:52<49:24, 102.21s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [37:35<47:53, 102.62s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [39:19<46:17, 102.87s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [41:01<44:31, 102.74s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [42:39<42:11, 101.25s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [44:21<40:33, 101.41s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [46:04<39:05, 101.97s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [47:24<35:00, 95.49s/it]Evaluating commonsenseqa :  58%|                              | 29/50 [49:07<34:10, 97.66s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [50:50<33:06, 99.31s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [52:32<31:42, 100.11s/it]Evaluating commonsenseqa :  64%|                         | 32/50 [54:14<30:11, 100.66s/it]Evaluating commonsenseqa :  66%|                        | 33/50 [55:59<28:50, 101.81s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [57:42<27:14, 102.16s/it]Evaluating commonsenseqa :  70%|                     | 35/50 [59:23<25:31, 102.08s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:01:07<23:56, 102.60s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:02:49<22:12, 102.49s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:04:32<20:31, 102.59s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:06:17<18:54, 103.11s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:07:59<17:09, 102.98s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:09:41<15:23, 102.62s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:11:24<13:42, 102.77s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:13:06<11:57, 102.50s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:14:37<09:54, 99.11s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:16:20<08:21, 100.30s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:18:03<06:43, 100.95s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:19:45<05:03, 101.26s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:21:28<03:23, 101.91s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:23:11<01:42, 102.21s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:24:53<00:00, 102.25s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:24:53<00:00, 101.88s/it]
name: commonsenseqa | avg. gen lenth: 398.152 | time: 5094.448546171188s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 10:14:31,018] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 10:14:31,031] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 10:14:31,049] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 10:14:31,068] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 5
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i5-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9736 [00:00<?, ?it/s]Loading data: 100%|| 9736/9736 [00:00<00:00, 392251.59it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.82s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.85s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.90s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.11s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.43s/it]
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.06s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.48s/it]
[2023-08-30 10:14:41,454] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.11s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.52s/it]
 > number of parameters: 6738415616
[2023-08-30 10:14:41,556] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.12s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.57s/it]
[2023-08-30 10:14:41,625] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 10:14:41,676] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 10:14:42,261] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 10:14:42,262] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 10:14:42,262] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 10:14:42,262] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 10:14:42,262] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 10:14:42,262] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe9befc12d0>
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 10:14:42,264] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. The sentence talks about a machine being very intricate.
2. The missing word would need to be something that can be described as being complex or intricate. 
3. An 'intricate box' doesn't make much sense as a box is often simple in its design. Hence option A is incorrect.
4. Similarly, 'intricate wash dishes' does not fit the given context. Hence option D is incorrect.
5. Option E, 'implement' often refers to tools or equipment for specific purposes, which can be intricate but it is less often used to describe machinery. Hence it might not be the best fit.
6. Although appliances can be intricate, they are generally things like a kitchen or household devices, which may not be the best fit for the general term of'machine'. Therefore, option C might not be the best fit.
7. That leaves 'apparatus'. An apparatus is a technical device or machinery, and can indeed be intricate, fitting the sentence context well.
8. Therefore, the answer is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: 1. The question is asking where petrol is obtained from or where we can usually get petrol. 
2. Option A: burn hot, does not pertain to the question since it does not have a location where petrol can be obtained. It seems more related to the process of burning or combustion.
3. Option B: fuel tank, while it is a place where petrol is stored, it is not typically where individuals go to obtain petrol. Therefore, this option is skipped.
4. Option C: burn hot, as it is the same as option A and it does not provide an answer related to where petrol is typically obtained. 
5. Option D: car; petrol is used by a car and is stored in its fuel tank for operation, but it is not typically the place where individuals go to obtain or purchase petrol. 
6. Option E: gas station; this is the correct answer as this is the typical place where individuals go to obtain or purchase petrol for their vehicles.
So the final answer is E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: Step 1: Consider the options. The question asks where a television, used at night, might be.
Step 2: Option A: Cabinet is not a valid option since televisions are typically stored in cabinets when not in use, not when they are being watched. 
Step 3: Option B: House is a valid option as televisions are typically found in houses. But it's quite broad and does not specify a particular location within a house where a television would be used at night.
Step 4: Option C: Apartment is also a valid option like a house. But the same reasoning as option B applies here - it does not specify a particular place where a television would usually be used at night.
Step 5: Option D: Bedroom is a valid answer. People often watch TV in their bedrooms before going to sleep at night, so this choice is more specific than just a house or an apartment.
Step 6: Option E: Woods is not a valid option. Televisions require a power source and strong signal connection to broadcast, which are typically not available in the woods.
Step 7: Among all the options, bedroom is the most specific one which answers the question of where a television might be used at night. Therefore, the answer is D: bedroom.
So the final answer is D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: 1. The question asks about where an expensive knowledge acquisition process takes place.
2. We start by examining each of the choices.
3. Choice B: Knowledge acquired from books can be expensive, but not necessarily as it can also be freely accessible through libraries or online sources. So, it might not be the correct answer.
4. Choice C: Getting knowledge through field or practical experience is not necessarily expensive, it depends more on the nature of the job or project. So, it's less likely to be the right answer.
5. Choice D: Obtaining knowledge via a meeting is not usually thought of as an expensive process, as meetings can often be attended for free or at a low cost. Therefore, it rules out the choice.
6. Choice E: While knowledge can be expensive to gain in classes, this could be any type of class, not necessarily one that comes with a high cost.
7. Choice A: Universities are commonly referred to as places where one gains knowledge, and it is typically an expensive process due to tuition fees, living expenses, etc. Thus, it fits best with the descriptor 'expensive.'
8. Therefore, the answer is A: University.
So the final answer is A: university

Input: Where are small grapes likely to be found? Choices:  A: lunch box B: food store C: wine country D: kitchen E: raisins
Output: 1. This question is asking for a place where small grapes are typically found.
2. If we look at choice A: lunch box, we can eliminate this option because any type of fruit, not specifically small grapes, could be in a lunch box.  
3. Choice B: food store is a broader option as it could contain all kinds of food, not specifically small grapes.
4. Choice D: kitchen, a place where all types of food are prepared and not solely devoted for small grapes.
5. Choice E: raisins. While grapes are indeed used to make raisins, the question asks for a place where one would find small grapes, not their byproducts.
6. The answer is therefore choice C: wine country. This is because small grapes are often used in making certain types of wine, and vineyards in wine countries are known to cultivate and harvest small grapes specifically for this purpose.
So the final answer is C: wine country

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i5-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                     | 1/50 [01:32<1:15:29, 92.44s/it]Evaluating commonsenseqa :   4%|                                                                    | 2/50 [03:03<1:13:10, 91.47s/it]Evaluating commonsenseqa :   6%|                                                                  | 3/50 [04:34<1:11:37, 91.44s/it]Evaluating commonsenseqa :   8%|                                                                 | 4/50 [06:06<1:10:05, 91.43s/it]Evaluating commonsenseqa :  10%|                                                                | 5/50 [07:37<1:08:33, 91.40s/it]Evaluating commonsenseqa :  12%|                                                              | 6/50 [09:08<1:06:56, 91.29s/it]Evaluating commonsenseqa :  14%|                                                             | 7/50 [10:40<1:05:42, 91.68s/it]Evaluating commonsenseqa :  16%|                                                           | 8/50 [12:13<1:04:21, 91.94s/it]Evaluating commonsenseqa :  18%|                                                          | 9/50 [13:45<1:02:44, 91.82s/it]Evaluating commonsenseqa :  20%|                                                        | 10/50 [15:16<1:01:03, 91.59s/it]Evaluating commonsenseqa :  22%|                                                        | 11/50 [16:46<59:21, 91.32s/it]Evaluating commonsenseqa :  24%|                                                      | 12/50 [18:18<57:49, 91.29s/it]Evaluating commonsenseqa :  26%|                                                     | 13/50 [19:48<56:12, 91.15s/it]Evaluating commonsenseqa :  28%|                                                   | 14/50 [21:20<54:48, 91.34s/it]Evaluating commonsenseqa :  30%|                                                  | 15/50 [22:52<53:26, 91.61s/it]Evaluating commonsenseqa :  32%|                                                 | 16/50 [24:24<51:56, 91.67s/it]Evaluating commonsenseqa :  34%|                                               | 17/50 [25:55<50:16, 91.40s/it]Evaluating commonsenseqa :  36%|                                              | 18/50 [27:26<48:41, 91.28s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [28:57<47:04, 91.12s/it]Evaluating commonsenseqa :  40%|                                           | 20/50 [30:27<45:30, 91.01s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [31:58<43:51, 90.75s/it]Evaluating commonsenseqa :  44%|                                        | 22/50 [33:30<42:31, 91.11s/it]Evaluating commonsenseqa :  46%|                                       | 23/50 [35:00<40:58, 91.04s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [36:31<39:26, 91.03s/it]Evaluating commonsenseqa :  50%|                                    | 25/50 [38:02<37:56, 91.04s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [39:34<36:26, 91.10s/it]Evaluating commonsenseqa :  54%|                                 | 27/50 [41:05<34:56, 91.16s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [42:37<33:33, 91.51s/it]Evaluating commonsenseqa :  58%|                              | 29/50 [44:08<31:58, 91.36s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [45:39<30:23, 91.15s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [47:10<28:50, 91.10s/it]Evaluating commonsenseqa :  64%|                          | 32/50 [48:41<27:19, 91.10s/it]Evaluating commonsenseqa :  66%|                        | 33/50 [50:13<25:50, 91.22s/it]Evaluating commonsenseqa :  68%|                       | 34/50 [51:45<24:24, 91.51s/it]Evaluating commonsenseqa :  70%|                     | 35/50 [53:16<22:52, 91.50s/it]Evaluating commonsenseqa :  72%|                    | 36/50 [54:47<21:18, 91.31s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [56:18<19:45, 91.16s/it]Evaluating commonsenseqa :  76%|                 | 38/50 [57:49<18:11, 90.98s/it]Evaluating commonsenseqa :  78%|               | 39/50 [59:20<16:41, 91.00s/it]Evaluating commonsenseqa :  80%|              | 40/50 [1:00:50<15:08, 90.86s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:02:22<13:41, 91.31s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:03:54<12:11, 91.45s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:05:26<10:40, 91.47s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:06:57<09:08, 91.46s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:08:28<07:36, 91.34s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:10:00<06:05, 91.36s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:11:31<04:34, 91.50s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:13:03<03:02, 91.46s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:14:34<01:31, 91.43s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:16:06<00:00, 91.67s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:16:06<00:00, 91.34s/it]
name: commonsenseqa | avg. gen lenth: 425.5 | time: 4567.483436584473s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 11:31:02,906] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 11:31:02,930] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 11:31:02,937] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 11:31:02,980] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 6
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i6-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9735 [00:00<?, ?it/s]Loading data: 100%|| 9735/9735 [00:00<00:00, 419581.25it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.78s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.09s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.14s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.31s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.99s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.41s/it]
[2023-08-30 11:31:13,213] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.17s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.61s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.18s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.62s/it]
[2023-08-30 11:31:13,572] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 11:31:13,623] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.37s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.81s/it]
 > number of parameters: 6738415616
[2023-08-30 11:31:14,062] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 11:31:14,726] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 11:31:14,728] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6c24dbd2d0>
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 11:31:14,728] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 11:31:14,729] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 11:31:14,729] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. We need to fill in the blank using one of the provided choices. 
2. The context implies that the machine is very intricate or complex. Therefore, we need to find a word among the choices that adequately represents a complex machine.
3. The words "box" and "wash dishes" don't fit because they don't convey the complexity of a machine.
4. Although "appliance" and "implement" might make sense of complexity, they're too specific. These words are used for machines designed for a specific purpose. Without knowing the specific function of the machine, these words might misrepresent it.
5. The word "apparatus" is a general term for a complex machine, irrespective of what specific function it performs, fitting the context of sentence.
6. Therefore, the answer is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: 1. First, eliminate irrelevant choices. In this context, the items "burn hot" and "car" can be eliminated as they do not specifically relate to the source of petrol.
2. Among the remaining options, "fuel tank" can be eliminated as a petrol source. Although petrol is stored in a fuel tank, it does not originate from there.
3. Therefore, the correct answer is "gas station", where petrol is usually purchased or obtained.
So the final answer is E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: 1. The question asks where a television used at night might be.
2. A television is typically an indoor item, so both the woods and a cabinet wouldn't provide the appropriate setting or the purpose of watching television. So options A and E can be eliminated.
3. Remaining choices are the house, the apartment, and the bedroom.
4. While a television can be located in both a house and an apartment, these choices lack the specificity needed to determine the exact location within those places, hence may not be suitable responses.
5. This leaves the bedroom where it's common for people to have a TV and use at night before sleeping. 
6. Thus, the answer is D: bedroom.
So the final answer is D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: 1. The question is asking for a place where gaining knowledge is considered expensive. 
2. Although knowledge can be gained in all of the options given (university, book, field, meeting, class), not all of them are notably expensive. 
3. From our everyday understanding, we know that buying a book is considerably cheaper than paying for a university or class. Similarly, learning in a field or a meeting does not necessarily imply a high cost. 
4. Therefore, amongst the given choices, getting a university education usually involves significant costs, making it the most expensive medium to gain knowledge. 
5. Thus, the answer is "A: university".
So the final answer is A: university

Input: Where are small grapes likely to be found? Choices:  A: lunch box B: food store C: wine country D: kitchen E: raisins
Output: 1. The question is asking where you are likely to find small grapes. 
2. Grapes can be found in all the options provided. However, the question specifically mentions "small" grapes. 
3. In a lunch box, kitchen, or food store, the size of the grapes might vary and are not necessarily small.
4. Raisins are dried grapes, so they wouldn't be considered as small grapes. 
5. Small grapes are generally used in wine production because they have a better skin to fruit ratio, which is ideal for wine-making.
6. Wine is predominantly made in the wine country.
7. Therefore, small grapes are most likely to be found in the wine country. Hence, the answer is C: wine country.
So the final answer is C: wine country

Input: The painter explained how he never achieved a flawless portrait, he said this was because all people are what? Choices:  A: imperfect B: disfigured C: damaged D: flawed E: defective
Output: 1. Begin with understanding the question: we're being asked to figure out what the painter associates with why he is unable to create a flawless portrait.
2. Consider all the choices given: imperfect, disfigured, damaged, flawed, defective. 
3. Evaluate all the presented choices. The painter's inability to create a perfect portrait does not necessarily mean that all people are disfigured, damaged, flawed, or defective. These are strong words and talk about physical or mental damage or deformities, which is not implied in the question.
4. The word 'flawless' is the clue it's synonymous with 'perfect'. So the reason why the painter never achieved a flawless portrait is because he believes all people are imperfect.
5. Therefore the answer is A: imperfect. 
6. Conclude that the reasoning behind this answer is that 'imperfect' is a less harsh term and is synonymous with 'flawless', making it the most fitting word choice in this context.
So the final answer is A: imperfect

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i6-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                     | 1/50 [01:36<1:18:47, 96.47s/it]Evaluating commonsenseqa :   4%|                                                                    | 2/50 [03:11<1:16:32, 95.68s/it]Evaluating commonsenseqa :   6%|                                                                  | 3/50 [04:46<1:14:45, 95.43s/it]Evaluating commonsenseqa :   8%|                                                                 | 4/50 [06:23<1:13:33, 95.95s/it]Evaluating commonsenseqa :  10%|                                                                | 5/50 [07:58<1:11:42, 95.61s/it]Evaluating commonsenseqa :  12%|                                                              | 6/50 [09:33<1:09:57, 95.39s/it]Evaluating commonsenseqa :  14%|                                                             | 7/50 [11:08<1:08:10, 95.12s/it]Evaluating commonsenseqa :  16%|                                                           | 8/50 [12:43<1:06:43, 95.33s/it]Evaluating commonsenseqa :  18%|                                                          | 9/50 [14:18<1:05:02, 95.18s/it]Evaluating commonsenseqa :  20%|                                                        | 10/50 [15:53<1:03:28, 95.22s/it]Evaluating commonsenseqa :  22%|                                                      | 11/50 [17:28<1:01:45, 95.01s/it]Evaluating commonsenseqa :  24%|                                                     | 12/50 [19:02<1:00:00, 94.74s/it]Evaluating commonsenseqa :  26%|                                                     | 13/50 [20:36<58:16, 94.49s/it]Evaluating commonsenseqa :  28%|                                                   | 14/50 [22:12<57:00, 95.02s/it]Evaluating commonsenseqa :  30%|                                                  | 15/50 [23:49<55:43, 95.53s/it]Evaluating commonsenseqa :  32%|                                                 | 16/50 [25:24<53:59, 95.29s/it]Evaluating commonsenseqa :  34%|                                               | 17/50 [26:59<52:20, 95.18s/it]Evaluating commonsenseqa :  36%|                                              | 18/50 [28:34<50:44, 95.13s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [30:09<49:08, 95.10s/it]Evaluating commonsenseqa :  40%|                                           | 20/50 [31:43<47:28, 94.96s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [33:17<45:45, 94.66s/it]Evaluating commonsenseqa :  44%|                                        | 22/50 [34:52<44:09, 94.64s/it]Evaluating commonsenseqa :  46%|                                       | 23/50 [36:27<42:39, 94.78s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [38:02<41:06, 94.86s/it]Evaluating commonsenseqa :  50%|                                    | 25/50 [39:36<39:26, 94.65s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [41:11<37:50, 94.59s/it]Evaluating commonsenseqa :  54%|                                 | 27/50 [42:45<36:14, 94.55s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [44:22<34:54, 95.22s/it]Evaluating commonsenseqa :  58%|                              | 29/50 [45:57<33:20, 95.28s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [47:32<31:42, 95.15s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [49:06<30:02, 94.88s/it]Evaluating commonsenseqa :  64%|                          | 32/50 [50:42<28:31, 95.08s/it]Evaluating commonsenseqa :  66%|                        | 33/50 [52:17<26:54, 95.00s/it]Evaluating commonsenseqa :  68%|                       | 34/50 [53:51<25:17, 94.86s/it]Evaluating commonsenseqa :  70%|                     | 35/50 [55:27<23:45, 95.04s/it]Evaluating commonsenseqa :  72%|                    | 36/50 [57:03<22:17, 95.53s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [58:38<20:36, 95.11s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:00:12<18:59, 95.00s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:01:48<17:28, 95.30s/it]Evaluating commonsenseqa :  80%|              | 40/50 [1:03:23<15:51, 95.17s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:04:58<14:15, 95.03s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:06:34<12:42, 95.36s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:08:09<11:05, 95.13s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:09:44<09:31, 95.25s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:11:19<07:55, 95.11s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:12:54<06:19, 94.97s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:14:29<04:45, 95.21s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:16:04<03:10, 95.10s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:17:41<01:35, 95.57s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:19:16<00:00, 95.45s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:19:16<00:00, 95.13s/it]
name: commonsenseqa | avg. gen lenth: 416.216 | time: 4756.975300788879s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 12:50:50,032] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 12:50:50,123] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 12:50:50,140] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 12:50:50,149] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 7
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i7-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9734 [00:00<?, ?it/s]Loading data: 100%|| 9734/9734 [00:00<00:00, 336296.10it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.32s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.71s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.11s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.14s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.80s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.18s/it]
[2023-08-30 12:51:00,192] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.41s/it]
 > number of parameters: 6738415616
[2023-08-30 12:51:00,448] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.31s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.73s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.30s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.73s/it]
[2023-08-30 12:51:01,119] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 12:51:01,164] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 12:51:01,771] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 12:51:01,772] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 12:51:01,772] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 12:51:01,772] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 12:51:01,772] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 12:51:01,772] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f23af1b9300>
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 12:51:01,773] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 12:51:01,774] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 12:51:01,774] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. The sentence in the question is describing a complicated machine. The right word must fit well into this context.
2. The choices are: A: box B: apparatus C: appliance D: wash dishes E: implement.
3. Looking up each choice, an 'apparatus' can denote a complex machine or device, 'appliance' stands for a piece of equipment designed to perform a specific task, typically a domestic one, 'implement' is a tool or utensil helping to do something(not complex enough), and 'box' is a container or 'wash dishes' obviously does not fit in the context.
4. Therefore, the appropriate word is apparatus as it is complex and it fits the context of the sentence well. 
5. So, the answer is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: 1. The question asks where we can obtain petrol.
2. Petrol, also known as gasoline, is a type of fuel commonly used in internal combustion engines.
3. Looking at the choices:
    - Option A and C suggest "burn hot". However, this is a description of what can happen when petrol is used, not a place where it can be obtained, so both options A and C can be eliminated.
    - Option B suggests a "fuel tank." While a fuel tank is where petrol is stored once obtained, it's not a place where one can go to get petrol, hence option B is not correct.
    - Option D suggests a "car." While it's true that petrol is used in cars, cars are not a place where you can get petrol. They are where you use petrol. So, option D can also be eliminated.
    - Option E suggests a "gas station." Gas stations are exactly where one would go to obtain petrol to fuel their cars or other engines.
4. Therefore, the answer is E: Gas station, which is the place where petrol can be obtained.
So the final answer is E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: 1. A television used at night most likely implies it is used for personal entertainment during leisure time, especially before sleep time.
2. If we go through the answer options: a cabinet is usually for storing items, so it's not likely to be the correct choice.
3. A house or an apartment could be a right answer, as they do have televisions. However, these options are pretty broad because they contain many other rooms/areas where people may or may not watch television.
4. Woods is an unlikely answer as it is open and not resided in, making it unfeasible for using a television, especially during the night.
5. That leaves us the option of a bedroom. People often watch TV in their bedrooms at night as a form of relaxation or to fall asleep. 
6. Therefore, the most accurate answer would be D. bedroom, because it's the most likely place for a television to be used at night.
So the final answer is D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: 1. The phrase "The knowledge was expensive to get" implies a place where extensive and high-level learning occurs, often requiring significant financial investment.
2. A book (choice B), could contain expensive knowledge, but the cost of a book is generally not high compared to other means of obtaining knowledge.
3. A field (choice C) typically refers to "field of study" or on the job training, this could be seen as costly in terms of time and commitment, but it's not inherently expensive.
4. Meetings (choice D) could present costly information if it's a high-end conference or seminar, but again meetings in general do not require a significant financial investment.
5. A class (choice E) might be expensive depending on the course or program, but a class is typically part of a larger educational institution or system.
6. Out of all the choices, a university (choice A) is the most likely to require a substantial financial investment. This is due to high tuition costs, cost of books, housing fees, and other related expenses.
7. Therefore, the answer is A: University.
So the final answer is A: university

Input: Where are small grapes likely to be found? Choices:  A: lunch box B: food store C: wine country D: kitchen E: raisins
Output: 1. The question is asking for a location where small grapes are likely found.
2. Looking at the answer choices, we have a lunch box, a food store, wine country, a kitchen and raisins.
3. Think about the context in which small grapes specifically can be seen.
4. Small grapes could exist in a lunch box, a food store, or a kitchen, but there's no specification in these places for the grapes to be small.
5. But in the case of wine country, wine grapes are typically smaller in size than the grapes typically eaten as fruit.
6. Raisins are not a location but are dried grapes. 
7. Therefore, the most likely answer would be wine country (C) where small grapes are specifically cultivated for wine production.
So the final answer is C: wine country

Input: The painter explained how he never achieved a flawless portrait, he said this was because all people are what? Choices:  A: imperfect B: disfigured C: damaged D: flawed E: defective
Output: 1. The question is asking for a word from the choices that could describe the reason why the painter never achieved a flawless portrait.
2. As per the painter's explanation, the reason is related to the inherent quality of people.
3. The choices offer various negative characteristics that could potentially describe people, which include being imperfect, disfigured, damaged, flawed, or defective.
4. The words 'disfigured', 'damaged', 'defective' could all be considered to mean 'broken' or 'wounded'. It's not in line with the painter's statement which intended to imply a natural perceived imperfection rather than a severe condition.
5. The word 'flawed' could also apply but it is more often used to describe faults or mistakes.
6. The word 'imperfect' best fits the explanation as it refers to the natural shortcomings or limitations that all people inherently have.
7. Therefore, the answer is A: imperfect.
So the final answer is A: imperfect

Input: If you're betting with a shark, where are you likely playing? Choices:  A: aquarium B: mediterranean sea C: south pacific D: pool hall E: pacific ocean
Output: 1. The term 'betting with a shark' is a slang term. 
2. This term does not refer to the animal shark, but to an experienced gambler who wins consistently, often in pool or billiards games.
3. Thus, it's necessary to identify which of the options provided is a place where this type of activity could happen. 
4. The options aquarium, mediterranean sea, south pacific and pacific ocean - while they could contain the animal species'shark' - do not relate to the context of gambling or pool games. 
5. The only option left and that fits the context of gambling specifically is 'pool hall', which is commonly associated with pool or billiards games. 
6. Therefore, the answer is D: pool hall.
So the final answer is D: pool hall

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i7-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                     | 1/50 [01:23<1:08:03, 83.34s/it]Evaluating commonsenseqa :   4%|                                                                    | 2/50 [02:45<1:06:00, 82.50s/it]Evaluating commonsenseqa :   6%|                                                                  | 3/50 [04:07<1:04:37, 82.50s/it]Evaluating commonsenseqa :   8%|                                                                 | 4/50 [05:29<1:03:08, 82.37s/it]Evaluating commonsenseqa :  10%|                                                                | 5/50 [06:52<1:01:47, 82.38s/it]Evaluating commonsenseqa :  12%|                                                              | 6/50 [08:15<1:00:35, 82.63s/it]Evaluating commonsenseqa :  14%|                                                              | 7/50 [09:37<59:11, 82.59s/it]Evaluating commonsenseqa :  16%|                                                             | 8/50 [10:59<57:38, 82.35s/it]Evaluating commonsenseqa :  18%|                                                           | 9/50 [12:22<56:21, 82.48s/it]Evaluating commonsenseqa :  20%|                                                         | 10/50 [13:44<54:50, 82.25s/it]Evaluating commonsenseqa :  22%|                                                        | 11/50 [15:06<53:33, 82.39s/it]Evaluating commonsenseqa :  24%|                                                      | 12/50 [16:29<52:08, 82.32s/it]Evaluating commonsenseqa :  26%|                                                     | 13/50 [17:52<51:01, 82.74s/it]Evaluating commonsenseqa :  28%|                                                   | 14/50 [19:16<49:45, 82.94s/it]Evaluating commonsenseqa :  30%|                                                  | 15/50 [20:38<48:15, 82.74s/it]Evaluating commonsenseqa :  32%|                                                 | 16/50 [22:00<46:41, 82.39s/it]Evaluating commonsenseqa :  34%|                                               | 17/50 [23:22<45:15, 82.27s/it]Evaluating commonsenseqa :  36%|                                              | 18/50 [24:44<43:50, 82.21s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [26:06<42:28, 82.21s/it]Evaluating commonsenseqa :  40%|                                           | 20/50 [27:29<41:13, 82.45s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [28:51<39:44, 82.23s/it]Evaluating commonsenseqa :  44%|                                        | 22/50 [30:13<38:21, 82.19s/it]Evaluating commonsenseqa :  46%|                                       | 23/50 [31:35<36:55, 82.07s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [32:58<35:42, 82.42s/it]Evaluating commonsenseqa :  50%|                                    | 25/50 [34:19<34:13, 82.16s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [35:41<32:51, 82.15s/it]Evaluating commonsenseqa :  54%|                                 | 27/50 [37:05<31:37, 82.50s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [38:27<30:15, 82.53s/it]Evaluating commonsenseqa :  58%|                              | 29/50 [39:50<28:54, 82.58s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [41:11<27:23, 82.17s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [42:34<26:02, 82.24s/it]Evaluating commonsenseqa :  64%|                          | 32/50 [43:56<24:40, 82.26s/it]Evaluating commonsenseqa :  66%|                        | 33/50 [45:18<23:17, 82.22s/it]Evaluating commonsenseqa :  68%|                       | 34/50 [46:41<21:56, 82.29s/it]Evaluating commonsenseqa :  70%|                     | 35/50 [48:03<20:36, 82.41s/it]Evaluating commonsenseqa :  72%|                    | 36/50 [49:26<19:13, 82.40s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [50:37<17:08, 79.10s/it]Evaluating commonsenseqa :  76%|                 | 38/50 [52:00<16:04, 80.33s/it]Evaluating commonsenseqa :  78%|               | 39/50 [53:23<14:50, 80.92s/it]Evaluating commonsenseqa :  80%|              | 40/50 [54:45<13:33, 81.37s/it]Evaluating commonsenseqa :  82%|             | 41/50 [56:08<12:16, 81.84s/it]Evaluating commonsenseqa :  84%|           | 42/50 [57:30<10:54, 81.85s/it]Evaluating commonsenseqa :  86%|          | 43/50 [58:53<09:35, 82.19s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:00:16<08:14, 82.36s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:01:38<06:51, 82.37s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:03:00<05:29, 82.34s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:04:23<04:07, 82.57s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:05:45<02:44, 82.44s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:07:07<01:22, 82.23s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:08:29<00:00, 82.18s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:08:29<00:00, 82.19s/it]
name: commonsenseqa | avg. gen lenth: 418.628 | time: 4110.270145893097s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 13:59:56,694] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 13:59:56,759] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 13:59:56,761] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 13:59:56,791] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 8
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9733 [00:00<?, ?it/s]Loading data: 100%|| 9733/9733 [00:00<00:00, 300971.42it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.92s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.98s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.98s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.07s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.50s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.11s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.54s/it]
[2023-08-30 14:00:07,394] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.11s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.54s/it]
 > number of parameters: 6738415616
[2023-08-30 14:00:07,467] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 14:00:07,579] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards:  50%|                                     | 1/2 [00:10<00:10, 10.94s/it]Loading checkpoint shards: 100%|| 2/2 [00:13<00:00,  5.74s/it]Loading checkpoint shards: 100%|| 2/2 [00:13<00:00,  6.52s/it]
[2023-08-30 14:00:11,449] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 14:00:12,104] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 14:00:12,106] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3b361bd300>
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 14:00:12,107] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 14:00:12,108] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 14:00:12,109] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 14:00:12,109] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. The sentence implies that the machine was quite complex or complicated. 
2. We need to find a word that could be used to refer to a complex machine.
3. Looking at the choices, "box" would not be correct because it is not typically used to describe something intricate or complex.
4. Similarly, "wash dishes" is incorrect as it's an action, not a descriptor.
5. While "appliance" and "implement" are words used to describe types of machines, they don't specifically describe a machine as being intricate.
6. Only the word "apparatus" fits well, as it can be used to refer to a complex equipment or machinery.
7. Therefore, the answer is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: 1. The question asks where you get petrol, which requires an understanding of what petrol is used for and where it is typically acquired.
2. Petrol is a type of fuel used to power internal combustion engines, which are commonly found in cars and other types of vehicles.
3. Looking at the choices, we can eliminate A and C because "burn hot" does not align with the concept of obtaining petrol.
4. Choice B, "fuel tank", is also not a place where you get petrol but rather where petrol is stored after it is acquired.
5. Choice D, "car", can also be eliminated because although we put petrol in a car, we do not obtain petrol from a car.
6. Through process of elimination, we are left with choice E, "gas station".
7. A gas station is where you generally go to acquire petrol for your car.
8. So the best answer to the question "Where do you get petrol?" would be E: gas station.
So the final answer is E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: Step 1: The question is asking for a location where a television is typically used at night.
Step 2: Let's consider each option individually.
Step 3: Option A is a "cabinet". A cabinet is not a location where a television is typically used at night. Therefore, a cabinet is not the correct answer.
Step 4: Next are options B and C, which are "house" and "apartment". While a television can certainly be used in both a house and an apartment, these are not the most specific answers because they could refer to anywhere within these buildings.
Step 5: We now consider option D, "bedroom". People often use a television in a bedroom at night before sleeping, making this a likely answer.
Step 6: The last option is E, "woods". The woods is not usually a place where people use a television, especially at night.
Step 7: Comparing all the options, the best and most specific answer is D, "bedroom". So, a television used at night might be in a bedroom.
So the final answer is D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: 1. The question is asking about the location knowledge was being gained.
2. The aspect of being "expensive" is a vital hint given in the question.
3. We know knowledge can be attained from various sources - a book, in the field, during meetings or in a class.
4. However, not all options associate with being "expensive". Books can be priced but it's not necessarily considered that high compared with other options.
5. Knowledge gained at a meeting or in the field does not typically require a large financial investment.
6. When we talk about an expensive learning environment, universities come into play as they often requires significant monetary investment in terms of tuition and other fees.
7. Therefore, the correct answer is A: university.
So the final answer is A: university

Input: Where are small grapes likely to be found? Choices:  A: lunch box B: food store C: wine country D: kitchen E: raisins
Output: 1. To solve this, you must examine each of the answer choices in relation to the original question. A lunch box, a food store, a kitchen, and raisins are all places where grapes of any size might be located. However, these options do not specify the size of the grapes.

2. The term'small grapes' can often be associated with the type of grapes used in wine making. These grapes are typically smaller than the type you'd find in a grocery store.

3. The term 'wine country' refers to regions that are famed for their vineyards and wine production. 

4. Thus option C: wine country is the place where small grapes are most likely to be found because it is the only choice that directly connects to the concept of small grapes used in wine making. 

5. Hence, the answer is C: wine country.
So the final answer is C: wine country

Input: The painter explained how he never achieved a flawless portrait, he said this was because all people are what? Choices:  A: imperfect B: disfigured C: damaged D: flawed E: defective
Output: Step 1: The question asks what the painter believes about all people, which is the reason he never achieves a flawless portrait. 
Step 2: To begin the reasoning process, we need to find a term from the choices that aligns with the notion that people are not without flaws. 
Step 3: A look at the choices presents us with the options: imperfect, disfigured, damaged, flawed, and defective. 
Step 4: Disfigured, damaged, and defective are eliminated because they imply physical or serious flaws which may not apply to all people and don't necessarily represent the common understanding of human nature. 
Step 5: Looking at the remaining options (imperfect and flawed), flawed carries a significantly negative connotation compared to imperfect. Since there is no indication from the question that the painter views all people in a negative light, we eliminate flawed. 
Final Step: Therefore, the only suitable choice that aligns with believing all people have some shortcomings but does not denote a negative connotation is "imperfect," making option A: Imperfect the correct answer.
So the final answer is A: imperfect

Input: If you're betting with a shark, where are you likely playing? Choices:  A: aquarium B: mediterranean sea C: south pacific D: pool hall E: pacific ocean
Output: 1. Understand the question and the set of answers given. It may seem that the question refers to a real shark, implying water-based environments like an aquarium, the Mediterranean Sea, South Pacific or Pacific Ocean.
2. Understand the slang term shark. In some contexts, especially in the field of gaming and betting, a "shark" is a slang term referring to a person who is an expert or highly skilled professional.
3. Apply the meaning of "shark" to the given options. Considering the "shark" as a skilled gamer, it can be deduced that a shark does not necessarily mean a real marine creature but someone good at games.
4. Analyze the answer choices. Among the options listed, the only place where you are likely to play or bet with a highly skilled professional or expert "shark" in gaming is D: pool hall. 
5. Therefore, the answer is D: Pool Hall. A pool hall is a place where "sharks" or skilled individuals play pool games and sometimes place bets.
So the final answer is D: pool hall

Input: Marmot's can be found in high places.  They have hands that they can use to get to the top of what? Choices:  A: outside B: hill C: jungle D: rocky hillside E: tree
Output: 1. The first step is to understand the main features of the question which talks about Marmot's ability to reach high places due to their hands.
2. As per the question, we should associate their hands with an ability to climb.
3. The next step is to identify the options that involve climbing.
4. Among the given options, 'outside' is too general and does not specify a particular place or object that a marmot could climb.
5. 'Hill' and 'rocky hillside' can be eliminated because marmots cannot use their hands to climb these surfaces due to their steepness and rough texture.
6. 'Jungle' is also eliminated because it does not indicate a specific high place that marmots can climb using their hands.
7. This leaves us with 'tree', which is an object that marmots can climb using their hands.
8. Therefore, the answer is E: Tree.
So the final answer is E: tree

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                     | 1/50 [01:17<1:03:35, 77.86s/it]Evaluating commonsenseqa :   4%|                                                                    | 2/50 [02:34<1:01:36, 77.02s/it]Evaluating commonsenseqa :   6%|                                                                  | 3/50 [03:51<1:00:17, 76.97s/it]Evaluating commonsenseqa :   8%|                                                                   | 4/50 [05:07<58:43, 76.59s/it]Evaluating commonsenseqa :  10%|                                                                 | 5/50 [06:24<57:32, 76.73s/it]Evaluating commonsenseqa :  12%|                                                                | 6/50 [07:41<56:18, 76.78s/it]Evaluating commonsenseqa :  14%|                                                              | 7/50 [08:57<54:53, 76.60s/it]Evaluating commonsenseqa :  16%|                                                             | 8/50 [10:14<53:41, 76.70s/it]Evaluating commonsenseqa :  18%|                                                           | 9/50 [11:30<52:24, 76.70s/it]Evaluating commonsenseqa :  20%|                                                         | 10/50 [12:46<50:57, 76.44s/it]Evaluating commonsenseqa :  22%|                                                        | 11/50 [14:02<49:37, 76.35s/it]Evaluating commonsenseqa :  24%|                                                      | 12/50 [15:19<48:22, 76.37s/it]Evaluating commonsenseqa :  26%|                                                     | 13/50 [16:36<47:11, 76.52s/it]Evaluating commonsenseqa :  28%|                                                   | 14/50 [17:52<45:49, 76.37s/it]Evaluating commonsenseqa :  30%|                                                  | 15/50 [19:09<44:44, 76.70s/it]Evaluating commonsenseqa :  32%|                                                 | 16/50 [20:26<43:24, 76.61s/it]Evaluating commonsenseqa :  34%|                                               | 17/50 [21:42<42:06, 76.55s/it]Evaluating commonsenseqa :  36%|                                              | 18/50 [22:58<40:46, 76.47s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [24:14<39:28, 76.39s/it]Evaluating commonsenseqa :  40%|                                           | 20/50 [25:31<38:11, 76.38s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [26:47<36:54, 76.35s/it]Evaluating commonsenseqa :  44%|                                        | 22/50 [28:03<35:33, 76.20s/it]Evaluating commonsenseqa :  46%|                                       | 23/50 [29:19<34:17, 76.20s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [30:36<33:03, 76.30s/it]Evaluating commonsenseqa :  50%|                                    | 25/50 [31:52<31:48, 76.33s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [33:08<30:29, 76.23s/it]Evaluating commonsenseqa :  54%|                                 | 27/50 [34:25<29:17, 76.42s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [35:41<27:58, 76.28s/it]Evaluating commonsenseqa :  58%|                              | 29/50 [36:57<26:42, 76.30s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [38:13<25:22, 76.12s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [39:30<24:10, 76.33s/it]Evaluating commonsenseqa :  64%|                          | 32/50 [40:46<22:52, 76.23s/it]Evaluating commonsenseqa :  66%|                        | 33/50 [42:02<21:34, 76.17s/it]Evaluating commonsenseqa :  68%|                       | 34/50 [43:18<20:18, 76.17s/it]Evaluating commonsenseqa :  70%|                     | 35/50 [44:35<19:06, 76.43s/it]Evaluating commonsenseqa :  72%|                    | 36/50 [45:52<17:50, 76.48s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [47:08<16:33, 76.39s/it]Evaluating commonsenseqa :  76%|                 | 38/50 [48:24<15:14, 76.22s/it]Evaluating commonsenseqa :  78%|               | 39/50 [49:40<13:57, 76.17s/it]Evaluating commonsenseqa :  80%|              | 40/50 [50:56<12:41, 76.16s/it]Evaluating commonsenseqa :  82%|             | 41/50 [52:12<11:25, 76.17s/it]Evaluating commonsenseqa :  84%|           | 42/50 [53:29<10:11, 76.41s/it]Evaluating commonsenseqa :  86%|          | 43/50 [54:46<08:55, 76.45s/it]Evaluating commonsenseqa :  88%|        | 44/50 [56:02<07:38, 76.45s/it]Evaluating commonsenseqa :  90%|       | 45/50 [57:19<06:23, 76.69s/it]Evaluating commonsenseqa :  92%|     | 46/50 [58:37<05:07, 76.88s/it]Evaluating commonsenseqa :  94%|    | 47/50 [59:53<03:50, 76.87s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:01:10<02:33, 76.77s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:02:26<01:16, 76.59s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:03:42<00:00, 76.49s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:03:42<00:00, 76.46s/it]
name: commonsenseqa | avg. gen lenth: 441.524 | time: 3823.510263442993s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 15:04:12,950] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 15:04:12,950] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 15:04:12,958] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 15:04:13,005] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 9
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i9-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9732 [00:00<?, ?it/s]Loading data: 100%|| 9732/9732 [00:00<00:00, 259844.46it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.11s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.13s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.17s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.20s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.13s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.58s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.17s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.62s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.17s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.62s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.19s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.64s/it]
[2023-08-30 15:04:23,889] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 15:04:23,893] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-30 15:04:23,946] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 15:04:23,992] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 15:04:24,555] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 15:04:24,556] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f95f07b92d0>
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 15:04:24,557] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 15:04:24,558] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 15:04:24,558] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. The problem here is to identify the most suitable word that fits the context of the sentence. 
2. From the given choices, we need to select a word that describes something that can be referred to as "very intricate".
3. An intricate thing is usually complex in design or system. 
4. 'Box' is a simple object to hold things, it doesn't necessarily have a complex arrangement, so it doesn't fit here.
5. 'Wash dishes' is not an object or thing, it is an action, and can't be referred to as something intricate, so we rule this out.
6. 'Implement' is a tool or a piece of equipment, which may or may not be complex, so it doesn't quite fit.
7. 'Appliance' can also represent a complex item, but appliances are usually referred to devices that perform specific tasks around the household, like a fridge, a microwave, or a dishwasher. 
8. 'Apparatus' is a more generic term used for a complex set of equipment or machine that performs a specific task. This fits the context well because it can be intricate, complex in its design or mechanism.
9. Therefore, by analyzing and eliminating the wrong answer choices, the correct answer is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: 1. The question is asking us to identify where we usually acquire petrol.
2. We begin by evaluating each of the given choices in relation to the question.
3. Choice A suggests that petrol is obtained from burning something hot, which is incorrect because petrol is a type of fuel, not a byproduct of heat.
4. Choice B suggests that petrol is obtained from a fuel tank, which isn't the case, because a fuel tank is where petrol is stored, not where it is sourced.
5. Choice C is the same as Choice A, suggesting that petrol is obtained from burning something hot, which is also incorrect.
6. Choice D suggests that petrol is obtained from a car. While cars use petrol to operate, they aren't where petrol is sourced.
7. Choice E suggests that petrol is obtained from a gas station, which is correct. Petrol is typically purchased from gas stations.
8. Therefore, the answer is E: gas station.
So the final answer is E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: 1. First, think about where a television is typically used.
2. Televisions are typically used in indoor surroundings, rather than outdoor locations, because of the need for electricity and a stable set up, thereby excluding option E: woods.
3. We narrow down the choices to A: cabinet, B: house, C: apartment, and D: bedroom.
4. The term "cabinet" usually refers to a piece of furniture for storage. While televisions could be stored in a TV cabinet, we generally use the term 'cabinet' to denote a place of storage and not active use, so we can exclude option A: cabinet.
5. This leaves options B: house, C: apartment, and D: bedroom.
6. Both a house and an apartment are environments where a TV is likely to be found. 
7. However, the question specifies that the television is used at night. 
8. Considering human behaviour and activities typically done at night, people usually retreat to their bedrooms after their day's work for relaxation and rest.
9. Ingenerally, bedrooms are the most likely places for people to be watching television at night.
10. Therefore, option D: bedroom is the most suitable answer.
So the final answer is D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: 1. The question is asking where knowledge is being gained that is considered "expensive" to get.
2. We can assume that the term "expensive" here implies a significant amount of money is spent to gain this knowledge.
3. Now look at given options and try to associate them with the possible places where people usually spend money to gain knowledge.
4. Option A: University - This is a very plausible answer as people often pay high fees for gaining knowledge in a university.
5. Option B: Book - While books can sometimes be pricey, they are not typically considered an expensive source of knowledge.
6. Option C: Field - This seems to refer to gaining knowledge through experience or on job training. Although it could be expensive in terms of time, it's not typically associated with a significant monetary cost.
7. Option D: Meeting - This option would not typically be associated with a high cost to gain knowledge.
8. Option E: Class - While taking classes can be expensive but it usually refers to smaller teaching sessions and not necessarily implying significant expenditure like a university degree does.
9. So, it can be reasonably deduced that the university is the most strongly associated with spending a significant amount of money to gain knowledge.
10. Therefore, the answer is A: university.
So the final answer is A: university

Input: Where are small grapes likely to be found? Choices:  A: lunch box B: food store C: wine country D: kitchen E: raisins
Output: 1. The question asks about where small grapes are most likely to be found.
2. We need to consider the best place where small grapes would typically be situated.
3. Option A (lunch box) could contain small grapes, but they are not naturally found there.
4. Option B (food store) may have small grapes but they are also not naturally found here, they are sold here which is dependent on if and when they are delivered to the store.
5. Option C is wine country, where small grapes would naturally grow, as wine is produced from grapes, often smaller ones.
6. Option D (kitchen) might have small grapes but they don't naturally appear there.
7. Option E (raisins) is not a location.
8. Based on the options, Wine Country (Option C) is the most logical choice. This is the place where small grapes are naturally grown and specifically used for wine production.
So the final answer is C: wine country

Input: The painter explained how he never achieved a flawless portrait, he said this was because all people are what? Choices:  A: imperfect B: disfigured C: damaged D: flawed E: defective
Output: 1. The question presents a statement from a painter about how he never achieved a flawless portrait.
2. The reasoning he gives for this is that all people are a certain way. 
3. The answer choices provided all represent different types of imperfections or deficiencies. 
4. However, some words such as "disfigured," "damaged," and "defective" imply physical harm or abnormal structure, which might not be the reason behind the lack of perfection in the painter's portraits. 
5. The options "flawed" and "imperfect" are more generic descriptions, applying to failures or inaccuracies in a broad sense. 
6. In this case, "imperfect" would be a more suitable answer as it can include a wider array of imperfections, both physical and non-physical.
7. Thus, the answer is A: imperfect.
So the final answer is A: imperfect

Input: If you're betting with a shark, where are you likely playing? Choices:  A: aquarium B: mediterranean sea C: south pacific D: pool hall E: pacific ocean
Output: 1. The term "betting with a shark" actually contains the clue. In this case, it is not a marine creature but an indirect reference to a cunning person with great skills in whatever game is being played.
2. The clue is that it is more about a place where games requiring skills and often involving wagers are played. 
3. The aquarium, the Mediterranean sea, the South Pacific, and the Pacific Ocean are associated with actual marine sharks, but they are not places typically associated with gambling or betting games.
4. Thus, we can rule out options A, B, C, and E, leading us to the conclusion that the phrase "betting with a shark" is referring to playing games in a 'pool hall' (option D) where the term'shark' is often used for a highly skilled pool player who often bets while playing.
5. So, the answer is D: pool hall.
So the final answer is D: pool hall

Input: Marmot's can be found in high places.  They have hands that they can use to get to the top of what? Choices:  A: outside B: hill C: jungle D: rocky hillside E: tree
Output: 1. Marmots are known to reside in high places.
2. The question asks for a place that the marmot can use its hands to get to the top of. 
3. The given options are: outside, hill, jungle, rocky hillside, and tree. 
4. "Outside", "hill", and "jungle" seem less suitable options as they aren't necessarily high places and it's not clear how a marmot would use its hands to climb them.
5. While a Rocky hillside could be a high place, it would require more than hands to climb it, perhaps a combination of both feet and hands.
6. Trees are often found in high places and could feasibly be climbed using hands. Therefore, this seems to be the most logical choice. 
7. Hence, the answer is E: tree.
So the final answer is E: tree

Input: Kids have vivid imaginations, they can take anything and use it to what? Choices:  A: play with toys B: play games C: learn things D: do homework E: swimming pool
Output: 1. The question asks what children can utilize their vivid imagination for. 

2. When thinking about the options: Toys is too specific and is often a part of what kids imagine about but not the only thing. 

3. Homework seems inappropriate, children don't use their imagination to complete their assignments. 

4. A swimming pool is also an inappropriate choice because it doesn't relate to the use of imagination. 

5. Learning things could be possible, but it is not the most probable choice because while kids learn, it's not the primary context for using their imagination.

6. Option B, playing games, involves a lot of imagination, more so than the other options. Kids often create pretend scenarios as part of their games.

7. Therefore, the answer is B: play games.
So the final answer is B: play games

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i9-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                       | 1/50 [01:10<57:23, 70.28s/it]Evaluating commonsenseqa :   4%|                                                                      | 2/50 [02:19<55:45, 69.70s/it]Evaluating commonsenseqa :   6%|                                                                    | 3/50 [03:29<54:30, 69.59s/it]Evaluating commonsenseqa :   8%|                                                                   | 4/50 [04:37<53:08, 69.32s/it]Evaluating commonsenseqa :  10%|                                                                 | 5/50 [05:47<51:58, 69.29s/it]Evaluating commonsenseqa :  12%|                                                                | 6/50 [06:55<50:38, 69.07s/it]Evaluating commonsenseqa :  14%|                                                              | 7/50 [08:05<49:39, 69.29s/it]Evaluating commonsenseqa :  16%|                                                             | 8/50 [09:14<48:30, 69.29s/it]Evaluating commonsenseqa :  18%|                                                           | 9/50 [10:24<47:21, 69.31s/it]Evaluating commonsenseqa :  20%|                                                         | 10/50 [11:32<46:04, 69.12s/it]Evaluating commonsenseqa :  22%|                                                        | 11/50 [12:42<45:05, 69.38s/it]Evaluating commonsenseqa :  24%|                                                      | 12/50 [13:51<43:46, 69.12s/it]Evaluating commonsenseqa :  26%|                                                     | 13/50 [15:00<42:38, 69.15s/it]Evaluating commonsenseqa :  28%|                                                   | 14/50 [16:09<41:29, 69.15s/it]Evaluating commonsenseqa :  30%|                                                  | 15/50 [17:18<40:17, 69.07s/it]Evaluating commonsenseqa :  32%|                                                 | 16/50 [18:27<39:07, 69.05s/it]Evaluating commonsenseqa :  34%|                                               | 17/50 [19:36<37:55, 68.94s/it]Evaluating commonsenseqa :  36%|                                              | 18/50 [20:45<36:51, 69.11s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [21:54<35:39, 69.02s/it]Evaluating commonsenseqa :  40%|                                           | 20/50 [23:04<34:38, 69.28s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [24:13<33:30, 69.33s/it]Evaluating commonsenseqa :  44%|                                        | 22/50 [25:22<32:17, 69.19s/it]Evaluating commonsenseqa :  46%|                                       | 23/50 [26:31<31:05, 69.08s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [27:40<29:53, 68.99s/it]Evaluating commonsenseqa :  50%|                                    | 25/50 [28:49<28:42, 68.89s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [29:57<27:33, 68.88s/it]Evaluating commonsenseqa :  54%|                                 | 27/50 [31:06<26:23, 68.85s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [32:15<25:13, 68.77s/it]Evaluating commonsenseqa :  58%|                              | 29/50 [33:24<24:08, 68.97s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [34:33<23:00, 69.02s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [35:42<21:50, 68.97s/it]Evaluating commonsenseqa :  64%|                          | 32/50 [36:51<20:42, 69.00s/it]Evaluating commonsenseqa :  66%|                        | 33/50 [38:01<19:36, 69.21s/it]Evaluating commonsenseqa :  68%|                       | 34/50 [39:10<18:27, 69.23s/it]Evaluating commonsenseqa :  70%|                     | 35/50 [40:19<17:16, 69.07s/it]Evaluating commonsenseqa :  72%|                    | 36/50 [41:28<16:05, 68.97s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [42:36<14:55, 68.88s/it]Evaluating commonsenseqa :  76%|                 | 38/50 [43:46<13:47, 68.98s/it]Evaluating commonsenseqa :  78%|               | 39/50 [44:54<12:37, 68.87s/it]Evaluating commonsenseqa :  80%|              | 40/50 [46:03<11:28, 68.83s/it]Evaluating commonsenseqa :  82%|             | 41/50 [47:12<10:18, 68.76s/it]Evaluating commonsenseqa :  84%|           | 42/50 [48:20<09:10, 68.81s/it]Evaluating commonsenseqa :  86%|          | 43/50 [49:29<08:01, 68.85s/it]Evaluating commonsenseqa :  88%|        | 44/50 [50:39<06:53, 68.94s/it]Evaluating commonsenseqa :  90%|       | 45/50 [51:48<05:45, 69.03s/it]Evaluating commonsenseqa :  92%|     | 46/50 [52:57<04:36, 69.03s/it]Evaluating commonsenseqa :  94%|    | 47/50 [54:06<03:26, 68.94s/it]Evaluating commonsenseqa :  96%|   | 48/50 [55:15<02:17, 68.95s/it]Evaluating commonsenseqa :  98%| | 49/50 [56:23<01:08, 68.93s/it]Evaluating commonsenseqa : 100%|| 50/50 [57:32<00:00, 68.88s/it]Evaluating commonsenseqa : 100%|| 50/50 [57:32<00:00, 69.05s/it]
name: commonsenseqa | avg. gen lenth: 430.692 | time: 3453.3211483955383s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s20-rFalse --seed 20 --max-prompt-length 4096 --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 16:02:18,938] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 16:02:18,971] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 16:02:18,991] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 16:02:19,015] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s20-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 1
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s20-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9740 [00:00<?, ?it/s]Loading data: 100%|| 9740/9740 [00:00<00:00, 1262243.81it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.54s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.77s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.16s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.10s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.90s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.29s/it]
 > number of parameters: 6738415616
[2023-08-30 16:02:28,763] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.97s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.39s/it]
[2023-08-30 16:02:28,984] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.16s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.61s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.14s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.58s/it]
[2023-08-30 16:02:29,422] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 16:02:29,475] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 16:02:30,089] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 16:02:30,090] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 16:02:30,090] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 16:02:30,090] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 16:02:30,090] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 16:02:30,090] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3f70dcd2a0>
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 16:02:30,091] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 16:02:30,092] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 16:02:30,092] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: B: apparatus

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s20-rFalse-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [02:13<1:49:21, 133.91s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [04:33<1:49:50, 137.30s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [06:53<1:48:35, 138.62s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [09:13<1:46:29, 138.90s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [11:36<1:45:18, 140.41s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [12:27<1:20:44, 110.11s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [14:51<1:26:51, 121.19s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [16:47<1:23:35, 119.41s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [19:10<1:26:37, 126.77s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [21:25<1:26:18, 129.45s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [23:45<1:26:19, 132.80s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [26:04<1:25:19, 134.71s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [28:28<1:24:39, 137.28s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [30:48<1:22:54, 138.17s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [33:07<1:20:50, 138.57s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [34:55<1:13:14, 129.25s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [37:17<1:13:08, 132.99s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [39:06<1:07:12, 126.02s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [41:28<1:07:30, 130.67s/it]Evaluating commonsenseqa :  40%|                                         | 20/50 [43:50<1:06:59, 133.99s/it]Evaluating commonsenseqa :  42%|                                        | 21/50 [46:09<1:05:34, 135.67s/it]Evaluating commonsenseqa :  44%|                                      | 22/50 [48:27<1:03:34, 136.24s/it]Evaluating commonsenseqa :  46%|                                     | 23/50 [50:45<1:01:33, 136.78s/it]Evaluating commonsenseqa :  48%|                                    | 24/50 [53:09<1:00:11, 138.89s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [55:29<58:05, 139.43s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [56:34<46:47, 116.99s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [58:53<47:20, 123.52s/it]Evaluating commonsenseqa :  56%|                              | 28/50 [1:01:13<47:09, 128.62s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [1:03:35<46:23, 132.54s/it]Evaluating commonsenseqa :  60%|                           | 30/50 [1:05:54<44:49, 134.50s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:07:58<41:33, 131.24s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:10:19<40:16, 134.25s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:12:40<38:34, 136.16s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:14:59<36:32, 137.04s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:16:58<32:57, 131.82s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:19:21<31:30, 135.07s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:21:42<29:38, 136.82s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:23:04<24:06, 120.50s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:25:24<23:09, 126.29s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:26:42<18:37, 111.77s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:29:03<18:05, 120.62s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:30:38<15:03, 112.92s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:32:59<14:08, 121.22s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:35:22<12:46, 127.75s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:37:43<10:58, 131.74s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:40:04<08:58, 134.56s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:42:24<06:48, 136.23s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:44:46<04:35, 137.93s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:46:40<02:10, 130.63s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:48:59<00:00, 133.25s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:48:59<00:00, 130.79s/it]
name: commonsenseqa | avg. gen lenth: 256.944 | time: 6539.69127368927s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s20-rFalse --seed 20 --max-prompt-length 4096 --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 17:53:38,182] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 17:53:38,197] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 17:53:38,197] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 17:53:38,250] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s20-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 2
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s20-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9739 [00:00<?, ?it/s]Loading data: 100%|| 9739/9739 [00:00<00:00, 1210823.06it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.99s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.02s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.10s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.22s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.13s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.56s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.13s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.56s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.18s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.62s/it]
 > number of parameters: 6738415616
[2023-08-30 17:53:48,757] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.18s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.63s/it]
[2023-08-30 17:53:48,770] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 17:53:48,871] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 17:53:48,920] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 17:53:49,492] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 17:53:49,494] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 17:53:49,494] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 17:53:49,494] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 17:53:49,494] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 17:53:49,494] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbdd71cd300>
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 17:53:49,495] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 17:53:49,496] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 17:53:49,496] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: E: gas station

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s20-rFalse-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [02:19<1:53:43, 139.25s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [04:40<1:52:27, 140.57s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [07:00<1:49:57, 140.37s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [09:22<1:47:59, 140.87s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [10:04<1:19:01, 105.36s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [12:26<1:26:12, 117.56s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [14:48<1:30:07, 125.76s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [17:09<1:31:23, 130.56s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [19:27<1:30:46, 132.84s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [21:46<1:29:49, 134.74s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [24:06<1:28:42, 136.49s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [24:55<1:09:31, 109.78s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [27:12<1:12:46, 118.01s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [29:32<1:14:49, 124.71s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [31:50<1:15:01, 128.62s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [34:09<1:14:38, 131.73s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [36:28<1:13:39, 133.93s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [38:46<1:12:04, 135.14s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [41:04<1:10:18, 136.06s/it]Evaluating commonsenseqa :  40%|                                         | 20/50 [43:23<1:08:24, 136.82s/it]Evaluating commonsenseqa :  42%|                                        | 21/50 [45:43<1:06:41, 137.99s/it]Evaluating commonsenseqa :  44%|                                      | 22/50 [48:02<1:04:29, 138.20s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [49:34<55:53, 124.19s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [51:50<55:26, 127.93s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [54:11<54:57, 131.90s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [55:55<49:23, 123.47s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [58:16<49:22, 128.79s/it]Evaluating commonsenseqa :  56%|                              | 28/50 [1:00:36<48:23, 132.00s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [1:02:03<41:27, 118.48s/it]Evaluating commonsenseqa :  60%|                           | 30/50 [1:03:08<34:11, 102.56s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:05:29<36:04, 113.94s/it]Evaluating commonsenseqa :  64%|                         | 32/50 [1:06:30<29:28, 98.22s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:08:15<28:22, 100.15s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:09:23<24:10, 90.64s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:11:44<26:24, 105.62s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:13:18<23:51, 102.22s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:15:01<22:11, 102.46s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:17:20<22:39, 113.31s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:19:37<22:04, 120.43s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:21:57<21:04, 126.41s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:24:17<19:32, 130.28s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:26:37<17:47, 133.40s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:27:34<12:53, 110.53s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:29:05<10:26, 104.46s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:30:29<08:11, 98.37s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:32:49<07:23, 110.95s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:35:04<05:54, 118.13s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:36:14<03:27, 103.58s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:38:33<01:54, 114.31s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:40:53<00:00, 121.94s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:40:53<00:00, 121.07s/it]
name: commonsenseqa | avg. gen lenth: 230.792 | time: 6053.575313329697s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s20-rFalse --seed 20 --max-prompt-length 4096 --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 19:42:51,792] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 19:42:52,315] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 19:42:52,357] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 19:42:52,368] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s20-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 3
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i3-s20-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9738 [00:00<?, ?it/s]Loading data: 100%|| 9738/9738 [00:00<00:00, 1156926.48it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.67s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.89s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.90s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.96s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.94s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.35s/it]
[2023-08-30 19:43:02,212] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.02s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.45s/it]
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.07s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.50s/it]
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.07s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.50s/it]
 > number of parameters: 6738415616
[2023-08-30 19:43:02,477] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 19:43:02,502] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 19:43:02,569] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 19:43:03,198] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 19:43:03,200] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f42a9bb9300>
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 19:43:03,200] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 19:43:03,201] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 19:43:03,201] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: D: bedroom

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i3-s20-rFalse-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [02:17<1:52:30, 137.76s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [04:34<1:49:33, 136.95s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [06:54<1:48:32, 138.56s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [09:10<1:45:22, 137.43s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [11:09<1:38:00, 130.67s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [13:27<1:37:46, 133.33s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [14:53<1:24:21, 117.71s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [17:09<1:26:34, 123.69s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [18:58<1:21:22, 119.08s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [21:18<1:23:37, 125.45s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [23:35<1:23:55, 129.13s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [25:40<1:20:54, 127.75s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [27:16<1:12:57, 118.32s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [29:32<1:14:05, 123.50s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [31:44<1:13:34, 126.13s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [34:02<1:13:31, 129.76s/it]Evaluating commonsenseqa :  34%|                                              | 17/50 [34:58<59:05, 107.44s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [37:15<1:02:04, 116.39s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [38:57<57:58, 112.20s/it]Evaluating commonsenseqa :  40%|                                         | 20/50 [41:16<1:00:07, 120.25s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [43:25<59:18, 122.70s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [45:43<59:27, 127.42s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [48:02<58:53, 130.86s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [50:21<57:46, 133.34s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [52:38<56:00, 134.41s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [54:56<54:07, 135.31s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [57:13<52:04, 135.87s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [59:27<49:40, 135.45s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [1:01:45<47:40, 136.23s/it]Evaluating commonsenseqa :  60%|                           | 30/50 [1:02:18<35:01, 105.09s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:04:34<36:16, 114.56s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:06:51<36:19, 121.06s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:07:55<29:27, 103.95s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:10:12<30:25, 114.06s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:11:25<25:23, 101.56s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:13:41<26:08, 112.00s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:16:00<26:01, 120.08s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:17:23<21:46, 108.85s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:18:46<18:34, 101.30s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:21:04<18:42, 112.26s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:22:26<15:29, 103.26s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:24:47<15:16, 114.51s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:26:52<13:44, 117.73s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:29:11<12:23, 123.89s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:31:26<10:36, 127.31s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:33:44<08:42, 130.55s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:36:00<06:36, 132.15s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:38:18<04:28, 134.05s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:39:17<01:51, 111.51s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:41:36<00:00, 119.57s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:41:36<00:00, 121.93s/it]
name: commonsenseqa | avg. gen lenth: 229.644 | time: 6096.510813713074s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s20-rFalse --seed 20 --max-prompt-length 4096 --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 21:27:25,983] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 21:27:26,037] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 21:27:26,097] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 21:27:26,139] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s20-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 4
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i4-s20-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9737 [00:00<?, ?it/s]Loading data: 100%|| 9737/9737 [00:00<00:00, 1097081.02it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.34s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.10s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.11s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.78s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.17s/it]
 > number of parameters: 6738415616
[2023-08-30 21:27:35,732] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards:  50%|                                     | 1/2 [00:08<00:08,  8.69s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.13s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.58s/it]
[2023-08-30 21:27:36,539] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.41s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.82s/it]
[2023-08-30 21:27:37,061] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:10<00:00,  4.77s/it]Loading checkpoint shards: 100%|| 2/2 [00:10<00:00,  5.36s/it]
[2023-08-30 21:27:38,095] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 21:27:38,686] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 21:27:38,687] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 21:27:38,687] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 21:27:38,687] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 21:27:38,687] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 21:27:38,687] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 21:27:38,687] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 21:27:38,687] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe594ecd2a0>
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 21:27:38,688] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 21:27:38,689] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 21:27:38,689] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: A: university

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i4-s20-rFalse-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [02:16<1:51:29, 136.53s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [04:29<1:47:36, 134.52s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [06:47<1:46:39, 136.16s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [08:26<1:32:57, 121.26s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [09:46<1:19:45, 106.33s/it]Evaluating commonsenseqa :  12%|                                                              | 6/50 [11:06<1:11:33, 97.57s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [13:24<1:19:23, 110.78s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [15:41<1:23:15, 118.95s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [17:57<1:24:57, 124.32s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [20:13<1:25:22, 128.05s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [21:28<1:12:45, 111.94s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [22:43<1:03:41, 100.58s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [25:00<1:08:50, 111.64s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [27:15<1:11:15, 118.76s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [28:40<1:03:17, 108.51s/it]Evaluating commonsenseqa :  32%|                                                 | 16/50 [29:52<55:10, 97.35s/it]Evaluating commonsenseqa :  34%|                                              | 17/50 [31:49<56:51, 103.39s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [34:07<1:00:39, 113.73s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [36:22<1:02:09, 120.30s/it]Evaluating commonsenseqa :  40%|                                         | 20/50 [38:41<1:02:49, 125.65s/it]Evaluating commonsenseqa :  42%|                                        | 21/50 [40:58<1:02:28, 129.27s/it]Evaluating commonsenseqa :  44%|                                      | 22/50 [43:15<1:01:24, 131.59s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [44:49<54:09, 120.37s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [47:03<53:55, 124.42s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [49:19<53:16, 127.86s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [51:34<51:58, 129.93s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [53:53<50:51, 132.69s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [54:40<39:12, 106.94s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [56:55<40:23, 115.41s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [59:11<40:33, 121.70s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:00:38<35:12, 111.20s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:02:55<35:38, 118.80s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:05:10<35:04, 123.79s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:06:41<30:22, 113.90s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:08:56<30:05, 120.35s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:11:11<29:02, 124.49s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:13:28<27:49, 128.44s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:15:42<25:59, 129.98s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:17:05<21:16, 116.05s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:19:23<20:26, 122.65s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:21:39<18:58, 126.46s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:23:59<17:23, 130.47s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:26:15<15:25, 132.15s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:28:09<12:40, 126.75s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:30:11<10:26, 125.32s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:32:14<08:18, 124.64s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:34:29<06:23, 127.74s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:36:46<04:21, 130.58s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:39:02<02:12, 132.14s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:41:20<00:00, 133.86s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:41:20<00:00, 121.60s/it]
name: commonsenseqa | avg. gen lenth: 232.472 | time: 6080.361018896103s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s20-rFalse --seed 20 --max-prompt-length 4096 --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 23:10:09,620] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 23:10:09,656] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 23:10:09,659] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 23:10:09,713] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s20-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 5
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i5-s20-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9736 [00:00<?, ?it/s]Loading data: 100%|| 9736/9736 [00:00<00:00, 1034575.86it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.09s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.22s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.29s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.85s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.08s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.53s/it]
[2023-08-30 23:10:19,969] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.27s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.71s/it]
[2023-08-30 23:10:20,349] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.32s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.77s/it]
[2023-08-30 23:10:20,512] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:10<00:00,  4.55s/it]Loading checkpoint shards: 100%|| 2/2 [00:10<00:00,  5.05s/it]
 > number of parameters: 6738415616
[2023-08-30 23:10:21,067] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 23:10:21,632] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 23:10:21,634] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7febbd7cd2d0>
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 23:10:21,634] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 23:10:21,635] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 23:10:21,635] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: A: university

Input: Where are small grapes likely to be found? Choices:  A: lunch box B: food store C: wine country D: kitchen E: raisins
Output: C: wine country

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i5-s20-rFalse-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [02:14<1:49:47, 134.45s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [04:14<1:40:43, 125.91s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [06:31<1:42:32, 130.91s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [08:46<1:41:39, 132.61s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [10:09<1:26:02, 114.73s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [12:26<1:29:44, 122.37s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [14:41<1:30:35, 126.41s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [16:56<1:30:24, 129.14s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [17:39<1:09:50, 102.20s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [19:53<1:14:43, 112.08s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [21:53<1:14:30, 114.63s/it]Evaluating commonsenseqa :  24%|                                                     | 12/50 [22:58<1:02:54, 99.32s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [25:13<1:07:51, 110.05s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [26:39<1:01:41, 102.82s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [28:52<1:05:17, 111.94s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [31:09<1:07:43, 119.53s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [33:26<1:08:35, 124.71s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [35:40<1:08:06, 127.71s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [37:53<1:06:42, 129.10s/it]Evaluating commonsenseqa :  40%|                                          | 20/50 [38:44<52:54, 105.83s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [40:58<55:16, 114.35s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [43:14<56:17, 120.64s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [44:56<51:51, 115.25s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [47:09<52:08, 120.32s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [49:21<51:37, 123.91s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [51:35<50:48, 127.04s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [53:48<49:19, 128.69s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [56:01<47:42, 130.11s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [56:50<36:57, 105.61s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [58:30<34:38, 103.91s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:00:11<32:39, 103.13s/it]Evaluating commonsenseqa :  64%|                         | 32/50 [1:01:03<26:20, 87.79s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:03:19<28:58, 102.28s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:04:35<25:09, 94.33s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:06:29<25:06, 100.42s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:08:45<25:55, 111.11s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:10:10<22:22, 103.28s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:11:28<19:07, 95.58s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:13:37<19:20, 105.49s/it]Evaluating commonsenseqa :  80%|              | 40/50 [1:15:03<16:36, 99.60s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:17:18<16:32, 110.27s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:19:31<15:37, 117.20s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:21:45<14:16, 122.34s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:24:00<12:35, 125.94s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:26:14<10:42, 128.56s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:28:26<08:37, 129.42s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:30:41<06:33, 131.12s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:31:43<03:40, 110.27s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:32:40<01:34, 94.54s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:34:57<00:00, 107.24s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:34:57<00:00, 113.96s/it]
name: commonsenseqa | avg. gen lenth: 219.396 | time: 5698.007262229919s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s20-rFalse --seed 20 --max-prompt-length 4096 --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-31 00:51:23,175] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 00:51:23,209] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 00:51:23,241] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 00:51:23,264] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s20-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 6
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i6-s20-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9735 [00:00<?, ?it/s]Loading data: 100%|| 9735/9735 [00:00<00:00, 970147.06it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.62s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.80s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:08<00:08,  8.23s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.90s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.31s/it]
[2023-08-31 00:51:33,204] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.05s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.46s/it]
[2023-08-31 00:51:33,499] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards:  50%|                                     | 1/2 [00:09<00:09,  9.24s/it]Loading checkpoint shards: 100%|| 2/2 [00:10<00:00,  4.60s/it]Loading checkpoint shards: 100%|| 2/2 [00:10<00:00,  5.14s/it]
 > number of parameters: 6738415616
[2023-08-31 00:51:34,881] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:11<00:00,  4.99s/it]Loading checkpoint shards: 100%|| 2/2 [00:11<00:00,  5.63s/it]
[2023-08-31 00:51:35,837] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 00:51:36,448] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-31 00:51:36,450] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 00:51:36,450] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 00:51:36,450] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 00:51:36,450] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 00:51:36,450] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 00:51:36,450] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 00:51:36,450] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 00:51:36,450] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 00:51:36,450] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 00:51:36,450] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 00:51:36,450] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f635a9c52d0>
[2023-08-31 00:51:36,450] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 00:51:36,450] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-31 00:51:36,451] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-31 00:51:36,452] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-31 00:51:36,452] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 00:51:36,452] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 00:51:36,452] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 00:51:36,452] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: A: university

Input: Where are small grapes likely to be found? Choices:  A: lunch box B: food store C: wine country D: kitchen E: raisins
Output: C: wine country

Input: The painter explained how he never achieved a flawless portrait, he said this was because all people are what? Choices:  A: imperfect B: disfigured C: damaged D: flawed E: defective
Output: A: imperfect

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i6-s20-rFalse-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [02:14<1:49:58, 134.67s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [04:24<1:45:29, 131.86s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [05:43<1:24:21, 107.70s/it]Evaluating commonsenseqa :   8%|                                                                 | 4/50 [07:04<1:14:29, 97.16s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [09:17<1:22:26, 109.91s/it]Evaluating commonsenseqa :  12%|                                                              | 6/50 [10:25<1:10:20, 95.93s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [12:36<1:16:56, 107.36s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [14:47<1:20:22, 114.83s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [16:58<1:21:59, 119.98s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [19:12<1:22:48, 124.22s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [21:23<1:22:06, 126.32s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [23:35<1:21:09, 128.16s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [25:46<1:19:33, 129.00s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [27:58<1:17:52, 129.80s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [30:09<1:15:52, 130.07s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [32:05<1:11:23, 125.97s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [34:19<1:10:30, 128.20s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [36:29<1:08:44, 128.88s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [38:39<1:06:46, 129.24s/it]Evaluating commonsenseqa :  40%|                                         | 20/50 [40:48<1:04:37, 129.26s/it]Evaluating commonsenseqa :  42%|                                        | 21/50 [42:59<1:02:40, 129.67s/it]Evaluating commonsenseqa :  44%|                                      | 22/50 [45:12<1:00:57, 130.62s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [47:23<58:52, 130.82s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [49:35<56:46, 131.04s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [51:01<48:59, 117.57s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [53:05<47:44, 119.37s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [55:17<47:18, 123.41s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [57:30<46:16, 126.22s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [59:40<44:32, 127.24s/it]Evaluating commonsenseqa :  60%|                           | 30/50 [1:01:47<42:26, 127.31s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:03:59<40:45, 128.70s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:06:10<38:50, 129.47s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:08:20<36:43, 129.59s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:09:06<27:51, 104.47s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:11:16<28:00, 112.03s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:13:30<27:40, 118.60s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:15:19<25:06, 115.89s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:17:31<24:08, 120.74s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:19:01<20:24, 111.28s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:21:12<19:33, 117.37s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:22:01<14:32, 96.94s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:24:13<14:17, 107.19s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:26:21<13:15, 113.65s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:28:34<11:56, 119.39s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:30:44<10:13, 122.69s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:31:27<06:35, 98.78s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:33:00<04:50, 96.83s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:35:13<03:35, 107.67s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:37:26<01:55, 115.31s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:39:36<00:00, 119.71s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:39:36<00:00, 119.53s/it]
name: commonsenseqa | avg. gen lenth: 248.06 | time: 5976.555188179016s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s20-rFalse --seed 20 --max-prompt-length 4096 --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-31 02:33:01,849] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 02:33:01,857] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 02:33:01,872] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 02:33:01,930] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s20-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 7
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i7-s20-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9734 [00:00<?, ?it/s]Loading data: 100%|| 9734/9734 [00:00<00:00, 929288.37it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.63s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.99s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.10s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.16s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.90s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.31s/it]
[2023-08-31 02:33:11,940] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.18s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.60s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.16s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.61s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.18s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.62s/it]
[2023-08-31 02:33:12,499] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 02:33:12,520] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-31 02:33:12,545] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 02:33:13,070] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-31 02:33:13,071] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 02:33:13,072] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 02:33:13,072] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 02:33:13,072] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 02:33:13,072] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 02:33:13,072] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe750dc9300>
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-31 02:33:13,073] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 02:33:13,074] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 02:33:13,074] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: A: university

Input: Where are small grapes likely to be found? Choices:  A: lunch box B: food store C: wine country D: kitchen E: raisins
Output: C: wine country

Input: The painter explained how he never achieved a flawless portrait, he said this was because all people are what? Choices:  A: imperfect B: disfigured C: damaged D: flawed E: defective
Output: A: imperfect

Input: If you're betting with a shark, where are you likely playing? Choices:  A: aquarium B: mediterranean sea C: south pacific D: pool hall E: pacific ocean
Output: D: pool hall

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i7-s20-rFalse-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [02:11<1:47:03, 131.09s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [04:01<1:35:11, 119.00s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [05:35<1:24:08, 107.41s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [07:45<1:29:14, 116.40s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [09:54<1:30:49, 121.11s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [12:00<1:30:00, 122.74s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [13:31<1:20:30, 112.34s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [15:39<1:22:10, 117.38s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [17:16<1:15:41, 110.76s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [18:38<1:07:56, 101.92s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [20:48<1:11:54, 110.62s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [22:22<1:06:52, 105.60s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [24:34<1:10:05, 113.66s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [26:44<1:11:03, 118.43s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [28:51<1:10:36, 121.03s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [30:41<1:06:44, 117.78s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [32:52<1:06:52, 121.59s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [35:00<1:06:01, 123.78s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [37:10<1:04:53, 125.59s/it]Evaluating commonsenseqa :  40%|                                         | 20/50 [39:20<1:03:25, 126.83s/it]Evaluating commonsenseqa :  42%|                                        | 21/50 [41:28<1:01:30, 127.25s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [42:11<47:32, 101.87s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [44:20<49:30, 110.00s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [46:28<50:00, 115.41s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [48:41<50:15, 120.63s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [50:35<47:32, 118.85s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [52:06<42:16, 110.30s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [54:16<42:36, 116.21s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [56:28<42:19, 120.94s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [57:44<35:52, 107.64s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [59:56<36:22, 114.88s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:02:05<35:46, 119.22s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:04:14<34:35, 122.08s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:05:41<29:44, 111.53s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:07:28<27:30, 110.04s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:09:35<26:51, 115.11s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:11:45<25:54, 119.57s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:13:56<24:38, 123.24s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:16:05<22:51, 124.72s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:18:16<21:06, 126.64s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:20:26<19:09, 127.76s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:22:34<17:01, 127.66s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:24:43<14:56, 128.10s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:26:50<12:46, 127.77s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:28:59<10:41, 128.26s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:31:07<08:33, 128.27s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:33:17<06:26, 128.75s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:35:26<04:17, 128.84s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:37:37<02:09, 129.53s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:39:45<00:00, 128.83s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:39:45<00:00, 119.70s/it]
name: commonsenseqa | avg. gen lenth: 239.46 | time: 5985.381655216217s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s20-rFalse --seed 20 --max-prompt-length 4096 --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-31 04:13:15,447] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 04:13:15,451] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 04:13:15,458] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 04:13:15,502] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s20-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 8
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s20-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9733 [00:00<?, ?it/s]Loading data: 100%|| 9733/9733 [00:00<00:00, 869632.55it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.85s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.97s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.03s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.22s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.44s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.09s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.52s/it]
[2023-08-31 04:13:25,646] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 04:13:25,778] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.19s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.62s/it]
 > number of parameters: 6738415616
[2023-08-31 04:13:26,026] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.28s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.72s/it]
[2023-08-31 04:13:26,249] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 04:13:26,877] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-31 04:13:26,879] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f744bbbd300>
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 04:13:26,881] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 04:13:26,882] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 04:13:26,883] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 04:13:26,883] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: A: university

Input: Where are small grapes likely to be found? Choices:  A: lunch box B: food store C: wine country D: kitchen E: raisins
Output: C: wine country

Input: The painter explained how he never achieved a flawless portrait, he said this was because all people are what? Choices:  A: imperfect B: disfigured C: damaged D: flawed E: defective
Output: A: imperfect

Input: If you're betting with a shark, where are you likely playing? Choices:  A: aquarium B: mediterranean sea C: south pacific D: pool hall E: pacific ocean
Output: D: pool hall

Input: Marmot's can be found in high places.  They have hands that they can use to get to the top of what? Choices:  A: outside B: hill C: jungle D: rocky hillside E: tree
Output: E: tree

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s20-rFalse-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [02:09<1:46:03, 129.87s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [04:17<1:42:44, 128.43s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [05:54<1:29:30, 114.26s/it]Evaluating commonsenseqa :   8%|                                                                 | 4/50 [06:53<1:10:49, 92.37s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [09:00<1:18:46, 105.02s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [11:09<1:22:50, 112.97s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [13:16<1:24:12, 117.49s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [15:23<1:24:25, 120.62s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [16:37<1:12:24, 105.97s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [18:44<1:15:07, 112.69s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [20:53<1:16:22, 117.49s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [23:03<1:16:47, 121.26s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [25:13<1:16:26, 123.95s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [27:21<1:15:07, 125.20s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [29:27<1:13:14, 125.56s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [31:07<1:06:39, 117.62s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [33:18<1:06:54, 121.66s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [35:23<1:05:24, 122.65s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [36:45<57:12, 110.72s/it]Evaluating commonsenseqa :  40%|                                          | 20/50 [38:55<58:07, 116.26s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [41:02<57:47, 119.56s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [43:01<55:40, 119.32s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [45:04<54:13, 120.50s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [45:39<41:07, 94.91s/it]Evaluating commonsenseqa :  50%|                                    | 25/50 [47:13<39:27, 94.72s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [48:15<33:55, 84.83s/it]Evaluating commonsenseqa :  54%|                                 | 27/50 [50:20<37:04, 96.71s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [52:27<38:48, 105.83s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [54:33<39:13, 112.09s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [55:45<33:19, 99.98s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [57:53<34:16, 108.22s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:00:01<34:17, 114.32s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:01:20<29:21, 103.59s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:03:24<29:15, 109.70s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:05:34<28:58, 115.90s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:07:41<27:49, 119.24s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:09:51<26:30, 122.36s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:11:59<24:48, 124.02s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:14:08<23:02, 125.71s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:16:16<21:02, 126.25s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:18:23<18:59, 126.65s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:19:03<13:25, 100.67s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:21:13<12:45, 109.38s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:23:21<11:29, 114.98s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:25:32<09:58, 119.69s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:27:37<08:05, 121.39s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:29:44<06:09, 123.14s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:31:01<03:38, 109.23s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:33:08<01:54, 114.54s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:34:18<00:00, 101.13s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:34:18<00:00, 113.17s/it]
name: commonsenseqa | avg. gen lenth: 241.268 | time: 5658.679126739502s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s20-rFalse --seed 20 --max-prompt-length 4096 --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-31 05:54:23,791] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 05:54:23,990] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 05:54:24,022] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 05:54:24,077] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s20-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 9
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i9-s20-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9732 [00:00<?, ?it/s]Loading data: 100%|| 9732/9732 [00:00<00:00, 830903.52it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.39s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.45s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.78s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.11s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.87s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.24s/it]
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.87s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.26s/it]
[2023-08-31 05:54:33,841] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 05:54:33,875] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.99s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.41s/it]
[2023-08-31 05:54:34,166] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.12s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.57s/it]
 > number of parameters: 6738415616
[2023-08-31 05:54:34,483] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 05:54:35,125] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-31 05:54:35,126] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f29821bd300>
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 05:54:35,127] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 05:54:35,128] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 05:54:35,128] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: A: university

Input: Where are small grapes likely to be found? Choices:  A: lunch box B: food store C: wine country D: kitchen E: raisins
Output: C: wine country

Input: The painter explained how he never achieved a flawless portrait, he said this was because all people are what? Choices:  A: imperfect B: disfigured C: damaged D: flawed E: defective
Output: A: imperfect

Input: If you're betting with a shark, where are you likely playing? Choices:  A: aquarium B: mediterranean sea C: south pacific D: pool hall E: pacific ocean
Output: D: pool hall

Input: Marmot's can be found in high places.  They have hands that they can use to get to the top of what? Choices:  A: outside B: hill C: jungle D: rocky hillside E: tree
Output: E: tree

Input: Kids have vivid imaginations, they can take anything and use it to what? Choices:  A: play with toys B: play games C: learn things D: do homework E: swimming pool
Output: B: play games

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i9-s20-rFalse-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [02:06<1:43:03, 126.19s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [04:10<1:40:08, 125.17s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [06:17<1:38:33, 125.81s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [08:23<1:36:42, 126.14s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [10:30<1:34:42, 126.28s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [12:34<1:32:09, 125.67s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [14:39<1:29:44, 125.21s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [16:41<1:26:57, 124.23s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [18:48<1:25:29, 125.11s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [20:37<1:20:11, 120.30s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [22:38<1:18:17, 120.46s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [24:41<1:16:44, 121.17s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [26:46<1:15:21, 122.21s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [28:52<1:14:08, 123.56s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [30:55<1:11:56, 123.33s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [33:03<1:10:43, 124.82s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [34:58<1:07:00, 121.84s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [37:04<1:05:34, 122.95s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [39:08<1:03:43, 123.35s/it]Evaluating commonsenseqa :  40%|                                         | 20/50 [41:11<1:01:35, 123.19s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [43:02<57:45, 119.50s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [45:07<56:34, 121.25s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [47:12<55:03, 122.35s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [49:19<53:39, 123.83s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [51:25<51:52, 124.51s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [53:30<49:49, 124.55s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [55:37<47:58, 125.16s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [56:42<39:19, 107.24s/it]Evaluating commonsenseqa :  58%|                              | 29/50 [57:29<31:15, 89.29s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [59:32<33:05, 99.26s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:01:38<34:01, 107.44s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:03:47<34:06, 113.67s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:05:12<29:48, 105.18s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:07:16<29:34, 110.91s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:09:23<28:52, 115.51s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:11:27<27:34, 118.16s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:13:34<26:09, 120.72s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:14:54<21:42, 108.56s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:16:57<20:43, 113.07s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:19:06<19:35, 117.58s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:19:59<14:45, 98.38s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:22:04<14:09, 106.25s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:24:07<12:59, 111.33s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:26:14<11:36, 116.05s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:28:21<09:56, 119.27s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:29:37<07:05, 106.43s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:31:42<05:35, 111.79s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:33:47<03:51, 115.81s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:35:53<01:58, 118.94s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:37:57<00:00, 120.57s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:37:57<00:00, 117.56s/it]
name: commonsenseqa | avg. gen lenth: 242.68 | time: 5878.108880996704s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-31 07:32:42,340] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 07:32:42,371] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 07:32:42,388] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 07:32:42,395] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 1
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9740 [00:00<?, ?it/s]Loading data: 100%|| 9740/9740 [00:00<00:00, 881068.88it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.93s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.14s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.19s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.26s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.10s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.52s/it]
[2023-08-31 07:32:52,705] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.18s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.62s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.19s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.64s/it]
[2023-08-31 07:32:52,976] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.26s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.71s/it]
 > number of parameters: 6738415616
[2023-08-31 07:32:53,021] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 07:32:53,146] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 07:32:53,818] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-31 07:32:53,820] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff994eb9300>
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 07:32:53,820] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 07:32:53,821] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 07:32:53,821] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The lab results had been compiled, the scientist began analysing the data because he wanted what? Choices:  A: learn more about B: headache C: do math D: enlightened E: better understanding
Output: Reasoning:

The first task is to understand the context of the question. Here, the scenario invokes a scientist analyzing lab results, which suggests he's searching for a certain outcome. That immediately eliminates irrelevant options such as "headache" (B) and "do math" (C).

The next task is to pin down the exact purpose behind the scientist's actions.
Option "learn more about" (A) is plausible, because analyzing the data would undoubtedly result in learning more. But it is vague as to what exactly the scientist wants to learn about. 

The option "enlightened" (D), while spiritual in nature, doesn't fit the scientific context of this situation. It would be uncommon to use the term "enlightenment" to describe the outcome of data analysis in a research study.

Finally, the last option is "better understanding" (E). It fits the context and purpose behind analyzing the data - that is, to gain a deepened understanding of the results. Therefore, this would be the most accurate answer to the question.
So the final answer is E: better understanding

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s30-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [02:11<1:47:21, 131.47s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [04:18<1:42:59, 128.75s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [06:25<1:40:24, 128.18s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [08:35<1:38:37, 128.63s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [10:43<1:36:17, 128.38s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [12:50<1:34:01, 128.21s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [14:59<1:31:53, 128.22s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [17:07<1:29:43, 128.18s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [19:14<1:27:21, 127.85s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [21:21<1:25:05, 127.64s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [23:31<1:23:22, 128.26s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [25:38<1:21:01, 127.93s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [27:45<1:18:43, 127.65s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [29:53<1:16:40, 127.80s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [32:02<1:14:44, 128.12s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [34:09<1:12:25, 127.80s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [36:18<1:10:31, 128.24s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [38:25<1:08:13, 127.92s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [40:33<1:06:06, 127.94s/it]Evaluating commonsenseqa :  40%|                                         | 20/50 [42:42<1:04:00, 128.03s/it]Evaluating commonsenseqa :  42%|                                        | 21/50 [44:49<1:01:45, 127.77s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [46:59<59:54, 128.39s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [49:08<57:51, 128.58s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [51:15<55:36, 128.32s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [53:24<53:31, 128.46s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [55:33<51:27, 128.65s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [57:42<49:18, 128.64s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [59:49<46:59, 128.15s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [1:01:59<45:00, 128.61s/it]Evaluating commonsenseqa :  60%|                           | 30/50 [1:04:05<42:39, 127.99s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:06:17<40:51, 129.03s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:08:26<38:42, 129.01s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:10:32<36:22, 128.40s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:12:41<34:13, 128.33s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:14:49<32:06, 128.42s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:16:56<29:49, 127.80s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:19:06<27:50, 128.49s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:21:15<25:44, 128.75s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:23:23<23:34, 128.60s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:25:32<21:25, 128.54s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:27:40<19:14, 128.30s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:29:48<17:07, 128.41s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:31:55<14:55, 127.91s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:34:02<12:46, 127.77s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:36:10<10:38, 127.79s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:38:19<08:32, 128.22s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:40:30<06:26, 128.90s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:42:38<04:17, 128.67s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:44:48<02:09, 129.05s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:46:56<00:00, 128.78s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:46:56<00:00, 128.33s/it]
name: commonsenseqa | avg. gen lenth: 392.536 | time: 6416.9806661605835s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-31 09:20:21,271] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 09:20:21,377] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 09:20:21,377] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 09:20:21,392] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 2
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9739 [00:00<?, ?it/s]Loading data: 100%|| 9739/9739 [00:00<00:00, 655219.14it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.03s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.07s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.08s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.22s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.09s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.53s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.13s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.57s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.16s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.60s/it]
 > number of parameters: 6738415616
[2023-08-31 09:20:31,778] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 09:20:31,823] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 09:20:31,918] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.35s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.78s/it]
[2023-08-31 09:20:32,295] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 09:20:32,905] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-31 09:20:32,906] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 09:20:32,906] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 09:20:32,906] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 09:20:32,906] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 09:20:32,906] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 09:20:32,906] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f87a23c9300>
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 09:20:32,907] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 09:20:32,908] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 09:20:32,908] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 09:20:32,908] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-31 09:20:32,908] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-31 09:20:32,908] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-31 09:20:32,908] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 09:20:32,908] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 09:20:32,908] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-31 09:20:32,908] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-31 09:20:32,908] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-31 09:20:32,908] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 09:20:32,908] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 09:20:32,908] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 09:20:32,908] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The lab results had been compiled, the scientist began analysing the data because he wanted what? Choices:  A: learn more about B: headache C: do math D: enlightened E: better understanding
Output: 1. Firstly, understand the question which asks about the scientist's motive for analyzing the data of his lab results.
2. This is a reasoning question, so we need to think logically about the situation.
3. Eliminate options which do not make any logical sense such as 'headache' and 'do math', because they are not relevant to the action of a scientist analysing data.
4. Next, make a comparison between the remaining options 'learn more about', 'enlightened' and 'better understanding'.
5. 'Learn more about' is a plausible answer but it demands more context like what he wants to learn about. Therefore, this option is incomplete.
6. The option 'enlightened' lacks specificity, as it could relate to any form of enlightenment, even non-scientific.
7. Therefore, the most logical answer given the context of the scientist's action is 'better understanding', because logically a scientist will analyse data to gain a deeper or better understanding of the results. Hence, the answer is E: better understanding.
So the final answer is E: better understanding

Input: I needed to find out how to contact a person with a certain name, where should I look? Choices:  A: directory B: michigan C: roster D: phone book E: certificate
Output: 1. Start by reading the question carefully and try to grasp what it is asking.
2. The question is asking about a place where you can find the contact information of a person with a certain name.
3. Checking and analyzing the given choices: directory, Michigan, roster, phone book, and certificate.
4. Realize that Michigan and a certificate don't make sense as Michigan is a state and a certificate wouldn't have contact information for a large variety of people.
5. The term 'roster' usually refers to a list of people belonging to a particular group or team and may or may not contain contact details.
6. A directory could possibly contain this information, but it could also be a directory of anything, not necessarily of people and their contact information.
7. However, a 'phone book' is a well-known and specific source where one can access contact details, including the names and numbers of people.  
8. Therefore, the most suitable and specific answer is D: Phone Book.
So the final answer is D: phone book

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s30-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [01:58<1:36:52, 118.62s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [03:55<1:34:05, 117.62s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [05:54<1:32:27, 118.03s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [07:50<1:30:07, 117.56s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [09:49<1:28:34, 118.10s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [11:46<1:26:19, 117.71s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [13:44<1:24:14, 117.54s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [15:42<1:22:22, 117.67s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [17:40<1:20:28, 117.77s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [19:36<1:18:11, 117.29s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [21:33<1:16:13, 117.28s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [23:30<1:14:12, 117.16s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [25:28<1:12:26, 117.49s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [27:25<1:10:27, 117.43s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [29:24<1:08:38, 117.66s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [31:22<1:06:48, 117.91s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [33:20<1:04:49, 117.87s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [35:18<1:02:54, 117.95s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [37:17<1:01:04, 118.21s/it]Evaluating commonsenseqa :  40%|                                          | 20/50 [39:16<59:13, 118.44s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [41:14<57:08, 118.21s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [43:13<55:21, 118.62s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [45:11<53:20, 118.55s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [47:08<51:07, 118.00s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [49:07<49:15, 118.21s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [51:05<47:18, 118.26s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [53:02<45:11, 117.90s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [55:00<43:10, 117.77s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [57:00<41:25, 118.37s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [58:58<39:26, 118.34s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:00:56<37:24, 118.15s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:02:55<35:33, 118.55s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:04:52<33:25, 117.97s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:06:51<31:34, 118.42s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:08:49<29:34, 118.33s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:10:47<27:34, 118.16s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:12:43<25:29, 117.62s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:14:41<23:29, 117.49s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:16:38<21:33, 117.61s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:18:35<19:33, 117.34s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:20:33<17:37, 117.49s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:22:30<15:40, 117.50s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:24:29<13:44, 117.80s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:26:29<11:50, 118.43s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:28:26<09:49, 117.98s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:30:23<07:50, 117.74s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:32:20<05:52, 117.60s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:34:19<03:55, 117.80s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:36:16<01:57, 117.64s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:38:14<00:00, 117.76s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:38:14<00:00, 117.89s/it]
name: commonsenseqa | avg. gen lenth: 405.976 | time: 5894.736633300781s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-31 10:59:29,744] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 10:59:29,749] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 10:59:29,771] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 10:59:29,793] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 3
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i3-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9738 [00:00<?, ?it/s]Loading data: 100%|| 9738/9738 [00:00<00:00, 526783.16it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.76s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.00s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.22s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:08<00:08,  8.39s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.98s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.40s/it]
[2023-08-31 10:59:40,144] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.18s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.60s/it]
 > number of parameters: 6738415616
[2023-08-31 10:59:40,510] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.21s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.66s/it]
[2023-08-31 10:59:40,694] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:10<00:00,  4.71s/it]Loading checkpoint shards: 100%|| 2/2 [00:10<00:00,  5.26s/it]
[2023-08-31 10:59:41,826] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 10:59:42,420] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-31 10:59:42,422] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 10:59:42,423] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 10:59:42,423] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 10:59:42,423] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 10:59:42,423] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 10:59:42,423] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 10:59:42,423] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 10:59:42,423] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 10:59:42,423] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 10:59:42,423] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 10:59:42,423] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f47c53cd2d0>
[2023-08-31 10:59:42,423] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 10:59:42,423] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 10:59:42,423] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 10:59:42,423] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 10:59:42,424] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 10:59:42,425] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 10:59:42,425] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The lab results had been compiled, the scientist began analysing the data because he wanted what? Choices:  A: learn more about B: headache C: do math D: enlightened E: better understanding
Output: 1. The question states that the scientist started analyzing the data after the lab results were compiled.
2. The reason for this behaviour is sought.
3. Options B and C, a headache and doing math, appear irrelevant as they don't directly relate to the process of analysing data in a laboratory context.
4. While options A and D, learning more and being enlightened, are related to the gaining of knowledge, they are not precise and clear enough to explain why a scientist specifically would analyse compiled lab data.
5. Therefore, the answer is option E: better understanding. This is the most likely reason a scientist would analyze data, as their primary goal is to gain a deeper comprehension of their research subject.
So the final answer is E: better understanding

Input: I needed to find out how to contact a person with a certain name, where should I look? Choices:  A: directory B: michigan C: roster D: phone book E: certificate
Output: 1. The question here is essentially asking for the best way to find contact information, specifically a phone number, for a person given only their name.
2. To evaluate the options, we need to think about what information each choice generally contains.
3. Option A suggests a directory. However, this option is quite vague as there are many types of directories, and they don't necessarily contain contact information.
4. Option B suggests the location Michigan. This option isn't likely to provide contact information for a specific individual unless we have more specific data about them.
5. Option C is a roster, which is usually a list of people in a particular team or organization. While this may have contact information, it's not generally publicly accessible or comprehensive for all individuals.
6. Option D is a phone book. This is traditionally the most reliable source for obtaining telephone numbers associated with names. Phone books are organized alphabetically by name, making it relatively easy to find a persons contact information.
7. Finally, option E suggests a certificate. Certificates usually don't contain contact information.
8. Therefore, based on the information typically contained within each option and their accessibility, the phone book (Option D) would be the best bet for finding someone's contact information by their name.
So the final answer is D: phone book

Input: What is likely to happen to someone's speech after becoming inebriated? Choices:  A: regret B: automobile accidents C: drunk driving D: talking nonsense E: talk intelligently
Output: 1. The question asks for what is likely to happen to someone's speech after becoming inebriated, which means after they have consumed alcohol to a point where it affects their mental and physical functions.
2. This suggests that the effect on speech would likely be a negative one, since alcohol usually inhibits clear thought and speech.
3. Thus, positive or neutral outcomes, such as regret (choice A) and talking intelligently (choice E), can be eliminated.
4. The options "automobile accidents" and "drunk driving" (choices B and C) are also irrelevant, as they don't pertain directly to someone's speech.
5. This leaves us with "talking nonsense" (D) which aligns with the expected effect of inebriation on speech: a decline in coherence and sensibility. 
6. So, the answer is D: talking nonsense.
So the final answer is D: talking nonsense

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i3-s30-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [01:51<1:31:00, 111.44s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [03:40<1:28:04, 110.09s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [05:30<1:26:12, 110.05s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [07:21<1:24:33, 110.29s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [09:11<1:22:40, 110.23s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [11:00<1:20:38, 109.97s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [12:49<1:18:31, 109.57s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [14:39<1:16:45, 109.65s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [16:28<1:14:52, 109.59s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [18:18<1:12:58, 109.46s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [20:08<1:11:16, 109.65s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [21:57<1:09:24, 109.59s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [23:47<1:07:34, 109.59s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [25:36<1:05:43, 109.54s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [27:27<1:04:13, 110.09s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [29:18<1:02:29, 110.29s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [31:08<1:00:33, 110.10s/it]Evaluating commonsenseqa :  36%|                                             | 18/50 [32:59<58:49, 110.29s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [34:47<56:43, 109.80s/it]Evaluating commonsenseqa :  40%|                                          | 20/50 [36:36<54:44, 109.49s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [38:26<52:59, 109.63s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [40:16<51:09, 109.63s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [42:04<49:09, 109.23s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [43:52<47:08, 108.80s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [45:41<45:23, 108.92s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [47:30<43:33, 108.89s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [49:21<42:01, 109.65s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [51:11<40:12, 109.65s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [53:00<38:20, 109.57s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [54:49<36:29, 109.47s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [56:39<34:40, 109.52s/it]Evaluating commonsenseqa :  64%|                         | 32/50 [58:29<32:53, 109.62s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:00:19<31:06, 109.77s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:02:11<29:25, 110.37s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:04:02<27:38, 110.57s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:05:51<25:40, 110.04s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:07:41<23:52, 110.20s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:09:32<22:02, 110.25s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:11:21<20:11, 110.09s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:13:13<18:25, 110.50s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:15:02<16:31, 110.12s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:16:52<14:39, 110.00s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:18:43<12:52, 110.43s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:20:32<11:00, 110.03s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:22:23<09:10, 110.14s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:24:12<07:20, 110.03s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:26:03<05:30, 110.08s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:27:53<03:40, 110.16s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:29:42<01:49, 109.74s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:31:32<00:00, 109.83s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:31:32<00:00, 109.85s/it]
name: commonsenseqa | avg. gen lenth: 400.412 | time: 5492.6968677043915s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-31 12:31:27,759] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 12:31:27,759] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 12:31:27,760] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 12:31:27,770] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 4
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i4-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9737 [00:00<?, ?it/s]Loading data: 100%|| 9737/9737 [00:00<00:00, 450493.49it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.85s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.87s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.99s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.16s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.99s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.42s/it]
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.00s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.43s/it]
[2023-08-31 12:31:38,082] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.07s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.51s/it]
 > number of parameters: 6738415616
[2023-08-31 12:31:38,138] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 12:31:38,288] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.14s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.60s/it]
[2023-08-31 12:31:38,408] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 12:31:39,018] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-31 12:31:39,019] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f46aa9cd300>
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 12:31:39,020] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 12:31:39,021] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 12:31:39,021] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The lab results had been compiled, the scientist began analysing the data because he wanted what? Choices:  A: learn more about B: headache C: do math D: enlightened E: better understanding
Output: 1. The question describes a situation where a scientist is analyzing data. It asks us to determine what he wants from the information. 
2. We can infer that the action of analyzing data is done for some purpose, likely to further the scientist's knowledge on a particular topic, not for unrelated objectives.
3. We can exclude option B: "headache" and C: "do math" because these fell into unrelated or consequence condition which doesn't reason why he analyses the data.
4. Option D "enlightened", while somewhat related to understanding, it's a broader and vague term that doesn't typically refer to gaining knowledge about a specific topic from data.
5. Between A "learn more about" and E "better understanding", E is a more detailed and direct interpretation of the scientist's motive.
6. Hence, the answer is E: "better understanding".
So the final answer is E: better understanding

Input: I needed to find out how to contact a person with a certain name, where should I look? Choices:  A: directory B: michigan C: roster D: phone book E: certificate
Output: 1. The question is about locating the contact information of a person with a certain name.
2. The options available are a directory, Michigan, a roster, a phone book, or a certificate.
3. Michigan is a location and does not provide specific information about individuals, so this option is not applicable.
4. A certificate typically contains something that certifies or proves something about a person, but it is not a common source of contact information. So this option doesn't make sense.
5. A roster might contain names, but it usually doesn't provide contact information. It's usually a list of names for a team or an organization, not a way to contact people.
6. A directory and a phone book both could potentially contain contact information, but a directory could refer to any kind of directory, including a directory of businesses or organizations, not necessarily individuals.
7. Therefore, the best answer is the phone book (option D). Phone books are designed specifically for the purpose of listing people's names along with their phone numbers, making it the most relevant and correct choice.
So the final answer is D: phone book

Input: What is likely to happen to someone's speech after becoming inebriated? Choices:  A: regret B: automobile accidents C: drunk driving D: talking nonsense E: talk intelligently
Output: Reasoning:

1. The question asks about the probable effects on someone's speech after they've drunk a significant amount of alcohol, i.e., become inebriated. 

2. It's known that alcohol can impair cognitive abilities and speech due to its depressive effect on the brain.

3. Looking at the options, we need to choose the one that directly relates to a possible effect on speech after drinking.

4. Choice A: regret doesn't specifically apply to speech. Regret is more of an emotion or feeling that might follow after realising the consequences of one's actions.

5. Choice B: automobile accidents and Choice C: drunk driving are potential effects of drinking alcohol but they don't necessarily relate to changes in someone's speech.

6. Choice E: talk intelligently is unlikely because alcohol usually impairs cognitive functions and could make you less coherent or logical in your speech.

7. Therefore, the most probable effect of drinking alcohol on speech would be Choice D: talking nonsense, as alcohol can lower inhibitions and impair cognitive function, making an individual's speech less coherent or sensible.
   
8. So, the answer is D: talking nonsense.
So the final answer is D: talking nonsense

Input: A rosebush or squirrel or other plants and animals are part of what? Choices:  A: nature B: neighbor's yard C: park D: these are not valid to the two places E: field
Output: 1: A rosebush or squirrel or other plants and animals are considered living things or forms of life.
2: These living things live and coexist in the natural environment, which can include forests, fields, parks, or even the neighbor's yard. 
3: While it's possible to find them in the neighbor's yard, parks, or fields, these are specific locations where they may exist and not the all-encompassing term we're looking for. 
4: The term that encapsulates all these places and living organisms in them is 'nature'.
5: So, the all-encompassing shared space of a rosebush, squirrel, or other plants and animals, regardless of specific location, is nature.
6: Therefore, the answer is A: nature.
So the final answer is A: nature

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i4-s30-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [01:40<1:21:51, 100.23s/it]Evaluating commonsenseqa :   4%|                                                                    | 2/50 [03:19<1:19:34, 99.47s/it]Evaluating commonsenseqa :   6%|                                                                  | 3/50 [04:58<1:17:59, 99.57s/it]Evaluating commonsenseqa :   8%|                                                                 | 4/50 [06:38<1:16:13, 99.43s/it]Evaluating commonsenseqa :  10%|                                                                | 5/50 [08:17<1:14:29, 99.33s/it]Evaluating commonsenseqa :  12%|                                                              | 6/50 [09:56<1:12:47, 99.26s/it]Evaluating commonsenseqa :  14%|                                                             | 7/50 [11:35<1:11:04, 99.16s/it]Evaluating commonsenseqa :  16%|                                                           | 8/50 [13:14<1:09:23, 99.13s/it]Evaluating commonsenseqa :  18%|                                                          | 9/50 [14:53<1:07:47, 99.21s/it]Evaluating commonsenseqa :  20%|                                                        | 10/50 [16:33<1:06:20, 99.50s/it]Evaluating commonsenseqa :  22%|                                                      | 11/50 [18:12<1:04:31, 99.28s/it]Evaluating commonsenseqa :  24%|                                                     | 12/50 [19:51<1:02:48, 99.16s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [21:29<1:00:55, 98.80s/it]Evaluating commonsenseqa :  28%|                                                   | 14/50 [23:09<59:29, 99.14s/it]Evaluating commonsenseqa :  30%|                                                  | 15/50 [24:48<57:47, 99.07s/it]Evaluating commonsenseqa :  32%|                                                 | 16/50 [26:27<56:06, 99.02s/it]Evaluating commonsenseqa :  34%|                                               | 17/50 [28:05<54:22, 98.86s/it]Evaluating commonsenseqa :  36%|                                              | 18/50 [29:45<52:56, 99.26s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [31:25<51:20, 99.36s/it]Evaluating commonsenseqa :  40%|                                           | 20/50 [33:04<49:34, 99.14s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [34:42<47:51, 99.01s/it]Evaluating commonsenseqa :  44%|                                        | 22/50 [36:20<46:03, 98.71s/it]Evaluating commonsenseqa :  46%|                                       | 23/50 [38:00<44:30, 98.92s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [39:39<42:54, 99.02s/it]Evaluating commonsenseqa :  50%|                                    | 25/50 [41:17<41:07, 98.70s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [42:56<39:27, 98.66s/it]Evaluating commonsenseqa :  54%|                                 | 27/50 [44:34<37:45, 98.49s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [46:12<36:07, 98.54s/it]Evaluating commonsenseqa :  58%|                              | 29/50 [47:52<34:34, 98.78s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [49:31<32:57, 98.90s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [51:09<31:16, 98.74s/it]Evaluating commonsenseqa :  64%|                          | 32/50 [52:48<29:35, 98.66s/it]Evaluating commonsenseqa :  66%|                        | 33/50 [54:26<27:55, 98.54s/it]Evaluating commonsenseqa :  68%|                       | 34/50 [56:05<26:20, 98.76s/it]Evaluating commonsenseqa :  70%|                     | 35/50 [57:45<24:47, 99.17s/it]Evaluating commonsenseqa :  72%|                    | 36/50 [59:25<23:10, 99.30s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:01:02<21:23, 98.76s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:02:41<19:44, 98.69s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:04:20<18:07, 98.88s/it]Evaluating commonsenseqa :  80%|              | 40/50 [1:05:59<16:28, 98.82s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:07:38<14:50, 98.91s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:09:17<13:11, 98.90s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:10:57<11:34, 99.15s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:12:36<09:55, 99.17s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:14:15<08:15, 99.09s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:15:54<06:36, 99.05s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:17:32<04:56, 98.93s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:19:12<03:18, 99.10s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:20:51<01:39, 99.01s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:22:31<00:00, 99.40s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:22:31<00:00, 99.03s/it]
name: commonsenseqa | avg. gen lenth: 431.796 | time: 4952.031927585602s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-31 13:54:44,612] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 13:54:45,094] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 13:54:45,096] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 13:54:45,103] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 5
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i5-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9736 [00:00<?, ?it/s]Loading data: 100%|| 9736/9736 [00:00<00:00, 374409.71it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.62s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.14s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.15s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.19s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.05s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.44s/it]
[2023-08-31 13:54:55,217] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.14s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.59s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.14s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.59s/it]
[2023-08-31 13:54:55,644] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 13:54:55,659] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.20s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.65s/it]
 > number of parameters: 6738415616
[2023-08-31 13:54:55,767] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 13:54:56,330] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-31 13:54:56,332] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7ad43c12a0>
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 13:54:56,333] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 13:54:56,334] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 13:54:56,335] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 13:54:56,335] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 13:54:56,335] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 13:54:56,335] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-31 13:54:56,335] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-31 13:54:56,335] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-31 13:54:56,335] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 13:54:56,335] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 13:54:56,335] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-31 13:54:56,335] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-31 13:54:56,335] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-31 13:54:56,335] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 13:54:56,335] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 13:54:56,335] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 13:54:56,335] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The lab results had been compiled, the scientist began analysing the data because he wanted what? Choices:  A: learn more about B: headache C: do math D: enlightened E: better understanding
Output: 1. The sentence describes a scientist who has begun analysing data they have compiled from lab results. 
2. The purpose for doing such an analysis is often to gain knowledge or deeper insights into the topic they are studying.
3. Out of the given options, various could be applied to this context. However, we need to decide which one fits best.
4. Option A: "learn more about" is incomplete, it lacks specificity about what the scientist wants to learn more about.
5. Option B: "headache" is irrelevant. There's nothing mentioned about the scientist suffering from physical discomfort or stress.
6. Option C: "do math" is not directly referring to the outcome or goal of the analysis undertaken by the scientist.
7. Option D: "enlightened" is a broad term which refers to gaining spiritual or philosophical knowledge, not specifically related to scientific work.
8. Option E: "better understanding" precisely fits the context. It indicates that the scientist is analyzing the data to gain a deeper, or enhanced understanding into their research.
9. Therefore, the answer is E: "better understanding".
So the final answer is E: better understanding

Input: I needed to find out how to contact a person with a certain name, where should I look? Choices:  A: directory B: michigan C: roster D: phone book E: certificate
Output: 1. First, we need to understand what each option means and where it could possibly help us find contact information.
2. A directory is a list of individuals or organizations with their contact details. This could be a potential option.
3. Michigan is a U.S. state. Without more context, this doesn't seem like the place to find a person's contact details. So, this can be eliminated.
4. A roster is typically a list of names, often associated with a team or group. While it may provide names, it's not generally a source for contact information. So, this can also be eliminated.
5. A certificate usually serves as proof or evidence of something and typically does not contain contact details. Thus, we can eliminate this option as well. 
6. A phone book, on the other hand, is specifically designed to provide names and contact details of individuals. This would be the most reliable source for finding someone's phone number.
7. Therefore, the answer is D: phone book.
So the final answer is D: phone book

Input: What is likely to happen to someone's speech after becoming inebriated? Choices:  A: regret B: automobile accidents C: drunk driving D: talking nonsense E: talk intelligently
Output: 1. The question is asking us to determine the likely effect of inebriation, or drunkenness, on a person's speech.
2. Before reaching a conclusion, let's consider each option.
3. Option A: Regret. Although some people may feel regret after becoming drunk, this choice does not specifically pertain to a person's speech. It refers more to an emotion or feeling that one may have after the fact.
4. Option B: Automobile accidents. While inebriation does increase the likelihood of automobile accidents, this doesn't have direct relation to someone's speech. It might be cause of drunken driving, but it is not related to any change in speech.
5. Option C: Drunk driving. Again, inebriation can lead to drunk driving, but the question is focusing on how it affects someone's speech. So this option is not appropriate.
6. Option D: Talking nonsense. This is directly related to the change in normal behavior or speech of a person due to inebriation. Inebriation is known to disrupt a person's ability to think clearly and articulate their thoughts, which would potentially lead to them talking nonsense.
7. Option E: Talk intelligently. Generally, inebriation is associated with reduced inhibition and impaired cognitive function, making it less likely for a person to talk intelligently.
8. Therefore, based on the elimination of inappropriate choices and logical inference, the answer should be D: Talking nonsense.
So the final answer is D: talking nonsense

Input: A rosebush or squirrel or other plants and animals are part of what? Choices:  A: nature B: neighbor's yard C: park D: these are not valid to the two places E: field
Output: 1. The question asks for a categorization of rosebush, squirrel, and other plants and animals. 
2. These items belong to the general category of living things. 
3. The options given are: nature, neighbor's yard, park, these are not valid to the two places, and field. 
4. The options "neighbor's yard", "park", and "field" refer to specific locations and not a category.
5. "These are not valid to the two places" is not an appropriate category, as the location is not asked in the question.
6. The only remaining option is "nature", which includes all forms of living and non-living things.
7. Therefore, the correct answer is A: nature.
So the final answer is A: nature

Input: The shark was sick of the confines of its tank, what was it looking for? Choices:  A: sea world B: tomales bay C: great oceans D: the key E: tropical waters
Output: 1. The question mentions that the shark is sick of the confines of its tank.
2. Therefore, we are looking for an option that describes a large open space that a shark may desire to be in.
3. Sea World is another enclosed space, so A is not valid.
4. Tomales Bay seems to be a large body of water, but it's also a specific location. The shark would likely seek broader expanses of space, thus option B is unlikely.
5. The key (option D) does not make sense in this context.
6. Tropical waters (option E) could be a possibility since it's a type of environment that some sharks like, however it's still a relatively limited location and not all sharks prefer tropical waters.
7. The "great oceans" (option C) could be the perfect habitat for a shark, as it provides the most expansive and free environment.
8. Bassed on our reasoning, thus, the answer is C: great oceans.
So the final answer is C: great oceans

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i5-s30-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                     | 1/50 [01:30<1:14:14, 90.90s/it]Evaluating commonsenseqa :   4%|                                                                    | 2/50 [03:00<1:11:53, 89.86s/it]Evaluating commonsenseqa :   6%|                                                                  | 3/50 [04:29<1:10:07, 89.52s/it]Evaluating commonsenseqa :   8%|                                                                 | 4/50 [05:58<1:08:37, 89.52s/it]Evaluating commonsenseqa :  10%|                                                                | 5/50 [07:28<1:07:13, 89.63s/it]Evaluating commonsenseqa :  12%|                                                              | 6/50 [08:58<1:05:42, 89.61s/it]Evaluating commonsenseqa :  14%|                                                             | 7/50 [10:27<1:04:14, 89.65s/it]Evaluating commonsenseqa :  16%|                                                           | 8/50 [11:56<1:02:33, 89.37s/it]Evaluating commonsenseqa :  18%|                                                          | 9/50 [13:26<1:01:10, 89.52s/it]Evaluating commonsenseqa :  20%|                                                         | 10/50 [14:55<59:38, 89.47s/it]Evaluating commonsenseqa :  22%|                                                        | 11/50 [16:25<58:07, 89.43s/it]Evaluating commonsenseqa :  24%|                                                      | 12/50 [17:54<56:37, 89.41s/it]Evaluating commonsenseqa :  26%|                                                     | 13/50 [19:24<55:12, 89.53s/it]Evaluating commonsenseqa :  28%|                                                   | 14/50 [20:54<53:48, 89.69s/it]Evaluating commonsenseqa :  30%|                                                  | 15/50 [22:23<52:12, 89.49s/it]Evaluating commonsenseqa :  32%|                                                 | 16/50 [23:53<50:44, 89.55s/it]Evaluating commonsenseqa :  34%|                                               | 17/50 [25:21<49:03, 89.18s/it]Evaluating commonsenseqa :  36%|                                              | 18/50 [26:50<47:31, 89.09s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [28:20<46:10, 89.36s/it]Evaluating commonsenseqa :  40%|                                           | 20/50 [29:49<44:41, 89.37s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [31:18<43:04, 89.11s/it]Evaluating commonsenseqa :  44%|                                        | 22/50 [32:46<41:28, 88.89s/it]Evaluating commonsenseqa :  46%|                                       | 23/50 [34:15<39:57, 88.80s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [35:44<38:35, 89.05s/it]Evaluating commonsenseqa :  50%|                                    | 25/50 [37:14<37:08, 89.14s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [38:42<35:34, 88.94s/it]Evaluating commonsenseqa :  54%|                                 | 27/50 [40:13<34:17, 89.45s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [41:41<32:41, 89.16s/it]Evaluating commonsenseqa :  58%|                              | 29/50 [43:10<31:09, 89.04s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [44:40<29:44, 89.20s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [46:09<28:18, 89.40s/it]Evaluating commonsenseqa :  64%|                          | 32/50 [47:39<26:52, 89.56s/it]Evaluating commonsenseqa :  66%|                        | 33/50 [49:09<25:20, 89.44s/it]Evaluating commonsenseqa :  68%|                       | 34/50 [50:38<23:52, 89.52s/it]Evaluating commonsenseqa :  70%|                     | 35/50 [52:08<22:22, 89.47s/it]Evaluating commonsenseqa :  72%|                    | 36/50 [53:38<20:56, 89.78s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [55:09<19:30, 90.07s/it]Evaluating commonsenseqa :  76%|                 | 38/50 [56:37<17:55, 89.64s/it]Evaluating commonsenseqa :  78%|               | 39/50 [58:07<16:25, 89.58s/it]Evaluating commonsenseqa :  80%|              | 40/50 [59:36<14:55, 89.51s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:01:05<13:24, 89.37s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:02:35<11:56, 89.54s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:04:06<10:29, 89.88s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:05:34<08:56, 89.49s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:07:05<07:28, 89.72s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:08:34<05:58, 89.67s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:10:04<04:28, 89.62s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:11:33<02:58, 89.35s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:13:02<01:29, 89.26s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:14:31<00:00, 89.24s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:14:31<00:00, 89.42s/it]
name: commonsenseqa | avg. gen lenth: 426.736 | time: 4471.7560222148895s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-31 15:09:36,913] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 15:09:36,915] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 15:09:36,946] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 15:09:36,966] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 6
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i6-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9735 [00:00<?, ?it/s]Loading data: 100%|| 9735/9735 [00:00<00:00, 337936.78it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.35s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.37s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.82s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.06s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.85s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.23s/it]
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.86s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.24s/it]
[2023-08-31 15:09:46,703] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 15:09:46,717] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.05s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.46s/it]
 > number of parameters: 6738415616
[2023-08-31 15:09:47,327] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.14s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.57s/it]
[2023-08-31 15:09:47,555] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 15:09:48,103] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-31 15:09:48,104] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 15:09:48,104] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 15:09:48,104] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 15:09:48,104] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 15:09:48,104] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 15:09:48,104] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 15:09:48,104] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 15:09:48,104] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 15:09:48,104] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 15:09:48,104] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 15:09:48,104] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7efee44bd300>
[2023-08-31 15:09:48,104] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 15:09:48,105] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-31 15:09:48,106] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-31 15:09:48,106] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-31 15:09:48,106] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 15:09:48,106] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 15:09:48,106] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 15:09:48,106] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The lab results had been compiled, the scientist began analysing the data because he wanted what? Choices:  A: learn more about B: headache C: do math D: enlightened E: better understanding
Output: Step 1: Identify the question - the scientist began analysing the data because he wanted what?
Step 2: The question asks us to determine the scientist's motive or purpose for analyzing the data.
Step 3: Start by excluding options that do not make sense in the context of the question.
Step 4: "headache" (B) and "do math" (C) are not reasonable motivations for analyzing data. We can exclude these options.
Step 5: Even though a scientist can learn more about (A) something through analyzing data, this choice is not specific enough and does not fully capture the goal of data analysis.
Step 6: Although "enlightened" (D) might mean gaining new knowledge or insight, it's a term more commonly used in spiritual or philosophical contexts and does not precisely describe the scientist's aim.
Step 7: Leaving us with option E: "better understanding", which is a common and accurate motivation for a scientist to analyse data.
Final Step: Therefore, the scientist began analysing the data because he wanted a "better understanding" (Option E).
So the final answer is E: better understanding

Input: I needed to find out how to contact a person with a certain name, where should I look? Choices:  A: directory B: michigan C: roster D: phone book E: certificate
Output: 1. The question is asking where to find contact information for a person.
2. Looking at the options, a directory (A), a roster (C), and a phone book (D) can contain contact information.
3. Option B, Michigan, is not a reasonable answer, as it is the name of a place and does not inherently have contact information.
4. Option E, a certificate, again, is unlikely to provide such detailed contact information, like a phone number or address, and thus can be ruled out.
5. Between directory, roster and phone book, a directory could contain listings of things rather than people, and a roster typically contains names and roles without personal contact information.
6. Thus, the most reliable source for detailed contact information such as phone number or address for individuals would be a phone book (D). Hence, the answer is D: phone book.
So the final answer is D: phone book

Input: What is likely to happen to someone's speech after becoming inebriated? Choices:  A: regret B: automobile accidents C: drunk driving D: talking nonsense E: talk intelligently
Output: Reasoning:

Inebriation, or intoxication, refers to the condition that occurs after consuming a significant amount of alcohol.
When inebriated, an individual's cognitive functions, such as thought, coordination, and judgment, are impaired.
Speech, being a cognitive function, also deteriorates under the influence of alcohol.
The impaired judgment and coordination can lead to the person not making much sense in their words or sentences, resulting in gibberish or nonsense.
Choosing between the options,'regret' and 'automobile accidents' are consequences of actions performed under inebriation and 'drunk driving' is an action one might do under inebriation. 'Talk intelligently' is unlikely due to the impairing effects of alcohol.
Therefore, the most likely outcome for someone's speech after becoming inebriated, as per the offered choices, is D: talking nonsense.
So the final answer is D: talking nonsense

Input: A rosebush or squirrel or other plants and animals are part of what? Choices:  A: nature B: neighbor's yard C: park D: these are not valid to the two places E: field
Output: 1. To answer this question, we first need to understand or identify what is common across the given examples that is a rosebush, squirrel and other plants and animals. 
2. The key elements mentioned in the question - a rosebush, other plants, and animals like a squirrel - are all living entities.
3. All of these living things naturally exist, grow, reproduce and perform all life processes in nature.
4. Regardless of their specific location (whether it's a neighbor's yard, a park, a field, or somewhere else), they are fundamentally part of nature because they're biotic components, interacting with abiotic components (such as air, soil, water, etc.) contributing to the ecosystem.
5. Therefore, despite other choices referring to specific locations, the overarching, most accurate category these examples fit into is 'nature'.
6. Hence, the answer is A: nature.
So the final answer is A: nature

Input: The shark was sick of the confines of its tank, what was it looking for? Choices:  A: sea world B: tomales bay C: great oceans D: the key E: tropical waters
Output: 1. The question revolves around a shark and it being discontented with the confined conditions of its tank.
2. Sharks being undersea creatures, their natural habitat is open water.
3. The context does not specify a geographic location or weather condition, so it can be any environment similar to the open water of their natural habitat.
4. The phrase "sick of the confines of its tank" suggests that the shark would be looking for a vast, open space to live in.
5. When we look at the options, "Sea World" is a marine mammal park, which is essentially another form of tank-based confinement.
6. "Tomales Bay" is a specific geographical location and may not necessarily offer the larger confines that a shark requires.
7. "The Key" is too abstract and undefined, it does not refer to a physical space.
8. "Tropical waters" refers to a specific type of water conditions, not necessarily an encompassing environment in terms of space.
9. The option "Great Oceans" is the most suitable choice as it indicates the vast, non-confined waters that make up a shark's natural habitat.
10. So, the answer is C: Great Oceans.
So the final answer is C: great oceans

Input: When people want to watch a new move, the often go see it at the? Choices:  A: town B: conference C: bathroom D: theater E: train station
Output: Reasoning:

The question is about a location where people usually go to watch a new movie. 

Step 1: Let's consider each option:

A: Town. Although towns do sometimes hold movie viewings, it is not a specific location where people go to see a new movie.

B: Conference. Conferences are usually for discussions or presentations on a specific topic. People don't usually go to conferences to see new movies.

C: Bathroom. This answer is not valid because people don't go to bathrooms to watch movies.

D: Theater. Movie theaters are places specially designed for screening films to the public, often featuring comfortable seating and high quality sound systems, making them a traditional and popular location for watching new movies.

E: Train Station. Train stations are for transportation, not for watching new movies.

Step 2: Based on the consideration above, the most suitable place where people go to watch a new movie is D: theater. Therefore, the answer is D: theater.
So the final answer is D: theater

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i6-s30-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                     | 1/50 [01:26<1:10:41, 86.57s/it]Evaluating commonsenseqa :   4%|                                                                    | 2/50 [02:52<1:08:49, 86.02s/it]Evaluating commonsenseqa :   6%|                                                                  | 3/50 [04:17<1:07:04, 85.63s/it]Evaluating commonsenseqa :   8%|                                                                 | 4/50 [05:41<1:05:08, 84.97s/it]Evaluating commonsenseqa :  10%|                                                                | 5/50 [07:05<1:03:28, 84.64s/it]Evaluating commonsenseqa :  12%|                                                              | 6/50 [08:29<1:02:02, 84.60s/it]Evaluating commonsenseqa :  14%|                                                             | 7/50 [09:53<1:00:20, 84.21s/it]Evaluating commonsenseqa :  16%|                                                             | 8/50 [11:17<58:58, 84.26s/it]Evaluating commonsenseqa :  18%|                                                           | 9/50 [12:42<57:44, 84.50s/it]Evaluating commonsenseqa :  20%|                                                         | 10/50 [14:06<56:12, 84.32s/it]Evaluating commonsenseqa :  22%|                                                        | 11/50 [15:31<54:51, 84.41s/it]Evaluating commonsenseqa :  24%|                                                      | 12/50 [16:54<53:19, 84.19s/it]Evaluating commonsenseqa :  26%|                                                     | 13/50 [18:18<51:51, 84.10s/it]Evaluating commonsenseqa :  28%|                                                   | 14/50 [19:43<50:32, 84.25s/it]Evaluating commonsenseqa :  30%|                                                  | 15/50 [21:07<49:06, 84.18s/it]Evaluating commonsenseqa :  32%|                                                 | 16/50 [22:31<47:36, 84.02s/it]Evaluating commonsenseqa :  34%|                                               | 17/50 [23:56<46:28, 84.49s/it]Evaluating commonsenseqa :  36%|                                              | 18/50 [25:20<44:59, 84.37s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [26:45<43:39, 84.49s/it]Evaluating commonsenseqa :  40%|                                           | 20/50 [28:09<42:13, 84.43s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [29:34<40:50, 84.51s/it]Evaluating commonsenseqa :  44%|                                        | 22/50 [30:58<39:23, 84.39s/it]Evaluating commonsenseqa :  46%|                                       | 23/50 [32:22<37:57, 84.34s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [33:46<36:29, 84.21s/it]Evaluating commonsenseqa :  50%|                                    | 25/50 [35:10<35:02, 84.09s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [36:35<33:44, 84.34s/it]Evaluating commonsenseqa :  54%|                                 | 27/50 [38:00<32:21, 84.42s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [39:24<30:58, 84.47s/it]Evaluating commonsenseqa :  58%|                              | 29/50 [40:49<29:33, 84.46s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [42:13<28:07, 84.37s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [43:37<26:45, 84.47s/it]Evaluating commonsenseqa :  64%|                          | 32/50 [45:03<25:24, 84.70s/it]Evaluating commonsenseqa :  66%|                        | 33/50 [46:27<23:58, 84.61s/it]Evaluating commonsenseqa :  68%|                       | 34/50 [47:53<22:38, 84.93s/it]Evaluating commonsenseqa :  70%|                     | 35/50 [49:17<21:08, 84.59s/it]Evaluating commonsenseqa :  72%|                    | 36/50 [50:41<19:41, 84.42s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [52:05<18:18, 84.53s/it]Evaluating commonsenseqa :  76%|                 | 38/50 [53:29<16:50, 84.24s/it]Evaluating commonsenseqa :  78%|               | 39/50 [54:54<15:28, 84.43s/it]Evaluating commonsenseqa :  80%|              | 40/50 [56:18<14:04, 84.47s/it]Evaluating commonsenseqa :  82%|             | 41/50 [57:43<12:41, 84.64s/it]Evaluating commonsenseqa :  84%|           | 42/50 [59:09<11:19, 84.97s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:00:33<09:52, 84.66s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:01:57<08:27, 84.51s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:03:21<07:01, 84.24s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:04:46<05:38, 84.66s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:06:12<04:14, 84.89s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:07:37<02:49, 84.90s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:09:02<01:24, 84.83s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:10:26<00:00, 84.63s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:10:26<00:00, 84.52s/it]
name: commonsenseqa | avg. gen lenth: 407.752 | time: 4226.7085020542145s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-31 16:20:23,552] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 16:20:23,560] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 16:20:23,581] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 16:20:23,604] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 7
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i7-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9734 [00:00<?, ?it/s]Loading data: 100%|| 9734/9734 [00:00<00:00, 399660.86it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.45s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.65s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.93s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.06s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.94s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.32s/it]
 > number of parameters: 6738415616
[2023-08-31 16:20:33,595] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.05s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.44s/it]
[2023-08-31 16:20:33,935] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.07s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.50s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.15s/it][2023-08-31 16:20:34,222] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.59s/it]
[2023-08-31 16:20:34,313] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 16:20:34,903] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-31 16:20:34,904] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 16:20:34,904] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 16:20:34,904] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 16:20:34,904] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 16:20:34,904] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 16:20:34,904] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f11be1b92a0>
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 16:20:34,905] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 16:20:34,906] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 16:20:34,906] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-31 16:20:34,906] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-31 16:20:34,906] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-31 16:20:34,906] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 16:20:34,906] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 16:20:34,906] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-31 16:20:34,906] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-31 16:20:34,906] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-31 16:20:34,906] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 16:20:34,906] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 16:20:34,906] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 16:20:34,906] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The lab results had been compiled, the scientist began analysing the data because he wanted what? Choices:  A: learn more about B: headache C: do math D: enlightened E: better understanding
Output: 1. In the question, a scientist is engaged in analyzing data gathered from lab results. 
2. It is unlikely that he is doing this to get a "headache" as this is usually an undesirable state.
3. Option C, "do math" could be a part of his process to understand the data but it is not the ultimate aim of his activity.
4. "Enlightened" could come close as the scientist may attain enlightenment through discovery, but it is not specific to his action of analysing data.
5. Similarly option A, "learn more about" is also close, but it doesn't specify what he's learning about.
6. So, option E, "better understanding" is the most probable choice because when scientists analyze data, their primary goal is typically to gain a clearer or better understanding of the results.
So the final answer is E: better understanding

Input: I needed to find out how to contact a person with a certain name, where should I look? Choices:  A: directory B: michigan C: roster D: phone book E: certificate
Output: 1. The first step is to identify what we are trying to find. We are not looking for a place, we are not trying to enroll in a course or program, and we are not trying to verify or authenticate some document or identity. Instead, we are looking for a medium that contains contact information about a person.
2. Next, we examine the options given. 
3. Let's consider Option A: a directory. This could potentially be an answer as directories often contain contact information. But what kind of named directory, we are not sure about.
4. Option B: Michigan is a place, not a medium or a source of information where one can find a persons contact information.
5. Option C: a roster typically contains names and roles within a specific context, such as a sports team or a classroom, but they do not always include contact information.
6. Option D: a phone book includes names of people or businesses along with their contact information, specifically their phone numbers and addresses. This seems like a good possibility.
7. Option E: a certificate often verifies completion or awarding of something and does not usually contain contact information.
8. By process of elimination and matching the appropriate resource to the needs of the question, we can see that the answer is D: phone book. A phone book is specifically designed to provide contact information for people and businesses, which is what we are searching for.
So the final answer is D: phone book

Input: What is likely to happen to someone's speech after becoming inebriated? Choices:  A: regret B: automobile accidents C: drunk driving D: talking nonsense E: talk intelligently
Output: Here are the intermediate steps to reach the answer:

Step 1: Understand the question, which asks about the possible effect on someone's speech after becoming inebriated or drunk.

Step 2: Go through the options and eliminate those choices that are not directly related to speech. The options "automobile accidents" and "drunk driving" can be excluded since they are not directly related to speech.

Step 3: Evaluate the remaining choices. "Regret" is a feeling or emotion and isn't directly related to speech, so eliminate this option.

Step 4: Compare the remaining options "talk intelligently" and "talking nonsense". Typically, intoxication or being inebriated often leads to impediments in motor control and cognitive function, including speech. Therefore, it is less likely for someone to 'talk intelligently' when inebriated. 

Step 5: Sous option D, "talking nonsense", seems the most likely since it is a common effect of inebriation on speech. Logic and coherence often diminish, leading to nonsensical or irrelevant conversation.

Hence, the correct answer is D: talking nonsense.
So the final answer is D: talking nonsense

Input: A rosebush or squirrel or other plants and animals are part of what? Choices:  A: nature B: neighbor's yard C: park D: these are not valid to the two places E: field
Output: 1. The question is asking which category rosebushes, squirrels, and other plants and animals fall under.
2. The options provided are nature, a neighbor's yard, a park, none of these places, and a field.
3. Starting from the last option, while it is possible for rosebushes, squirrels and other plants and animals to be found in a field, this is not exclusively true as there are many other types of places where these things can exist. Therefore, the answer is not E: field.
4. Similarly, while it is possible for rosebushes, squirrels, and other plants and animals to exist in someone's yard or a park, these are not the only locations where these could exist. Therefore, B: neighbor's yard and C: park can also be eliminated.
5. Option D: these are not valid to the two places is ambiguous and does not provide a clear category for rosebushes, squirrels and other plants and animals.
6. This leaves us with option A: nature, which is a broad enough category to include rosebushes, squirrels, and all other plants and animals, no matter where they are located. 
7. Thus, the answer is A: nature.
So the final answer is A: nature

Input: The shark was sick of the confines of its tank, what was it looking for? Choices:  A: sea world B: tomales bay C: great oceans D: the key E: tropical waters
Output: 1. The question gives us a hint that the shark is feeling confined in its tank.
2. This indicates that the shark is wanting more space, typically found in larger bodies of water.
3. Looking at the answer choices, sea world and tomales bay are both specific places, but they might not provide the large amount of space a great white shark would desire.
4. "The key" is very vague and doesn't specify anything related to space or water, hence it can be excluded.
5. Tropical waters could fit the shark's desire for more space, but the question does not specify the shark wanting a particular climate or environment. 
6. Therefore, the only plausible answer that fits the context of a shark needing more expansive space is "great oceans". So, the answer is C: great oceans.
So the final answer is C: great oceans

Input: When people want to watch a new move, the often go see it at the? Choices:  A: town B: conference C: bathroom D: theater E: train station
Output: 1. The question asks where people often go to watch a new movie.
2. This suggests we need a location which is associated with movie showings.
3. Let's go through the choices.
4. Choice A: Town is a broad term and is not specifically associated with showing movies. It can be eliminated.
5. Choice B: Conference is a place associated with meetings and presentations. It doesn't directly relate to watching a new movie.  So, it can be eliminated.
6. Choice C: Bathroom is clearly not a place for watching movies. So, it can be dismissed.
7. Choice E: Train Station is a place where people catch trains. It's not related to watching movies. Hence it can be eliminated.
8. Choice D: Theater is a place where movies, especially new releases, are often shown. 
9. So, by a process of elimination, the answer is D: Theater.
So the final answer is D: theater

Input: If someone is talent but doesn't like performing, they probably suffer from what? Choices:  A: injury B: talent persons cannot suffer from injury and butterflies C: anxiety D: happiness E: energetic
Output: 1. The question is asking us about a condition that someone who has talent but doesn't like performing might have.
2. We know from this information that the person has a talent so option B is not applicable because it says 'talent persons cannot suffer from injury and butterflies' which contradicts the given information.
3. Option A seems to not be the correct answer, as the person having an injury is not specifically related to having a talent or liking to perform.
4. Similarly, options D and E do not intuitively make sense since being happy or energetic aren't reasons for not wanting to perform.
5. By process of elimination, the most likely answer is option C - anxiety. This is because someone who has a talent, but doesn't like to perform, might be suffering from performance anxiety, which often leads people to avoid situations where they have to perform. Thus the answer is C.
So the final answer is C: anxiety

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i7-s30-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                     | 1/50 [01:19<1:04:43, 79.26s/it]Evaluating commonsenseqa :   4%|                                                                    | 2/50 [02:36<1:02:28, 78.10s/it]Evaluating commonsenseqa :   6%|                                                                  | 3/50 [03:54<1:01:11, 78.11s/it]Evaluating commonsenseqa :   8%|                                                                   | 4/50 [05:12<59:41, 77.85s/it]Evaluating commonsenseqa :  10%|                                                                 | 5/50 [06:29<58:15, 77.67s/it]Evaluating commonsenseqa :  12%|                                                                | 6/50 [07:46<56:49, 77.48s/it]Evaluating commonsenseqa :  14%|                                                              | 7/50 [09:03<55:26, 77.36s/it]Evaluating commonsenseqa :  16%|                                                             | 8/50 [10:21<54:17, 77.56s/it]Evaluating commonsenseqa :  18%|                                                           | 9/50 [11:38<52:56, 77.46s/it]Evaluating commonsenseqa :  20%|                                                         | 10/50 [12:56<51:35, 77.39s/it]Evaluating commonsenseqa :  22%|                                                        | 11/50 [14:13<50:19, 77.42s/it]Evaluating commonsenseqa :  24%|                                                      | 12/50 [15:30<48:58, 77.34s/it]Evaluating commonsenseqa :  26%|                                                     | 13/50 [16:49<47:59, 77.83s/it]Evaluating commonsenseqa :  28%|                                                   | 14/50 [18:07<46:40, 77.79s/it]Evaluating commonsenseqa :  30%|                                                  | 15/50 [19:24<45:19, 77.71s/it]Evaluating commonsenseqa :  32%|                                                 | 16/50 [20:41<43:54, 77.49s/it]Evaluating commonsenseqa :  34%|                                               | 17/50 [21:58<42:31, 77.31s/it]Evaluating commonsenseqa :  36%|                                              | 18/50 [23:15<41:10, 77.22s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [24:33<39:53, 77.22s/it]Evaluating commonsenseqa :  40%|                                           | 20/50 [25:50<38:39, 77.31s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [27:08<37:24, 77.41s/it]Evaluating commonsenseqa :  44%|                                        | 22/50 [28:25<36:02, 77.22s/it]Evaluating commonsenseqa :  46%|                                       | 23/50 [29:42<34:50, 77.44s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [31:00<33:37, 77.61s/it]Evaluating commonsenseqa :  50%|                                    | 25/50 [32:19<32:23, 77.74s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [33:36<31:06, 77.78s/it]Evaluating commonsenseqa :  54%|                                 | 27/50 [34:55<29:52, 77.94s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [36:12<28:30, 77.77s/it]Evaluating commonsenseqa :  58%|                              | 29/50 [37:30<27:13, 77.80s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [38:48<25:57, 77.89s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [40:06<24:42, 78.05s/it]Evaluating commonsenseqa :  64%|                          | 32/50 [41:25<23:26, 78.14s/it]Evaluating commonsenseqa :  66%|                        | 33/50 [42:42<22:05, 77.94s/it]Evaluating commonsenseqa :  68%|                       | 34/50 [44:00<20:44, 77.79s/it]Evaluating commonsenseqa :  70%|                     | 35/50 [45:17<19:25, 77.70s/it]Evaluating commonsenseqa :  72%|                    | 36/50 [46:34<18:05, 77.55s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [47:53<16:51, 77.82s/it]Evaluating commonsenseqa :  76%|                 | 38/50 [49:11<15:33, 77.83s/it]Evaluating commonsenseqa :  78%|               | 39/50 [50:28<14:15, 77.76s/it]Evaluating commonsenseqa :  80%|              | 40/50 [51:46<12:57, 77.78s/it]Evaluating commonsenseqa :  82%|             | 41/50 [53:04<11:39, 77.77s/it]Evaluating commonsenseqa :  84%|           | 42/50 [54:22<10:21, 77.74s/it]Evaluating commonsenseqa :  86%|          | 43/50 [55:39<09:02, 77.56s/it]Evaluating commonsenseqa :  88%|        | 44/50 [56:58<07:47, 77.96s/it]Evaluating commonsenseqa :  90%|       | 45/50 [58:15<06:29, 77.88s/it]Evaluating commonsenseqa :  92%|     | 46/50 [59:33<05:11, 77.77s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:00:51<03:54, 78.04s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:02:09<02:35, 77.85s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:03:28<01:18, 78.14s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:04:45<00:00, 77.97s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:04:45<00:00, 77.72s/it]
name: commonsenseqa | avg. gen lenth: 429.932 | time: 3886.3620085716248s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-31 17:25:28,995] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 17:25:29,439] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 17:25:29,507] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 17:25:29,514] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 8
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9733 [00:00<?, ?it/s]Loading data: 100%|| 9733/9733 [00:00<00:00, 262357.96it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.88s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.97s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.09s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.51s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.09s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.53s/it]
[2023-08-31 17:25:40,087] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-31 17:25:40,118] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards:  50%|                                     | 1/2 [00:09<00:09,  9.16s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:09<00:09,  9.44s/it]Loading checkpoint shards: 100%|| 2/2 [00:11<00:00,  4.95s/it]Loading checkpoint shards: 100%|| 2/2 [00:11<00:00,  5.58s/it]
[2023-08-31 17:25:42,270] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:11<00:00,  5.03s/it]Loading checkpoint shards: 100%|| 2/2 [00:11<00:00,  5.69s/it]
[2023-08-31 17:25:42,422] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 17:25:43,045] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-31 17:25:43,047] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 17:25:43,047] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 17:25:43,047] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 17:25:43,047] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 17:25:43,047] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe3c93c52d0>
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 17:25:43,048] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 17:25:43,049] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-31 17:25:43,050] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-31 17:25:43,050] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-31 17:25:43,050] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 17:25:43,050] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 17:25:43,050] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 17:25:43,050] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The lab results had been compiled, the scientist began analysing the data because he wanted what? Choices:  A: learn more about B: headache C: do math D: enlightened E: better understanding
Output: 1. The question involves a scientist who has compiled lab results and begun analyzing the data. 
2. The question specifies that the scientist is doing this analysis because he "wanted" something, indicating a motive or a purpose behind his actions. 
3. Choice A: 'learn more about' is likely incorrect, because it is incomplete  it does not specify what the scientist wants to learn more about.
4. Choice B: 'headache' does not make sense, as scientists generally do not seek to achieve a headache through their work.
5. Choice C: 'do math' could be potentially correct, as scientists often incorporate math into their analyses. However, the question does not specify math as the object of the scientist's desire. 
6. Choice D: 'enlightened' is grammatically incorrect, as the sentence would require 'to be enlightened.' Moreover, it is also vague because enlightenment could be achieved through several avenues and not necessarily through analyzing data.
7. Choice E: 'better understanding' is the most appropriate choice. It aligns logically with the context given  scientists analyze data to gain a better understanding of their subject  and it is grammatically correct. Thus, choice E is the correct answer.
So the final answer is E: better understanding

Input: I needed to find out how to contact a person with a certain name, where should I look? Choices:  A: directory B: michigan C: roster D: phone book E: certificate
Output: 1. To find out how to contact a person, we need a resource that lists individuals along with their contact information.
2. Evaluating the choices, A: directory is a good possibility as it often includes contact information, but it could also just be a directory of people without their contact details.
3. Choice B: Michigan is a place, not a resource for finding contact information.
4. Choice C: a roster typically lists names and sometimes roles or responsibilities, but does not usually include contact information.
5. Choice E: a certificate can have a person's name, but it generally does not include contact information.
6. Choice D: a phone book typically includes both names and phone numbers, making it an ideal resource for finding how to contact someone. 
7. Therefore, the answer is D: phone book.
So the final answer is D: phone book

Input: What is likely to happen to someone's speech after becoming inebriated? Choices:  A: regret B: automobile accidents C: drunk driving D: talking nonsense E: talk intelligently
Output: 1. The question asks about the effect of being inebriated on someone's speech. 
2. Inebriation means being drunk or highly intoxicated because of alcohol consumption. 
3. The effects of heavy drinking or inebriation are well-documented. In general, it impacts cognitive functions, including speech.
4. The first choice, "A: regret", isn't plausible as it's an emotion however, feelings of regret may come as a result of actions taken while inebriated but it does not direct impact on speech.
5. The next two choices, "B: automobile accidents" and "C: drunk driving", are related to actions that might be taken while inebriated, but they are not ones that directly relate to the effect on someone's speech.
6. The fourth choice, "D: talking nonsense", shows a direct impact on speech. Under the influence of alcohol, people might start to make less sense in their speech, mispronounce words, or ramble. Hence, this meets the criteria specified in the question. 
7. The final choice, "E: talk intelligently", is unlikely due to alcohol's inhibitory impact on mental functions.
8. Therefore, the most appropriate answer to the question is "D: talking nonsense", as it directly relates to the effect of inebriation on speech.
So the final answer is D: talking nonsense

Input: A rosebush or squirrel or other plants and animals are part of what? Choices:  A: nature B: neighbor's yard C: park D: these are not valid to the two places E: field
Output: 1. Analyze the question, it asks about the general group or category that a rosebush, squirrel, and other plants and animals can be part of.
2. Look at the options one by one.
3. From option B: not all the rosebush, squirrel and other plants and animals are part of the "neighbor's yard," so this option is not correct because it's too specific and not applicable globally.
4. From option C: Similarly, not all the rosebush, squirrel and other plants and animals are part of a "park". This too is a limited area and does not account for ones in wild or other areas. So, this option is also not correct.
5. From option D: The question does not specify any two places or contexts, thus this option is not relevant and therefore incorrect.
6. From option E: similar to option B and C, not all the rosebush, squirrel and other plants and animals are part of a "field". This option is also incorrect.
7. Finally, looking at option A: All plants and animals including rosebush and squirrel indeed are part of "nature," no matter where they are or whether they are domesticated or wild. Considering what has been learned from each step, we can come to the conclusion that the answer is A: nature.
So the final answer is A: nature

Input: The shark was sick of the confines of its tank, what was it looking for? Choices:  A: sea world B: tomales bay C: great oceans D: the key E: tropical waters
Output: 1. The question centers on a shark that is tired of its tank, implying that it is seeking a wider, natural habitat.
2. Among the given choices, we need to identify the most appropriate dwelling place for a shark.
3. Sea World (option A) is a theme park. Despite having large aquariums, it still doesn't equate to the broadness of a shark's natural habitat. 
4. Tomales Bay (option B) is indeed a body of water, but it is a narrow inlet, not a wide expanse. 
5. "The key" (option D) is too vague and doesn't directly relate to a shark's habitat.
6. "Tropical waters" (option E) might be appealing, but it doesn't necessarily offer the broad expanse the shark may desire.
7. Comparatively, "great oceans" (option C) offers the vastness and freedom that the shark is likely seeking. Therefore, the answer is "C: great oceans".
So the final answer is C: great oceans

Input: When people want to watch a new move, the often go see it at the? Choices:  A: town B: conference C: bathroom D: theater E: train station
Output: 1. The question is asking where people often go to watch a new movie.
2. We need to find a place that is commonly associated with screening movies.
3. Option A, town, is not specific as movies can be watched in various places within a town.
4. Option B, conference, is not suitable as conferences are usually associated with business meetings, academic gatherings, or discussions on specific topics, not movie screenings.
5. Option C, bathroom, is incorrect too. Bathrooms are usually used for personal hygiene, not for watching movies.
6. Option E, train station, is also an unsuitable place for watching movies, as it's primarily used for transportation.
7. Option D, theater, is the correct answer since theaters are specifically designed for screening movies and it is the most common place people go to watch new movies.
So the final answer is D: theater

Input: If someone is talent but doesn't like performing, they probably suffer from what? Choices:  A: injury B: talent persons cannot suffer from injury and butterflies C: anxiety D: happiness E: energetic
Output: 1. The question suggests that a person possesses talent but has an aversion to performing it.
2. This aversion or reluctance to perform could be due to a number of factors.
3. If we go through the choices: A: injury isn't relevant as the question doesn't mention any physical hurt. 
4. B: "talent persons cannot suffer from injury and butterflies" doesn't make sense and doesn't provide a reasonable conclusion.
5. C: anxiety could be a reasonable answer as it encompasses the fear or nervousness someone might feel about performing, even if they are skilful.
6. D: happiness is not a probable answer as people who are happy generally do not avoid things they are talented at, quite the contrary. 
7. E: being energetic also does not imply an aversion to performance.
8. Therefore, comparing all choices, we conclude that 'C: anxiety' is the most probable reason a talented person might avoid performing.
So the final answer is C: anxiety

Input: Where would you put bread immediately after buying it? Choices:  A: toaster B: pantry C: plastic bag D: refrigerator E: box
Output: 1. The question asks where you would put bread immediately after buying it.
2. This implies that the correct answer should be a place or container where bread is commonly stored immediately after it is purchased.
3. Choice A, the toaster, is not the correct answer because bread is not immediately toasted after being bought.
4. Choice B, the pantry, could be an option but not immediately after buying it. In most homes, bread goes through another step before reaching the pantry.
5. Choice C is a plastic bag. After purchasing bread, particularly in a grocery store, items are usually put in plastic bags before leaving the store. So this could be a correct answer.
6. Choice D, the refrigerator, could be a place to store bread, but usually, you don't put it there immediately after buying.
7. Choice E, the box, does not seem like a standard place where you would store bread immediately after buying it.
8. Considering all these factors, the most likely answer is C: plastic bag, since that is largely where bread would go right after purchase before it's taken home.
So the final answer is C: plastic bag

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s30-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                       | 1/50 [01:12<59:05, 72.36s/it]Evaluating commonsenseqa :   4%|                                                                      | 2/50 [02:23<57:17, 71.61s/it]Evaluating commonsenseqa :   6%|                                                                    | 3/50 [03:34<55:59, 71.47s/it]Evaluating commonsenseqa :   8%|                                                                   | 4/50 [04:45<54:30, 71.10s/it]Evaluating commonsenseqa :  10%|                                                                 | 5/50 [05:55<53:10, 70.90s/it]Evaluating commonsenseqa :  12%|                                                                | 6/50 [07:06<52:02, 70.97s/it]Evaluating commonsenseqa :  14%|                                                              | 7/50 [08:17<50:47, 70.88s/it]Evaluating commonsenseqa :  16%|                                                             | 8/50 [09:29<49:43, 71.04s/it]Evaluating commonsenseqa :  18%|                                                           | 9/50 [10:39<48:28, 70.95s/it]Evaluating commonsenseqa :  20%|                                                         | 10/50 [11:50<47:12, 70.82s/it]Evaluating commonsenseqa :  22%|                                                        | 11/50 [13:00<46:00, 70.77s/it]Evaluating commonsenseqa :  24%|                                                      | 12/50 [14:11<44:50, 70.79s/it]Evaluating commonsenseqa :  26%|                                                     | 13/50 [15:22<43:34, 70.68s/it]Evaluating commonsenseqa :  28%|                                                   | 14/50 [16:33<42:25, 70.72s/it]Evaluating commonsenseqa :  30%|                                                  | 15/50 [17:44<41:18, 70.80s/it]Evaluating commonsenseqa :  32%|                                                 | 16/50 [18:54<40:03, 70.70s/it]Evaluating commonsenseqa :  34%|                                               | 17/50 [20:05<38:51, 70.66s/it]Evaluating commonsenseqa :  36%|                                              | 18/50 [21:15<37:41, 70.66s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [22:26<36:30, 70.65s/it]Evaluating commonsenseqa :  40%|                                           | 20/50 [23:36<35:18, 70.62s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [24:47<34:06, 70.57s/it]Evaluating commonsenseqa :  44%|                                        | 22/50 [25:58<32:58, 70.65s/it]Evaluating commonsenseqa :  46%|                                       | 23/50 [27:09<31:52, 70.82s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [28:20<30:44, 70.95s/it]Evaluating commonsenseqa :  50%|                                    | 25/50 [29:31<29:34, 70.99s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [30:42<28:20, 70.86s/it]Evaluating commonsenseqa :  54%|                                 | 27/50 [31:52<27:08, 70.80s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [33:04<26:03, 71.06s/it]Evaluating commonsenseqa :  58%|                              | 29/50 [34:15<24:52, 71.05s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [35:26<23:41, 71.05s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [36:37<22:27, 70.90s/it]Evaluating commonsenseqa :  64%|                          | 32/50 [37:48<21:16, 70.91s/it]Evaluating commonsenseqa :  66%|                        | 33/50 [38:59<20:06, 70.98s/it]Evaluating commonsenseqa :  68%|                       | 34/50 [40:10<18:54, 70.92s/it]Evaluating commonsenseqa :  70%|                     | 35/50 [41:20<17:43, 70.91s/it]Evaluating commonsenseqa :  72%|                    | 36/50 [42:31<16:31, 70.82s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [43:42<15:22, 70.99s/it]Evaluating commonsenseqa :  76%|                 | 38/50 [44:53<14:10, 70.88s/it]Evaluating commonsenseqa :  78%|               | 39/50 [46:04<12:59, 70.89s/it]Evaluating commonsenseqa :  80%|              | 40/50 [47:15<11:49, 70.92s/it]Evaluating commonsenseqa :  82%|             | 41/50 [48:26<10:39, 71.06s/it]Evaluating commonsenseqa :  84%|           | 42/50 [49:37<09:28, 71.03s/it]Evaluating commonsenseqa :  86%|          | 43/50 [50:48<08:17, 71.04s/it]Evaluating commonsenseqa :  88%|        | 44/50 [52:00<07:07, 71.24s/it]Evaluating commonsenseqa :  90%|       | 45/50 [53:12<05:57, 71.42s/it]Evaluating commonsenseqa :  92%|     | 46/50 [54:23<04:45, 71.40s/it]Evaluating commonsenseqa :  94%|    | 47/50 [55:34<03:33, 71.25s/it]Evaluating commonsenseqa :  96%|   | 48/50 [56:45<02:22, 71.07s/it]Evaluating commonsenseqa :  98%| | 49/50 [57:56<01:11, 71.13s/it]Evaluating commonsenseqa : 100%|| 50/50 [59:07<00:00, 70.98s/it]Evaluating commonsenseqa : 100%|| 50/50 [59:07<00:00, 70.95s/it]
name: commonsenseqa | avg. gen lenth: 448.472 | time: 3547.9181258678436s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-31 18:24:59,986] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 18:25:00,455] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 18:25:00,518] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 18:25:00,519] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 9
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i9-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9732 [00:00<?, ?it/s]Loading data: 100%|| 9732/9732 [00:00<00:00, 247101.64it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.73s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.80s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.90s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.58s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.96s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.38s/it]
 > number of parameters: 6738415616
[2023-08-31 18:25:10,774] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.02s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.43s/it]
[2023-08-31 18:25:10,874] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.07s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.50s/it]
[2023-08-31 18:25:11,014] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.28s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.78s/it]
[2023-08-31 18:25:11,616] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 18:25:12,241] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-31 18:25:12,243] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 18:25:12,243] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 18:25:12,243] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 18:25:12,243] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 18:25:12,243] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 18:25:12,243] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 18:25:12,243] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 18:25:12,243] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 18:25:12,243] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 18:25:12,243] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 18:25:12,243] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f721e4cd300>
[2023-08-31 18:25:12,243] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 18:25:12,244] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 18:25:12,245] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 18:25:12,245] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The lab results had been compiled, the scientist began analysing the data because he wanted what? Choices:  A: learn more about B: headache C: do math D: enlightened E: better understanding
Output: 1. The question revolves around finding out the reason why the scientist began analysing the data.
2. The first step is to understand the context of the sentence. Since the scientist is analysing the data of lab results, it is plausible to rule out irrelevant answers.
3. Choices B: headache and C: do math are not related to the activity of the scientist and neither provide a logical reason for his action, so both these options can be discarded.
4. Looking at the rest of the options, choices A: learn more about, D: enlightened and E: better understanding, seem plausible as they are related to data analysis.
5. Choice D: enlightened is a bit vague and doesn't specifically align with the goal of analysing data. So, it can be ruled out.
6. Considering options A: learn more about and E: better understanding, both appear connected to the task of data analysis. 
7. The best choice would be the one which gives a more comprehensive and genuine reason why scientists analyse data. Though "learn more about" is a valid reason, it is incomplete without knowing what the scientist wants to learn more about.
8. From this reasoning, the answer is E: better understanding as this option accurately captures the broader aim of data analysis, implying the scientist wants to gain a deeper or better understanding of the lab results. This also makes sense in a scientific context as research work generally involves analysis to improve understanding about a given subject or data.
So the final answer is E: better understanding

Input: I needed to find out how to contact a person with a certain name, where should I look? Choices:  A: directory B: michigan C: roster D: phone book E: certificate
Output: 1. The question is asking for a resource where one might find contact information for a person knowing only their name.
2. Option A, "directory", can contain contact information, but it is unclear what type of directory is being referred to here. It could mean anything from a phone book to a school yearbook, and therefore its broad nature makes it less accurate.
3. Option B, "Michigan", does not seem viable as it is a state, not a tool or resource for finding contact information.
4. Option C, "roster", often contains names and roles but does not routinely provide contact information.
5. Option E, "certificate", also does not regularly contain contact information; it has more to do with qualifications or achievements.
6. Option D, "phone book", is a known resource for obtaining a person's contact information based on their name.
7. Therefore, the best answer and most fitting choice to the question is D: phone book.
So the final answer is D: phone book

Input: What is likely to happen to someone's speech after becoming inebriated? Choices:  A: regret B: automobile accidents C: drunk driving D: talking nonsense E: talk intelligently
Output: 1. The question is asking about the effects of becoming inebriated, or drunk, on someone's speech.
2. The choices provided include a range of outcomes: regret, automobile accidents, drunk driving, talking nonsense, and talk intelligently. 
3. We can rule out A: regret, because this is an emotional response, not a direct consequence on speech. 
4. We can rule out B: automobile accidents, and C: drunk driving, as they both are not directly related to someone's speech.
5. Between remaining options D: talking nonsense and E: talk intelligently, we need to consider the typical effects of alcohol.
6. Generally, being inebriated impairs cognitive function and coordination, not making someone more articulate or logical.
7. Therefore, the answer is D: talking nonsense, as this choice most accurately describes the likely effect of alcohol on someone's speech.
So the final answer is D: talking nonsense

Input: A rosebush or squirrel or other plants and animals are part of what? Choices:  A: nature B: neighbor's yard C: park D: these are not valid to the two places E: field
Output: 1. The question is asking about the general category that includes a rosebush, a squirrel and other plants and animals.
2. Choices B, C, and E imply specific places like a neighbor's yard, a park, or a field. While these may contain a rosebush, a squirrel or other plants and animals, they don't apply to all such living things in general.
3. Choice D is incorrect because it doesn't valid in regards to the question as it doesn't necessarily denote a place or category in which these things exist.
4. Thus, the only option left is A: Nature, which is a broad category that includes all plants and animals, including a rosebush and a squirrel, regardless of their specific location. Therefore, the answer is A: Nature.
So the final answer is A: nature

Input: The shark was sick of the confines of its tank, what was it looking for? Choices:  A: sea world B: tomales bay C: great oceans D: the key E: tropical waters
Output: 1. The question states the shark was "sick of the confines of its tank," meaning it was not content with the limits of the tank.
2. This implies that the shark was seeking more space or a larger environment.
3. Let's examine the options: Sea World and Tomales Bay are both specific places, still considered enclosed environments and not known for having vast spaces; "the key" is not related to an aquatic environment; "tropical waters" is a type of aquatic environment, but not necessarily larger in scope.
4. "Great oceans", however, offers a vast, expansive body of water, which would provide the shark an unlimited space and environment.
5. Therefore, based on the clues in the question, the answer is likely C: great oceans.
So the final answer is C: great oceans

Input: When people want to watch a new move, the often go see it at the? Choices:  A: town B: conference C: bathroom D: theater E: train station
Output: 1. The question is asking about where people often go to watch a new movie. 
2. We need to determine the most common place among the choices where movies are typically shown.
3. We can quickly eliminate options that clearly don't make sense based on common knowledge. People don't typically watch movies in the bathroom (C), or at the train station (E).
4. Similarly, while towns (A) and conferences (B) might host special movie screenings sometimes, they are not the standard places for watching new movies.
5. The only remaining option is a theater (D), which is indeed a place where new movies are usually screened.
6. Therefore, the answer is D: theater.
So the final answer is D: theater

Input: If someone is talent but doesn't like performing, they probably suffer from what? Choices:  A: injury B: talent persons cannot suffer from injury and butterflies C: anxiety D: happiness E: energetic
Output: 1. The question suggests that despite having a talent, the person does not like performing.
2. This behavior might occur due to a psychological barrier that prevents a talented person from showcasing their capabilities.
3. Looking at the options, we can dismiss option A: injury. Injury is a physical issue, not directly related to one's will to perform.
4. The second option, 'talent persons cannot suffer from injury and butterflies,' is irrelevant. But butterflies in the stomach can symbolize nervousness, which is a part of anxiety. 
5. Option D: happiness and Option E: energetic are positive states or qualities, which would not provide a reason for not wanting to perform.
6. Anxiety can cause people to avoid situations that make them uncomfortable, like performing in front of others, despite having the talent to do so.
7. Therefore, option C: anxiety provides the most suitable answer to the question.
So the final answer is C: anxiety

Input: Where would you put bread immediately after buying it? Choices:  A: toaster B: pantry C: plastic bag D: refrigerator E: box
Output: 1. The question is asking where to put bread immediately after buying it.
2. Once you buy bread from the store, you may put the bread into a variety of places based on the future immediate requirement. But we need to think about the very first action immediately after we purchase it, with the choices provided.
3. Looking at the options, starting with A: toaster. This is not typically the first place we put bread after buying it as we usually do so when we are ready to eat it. So, A is not the answer.
4. Option B: Pantry. This is where we generally store bread for daily usage. But usually after we take it home, not immediately after buying it. So, it's not the likely answer.
5. Option C: Plastic bag. This is the most immediate step after buying bread. Usually, in many stores, bread is placed in a plastic bag either for sanitary reasons or ease of carrying. So, this could be the answer.
6. Option D: Refrigerator. We usually put the bread in the refrigerator to extend its shelf life, but typically not immediately after buying it. So, this is not the answer.
7. Option E: Box. This is not where we typically place bread at any stage. So, it's not the answer.
8. Therefore, noticing all the choices and their relevance to the question, the most sensible answer is C: plastic bag.
So the final answer is C: plastic bag

Input: She complained about the radio, what was it doing? Choices:  A: radiotracking B: turn on C: train D: blaring E: play music
Output: 1. The question asks about a problem concerning a radio, which is a device used for audio transmission.
2. The options provided are actions that a radio can perform: radiotracking, turn on, train, blaring, and play music.
3. We have to search for an option that presents an issue or complication with a radio which could cause someone to complain.
4. Looking at the options, radiotracking doesn't apply to a regular radio, so we can eliminate it.
5. The option "turn on" doesn't usually cause any complaint unless the radio turns on by itself which is not implied in the question. So, we can eliminate this option as well.
6. The option "train" seems out of context as operations related to training don't relate to a common use of a radio, we can disregard this option.
7. The option "play music" is the primary function of a radio and won't usually cause a complaint, so we can eliminate this too.
8. The only option left is "blaring", which refers to a radio playing music too loudly. This can be a reason for a complaint.
9. Therefore, the answer is D: blaring.
So the final answer is D: blaring

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i9-s30-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                       | 1/50 [01:10<57:16, 70.13s/it]Evaluating commonsenseqa :   4%|                                                                      | 2/50 [02:19<55:37, 69.54s/it]Evaluating commonsenseqa :   6%|                                                                    | 3/50 [03:27<53:59, 68.92s/it]Evaluating commonsenseqa :   8%|                                                                   | 4/50 [04:36<52:57, 69.07s/it]Evaluating commonsenseqa :  10%|                                                                 | 5/50 [05:45<51:39, 68.89s/it]Evaluating commonsenseqa :  12%|                                                                | 6/50 [06:53<50:22, 68.69s/it]Evaluating commonsenseqa :  14%|                                                              | 7/50 [08:01<49:06, 68.53s/it]Evaluating commonsenseqa :  16%|                                                             | 8/50 [09:10<47:59, 68.56s/it]Evaluating commonsenseqa :  18%|                                                           | 9/50 [10:18<46:49, 68.52s/it]Evaluating commonsenseqa :  20%|                                                         | 10/50 [11:27<45:43, 68.58s/it]Evaluating commonsenseqa :  22%|                                                        | 11/50 [12:36<44:37, 68.66s/it]Evaluating commonsenseqa :  24%|                                                      | 12/50 [13:44<43:27, 68.63s/it]Evaluating commonsenseqa :  26%|                                                     | 13/50 [14:53<42:18, 68.60s/it]Evaluating commonsenseqa :  28%|                                                   | 14/50 [16:02<41:10, 68.62s/it]Evaluating commonsenseqa :  30%|                                                  | 15/50 [17:10<39:55, 68.44s/it]Evaluating commonsenseqa :  32%|                                                 | 16/50 [18:18<38:45, 68.40s/it]Evaluating commonsenseqa :  34%|                                               | 17/50 [19:27<37:39, 68.47s/it]Evaluating commonsenseqa :  36%|                                              | 18/50 [20:36<36:38, 68.71s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [21:45<35:30, 68.71s/it]Evaluating commonsenseqa :  40%|                                           | 20/50 [22:54<34:23, 68.78s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [24:02<33:15, 68.82s/it]Evaluating commonsenseqa :  44%|                                        | 22/50 [25:11<32:02, 68.65s/it]Evaluating commonsenseqa :  46%|                                       | 23/50 [26:19<30:51, 68.57s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [27:28<29:44, 68.62s/it]Evaluating commonsenseqa :  50%|                                    | 25/50 [28:36<28:32, 68.48s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [29:44<27:22, 68.43s/it]Evaluating commonsenseqa :  54%|                                 | 27/50 [30:53<26:16, 68.56s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [32:01<25:04, 68.41s/it]Evaluating commonsenseqa :  58%|                              | 29/50 [33:09<23:55, 68.35s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [34:18<22:48, 68.43s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [35:27<21:40, 68.44s/it]Evaluating commonsenseqa :  64%|                          | 32/50 [36:35<20:30, 68.37s/it]Evaluating commonsenseqa :  66%|                        | 33/50 [37:43<19:21, 68.34s/it]Evaluating commonsenseqa :  68%|                       | 34/50 [38:52<18:17, 68.57s/it]Evaluating commonsenseqa :  70%|                     | 35/50 [40:01<17:08, 68.55s/it]Evaluating commonsenseqa :  72%|                    | 36/50 [41:09<16:00, 68.62s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [42:18<14:52, 68.63s/it]Evaluating commonsenseqa :  76%|                 | 38/50 [43:27<13:45, 68.76s/it]Evaluating commonsenseqa :  78%|               | 39/50 [44:36<12:35, 68.66s/it]Evaluating commonsenseqa :  80%|              | 40/50 [45:44<11:26, 68.68s/it]Evaluating commonsenseqa :  82%|             | 41/50 [46:53<10:17, 68.62s/it]Evaluating commonsenseqa :  84%|           | 42/50 [48:02<09:09, 68.69s/it]Evaluating commonsenseqa :  86%|          | 43/50 [49:10<08:00, 68.71s/it]Evaluating commonsenseqa :  88%|        | 44/50 [50:20<06:53, 68.85s/it]Evaluating commonsenseqa :  90%|       | 45/50 [51:28<05:43, 68.79s/it]Evaluating commonsenseqa :  92%|     | 46/50 [52:36<04:34, 68.58s/it]Evaluating commonsenseqa :  94%|    | 47/50 [53:44<03:25, 68.46s/it]Evaluating commonsenseqa :  96%|   | 48/50 [54:53<02:17, 68.60s/it]Evaluating commonsenseqa :  98%| | 49/50 [56:02<01:08, 68.58s/it]Evaluating commonsenseqa : 100%|| 50/50 [57:10<00:00, 68.53s/it]Evaluating commonsenseqa : 100%|| 50/50 [57:10<00:00, 68.62s/it]
name: commonsenseqa | avg. gen lenth: 425.064 | time: 3431.481323003769s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s30-rFalse --seed 30 --max-prompt-length 4096 --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-31 19:22:41,235] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 19:22:41,304] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 19:22:41,329] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 19:22:41,332] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s30-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 1
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s30-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9740 [00:00<?, ?it/s]Loading data: 100%|| 9740/9740 [00:00<00:00, 1646681.48it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.48s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.52s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  7.00s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.18s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.94s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.32s/it]
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.96s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.35s/it]
[2023-08-31 19:22:51,094] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-31 19:22:51,127] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.16s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.58s/it]
[2023-08-31 19:22:51,734] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.23s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.67s/it]
[2023-08-31 19:22:51,912] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 19:22:52,503] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-31 19:22:52,504] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9f79fc9300>
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 19:22:52,505] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 19:22:52,506] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 19:22:52,506] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The lab results had been compiled, the scientist began analysing the data because he wanted what? Choices:  A: learn more about B: headache C: do math D: enlightened E: better understanding
Output: E: better understanding

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s30-rFalse-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [02:20<1:54:25, 140.11s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [04:41<1:52:40, 140.84s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [07:04<1:51:00, 141.72s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [09:24<1:48:15, 141.21s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [10:45<1:29:40, 119.56s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [13:06<1:33:00, 126.84s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [15:26<1:33:54, 131.04s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [17:48<1:34:11, 134.56s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [20:12<1:33:53, 137.41s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [22:31<1:32:00, 138.02s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [24:53<1:30:31, 139.26s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [27:12<1:28:09, 139.20s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [29:34<1:26:21, 140.03s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [31:55<1:24:11, 140.31s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [34:17<1:22:08, 140.82s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [36:39<1:19:57, 141.11s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [38:57<1:17:03, 140.10s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [41:11<1:13:46, 138.32s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [43:32<1:11:54, 139.17s/it]Evaluating commonsenseqa :  40%|                                         | 20/50 [45:52<1:09:45, 139.53s/it]Evaluating commonsenseqa :  42%|                                        | 21/50 [48:12<1:07:26, 139.53s/it]Evaluating commonsenseqa :  44%|                                      | 22/50 [50:32<1:05:12, 139.72s/it]Evaluating commonsenseqa :  46%|                                     | 23/50 [52:51<1:02:49, 139.62s/it]Evaluating commonsenseqa :  48%|                                    | 24/50 [55:10<1:00:20, 139.27s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [57:33<58:30, 140.42s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [59:52<55:59, 139.97s/it]Evaluating commonsenseqa :  54%|                               | 27/50 [1:02:10<53:29, 139.53s/it]Evaluating commonsenseqa :  56%|                              | 28/50 [1:04:30<51:10, 139.58s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [1:06:53<49:14, 140.69s/it]Evaluating commonsenseqa :  60%|                           | 30/50 [1:08:44<43:54, 131.74s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:11:03<42:20, 133.70s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:13:23<40:41, 135.63s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:15:42<38:43, 136.68s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:18:00<36:36, 137.27s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:20:20<34:27, 137.84s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:22:40<32:22, 138.75s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:24:58<29:57, 138.30s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:27:18<27:47, 138.92s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:29:39<25:34, 139.53s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:31:28<21:42, 130.23s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:33:52<20:10, 134.53s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:36:12<18:08, 136.05s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:38:33<16:02, 137.54s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:40:51<13:46, 137.76s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:43:13<11:35, 139.13s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:45:34<09:18, 139.57s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:47:40<06:46, 135.52s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:50:00<04:33, 136.97s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:51:25<02:01, 121.34s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:52:08<00:00, 97.72s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:52:08<00:00, 134.57s/it]
name: commonsenseqa | avg. gen lenth: 284.996 | time: 6728.631060361862s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s30-rFalse --seed 30 --max-prompt-length 4096 --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-31 21:16:50,331] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 21:16:50,333] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 21:16:50,335] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 21:16:50,354] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s30-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 2
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s30-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9739 [00:00<?, ?it/s]Loading data: 100%|| 9739/9739 [00:00<00:00, 1149556.11it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.94s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.01s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.04s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.12s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.54s/it]
[2023-08-31 21:17:00,705] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.22s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.64s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.24s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.66s/it]
[2023-08-31 21:17:00,867] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-31 21:17:01,001] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards:  50%|                                     | 1/2 [00:10<00:10, 10.33s/it]Loading checkpoint shards: 100%|| 2/2 [00:12<00:00,  5.48s/it]Loading checkpoint shards: 100%|| 2/2 [00:12<00:00,  6.21s/it]
[2023-08-31 21:17:04,071] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-31 21:17:04,735] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-31 21:17:04,737] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 21:17:04,737] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 21:17:04,737] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 21:17:04,737] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 21:17:04,737] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f66a9ccd2d0>
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 21:17:04,738] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 21:17:04,739] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-31 21:17:04,740] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-31 21:17:04,740] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-31 21:17:04,740] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 21:17:04,740] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 21:17:04,740] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-31 21:17:04,740] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-31 21:17:04,740] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-31 21:17:04,740] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 21:17:04,740] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 21:17:04,740] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 21:17:04,740] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The lab results had been compiled, the scientist began analysing the data because he wanted what? Choices:  A: learn more about B: headache C: do math D: enlightened E: better understanding
Output: E: better understanding

Input: I needed to find out how to contact a person with a certain name, where should I look? Choices:  A: directory B: michigan C: roster D: phone book E: certificate
Output: D: phone book

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s30-rFalse-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [02:16<1:51:50, 136.95s/it]Evaluating commonsenseqa :   4%|                                                                    | 2/50 [03:16<1:13:21, 91.71s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [05:35<1:28:38, 113.16s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [07:55<1:34:44, 123.58s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [10:12<1:36:28, 128.64s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [12:32<1:36:59, 132.26s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [14:49<1:36:02, 134.00s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [17:09<1:35:00, 135.72s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [19:29<1:33:44, 137.19s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [21:47<1:31:40, 137.50s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [24:05<1:29:30, 137.70s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [26:22<1:27:03, 137.45s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [28:40<1:24:49, 137.54s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [30:56<1:22:11, 136.99s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [33:18<1:20:45, 138.44s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [35:38<1:18:46, 139.01s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [37:02<1:07:26, 122.64s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [39:21<1:07:56, 127.39s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [41:39<1:07:25, 130.48s/it]Evaluating commonsenseqa :  40%|                                         | 20/50 [43:54<1:05:55, 131.84s/it]Evaluating commonsenseqa :  42%|                                        | 21/50 [46:13<1:04:45, 133.99s/it]Evaluating commonsenseqa :  44%|                                      | 22/50 [48:31<1:03:08, 135.29s/it]Evaluating commonsenseqa :  46%|                                     | 23/50 [50:48<1:01:06, 135.79s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [53:06<59:09, 136.53s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [55:27<57:26, 137.86s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [57:43<54:58, 137.43s/it]Evaluating commonsenseqa :  54%|                               | 27/50 [1:00:02<52:45, 137.62s/it]Evaluating commonsenseqa :  56%|                              | 28/50 [1:02:22<50:43, 138.35s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [1:04:39<48:17, 137.97s/it]Evaluating commonsenseqa :  60%|                           | 30/50 [1:06:37<44:03, 132.15s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:08:54<42:18, 133.60s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:11:14<40:38, 135.49s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:13:32<38:37, 136.33s/it]