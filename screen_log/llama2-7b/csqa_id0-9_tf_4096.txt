PYTHONPATH=/home/ylu130/workspace/in-context-generalization
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i0-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 0
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:15:19,067] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:15:19,575] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:15:19,589] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:15:19,594] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:15:25,816] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:15:26,405] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:15:26,443] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:15:26,448] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:15:32,828] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:15:33,089] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:15:33,094] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:15:33,365] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:15:39,733] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:15:39,864] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:15:39,891] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-28 22:15:40,244] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:15:46,590] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:15:46,698] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:15:46,698] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-28 22:15:47,258] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:15:53,604] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:15:53,745] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-28 22:15:54,225] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:15:54,238] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:16:00,727] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:00,802] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:00,814] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-28 22:16:01,316] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:16:07,708] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:07,817] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-28 22:16:08,319] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:08,319] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:16:14,661] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:14,661] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:14,662] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-28 22:16:15,189] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...Answers already exist, exiting...

torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:16:21,615] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:21,615] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:21,977] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:22,142] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:16:28,635] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:28,635] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-28 22:16:29,117] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:29,158] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:16:35,499] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:35,503] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:35,503] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:35,503] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:16:42,452] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:42,463] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:42,465] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:42,980] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:16:49,778] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:49,919] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:49,928] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-28 22:16:50,265] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:16:57,502] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:57,634] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:57,690] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:16:58,011] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:17:04,389] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:17:04,441] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:17:04,926] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:17:04,942] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:17:11,428] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:17:11,429] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:17:11,429] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:17:11,736] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:17:18,403] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:17:18,423] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:17:18,448] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:17:18,728] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:17:25,353] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:17:25,353] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:17:25,362] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-28 22:17:25,853] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 22:17:32,523] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:17:32,523] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 22:17:32,575] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-28 22:17:32,940] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 1
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                        | 0/9740 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████| 9740/9740 [00:00<00:00, 888813.20it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.20s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.24s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.34s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.36s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.16s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.62s/it]
[2023-08-28 22:17:43,601] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.26s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.71s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.25s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.71s/it]
[2023-08-28 22:17:43,767] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-28 22:17:43,801] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.32s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.78s/it]
[2023-08-28 22:17:44,027] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 22:17:44,611] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-28 22:17:44,612] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-28 22:17:44,612] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-28 22:17:44,612] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-28 22:17:44,612] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-28 22:17:44,612] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-28 22:17:44,612] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fdada453520>
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-28 22:17:44,613] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-28 22:17:44,614] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-28 22:17:44,614] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-28 22:17:44,614] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-28 22:17:44,614] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-28 22:17:44,614] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-28 22:17:44,614] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-28 22:17:44,614] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-28 22:17:44,614] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-28 22:17:44,614] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-28 22:17:44,614] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-28 22:17:44,614] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-28 22:17:44,614] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-28 22:17:44,614] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-28 22:17:44,614] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-28 22:17:44,614] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-28 22:17:44,614] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-28 22:17:44,614] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                             | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: 1. Analyze the question: The question asks for a type of'seller' where fabric is cut to order.
2. Consider each option in relation to fabric being cut to order. 
3. Option A: Curtains - These are a type of product, not a place where fabric is cut to order.
4. Option B: Tailor shop - This location specializes in custom fitting and modifying garments, which would require cutting fabric to order in order for each piece to be customized to the specifications of the customer.
5. Option C: Clothing store - Most clothing stores sell ready-made clothes and do not typically provide a service to cut fabric to order.
6. Option D: Sewing room - This is not a seller, but rather a location where someone might cut fabric for personal use.
7. Option E: Hardware store - This is a store that typically sells tools and building materials, not fabric or clothing.
8. Therefore, the answer is B: tailor shop as it is the only seller in the provided options that cuts fabric specific to the customer's measurements.
So the final answer is B: tailor shop

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s10-rTrue-m4096
Evaluating commonsenseqa :   2%|▌                                  | 1/63 [01:26<1:29:32, 86.66s/it]Evaluating commonsenseqa :   3%|█                                  | 2/63 [03:13<1:40:07, 98.49s/it]Evaluating commonsenseqa :   5%|█▌                                | 3/63 [05:01<1:42:40, 102.68s/it]Evaluating commonsenseqa :   6%|██▏                               | 4/63 [06:42<1:40:33, 102.26s/it]Evaluating commonsenseqa :   8%|██▋                               | 5/63 [08:29<1:40:25, 103.89s/it]Evaluating commonsenseqa :  10%|███▏                              | 6/63 [10:15<1:39:19, 104.55s/it]Evaluating commonsenseqa :  11%|███▉                               | 7/63 [11:14<1:23:38, 89.62s/it]Evaluating commonsenseqa :  13%|████▍                              | 8/63 [13:01<1:27:22, 95.32s/it]Evaluating commonsenseqa :  14%|█████                              | 9/63 [14:48<1:29:05, 99.00s/it]Evaluating commonsenseqa :  16%|█████▏                           | 10/63 [16:34<1:29:11, 100.97s/it]Evaluating commonsenseqa :  17%|█████▊                           | 11/63 [18:19<1:28:39, 102.30s/it]Evaluating commonsenseqa :  19%|██████▎                          | 12/63 [20:06<1:28:06, 103.65s/it]Evaluating commonsenseqa :  21%|██████▊                          | 13/63 [21:51<1:26:49, 104.18s/it]Evaluating commonsenseqa :  22%|███████▎                         | 14/63 [23:37<1:25:22, 104.55s/it]Evaluating commonsenseqa :  24%|███████▊                         | 15/63 [25:24<1:24:22, 105.47s/it]Evaluating commonsenseqa :  25%|████████▍                        | 16/63 [27:12<1:23:10, 106.19s/it]Evaluating commonsenseqa :  27%|████████▉                        | 17/63 [29:00<1:21:47, 106.69s/it]Evaluating commonsenseqa :  29%|█████████▍                       | 18/63 [30:45<1:19:33, 106.08s/it]Evaluating commonsenseqa :  30%|█████████▉                       | 19/63 [32:31<1:17:48, 106.09s/it]Evaluating commonsenseqa :  32%|██████████▍                      | 20/63 [34:16<1:15:48, 105.77s/it]Evaluating commonsenseqa :  33%|███████████                      | 21/63 [36:02<1:14:12, 106.01s/it]Evaluating commonsenseqa :  35%|███████████▌                     | 22/63 [37:46<1:12:01, 105.39s/it]Evaluating commonsenseqa :  37%|████████████                     | 23/63 [39:31<1:10:05, 105.14s/it]Evaluating commonsenseqa :  38%|████████████▌                    | 24/63 [41:18<1:08:45, 105.79s/it]Evaluating commonsenseqa :  40%|█████████████                    | 25/63 [43:04<1:07:03, 105.87s/it]Evaluating commonsenseqa :  41%|█████████████▌                   | 26/63 [44:50<1:05:16, 105.84s/it]Evaluating commonsenseqa :  43%|███████████████▍                    | 27/63 [46:13<59:27, 99.09s/it]Evaluating commonsenseqa :  44%|███████████████▌                   | 28/63 [47:59<58:59, 101.13s/it]Evaluating commonsenseqa :  46%|████████████████                   | 29/63 [49:47<58:26, 103.14s/it]Evaluating commonsenseqa :  48%|████████████████▋                  | 30/63 [51:32<56:59, 103.63s/it]Evaluating commonsenseqa :  49%|█████████████████▏                 | 31/63 [53:17<55:36, 104.26s/it]Evaluating commonsenseqa :  51%|█████████████████▊                 | 32/63 [55:04<54:10, 104.86s/it]Evaluating commonsenseqa :  52%|██████████████████▎                | 33/63 [56:49<52:30, 105.00s/it]Evaluating commonsenseqa :  54%|██████████████████▉                | 34/63 [58:34<50:44, 104.97s/it]Evaluating commonsenseqa :  56%|████████████████████                | 35/63 [59:38<43:17, 92.76s/it]Evaluating commonsenseqa :  57%|███████████████████▍              | 36/63 [1:01:23<43:21, 96.35s/it]Evaluating commonsenseqa :  59%|███████████████████▉              | 37/63 [1:03:09<42:58, 99.17s/it]Evaluating commonsenseqa :  60%|███████████████████▉             | 38/63 [1:04:54<42:05, 101.03s/it]Evaluating commonsenseqa :  62%|████████████████████▍            | 39/63 [1:06:38<40:46, 101.93s/it]Evaluating commonsenseqa :  63%|█████████████████████▌            | 40/63 [1:07:52<35:48, 93.42s/it]Evaluating commonsenseqa :  65%|██████████████████████▏           | 41/63 [1:09:37<35:37, 97.14s/it]Evaluating commonsenseqa :  67%|██████████████████████▋           | 42/63 [1:11:23<34:52, 99.62s/it]Evaluating commonsenseqa :  68%|██████████████████████▌          | 43/63 [1:13:10<33:54, 101.74s/it]Evaluating commonsenseqa :  70%|███████████████████████▋          | 44/63 [1:14:11<28:25, 89.78s/it]Evaluating commonsenseqa :  71%|████████████████████████▎         | 45/63 [1:15:58<28:24, 94.72s/it]Evaluating commonsenseqa :  73%|████████████████████████▊         | 46/63 [1:17:42<27:37, 97.53s/it]Evaluating commonsenseqa :  75%|█████████████████████████▎        | 47/63 [1:19:27<26:36, 99.77s/it]Evaluating commonsenseqa :  76%|█████████████████████████▏       | 48/63 [1:21:12<25:19, 101.29s/it]Evaluating commonsenseqa :  78%|█████████████████████████▋       | 49/63 [1:22:56<23:52, 102.30s/it]Evaluating commonsenseqa :  79%|██████████████████████████▏      | 50/63 [1:24:42<22:25, 103.48s/it]Evaluating commonsenseqa :  81%|███████████████████████████▌      | 51/63 [1:26:07<19:34, 97.85s/it]Evaluating commonsenseqa :  83%|███████████████████████████▏     | 52/63 [1:27:53<18:23, 100.29s/it]Evaluating commonsenseqa :  84%|███████████████████████████▊     | 53/63 [1:29:37<16:54, 101.48s/it]Evaluating commonsenseqa :  86%|████████████████████████████▎    | 54/63 [1:31:23<15:25, 102.85s/it]Evaluating commonsenseqa :  87%|████████████████████████████▊    | 55/63 [1:33:10<13:50, 103.84s/it]Evaluating commonsenseqa :  89%|█████████████████████████████▎   | 56/63 [1:34:54<12:07, 103.87s/it]Evaluating commonsenseqa :  90%|█████████████████████████████▊   | 57/63 [1:36:39<10:26, 104.38s/it]Evaluating commonsenseqa :  92%|██████████████████████████████▍  | 58/63 [1:38:25<08:43, 104.70s/it]Evaluating commonsenseqa :  94%|██████████████████████████████▉  | 59/63 [1:40:11<07:00, 105.22s/it]Evaluating commonsenseqa :  95%|███████████████████████████████▍ | 60/63 [1:41:57<05:16, 105.40s/it]Evaluating commonsenseqa :  97%|███████████████████████████████▉ | 61/63 [1:43:37<03:27, 103.91s/it]Evaluating commonsenseqa :  98%|█████████████████████████████████▍| 62/63 [1:44:34<01:29, 89.62s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:45:05<00:00, 72.29s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:45:05<00:00, 100.09s/it]
name: commonsenseqa | avg. gen lenth: 330.38 | time: 6306.246385097504s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-29 00:05:12,050] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 00:05:12,051] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 00:05:12,066] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 00:05:12,066] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 2
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                        | 0/9739 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████| 9739/9739 [00:00<00:00, 971053.27it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:06<00:06,  6.50s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:06<00:06,  6.99s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:06<00:06,  6.95s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.02s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.39s/it]
 > number of parameters: 6738415616
[2023-08-29 00:05:22,092] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.12s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.55s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.12s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.54s/it]
[2023-08-29 00:05:22,486] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 00:05:22,584] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:10<00:10, 10.13s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:12<00:00,  5.42s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:12<00:00,  6.12s/it]
[2023-08-29 00:05:25,731] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 00:05:26,416] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-29 00:05:26,419] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-29 00:05:26,420] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-29 00:05:26,420] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-29 00:05:26,420] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-29 00:05:26,420] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc42d237520>
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-29 00:05:26,421] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-29 00:05:26,422] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-29 00:05:26,423] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-29 00:05:26,423] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-29 00:05:26,423] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-29 00:05:26,423] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-29 00:05:26,423] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-29 00:05:26,423] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-29 00:05:26,423] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-29 00:05:26,423] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-29 00:05:26,423] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-29 00:05:26,423] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-29 00:05:26,423] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-29 00:05:26,423] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-29 00:05:26,423] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-29 00:05:26,423] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-29 00:05:26,423] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                             | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: 1. The question is asking where fabric is cut to order. This means looking for a place where fabric is used and customized based on specific measurements and requirements.
2. The first option, A: curtains, is not a place but a type of fabric product, so we can eliminate it.
3. The second option, B: tailor shop, is a place where fabric is indeed cut to order to make clothing. This stands as a plausible answer until we assess the other options.
4. The third option, C: clothing store, could initially seem like a potential answer, but it's important to remember that while clothing stores sell clothes, they don't usually cut fabric to order. They offer pre-made clothes, so we can eliminate this option.
5. The fourth option, D: sewing room, could potentially work as well. However, a sewing room is more of a general area where any form of sewing (including fabric cutting) might take place, rather than a commercial place where fabric is specifically cut to order.
6. The last option, E: hardware store, is not relevant since hardware stores sell tools and materials for household repairs, but not typically fabric or sewing related items.
7. After evaluating all the options, we see that B: tailor shop is the only one that fits the bill. Tailors indeed cut fabric to specific orders to tailor-make clothes. Thus, the correct answer is B: tailor shop.
So the final answer is B: tailor shop

Input: Where are you if your reading magazines while waiting for a vehicle on rails? Choices:  A: vegetables B: market C: doctor D: train station E: bookstore
Output: 1. The question asks about a place where one would wait for a vehicle on rails while reading magazines. 
2. The term'vehicle on rails' refers to a train. Trains travel on railways, hence the use of 'rails'. 
3. Hence, the location where one would most likely wait for a train is a train station. 
4. Even though one can read magazines in a market, doctor's office, or bookstore, these options do not involve waiting for a vehicle on rails. 
5. Vegetables is not a location and therefore, not a suitable option. 
6. Combining all the above information, the correct answer therefore is D: train station as it's the place where one waits for a train(returning to the question's hint'vehicle on rails') and can possibly read magazines while waiting.
So the final answer is D: train station

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s10-rTrue-m4096
Evaluating commonsenseqa :   2%|▌                                  | 1/63 [01:37<1:40:32, 97.30s/it]Evaluating commonsenseqa :   3%|█                                  | 2/63 [03:12<1:37:51, 96.26s/it]Evaluating commonsenseqa :   5%|█▋                                 | 3/63 [04:47<1:35:41, 95.69s/it]Evaluating commonsenseqa :   6%|██▏                                | 4/63 [06:24<1:34:25, 96.03s/it]Evaluating commonsenseqa :   8%|██▊                                | 5/63 [07:59<1:32:40, 95.86s/it]Evaluating commonsenseqa :  10%|███▎                               | 6/63 [09:35<1:31:01, 95.81s/it]Evaluating commonsenseqa :  11%|███▉                               | 7/63 [11:10<1:29:16, 95.65s/it]Evaluating commonsenseqa :  13%|████▍                              | 8/63 [12:45<1:27:26, 95.39s/it]Evaluating commonsenseqa :  14%|█████                              | 9/63 [14:22<1:26:07, 95.70s/it]Evaluating commonsenseqa :  16%|█████▍                            | 10/63 [15:57<1:24:30, 95.67s/it]Evaluating commonsenseqa :  17%|█████▉                            | 11/63 [17:32<1:22:46, 95.51s/it]Evaluating commonsenseqa :  19%|██████▍                           | 12/63 [19:08<1:21:15, 95.60s/it]Evaluating commonsenseqa :  21%|███████                           | 13/63 [20:38<1:18:11, 93.83s/it]Evaluating commonsenseqa :  22%|███████▌                          | 14/63 [22:14<1:17:16, 94.62s/it]Evaluating commonsenseqa :  24%|████████                          | 15/63 [23:50<1:15:57, 94.95s/it]Evaluating commonsenseqa :  25%|████████▋                         | 16/63 [25:27<1:14:48, 95.50s/it]Evaluating commonsenseqa :  27%|█████████▏                        | 17/63 [27:02<1:13:05, 95.35s/it]Evaluating commonsenseqa :  29%|█████████▋                        | 18/63 [28:37<1:11:32, 95.38s/it]Evaluating commonsenseqa :  30%|██████████▎                       | 19/63 [30:13<1:10:01, 95.49s/it]Evaluating commonsenseqa :  32%|██████████▊                       | 20/63 [31:48<1:08:18, 95.32s/it]Evaluating commonsenseqa :  33%|███████████▎                      | 21/63 [33:24<1:06:50, 95.49s/it]Evaluating commonsenseqa :  35%|███████████▊                      | 22/63 [35:00<1:05:22, 95.66s/it]Evaluating commonsenseqa :  37%|████████████▍                     | 23/63 [36:35<1:03:40, 95.51s/it]Evaluating commonsenseqa :  38%|████████████▉                     | 24/63 [38:10<1:01:58, 95.34s/it]Evaluating commonsenseqa :  40%|█████████████▍                    | 25/63 [39:45<1:00:20, 95.27s/it]Evaluating commonsenseqa :  41%|██████████████▊                     | 26/63 [41:21<58:55, 95.57s/it]Evaluating commonsenseqa :  43%|███████████████▍                    | 27/63 [42:58<57:27, 95.76s/it]Evaluating commonsenseqa :  44%|████████████████                    | 28/63 [44:34<55:53, 95.83s/it]Evaluating commonsenseqa :  46%|████████████████▌                   | 29/63 [46:08<54:03, 95.41s/it]Evaluating commonsenseqa :  48%|█████████████████▏                  | 30/63 [47:42<52:17, 95.07s/it]Evaluating commonsenseqa :  49%|█████████████████▋                  | 31/63 [49:19<50:55, 95.48s/it]Evaluating commonsenseqa :  51%|██████████████████▎                 | 32/63 [50:53<49:09, 95.15s/it]Evaluating commonsenseqa :  52%|██████████████████▊                 | 33/63 [52:28<47:31, 95.06s/it]Evaluating commonsenseqa :  54%|███████████████████▍                | 34/63 [54:04<46:08, 95.48s/it]Evaluating commonsenseqa :  56%|████████████████████                | 35/63 [55:40<44:37, 95.62s/it]Evaluating commonsenseqa :  57%|████████████████████▌               | 36/63 [57:15<42:56, 95.43s/it]Evaluating commonsenseqa :  59%|█████████████████████▏              | 37/63 [58:52<41:27, 95.67s/it]Evaluating commonsenseqa :  60%|████████████████████▌             | 38/63 [1:00:30<40:10, 96.42s/it]Evaluating commonsenseqa :  62%|█████████████████████             | 39/63 [1:02:06<38:29, 96.25s/it]Evaluating commonsenseqa :  63%|█████████████████████▌            | 40/63 [1:03:41<36:48, 96.02s/it]Evaluating commonsenseqa :  65%|██████████████████████▏           | 41/63 [1:05:17<35:11, 96.00s/it]Evaluating commonsenseqa :  67%|██████████████████████▋           | 42/63 [1:06:53<33:35, 95.98s/it]Evaluating commonsenseqa :  68%|███████████████████████▏          | 43/63 [1:08:26<31:43, 95.16s/it]Evaluating commonsenseqa :  70%|███████████████████████▋          | 44/63 [1:10:01<30:04, 94.98s/it]Evaluating commonsenseqa :  71%|████████████████████████▎         | 45/63 [1:11:36<28:28, 94.91s/it]Evaluating commonsenseqa :  73%|████████████████████████▊         | 46/63 [1:13:11<26:55, 95.01s/it]Evaluating commonsenseqa :  75%|█████████████████████████▎        | 47/63 [1:14:47<25:23, 95.24s/it]Evaluating commonsenseqa :  76%|█████████████████████████▉        | 48/63 [1:16:22<23:50, 95.35s/it]Evaluating commonsenseqa :  78%|██████████████████████████▍       | 49/63 [1:17:58<22:17, 95.50s/it]Evaluating commonsenseqa :  79%|██████████████████████████▉       | 50/63 [1:19:34<20:44, 95.73s/it]Evaluating commonsenseqa :  81%|███████████████████████████▌      | 51/63 [1:20:57<18:22, 91.92s/it]Evaluating commonsenseqa :  83%|████████████████████████████      | 52/63 [1:22:33<17:02, 92.92s/it]Evaluating commonsenseqa :  84%|████████████████████████████▌     | 53/63 [1:24:08<15:36, 93.62s/it]Evaluating commonsenseqa :  86%|█████████████████████████████▏    | 54/63 [1:25:45<14:11, 94.65s/it]Evaluating commonsenseqa :  87%|█████████████████████████████▋    | 55/63 [1:27:20<12:37, 94.64s/it]Evaluating commonsenseqa :  89%|██████████████████████████████▏   | 56/63 [1:28:55<11:04, 95.00s/it]Evaluating commonsenseqa :  90%|██████████████████████████████▊   | 57/63 [1:30:31<09:31, 95.28s/it]Evaluating commonsenseqa :  92%|███████████████████████████████▎  | 58/63 [1:31:53<07:36, 91.25s/it]Evaluating commonsenseqa :  94%|███████████████████████████████▊  | 59/63 [1:33:29<06:10, 92.53s/it]Evaluating commonsenseqa :  95%|████████████████████████████████▍ | 60/63 [1:35:05<04:40, 93.54s/it]Evaluating commonsenseqa :  97%|████████████████████████████████▉ | 61/63 [1:36:39<03:07, 93.74s/it]Evaluating commonsenseqa :  98%|█████████████████████████████████▍| 62/63 [1:38:13<01:33, 93.94s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:39:06<00:00, 81.74s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:39:06<00:00, 94.40s/it]
name: commonsenseqa | avg. gen lenth: 382.372 | time: 5947.353152751923s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-29 01:44:40,023] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-29 01:44:40,503] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 01:44:40,531] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 01:44:40,546] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 3
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i3-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                        | 0/9738 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████| 9738/9738 [00:00<00:00, 547949.19it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.26s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.37s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.40s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.40s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.31s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.77s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.37s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.81s/it]
 > number of parameters: 6738415616
[2023-08-29 01:44:51,355] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.41s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.86s/it]
[2023-08-29 01:44:51,445] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.44s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.88s/it]
[2023-08-29 01:44:51,563] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 01:44:51,644] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 01:44:52,243] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-29 01:44:52,244] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-29 01:44:52,245] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-29 01:44:52,245] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-29 01:44:52,245] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-29 01:44:52,245] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-29 01:44:52,245] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-29 01:44:52,245] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-29 01:44:52,245] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-29 01:44:52,245] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-29 01:44:52,245] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-29 01:44:52,245] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fcc55d5b520>
[2023-08-29 01:44:52,245] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-29 01:44:52,245] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-29 01:44:52,245] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-29 01:44:52,245] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-29 01:44:52,246] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-29 01:44:52,247] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-29 01:44:52,247] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-29 01:44:52,247] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-29 01:44:52,247] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-29 01:44:52,247] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-29 01:44:52,247] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-29 01:44:52,247] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-29 01:44:52,247] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                             | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: 1. The question asks where fabric is cut to order, meaning where fabric is cut according to specific measurements given by the buyer. 
2. Option A, curtains, isn't a seller but rather a type of item that could be made from cut fabric.
3. Option C, clothing store, mainly sells ready-to-wear outfits and does not typically offer fabric cutting services.
4. Option D, a sewing room, often refers to a space within a home where sewing projects are done, not a commercial seller.
5. Option E, a hardware store, primarily sells tools and equipment for home improvement and doesn't deal with fabric. 
6. Therefore, the correct answer is option B, a tailor shop. A tailor shop performs alterations and custom clothing designs, which involves cutting fabric to a customer's specific measurements.
So the final answer is B: tailor shop

Input: Where are you if your reading magazines while waiting for a vehicle on rails? Choices:  A: vegetables B: market C: doctor D: train station E: bookstore
Output: 1. Evaluating each option is the best approach to find the correct answer.
2. The activity in the question is reading magazines while waiting for a vehicle on rails, which hints that the location should be somewhere trains are boarding.
3. Option A, vegetables, is not a location. Therefore, this is incorrect.
4. Option B, market, does not correspond to waiting for a vehicle on rails, so this is also ruled out.
5. Option C, doctor, isn't a plausible solution because doctors offices don't typically have vehicles on rails, making this option irrelevant.
6. Option D, train station, is a place where people wait for vehicles on rails which are trains, and it's possible to read magazines while waiting. This option matches with the given situation, so this might be the correct answer.
7. Option E, bookstore, may offer magazines to read, but it's not a place where you wait for a vehicle on rails. Therefore, this option can be ruled out.
8. Thus, the correct answer is D: train station.
So the final answer is D: train station

Input: What would need oil to be used? Choices:  A: ground B: human  body C: repair shop D: combustion engines E: service station
Output: Step 1: Consider each option individually and whether oil is needed for its operation or maintenance.
Step 2: The first option is "ground". While oil can sometimes be used in the soil for certain purposes, it's hardly a necessity.
Step 3: The second option is "human body", which does need certain types of oils (like fats) for nutrition, but it doesn't use the type of oil meant in the question.
Step 4: The third option is "repair shop", this place will indeed need oil to carry out certain services, but the shop itself doesn't need oil to function. 
Step 5: Analysis of the fourth option "combustion engine" shows it indeed needs oil to function. The oil in a combustion engine is used to cool, clean, seal and lubricate the engine's parts.
Step 6: The last option is "service station". Similar to "repair shop", a service station will need oil to carry out some services, but doesn't need oil itself to function.
Step 7: After comparing all options, it's clear that "combustion engine" is the option to which oil is most vital. Thus, Option D is the correct answer.
So the final answer is D: combustion engines

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i3-s10-rTrue-m4096
Evaluating commonsenseqa :   2%|▌                                  | 1/63 [01:31<1:34:44, 91.69s/it]Evaluating commonsenseqa :   3%|█                                  | 2/63 [02:55<1:28:15, 86.80s/it]Evaluating commonsenseqa :   5%|█▋                                 | 3/63 [04:23<1:27:37, 87.62s/it]Evaluating commonsenseqa :   6%|██▏                                | 4/63 [05:51<1:26:22, 87.84s/it]Evaluating commonsenseqa :   8%|██▊                                | 5/63 [07:20<1:25:19, 88.27s/it]Evaluating commonsenseqa :  10%|███▎                               | 6/63 [08:48<1:23:36, 88.01s/it]Evaluating commonsenseqa :  11%|███▉                               | 7/63 [10:17<1:22:31, 88.43s/it]Evaluating commonsenseqa :  13%|████▍                              | 8/63 [11:45<1:20:50, 88.20s/it]Evaluating commonsenseqa :  14%|█████                              | 9/63 [13:13<1:19:27, 88.29s/it]Evaluating commonsenseqa :  16%|█████▍                            | 10/63 [14:42<1:18:02, 88.34s/it]Evaluating commonsenseqa :  17%|█████▉                            | 11/63 [16:11<1:16:46, 88.60s/it]Evaluating commonsenseqa :  19%|██████▍                           | 12/63 [17:40<1:15:20, 88.65s/it]Evaluating commonsenseqa :  21%|███████                           | 13/63 [19:08<1:13:51, 88.64s/it]Evaluating commonsenseqa :  22%|███████▌                          | 14/63 [20:39<1:12:49, 89.18s/it]Evaluating commonsenseqa :  24%|████████                          | 15/63 [22:09<1:11:33, 89.45s/it]Evaluating commonsenseqa :  25%|████████▋                         | 16/63 [23:38<1:10:00, 89.38s/it]Evaluating commonsenseqa :  27%|█████████▏                        | 17/63 [25:08<1:08:32, 89.41s/it]Evaluating commonsenseqa :  29%|█████████▋                        | 18/63 [26:37<1:06:59, 89.33s/it]Evaluating commonsenseqa :  30%|██████████▎                       | 19/63 [28:08<1:06:01, 90.02s/it]Evaluating commonsenseqa :  32%|██████████▊                       | 20/63 [29:36<1:04:06, 89.45s/it]Evaluating commonsenseqa :  33%|███████████▎                      | 21/63 [31:07<1:02:51, 89.81s/it]Evaluating commonsenseqa :  35%|███████████▊                      | 22/63 [32:37<1:01:21, 89.79s/it]Evaluating commonsenseqa :  37%|█████████████▏                      | 23/63 [34:05<59:37, 89.43s/it]Evaluating commonsenseqa :  38%|█████████████▋                      | 24/63 [35:36<58:18, 89.72s/it]Evaluating commonsenseqa :  40%|██████████████▎                     | 25/63 [37:07<57:03, 90.10s/it]Evaluating commonsenseqa :  41%|██████████████▊                     | 26/63 [38:35<55:14, 89.59s/it]Evaluating commonsenseqa :  43%|███████████████▍                    | 27/63 [40:05<53:45, 89.60s/it]Evaluating commonsenseqa :  44%|████████████████                    | 28/63 [41:35<52:23, 89.81s/it]Evaluating commonsenseqa :  46%|████████████████▌                   | 29/63 [43:04<50:42, 89.48s/it]Evaluating commonsenseqa :  48%|█████████████████▏                  | 30/63 [44:33<49:11, 89.45s/it]Evaluating commonsenseqa :  49%|█████████████████▋                  | 31/63 [46:02<47:39, 89.35s/it]Evaluating commonsenseqa :  51%|██████████████████▎                 | 32/63 [47:32<46:08, 89.31s/it]Evaluating commonsenseqa :  52%|██████████████████▊                 | 33/63 [49:00<44:33, 89.12s/it]Evaluating commonsenseqa :  54%|███████████████████▍                | 34/63 [50:29<43:01, 89.02s/it]Evaluating commonsenseqa :  56%|████████████████████                | 35/63 [51:58<41:35, 89.13s/it]Evaluating commonsenseqa :  57%|████████████████████▌               | 36/63 [53:28<40:10, 89.29s/it]Evaluating commonsenseqa :  59%|█████████████████████▏              | 37/63 [54:59<38:56, 89.86s/it]Evaluating commonsenseqa :  60%|█████████████████████▋              | 38/63 [56:29<37:23, 89.72s/it]Evaluating commonsenseqa :  62%|██████████████████████▎             | 39/63 [57:33<32:51, 82.16s/it]Evaluating commonsenseqa :  63%|██████████████████████▊             | 40/63 [59:03<32:21, 84.40s/it]Evaluating commonsenseqa :  65%|██████████████████████▏           | 41/63 [1:00:33<31:34, 86.09s/it]Evaluating commonsenseqa :  67%|██████████████████████▋           | 42/63 [1:02:02<30:28, 87.05s/it]Evaluating commonsenseqa :  68%|███████████████████████▏          | 43/63 [1:03:32<29:14, 87.74s/it]Evaluating commonsenseqa :  70%|███████████████████████▋          | 44/63 [1:05:02<28:00, 88.47s/it]Evaluating commonsenseqa :  71%|████████████████████████▎         | 45/63 [1:06:31<26:35, 88.63s/it]Evaluating commonsenseqa :  73%|████████████████████████▊         | 46/63 [1:07:24<22:07, 78.10s/it]Evaluating commonsenseqa :  75%|█████████████████████████▎        | 47/63 [1:08:54<21:46, 81.64s/it]Evaluating commonsenseqa :  76%|█████████████████████████▉        | 48/63 [1:10:24<21:02, 84.19s/it]Evaluating commonsenseqa :  78%|██████████████████████████▍       | 49/63 [1:11:55<20:07, 86.23s/it]Evaluating commonsenseqa :  79%|██████████████████████████▉       | 50/63 [1:13:24<18:51, 87.06s/it]Evaluating commonsenseqa :  81%|███████████████████████████▌      | 51/63 [1:14:53<17:32, 87.70s/it]Evaluating commonsenseqa :  83%|████████████████████████████      | 52/63 [1:16:24<16:13, 88.49s/it]Evaluating commonsenseqa :  84%|████████████████████████████▌     | 53/63 [1:17:54<14:49, 88.95s/it]Evaluating commonsenseqa :  86%|█████████████████████████████▏    | 54/63 [1:19:26<13:29, 89.90s/it]Evaluating commonsenseqa :  87%|█████████████████████████████▋    | 55/63 [1:20:57<12:02, 90.33s/it]Evaluating commonsenseqa :  89%|██████████████████████████████▏   | 56/63 [1:22:26<10:29, 89.97s/it]Evaluating commonsenseqa :  90%|██████████████████████████████▊   | 57/63 [1:23:56<08:58, 89.74s/it]Evaluating commonsenseqa :  92%|███████████████████████████████▎  | 58/63 [1:25:25<07:27, 89.53s/it]Evaluating commonsenseqa :  94%|███████████████████████████████▊  | 59/63 [1:26:53<05:56, 89.10s/it]Evaluating commonsenseqa :  95%|████████████████████████████████▍ | 60/63 [1:28:21<04:26, 88.85s/it]Evaluating commonsenseqa :  97%|████████████████████████████████▉ | 61/63 [1:29:50<02:58, 89.03s/it]Evaluating commonsenseqa :  98%|█████████████████████████████████▍| 62/63 [1:31:20<01:29, 89.13s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:32:10<00:00, 77.31s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:32:10<00:00, 87.78s/it]
name: commonsenseqa | avg. gen lenth: 374.12 | time: 5530.4454433918s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-29 03:18:33,679] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 03:18:33,696] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 03:18:33,699] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 03:18:33,713] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 4
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i4-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                        | 0/9737 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████| 9737/9737 [00:00<00:00, 417164.00it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:06<00:06,  6.88s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.16s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.18s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.25s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.04s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.47s/it]
[2023-08-29 03:18:44,167] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.30s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.73s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.32s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.75s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.36s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.79s/it]
 > number of parameters: 6738415616
[2023-08-29 03:18:44,711] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 03:18:44,726] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 03:18:44,851] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 03:18:45,506] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-29 03:18:45,508] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f325517f640>
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-29 03:18:45,508] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-29 03:18:45,509] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-29 03:18:45,510] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-29 03:18:45,510] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                             | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: 1. We first need to understand the question, which is asking us where fabric is cut according to a specific order.
2. Then, we examine each of the answer choices and think about which type of seller has the capability to cut fabric to order.
3. Choice A is curtains and while fabric is often used to make curtains, a curtain store would typically sell pre-made curtains, rather than cutting fabric.
4. Choice C is a clothing store. While clothing stores do sell products made from fabric, they usually are not involved in the cutting and tailoring process of the fabric itself.
5. Choice D is a sewing room. This is a broad term and could refer to many different scenarios. While fabric might be cut in a sewing room, it is not a "seller".
6. Choice E is a hardware store, which is commonly associated with tools and equipment for home and construction use, so it does not seem to match the requirement of the question.
7. Therefore, the most logical answer is choice B: tailor shop. Tailor shops are where clothes are tailored to a customer's specific measurement, which requires cutting fabric according to orders.
So the final answer is B: tailor shop

Input: Where are you if your reading magazines while waiting for a vehicle on rails? Choices:  A: vegetables B: market C: doctor D: train station E: bookstore
Output: 1. The question asked where you are if you're waiting for a vehicle on rails while reading magazines. 
2. First, we look at what types of vehicles use rails. The most common type of rail vehicle is a train. 
3. The places where you generally wait for a train are train stations. 
4. While reading magazines can technically be done anywhere, the phrase is included in the same sentence as waiting for a rail vehicle, implying that these two activities are occurring in the same location. 
5. Therefore, given the options available, the answer that makes most sense is D: train station, as that is the location where you would typically wait for a vehicle like a train and it is plausible to read magazines while waiting.
So the final answer is D: train station

Input: What would need oil to be used? Choices:  A: ground B: human  body C: repair shop D: combustion engines E: service station
Output: 1. The question asks for a situation or a thing that requires oil for operational use.
2. We begin by analyzing each of the provided options. 
3. The first option is "Ground". Ground or soil generally does not require the use of oil. There are some instances where oil is applied to soil, such as in the case of oil spill cleanups, but in general context, ground does not require oil to function.
4. The second option is "Human body". While our bodies do need fats and oils as part of a balanced diet for our general health, we do not use or consume 'oil' as it is made for machineries in general setting. Therefore, this option doesn't fit well.
5. The third option is "Repair shop". Repair shops may use oil for various purposes, particularly to lubricate machinery parts. However, the oil is not used 'by' the repair shop itself, instead it's used for the objects being repaired within the shop.
6. The fourth option is "Combustion engines". Combustion engines, which are prevalent in many cars and other vehicles, require oil as a lubricant to ensure the parts inside the engine operate smoothly and without excess heat. This fits the requirement perfectly.
7. The fifth option is "Service station". Similar to "Repair shop", Service stations themselves do not'require' oil but they do keep or sell oil for the vehicles which need it.
8. Thus, by analyzing each of the answer choices, we find that 'Combustion engines' is the best answer because it requires oil for its operation.
So the final answer is D: combustion engines

Input: What is person probably feeling that plans on stopping being married to their spouse? Choices:  A: detachment B: bankruptcy C: sad D: fights E: wrong
Output: 1. The question pertains to what a person is probably feeling if they are planning on stopping being married to their spouse. This means we need to find a choice that relates to emotions or feelings associated with ending a marriage.
2. Let’s evaluate the given choices: A: detachment B: bankruptcy C: sad D: fights E: wrong
3. Choice B: Bankruptcy, is more related to financial problems, not to feelings or emotions.
4. Choice C: Sad, could be a possible choice as it's an emotion but it might not necessarily be what someone plans on stopping being married is primarily feeling. 
5. Choice D: Fights, is a situation or event rather than a feeling so it doesn't quite fit.
6. Choice E: Wrong, though a feeling, it does not uniquely or specifically relate to the situation of planning to end a marriage.
7. Choice A: Detachment, it refers to a state of being disengaged or separated. In the context of marriage, it is often felt when someone plans on ending the relationship. It represents the emotional state of no longer feeling connected or committed to their spouse.
8. Hence, choice A: Detachment, is the most suitable answer to this situation.
So the final answer is A: detachment

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i4-s10-rTrue-m4096
Evaluating commonsenseqa :   2%|▌                                  | 1/63 [01:20<1:23:19, 80.64s/it]Evaluating commonsenseqa :   3%|█                                  | 2/63 [02:41<1:21:51, 80.52s/it]Evaluating commonsenseqa :   5%|█▋                                 | 3/63 [03:59<1:19:45, 79.77s/it]Evaluating commonsenseqa :   6%|██▏                                | 4/63 [05:20<1:18:33, 79.89s/it]Evaluating commonsenseqa :   8%|██▊                                | 5/63 [06:38<1:16:37, 79.27s/it]Evaluating commonsenseqa :  10%|███▎                               | 6/63 [07:57<1:15:11, 79.15s/it]Evaluating commonsenseqa :  11%|███▉                               | 7/63 [09:16<1:13:53, 79.18s/it]Evaluating commonsenseqa :  13%|████▍                              | 8/63 [10:36<1:12:43, 79.33s/it]Evaluating commonsenseqa :  14%|█████                              | 9/63 [11:54<1:11:05, 78.99s/it]Evaluating commonsenseqa :  16%|█████▍                            | 10/63 [13:13<1:09:46, 79.00s/it]Evaluating commonsenseqa :  17%|█████▉                            | 11/63 [14:32<1:08:32, 79.08s/it]Evaluating commonsenseqa :  19%|██████▍                           | 12/63 [15:51<1:07:13, 79.10s/it]Evaluating commonsenseqa :  21%|███████                           | 13/63 [17:10<1:05:46, 78.94s/it]Evaluating commonsenseqa :  22%|███████▌                          | 14/63 [18:28<1:04:19, 78.78s/it]Evaluating commonsenseqa :  24%|████████                          | 15/63 [19:48<1:03:18, 79.14s/it]Evaluating commonsenseqa :  25%|████████▋                         | 16/63 [21:07<1:01:52, 78.99s/it]Evaluating commonsenseqa :  27%|█████████▏                        | 17/63 [22:26<1:00:37, 79.07s/it]Evaluating commonsenseqa :  29%|██████████▎                         | 18/63 [23:44<59:09, 78.89s/it]Evaluating commonsenseqa :  30%|██████████▊                         | 19/63 [25:03<57:46, 78.78s/it]Evaluating commonsenseqa :  32%|███████████▍                        | 20/63 [26:22<56:23, 78.70s/it]Evaluating commonsenseqa :  33%|████████████                        | 21/63 [27:40<55:05, 78.70s/it]Evaluating commonsenseqa :  35%|████████████▌                       | 22/63 [29:00<53:59, 79.00s/it]Evaluating commonsenseqa :  37%|█████████████▏                      | 23/63 [30:19<52:35, 78.88s/it]Evaluating commonsenseqa :  38%|█████████████▋                      | 24/63 [31:37<51:11, 78.75s/it]Evaluating commonsenseqa :  40%|██████████████▎                     | 25/63 [32:56<49:55, 78.82s/it]Evaluating commonsenseqa :  41%|██████████████▊                     | 26/63 [34:14<48:32, 78.71s/it]Evaluating commonsenseqa :  43%|███████████████▍                    | 27/63 [35:33<47:12, 78.69s/it]Evaluating commonsenseqa :  44%|████████████████                    | 28/63 [36:51<45:50, 78.58s/it]Evaluating commonsenseqa :  46%|████████████████▌                   | 29/63 [38:10<44:30, 78.53s/it]Evaluating commonsenseqa :  48%|█████████████████▏                  | 30/63 [39:29<43:16, 78.67s/it]Evaluating commonsenseqa :  49%|█████████████████▋                  | 31/63 [40:48<42:00, 78.77s/it]Evaluating commonsenseqa :  51%|██████████████████▎                 | 32/63 [41:50<38:04, 73.68s/it]Evaluating commonsenseqa :  52%|██████████████████▊                 | 33/63 [43:08<37:37, 75.24s/it]Evaluating commonsenseqa :  54%|███████████████████▍                | 34/63 [44:28<36:56, 76.45s/it]Evaluating commonsenseqa :  56%|████████████████████                | 35/63 [45:47<36:00, 77.17s/it]Evaluating commonsenseqa :  57%|████████████████████▌               | 36/63 [47:06<35:03, 77.90s/it]Evaluating commonsenseqa :  59%|█████████████████████▏              | 37/63 [47:59<30:30, 70.41s/it]Evaluating commonsenseqa :  60%|█████████████████████▋              | 38/63 [49:18<30:24, 72.99s/it]Evaluating commonsenseqa :  62%|██████████████████████▎             | 39/63 [50:36<29:49, 74.55s/it]Evaluating commonsenseqa :  63%|██████████████████████▊             | 40/63 [51:55<29:04, 75.85s/it]Evaluating commonsenseqa :  65%|███████████████████████▍            | 41/63 [53:14<28:10, 76.83s/it]Evaluating commonsenseqa :  67%|████████████████████████            | 42/63 [54:33<27:05, 77.39s/it]Evaluating commonsenseqa :  68%|████████████████████████▌           | 43/63 [55:52<25:55, 77.76s/it]Evaluating commonsenseqa :  70%|█████████████████████████▏          | 44/63 [57:11<24:44, 78.15s/it]Evaluating commonsenseqa :  71%|█████████████████████████▋          | 45/63 [58:30<23:31, 78.39s/it]Evaluating commonsenseqa :  73%|██████████████████████████▎         | 46/63 [59:48<22:12, 78.37s/it]Evaluating commonsenseqa :  75%|█████████████████████████▎        | 47/63 [1:01:07<20:57, 78.59s/it]Evaluating commonsenseqa :  76%|█████████████████████████▉        | 48/63 [1:02:26<19:39, 78.60s/it]Evaluating commonsenseqa :  78%|██████████████████████████▍       | 49/63 [1:03:45<18:21, 78.71s/it]Evaluating commonsenseqa :  79%|██████████████████████████▉       | 50/63 [1:05:03<17:02, 78.67s/it]Evaluating commonsenseqa :  81%|███████████████████████████▌      | 51/63 [1:06:22<15:43, 78.65s/it]Evaluating commonsenseqa :  83%|████████████████████████████      | 52/63 [1:07:40<14:25, 78.64s/it]Evaluating commonsenseqa :  84%|████████████████████████████▌     | 53/63 [1:08:59<13:06, 78.69s/it]Evaluating commonsenseqa :  86%|█████████████████████████████▏    | 54/63 [1:10:13<11:34, 77.13s/it]Evaluating commonsenseqa :  87%|█████████████████████████████▋    | 55/63 [1:11:33<10:23, 77.97s/it]Evaluating commonsenseqa :  89%|██████████████████████████████▏   | 56/63 [1:12:51<09:07, 78.22s/it]Evaluating commonsenseqa :  90%|██████████████████████████████▊   | 57/63 [1:14:11<07:51, 78.54s/it]Evaluating commonsenseqa :  92%|███████████████████████████████▎  | 58/63 [1:15:30<06:33, 78.67s/it]Evaluating commonsenseqa :  94%|███████████████████████████████▊  | 59/63 [1:16:48<05:14, 78.65s/it]Evaluating commonsenseqa :  95%|████████████████████████████████▍ | 60/63 [1:18:08<03:56, 78.91s/it]Evaluating commonsenseqa :  97%|████████████████████████████████▉ | 61/63 [1:19:26<02:37, 78.78s/it]Evaluating commonsenseqa :  98%|█████████████████████████████████▍| 62/63 [1:20:45<01:18, 78.79s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:21:30<00:00, 68.67s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:21:30<00:00, 77.63s/it]
name: commonsenseqa | avg. gen lenth: 430.424 | time: 4891.2073175907135s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-29 04:41:16,077] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 04:41:16,077] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 04:41:16,123] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 04:41:16,131] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 5
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i5-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                        | 0/9736 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████| 9736/9736 [00:00<00:00, 579205.76it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:06<00:06,  6.93s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.04s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.08s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.15s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.12s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.54s/it]
 > number of parameters: 6738415616
[2023-08-29 04:41:26,871] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.08s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.52s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.11s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.55s/it]
[2023-08-29 04:41:27,035] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.19s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.63s/it]
[2023-08-29 04:41:27,372] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 04:41:28,495] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 04:41:29,023] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-29 04:41:29,024] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f856e47f640>
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-29 04:41:29,025] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-29 04:41:29,026] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-29 04:41:29,026] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                             | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: Step 1: Interpret the question. The question is asking where fabric is specifically cut to order. This would typically be a place that uses fabric for custom-made items. 
Step 2: Distinguish the choices. Evaluate each answer choice and what it typically is associated with.
A: Curtains - this choice is an item, not a seller, so it can be ruled out.
B: Tailor shop - This is a place where clothing is custom-made, therefore fabric would likely be cut to order here.
C: Clothing store - While fabric can be found here, it often already made into finished clothes not cut to order.
D: Sewing room - This is more of a location inside a home, not typically a seller.
E: Hardware store - These stores typically do not deal with fabric, so it can be ruled out.
Step 3: Conclude. The tailor shop is the type of seller where fabric is custom cut to order.
So the final answer is B: tailor shop

Input: Where are you if your reading magazines while waiting for a vehicle on rails? Choices:  A: vegetables B: market C: doctor D: train station E: bookstore
Output: 1. The question asks for a location where you could be waiting for a vehicle that moves on rails. 
2. This implies that the location must be some form of transportation hub or station, as these are the typical places where people wait for vehicles.
3. Out of all the choices given, options A, B, C and E, refer to places where people don't usually wait for vehicles that move on rails. 
4. A Train Station, choice D, is a place where people often wait for a vehicle which runs on rails - this vehicle being a train. 
5. Additionally, it is common for people to pass the time by reading magazines while waiting at a train station, which matches the content of the question.
6. Therefore, the correct answer is D: Train Station.
So the final answer is D: train station

Input: What would need oil to be used? Choices:  A: ground B: human  body C: repair shop D: combustion engines E: service station
Output: Step 1: Analyze each of the provided choices in relation to the need for oil.
Step 2: Choice A, the ground doesn't need oil to function or operate. Oil is usually extracted from the ground, not added to it.
Step 3: Choice B, human body doesn't typically require oil for any physiological processes. Although some oil-based products can be used for skincare or cooking, the human body itself does not require oil to function.
Step 4: Choices C and E, repair shop and service station, might use oil in the course of their operations, such as for oil changes, but they themselves don't require oil to exist or operate fundamentally.
Step 5: Choice D, combustion engines, require oil for proper lubrication and functioning. In the context of this question, 'what would need oil to be used', combustion engines is the most appropriate answer.
Step 6: Thus, the answer is D, combustion engines.
So the final answer is D: combustion engines

Input: What is person probably feeling that plans on stopping being married to their spouse? Choices:  A: detachment B: bankruptcy C: sad D: fights E: wrong
Output: 1. In answering this question, we need to consider the emotional state of someone who intends to end a marital relationship.
2. Choosing the option B: bankruptcy is not relevant because it pertains to financial situation, not the feelings of a person who plans on stopping their marriage.
3. Option C: Sad, could be a possible feeling, but it's not necessarily the predominant emotion someone intending to end a marriage might feel.
4. Option D: fights, describes a situation or occurrence rather than a person's emotional state.
5. Option E: wrong, also does not describe a person's emotional state correctly in the context of ending a marriage.
6. Hence, by process of elimination, we are left with option A: detachment.
7. Detachment probably best describes the feeling of emotional disconnection that a person planning to end their marriage is likely experiencing. They may have become unattached or withdrawn from their spouse, leading to the decision to end the marriage.
8. Therefore, the answer is A: Detachment.
So the final answer is A: detachment

Input: What could you use to store a clock? Choices:  A: shelf B: own bedroom C: desk D: wall E: car
Output: 1. In order to answer this question, we need to first understand the context. The question is requesting to name a place where a clock can be stored.
2. We then look at each of the options given to decide if it's appropriate for storing a clock.
3. Option A is'shelf'. It is common and practical to store or place a clock on a shelf. As it provides an adequate surface and visibility, a clock can function properly and can easily be seen when placed on a shelf.
4. Option B is 'own bedroom'. This is a large space that could contain a clock, but it's not specific about where inside the bedroom the clock would be stored. The clock would still need a specific surface or location within the bedroom and hence isn't the best answer.
5. Option C is 'desk'. A desk could also serve as a surface for placing a clock. However, it isn't generally the primary location for a clock and a desk already has many other objects like books, a computer, etc.
6. Option D is 'wall'. Although a wall is a typical place for wall clocks, the question does not specify what kind of clock is being discussed, making this choice less optimal.
7. Option E is 'car'. It is unusual to store or place a clock in a car, as most cars already have built-in clocks.
8. After reviewing all the given options, we can conclude that while a few options could be feasible, option A:'shelf' could be the best place to store a clock.
9. Therefore, the answer is A: shelf.
So the final answer is A: shelf

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i5-s10-rTrue-m4096
Evaluating commonsenseqa :   2%|▌                                  | 1/63 [01:17<1:20:09, 77.57s/it]Evaluating commonsenseqa :   3%|█                                  | 2/63 [02:32<1:17:10, 75.91s/it]Evaluating commonsenseqa :   5%|█▋                                 | 3/63 [03:47<1:15:25, 75.43s/it]Evaluating commonsenseqa :   6%|██▏                                | 4/63 [05:02<1:13:57, 75.20s/it]Evaluating commonsenseqa :   8%|██▊                                | 5/63 [06:16<1:12:29, 74.98s/it]Evaluating commonsenseqa :  10%|███▎                               | 6/63 [07:32<1:11:35, 75.36s/it]Evaluating commonsenseqa :  11%|███▉                               | 7/63 [08:47<1:10:06, 75.12s/it]Evaluating commonsenseqa :  13%|████▍                              | 8/63 [10:03<1:09:02, 75.32s/it]Evaluating commonsenseqa :  14%|█████                              | 9/63 [11:18<1:07:45, 75.29s/it]Evaluating commonsenseqa :  16%|█████▍                            | 10/63 [12:33<1:06:31, 75.31s/it]Evaluating commonsenseqa :  17%|█████▉                            | 11/63 [13:48<1:05:12, 75.24s/it]Evaluating commonsenseqa :  19%|██████▍                           | 12/63 [15:03<1:03:46, 75.03s/it]Evaluating commonsenseqa :  21%|███████                           | 13/63 [16:19<1:02:45, 75.31s/it]Evaluating commonsenseqa :  22%|███████▌                          | 14/63 [17:35<1:01:36, 75.44s/it]Evaluating commonsenseqa :  24%|████████                          | 15/63 [18:50<1:00:26, 75.55s/it]Evaluating commonsenseqa :  25%|█████████▏                          | 16/63 [20:05<58:58, 75.29s/it]Evaluating commonsenseqa :  27%|█████████▋                          | 17/63 [21:20<57:39, 75.22s/it]Evaluating commonsenseqa :  29%|██████████▎                         | 18/63 [22:35<56:15, 75.00s/it]Evaluating commonsenseqa :  30%|██████████▊                         | 19/63 [23:50<55:04, 75.11s/it]Evaluating commonsenseqa :  32%|███████████▍                        | 20/63 [25:05<53:53, 75.19s/it]Evaluating commonsenseqa :  33%|████████████                        | 21/63 [26:21<52:45, 75.38s/it]Evaluating commonsenseqa :  35%|████████████▌                       | 22/63 [27:36<51:26, 75.29s/it]Evaluating commonsenseqa :  37%|█████████████▏                      | 23/63 [28:52<50:15, 75.39s/it]Evaluating commonsenseqa :  38%|█████████████▋                      | 24/63 [30:07<49:02, 75.44s/it]Evaluating commonsenseqa :  40%|██████████████▎                     | 25/63 [31:22<47:39, 75.25s/it]Evaluating commonsenseqa :  41%|██████████████▊                     | 26/63 [32:37<46:22, 75.21s/it]Evaluating commonsenseqa :  43%|███████████████▍                    | 27/63 [33:52<45:04, 75.13s/it]Evaluating commonsenseqa :  44%|████████████████                    | 28/63 [35:07<43:47, 75.06s/it]Evaluating commonsenseqa :  46%|████████████████▌                   | 29/63 [36:22<42:27, 74.93s/it]Evaluating commonsenseqa :  48%|█████████████████▏                  | 30/63 [37:36<41:05, 74.70s/it]Evaluating commonsenseqa :  49%|█████████████████▋                  | 31/63 [38:50<39:47, 74.60s/it]Evaluating commonsenseqa :  51%|██████████████████▎                 | 32/63 [40:06<38:42, 74.92s/it]Evaluating commonsenseqa :  52%|██████████████████▊                 | 33/63 [41:21<37:32, 75.08s/it]Evaluating commonsenseqa :  54%|███████████████████▍                | 34/63 [42:36<36:16, 75.07s/it]Evaluating commonsenseqa :  56%|████████████████████                | 35/63 [43:51<34:57, 74.91s/it]Evaluating commonsenseqa :  57%|████████████████████▌               | 36/63 [45:07<33:48, 75.13s/it]Evaluating commonsenseqa :  59%|█████████████████████▏              | 37/63 [46:21<32:29, 74.97s/it]Evaluating commonsenseqa :  60%|█████████████████████▋              | 38/63 [47:36<31:15, 75.03s/it]Evaluating commonsenseqa :  62%|██████████████████████▎             | 39/63 [48:52<30:02, 75.09s/it]Evaluating commonsenseqa :  63%|██████████████████████▊             | 40/63 [50:06<28:45, 75.02s/it]Evaluating commonsenseqa :  65%|███████████████████████▍            | 41/63 [51:22<27:31, 75.08s/it]Evaluating commonsenseqa :  67%|████████████████████████            | 42/63 [52:37<26:18, 75.19s/it]Evaluating commonsenseqa :  68%|████████████████████████▌           | 43/63 [53:53<25:07, 75.38s/it]Evaluating commonsenseqa :  70%|█████████████████████████▏          | 44/63 [55:08<23:49, 75.21s/it]Evaluating commonsenseqa :  71%|█████████████████████████▋          | 45/63 [56:23<22:32, 75.15s/it]Evaluating commonsenseqa :  73%|██████████████████████████▎         | 46/63 [57:38<21:17, 75.14s/it]Evaluating commonsenseqa :  75%|██████████████████████████▊         | 47/63 [58:53<20:00, 75.03s/it]Evaluating commonsenseqa :  76%|█████████████████████████▉        | 48/63 [1:00:08<18:47, 75.20s/it]Evaluating commonsenseqa :  78%|██████████████████████████▍       | 49/63 [1:01:25<17:39, 75.65s/it]Evaluating commonsenseqa :  79%|██████████████████████████▉       | 50/63 [1:02:41<16:26, 75.90s/it]Evaluating commonsenseqa :  81%|███████████████████████████▌      | 51/63 [1:03:56<15:06, 75.53s/it]Evaluating commonsenseqa :  83%|████████████████████████████      | 52/63 [1:05:11<13:47, 75.22s/it]Evaluating commonsenseqa :  84%|████████████████████████████▌     | 53/63 [1:06:25<12:30, 75.07s/it]Evaluating commonsenseqa :  86%|█████████████████████████████▏    | 54/63 [1:07:41<11:17, 75.24s/it]Evaluating commonsenseqa :  87%|█████████████████████████████▋    | 55/63 [1:08:56<10:02, 75.29s/it]Evaluating commonsenseqa :  89%|██████████████████████████████▏   | 56/63 [1:10:11<08:46, 75.23s/it]Evaluating commonsenseqa :  90%|██████████████████████████████▊   | 57/63 [1:11:28<07:33, 75.52s/it]Evaluating commonsenseqa :  92%|███████████████████████████████▎  | 58/63 [1:12:42<06:15, 75.19s/it]Evaluating commonsenseqa :  94%|███████████████████████████████▊  | 59/63 [1:13:58<05:01, 75.45s/it]Evaluating commonsenseqa :  95%|████████████████████████████████▍ | 60/63 [1:15:14<03:47, 75.70s/it]Evaluating commonsenseqa :  97%|████████████████████████████████▉ | 61/63 [1:16:29<02:30, 75.50s/it]Evaluating commonsenseqa :  98%|█████████████████████████████████▍| 62/63 [1:17:45<01:15, 75.45s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:18:28<00:00, 65.76s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:18:28<00:00, 74.74s/it]
name: commonsenseqa | avg. gen lenth: 427.972 | time: 4708.944651603699s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-29 06:00:03,322] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 06:00:03,347] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 06:00:03,347] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 06:00:03,357] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 6
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i6-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                        | 0/9735 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████| 9735/9735 [00:00<00:00, 366824.03it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:06<00:06,  6.70s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:06<00:06,  6.95s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.03s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.28s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.41s/it]
[2023-08-29 06:00:13,660] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.04s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.47s/it]
 > number of parameters: 6738415616
[2023-08-29 06:00:13,874] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.17s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.60s/it]
[2023-08-29 06:00:14,040] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.33s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.77s/it]
[2023-08-29 06:00:14,551] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 06:00:15,142] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-29 06:00:15,143] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-29 06:00:15,143] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-29 06:00:15,143] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-29 06:00:15,143] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-29 06:00:15,143] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-29 06:00:15,143] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-29 06:00:15,143] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-29 06:00:15,143] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-29 06:00:15,143] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-29 06:00:15,143] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-29 06:00:15,143] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fde01e57610>
[2023-08-29 06:00:15,143] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-29 06:00:15,143] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-29 06:00:15,143] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-29 06:00:15,143] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-29 06:00:15,144] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-29 06:00:15,145] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-29 06:00:15,145] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-29 06:00:15,145] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-29 06:00:15,145] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-29 06:00:15,145] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                             | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: 1. The question asks for a type of seller where fabric is cut to order.
2. To answer this question, we need to understand what it means for fabric to be "cut to order". This term implies a certain degree of customization, likely related to a customer's specific needs, and not something manufactured or already made.
3. Now, let's examine each choice closely:
   - A: curtains - This normally does not require customization, plus curtains are usually a finished product, not a place where fabric is cut.
   - B: tailor shop - This is a place where clothes are custom made, and fabric would be cut according to specific measurements or customer requirements. This fits the description perfectly.
   - C: clothing store - Clothing stores usually sell ready-made clothing, not custom-cut fabric.
   - D: sewing room - This is more of a location within a household or a general place where any sewing can occur, not necessarily a type of seller.
   - E: hardware store - A hardware store sells products related to home improvement and construction; fabric is not typically among these products.
4. From the analysis, only a tailor shop corresponds to a seller that would cut fabric according to custom orders.
5. Therefore, the answer is B: tailor shop.
So the final answer is B: tailor shop

Input: Where are you if your reading magazines while waiting for a vehicle on rails? Choices:  A: vegetables B: market C: doctor D: train station E: bookstore
Output: 1. The question asks for a location where it's possible to read magazines while waiting for a vehicle on rails. This implies some type of public transportation.
2. The vehicle is specified as moving on rails which rules out options other than a train.
3. Both a train station or a bookstore have areas where one might read a magazine, so these are the most likely choices at first glance.
4. However, a bookstore doesn't make sense with the passage since one doesn't typically wait for transportation at a bookstore.
5. Hence, considering all the given clues, the correct choice is a train station where passengers read magazines while waiting for their train. Therefore, the answer is D: train station.
So the final answer is D: train station

Input: What would need oil to be used? Choices:  A: ground B: human  body C: repair shop D: combustion engines E: service station
Output: 1. The question is asking for contexts where oil might be needed to function.
2. Considering the options:
- For option A, grounds generally do not require oil for their functionality. They require water and nutrients to sustain plant and animal life, but not oil.
- For option B, the human body also does not use oil. It requires water, nutrients, and oxygen to function properly, but oil is not a biological requirement.
- For option C and E, a repair shop and service station could use oil in their operations, particularly when working on machines that require oil. However, the stations and shops themselves don't require oil to function, instead, the machinery and vehicles that come in might.
- For option D, combustion engines specifically require oil to function. The oil is used to lubricate the engine's parts, maintain their temperature, and to prevent rust.
3. Thus, the option that directly necessitates the use of oil is D: combustion engines.
So the final answer is D: combustion engines

Input: What is person probably feeling that plans on stopping being married to their spouse? Choices:  A: detachment B: bankruptcy C: sad D: fights E: wrong
Output: 1. The question asks about a person's probable emotional state who plans on ending their marriage. 
2. From the multiple choices given, we need to pick a choice that reflects an emotion or mindset related to ending a marriage.
3. Two answers, "sad" and "detachment," are related to emotions.
4. Although ending a marriage can be sad, the question specifically asks about someone who plans on stopping being married, implying the decision has already been made.
5. This suggests the person has emotionally prepared themselves to end the relationship, which indicates a state of emotional withdrawal or separation.
6. Therefore, the answer is A: detachment. This term best reflects the emotional distance someone would likely feel when deciding to no longer be married to their spouse.
So the final answer is A: detachment

Input: What could you use to store a clock? Choices:  A: shelf B: own bedroom C: desk D: wall E: car
Output: Step 1: The question is asking what could be used to store a clock.
Step 2: Analyze each possible answer.
A: shelf - A shelf could be used to store a clock on it.
B: own bedroom - A bedroom is not a storage device but a place where storage devices can be placed.
C: desk - While a desk can be used to keep a clock, it is not typically used for storage.
D: wall - A wall can be used for hanging a clock, but it does not store it.
E: car - A car is not a practical place to store a clock. 
Step 3: Based on the analysis, answer A (shelf) is the best answer.
So the final answer is A: shelf

Input: The person put on lotion, what did they want? Choices:  A: fresh smell B: good credit C: smooth skin D: fresh produce E: headache
Output: 1. The question is about discerning the reason behind the action of a person putting on lotion.
2. We must identify the result that commonly occurs from this action, among the options given.
3. Option A refers to a fresh smell, which could be a reason, but lotions are not primarily used for scent.
4. Option B refers to good credit, which is unrelated to the action of putting on lotion.
5. Option C refers to smooth skin, which is a common reason for people to put on lotion. Most lotions contain moisturizing agents that help retain skin's moisture, leading to smoother skin.
6. Option D refers to fresh produce, which is entirely not related to the action.
7. Option E suggests the person might want a headache, which is an adverse state anyone would usually want to avoid and also it is unrelated to applying lotion.
8. So, after examining all the options, we can determine that the person put on lotion to get smooth skin - making Option C the correct answer.
So the final answer is C: smooth skin

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i6-s10-rTrue-m4096
Evaluating commonsenseqa :   2%|▌                                  | 1/63 [01:14<1:17:13, 74.74s/it]Evaluating commonsenseqa :   3%|█                                  | 2/63 [02:27<1:14:45, 73.53s/it]Evaluating commonsenseqa :   5%|█▋                                 | 3/63 [03:40<1:13:10, 73.18s/it]Evaluating commonsenseqa :   6%|██▏                                | 4/63 [04:53<1:11:50, 73.06s/it]Evaluating commonsenseqa :   8%|██▊                                | 5/63 [06:05<1:10:21, 72.78s/it]Evaluating commonsenseqa :  10%|███▎                               | 6/63 [07:18<1:09:21, 73.01s/it]Evaluating commonsenseqa :  11%|███▉                               | 7/63 [08:31<1:08:01, 72.88s/it]Evaluating commonsenseqa :  13%|████▍                              | 8/63 [09:43<1:06:41, 72.75s/it]Evaluating commonsenseqa :  14%|█████                              | 9/63 [10:53<1:04:39, 71.85s/it]Evaluating commonsenseqa :  16%|█████▍                            | 10/63 [12:06<1:03:47, 72.21s/it]Evaluating commonsenseqa :  17%|█████▉                            | 11/63 [13:19<1:02:51, 72.52s/it]Evaluating commonsenseqa :  19%|██████▍                           | 12/63 [14:33<1:01:56, 72.87s/it]Evaluating commonsenseqa :  21%|███████                           | 13/63 [15:46<1:00:43, 72.87s/it]Evaluating commonsenseqa :  22%|████████                            | 14/63 [16:58<59:23, 72.73s/it]Evaluating commonsenseqa :  24%|████████▌                           | 15/63 [18:11<58:12, 72.76s/it]Evaluating commonsenseqa :  25%|█████████▏                          | 16/63 [19:24<56:58, 72.72s/it]Evaluating commonsenseqa :  27%|█████████▋                          | 17/63 [20:36<55:39, 72.61s/it]Evaluating commonsenseqa :  29%|██████████▎                         | 18/63 [21:49<54:34, 72.76s/it]Evaluating commonsenseqa :  30%|██████████▊                         | 19/63 [23:02<53:25, 72.85s/it]Evaluating commonsenseqa :  32%|███████████▍                        | 20/63 [24:16<52:23, 73.10s/it]Evaluating commonsenseqa :  33%|████████████                        | 21/63 [25:28<51:00, 72.86s/it]Evaluating commonsenseqa :  35%|████████████▌                       | 22/63 [26:41<49:40, 72.70s/it]Evaluating commonsenseqa :  37%|█████████████▏                      | 23/63 [27:53<48:27, 72.69s/it]Evaluating commonsenseqa :  38%|█████████████▋                      | 24/63 [29:06<47:11, 72.59s/it]Evaluating commonsenseqa :  40%|██████████████▎                     | 25/63 [30:19<46:01, 72.68s/it]Evaluating commonsenseqa :  41%|██████████████▊                     | 26/63 [31:31<44:46, 72.61s/it]Evaluating commonsenseqa :  43%|███████████████▍                    | 27/63 [32:44<43:42, 72.84s/it]Evaluating commonsenseqa :  44%|████████████████                    | 28/63 [33:58<42:33, 72.96s/it]Evaluating commonsenseqa :  46%|████████████████▌                   | 29/63 [35:10<41:11, 72.69s/it]Evaluating commonsenseqa :  48%|█████████████████▏                  | 30/63 [36:22<39:53, 72.52s/it]Evaluating commonsenseqa :  49%|█████████████████▋                  | 31/63 [37:35<38:42, 72.58s/it]Evaluating commonsenseqa :  51%|██████████████████▎                 | 32/63 [38:48<37:34, 72.73s/it]Evaluating commonsenseqa :  52%|██████████████████▊                 | 33/63 [40:00<36:18, 72.61s/it]Evaluating commonsenseqa :  54%|███████████████████▍                | 34/63 [41:13<35:12, 72.86s/it]Evaluating commonsenseqa :  56%|████████████████████                | 35/63 [42:26<33:54, 72.66s/it]Evaluating commonsenseqa :  57%|████████████████████▌               | 36/63 [43:39<32:48, 72.92s/it]Evaluating commonsenseqa :  59%|█████████████████████▏              | 37/63 [44:52<31:32, 72.78s/it]Evaluating commonsenseqa :  60%|█████████████████████▋              | 38/63 [45:47<28:07, 67.51s/it]Evaluating commonsenseqa :  62%|██████████████████████▎             | 39/63 [46:59<27:34, 68.94s/it]Evaluating commonsenseqa :  63%|██████████████████████▊             | 40/63 [48:12<26:51, 70.07s/it]Evaluating commonsenseqa :  65%|███████████████████████▍            | 41/63 [49:24<25:54, 70.66s/it]Evaluating commonsenseqa :  67%|████████████████████████            | 42/63 [50:36<24:54, 71.16s/it]Evaluating commonsenseqa :  68%|████████████████████████▌           | 43/63 [51:49<23:50, 71.52s/it]Evaluating commonsenseqa :  70%|█████████████████████████▏          | 44/63 [53:01<22:42, 71.72s/it]Evaluating commonsenseqa :  71%|█████████████████████████▋          | 45/63 [54:13<21:33, 71.88s/it]Evaluating commonsenseqa :  73%|██████████████████████████▎         | 46/63 [55:27<20:32, 72.48s/it]Evaluating commonsenseqa :  75%|██████████████████████████▊         | 47/63 [56:40<19:24, 72.80s/it]Evaluating commonsenseqa :  76%|███████████████████████████▍        | 48/63 [57:53<18:10, 72.67s/it]Evaluating commonsenseqa :  78%|████████████████████████████        | 49/63 [59:06<16:58, 72.75s/it]Evaluating commonsenseqa :  79%|██████████████████████████▉       | 50/63 [1:00:19<15:46, 72.81s/it]Evaluating commonsenseqa :  81%|███████████████████████████▌      | 51/63 [1:01:31<14:32, 72.71s/it]Evaluating commonsenseqa :  83%|████████████████████████████      | 52/63 [1:02:44<13:18, 72.63s/it]Evaluating commonsenseqa :  84%|████████████████████████████▌     | 53/63 [1:03:56<12:06, 72.63s/it]Evaluating commonsenseqa :  86%|█████████████████████████████▏    | 54/63 [1:05:08<10:52, 72.49s/it]Evaluating commonsenseqa :  87%|█████████████████████████████▋    | 55/63 [1:06:22<09:42, 72.83s/it]Evaluating commonsenseqa :  89%|██████████████████████████████▏   | 56/63 [1:07:35<08:29, 72.77s/it]Evaluating commonsenseqa :  90%|██████████████████████████████▊   | 57/63 [1:08:48<07:17, 72.91s/it]Evaluating commonsenseqa :  92%|███████████████████████████████▎  | 58/63 [1:10:00<06:03, 72.64s/it]Evaluating commonsenseqa :  94%|███████████████████████████████▊  | 59/63 [1:11:12<04:50, 72.61s/it]Evaluating commonsenseqa :  95%|████████████████████████████████▍ | 60/63 [1:12:25<03:38, 72.73s/it]Evaluating commonsenseqa :  97%|████████████████████████████████▉ | 61/63 [1:13:38<02:25, 72.57s/it]Evaluating commonsenseqa :  98%|█████████████████████████████████▍| 62/63 [1:14:50<01:12, 72.42s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:15:32<00:00, 63.28s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:15:32<00:00, 71.94s/it]
name: commonsenseqa | avg. gen lenth: 427.392 | time: 4532.692110300064s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-29 07:16:50,373] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 07:16:50,381] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 07:16:50,390] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 07:16:50,430] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 7
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i7-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                        | 0/9734 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████| 9734/9734 [00:00<00:00, 323492.61it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.33s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.41s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.41s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:08<00:08,  8.20s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.43s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.86s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.42s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.87s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.44s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.89s/it]
 > number of parameters: 6738415616
[2023-08-29 07:17:01,630] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 07:17:01,646] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 07:17:01,707] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:10<00:00,  4.56s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:10<00:00,  5.10s/it]
[2023-08-29 07:17:01,903] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 07:17:02,503] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-29 07:17:02,504] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f809123b310>
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-29 07:17:02,505] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-29 07:17:02,506] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-29 07:17:02,506] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                             | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: 1. The question is asking where fabric is cut to order, which means a place where fabric is cut according to specific measurements given by the customer.
2. Looking at the choices, first we can eliminate E: hardware store as it usually deals with tools and materials related to construction, not fabric.
3. Next, we can eliminate A: curtains, because even though fabric is indeed used to make curtains, the question is asking where the fabric is cut to order, not what it is used for.
4. Then we eliminate C: clothing store. Even though clothing stores do sell fabric products, they generally do not cut fabric to order; rather, they sell finished garments.
5. D: Sewing room can also be eliminated because it is a place where someone might cut and sew fabric, but it is not typically a business where fabric is cut to order.
6. This leaves us with B: tailor shop. A tailor shop is a place where clothes are made and altered to fit specific individuals. Fabric is cut to order based on customers' measurements, making this the most sensible choice. So, the answer is B: tailor shop.
So the final answer is B: tailor shop

Input: Where are you if your reading magazines while waiting for a vehicle on rails? Choices:  A: vegetables B: market C: doctor D: train station E: bookstore
Output: 1. The question is asking about a location where someone could be waiting for a vehicle on rails while reading magazines.
2. "Vehicles on rails" refer to trains or trams. Vegetable, market, doctor, train station, and bookstore are the given choices. We need to identify which one of these is a place where trains or trams stop.
3. A vegetable is not a place but a type of food, so eliminate choice A. A doctor is a profession and not a place where trains stop, so eliminate choice C. A bookstore is a place, but trains do not usually stop there, so eliminate choice E.
4. A market is a place where people buy and sell goods. However, this is generally not a designated area for waiting for trains, so eliminate choice B.
5. The only option left is D: train station. In the context of the question, a train station is a place where trains or trams stop and where people could be waiting and likely have time to read magazines. 
6. Thus, the answer is D: train station.
So the final answer is D: train station

Input: What would need oil to be used? Choices:  A: ground B: human  body C: repair shop D: combustion engines E: service station
Output: Step 1: Understand the question which is asking to select an item from the choices that would definitely need oil for its operation or function. 
Step 2: Clarify what each choice could mean in the context of needing oil.
   A: Ground: We usually don't use oil for dirt or soil operation or behavior.
   B: Human body: Our body needs fats, not oil, for its operation.
   C: Repair Shop: This option seems plausible as oil is frequently used in a repair shop on various instruments. However, the shop itself does not need oil to function.
   D: Combustion Engines: A combustion engine indeed requires oil for lubrication and smooth operation.
   E: Service Station: Similar to repair shop, a service station will utilize oil, yet it isn't necessary for the function of the station itself.
Step 3: Evaluate the choices based upon the provided reasoning and choose the best one.
The combustion engine (choice D) is the option that directly needs oil for its operation. 
Therefore, the answer is D: combustion engines.
So the final answer is D: combustion engines

Input: What is person probably feeling that plans on stopping being married to their spouse? Choices:  A: detachment B: bankruptcy C: sad D: fights E: wrong
Output: 1. The question is about the probable feelings of a person who is planning to end their marriage.
2. Ending a marriage is usually a difficult decision that may invoke several emotional responses.
3. The given options are detachment, bankruptcy, sad, fights, and wrong.
4. We can eliminate 'bankruptcy' as it is a financial state, not an emotion.
5. 'Fights' can be a cause or consequence of ending a marriage, but it is not a feeling.
6. 'Wrong' here is ambiguous. It could refer to feeling like the decision is morally wrong, or feeling wronged by the spouse. But, without context to clarify this, it's not the best answer.
7. 'Sad' is a plausible option as many people feel sadness when ending a marriage. But the phrasing of the question implies a readiness and decision to end the marriage, which suggests a level of emotional distance, or detachment, has been achieved.
8. That leaves us with 'detachment,' which fits the situation most accurately as it indicates a sense of emotional disconnection that often precedes the decision to end a marriage.
9. Therefore, the answer is 'A: detachment.'
So the final answer is A: detachment

Input: What could you use to store a clock? Choices:  A: shelf B: own bedroom C: desk D: wall E: car
Output: 1. The question asks what you could use to store a clock. 
2. This eliminates B: own bedroom and E: car. Although a clock can exist in these locations, we wouldn't necessarily consider them as storage for a clock. 
3. Between A: shelf, C: desk, and D: wall, all could potentially store a clock due to their flat surfaces. 
4. However, while a clock may be placed or hung on a desk or wall, we would not typically consider these as places of storage rather than usage. 
5. Meanwhile, a shelf ideally serves as a storage place where we can place various objects, including a clock. Therefore, a clock can be stored there when not in use. 
6. Hence, the answer is A: shelf.
So the final answer is A: shelf

Input: The person put on lotion, what did they want? Choices:  A: fresh smell B: good credit C: smooth skin D: fresh produce E: headache
Output: 1. The question asks about the purpose of a person putting on lotion.
2. Consider all the options given. 
3. Option A suggests that the person might want a fresh smell. While it's true that some lotions do have fragrances, the primary purpose of lotion is not to give a fresh smell. Thus, this is not the most likely answer.
4. Option B suggests that the person might want good credit. This is not related to use of lotion at all, so this answer is not correct. 
5. Option C suggests that the person might want smooth skin. This aligns with a common purpose of applying lotion, which is to moisturize and smooth the skin. This is a plausible answer. 
6. Option D suggests they want fresh produce. This is not relevant to the use of lotion, so D is not the correct answer.
7. Option E suggests they might want a headache. This does not make sense because lotion is not associated with causing or relieving headaches.
8. Therefore, the most reasonable answer is C, the person put on lotion to have smooth skin.
So the final answer is C: smooth skin

Input: They burned the record, they were trying to do what to history? Choices:  A: compact disc B: tape C: rewrite D: play music E: erase
Output: 1. The question elaborates on a situation where a record is being burned.
2. A record, analogous to history here, contains information or data.
3. When a record is burned, the information it contains is destroyed or removed.
4. This is analogous to trying to erase some parts of history or forget about them.
5. Therefore, if they were trying to burn a record, they were trying to erase history.
6. Hence, the answer is E: erase.
So the final answer is E: erase

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i7-s10-rTrue-m4096
Evaluating commonsenseqa :   2%|▌                                  | 1/63 [01:10<1:12:28, 70.13s/it]Evaluating commonsenseqa :   3%|█                                  | 2/63 [02:17<1:09:39, 68.51s/it]Evaluating commonsenseqa :   5%|█▋                                 | 3/63 [03:24<1:07:47, 67.80s/it]Evaluating commonsenseqa :   6%|██▏                                | 4/63 [04:31<1:06:19, 67.46s/it]Evaluating commonsenseqa :   8%|██▊                                | 5/63 [05:37<1:04:46, 67.01s/it]Evaluating commonsenseqa :  10%|███▎                               | 6/63 [06:44<1:03:32, 66.89s/it]Evaluating commonsenseqa :  11%|███▉                               | 7/63 [07:51<1:02:29, 66.96s/it]Evaluating commonsenseqa :  13%|████▍                              | 8/63 [08:57<1:01:13, 66.79s/it]Evaluating commonsenseqa :  14%|█████                              | 9/63 [10:04<1:00:00, 66.67s/it]Evaluating commonsenseqa :  16%|█████▋                              | 10/63 [11:11<58:56, 66.72s/it]Evaluating commonsenseqa :  17%|██████▎                             | 11/63 [12:17<57:39, 66.54s/it]Evaluating commonsenseqa :  19%|██████▊                             | 12/63 [13:23<56:29, 66.45s/it]Evaluating commonsenseqa :  21%|███████▍                            | 13/63 [14:30<55:28, 66.57s/it]Evaluating commonsenseqa :  22%|████████                            | 14/63 [15:36<54:13, 66.40s/it]Evaluating commonsenseqa :  24%|████████▌                           | 15/63 [16:13<46:02, 57.55s/it]Evaluating commonsenseqa :  25%|█████████▏                          | 16/63 [17:19<47:11, 60.25s/it]Evaluating commonsenseqa :  27%|█████████▋                          | 17/63 [18:26<47:37, 62.13s/it]Evaluating commonsenseqa :  29%|██████████▎                         | 18/63 [19:32<47:27, 63.28s/it]Evaluating commonsenseqa :  30%|██████████▊                         | 19/63 [20:38<47:04, 64.19s/it]Evaluating commonsenseqa :  32%|███████████▍                        | 20/63 [21:45<46:29, 64.88s/it]Evaluating commonsenseqa :  33%|████████████                        | 21/63 [22:53<46:05, 65.85s/it]Evaluating commonsenseqa :  35%|████████████▌                       | 22/63 [24:00<45:12, 66.16s/it]Evaluating commonsenseqa :  37%|█████████████▏                      | 23/63 [25:05<44:02, 66.06s/it]Evaluating commonsenseqa :  38%|█████████████▋                      | 24/63 [26:12<43:04, 66.26s/it]Evaluating commonsenseqa :  40%|██████████████▎                     | 25/63 [27:19<42:06, 66.49s/it]Evaluating commonsenseqa :  41%|██████████████▊                     | 26/63 [28:25<40:53, 66.31s/it]Evaluating commonsenseqa :  43%|███████████████▍                    | 27/63 [29:31<39:45, 66.25s/it]Evaluating commonsenseqa :  44%|████████████████                    | 28/63 [30:38<38:44, 66.41s/it]Evaluating commonsenseqa :  46%|████████████████▌                   | 29/63 [31:44<37:37, 66.40s/it]Evaluating commonsenseqa :  48%|█████████████████▏                  | 30/63 [32:50<36:26, 66.27s/it]Evaluating commonsenseqa :  49%|█████████████████▋                  | 31/63 [33:57<35:23, 66.37s/it]Evaluating commonsenseqa :  51%|██████████████████▎                 | 32/63 [35:03<34:19, 66.45s/it]Evaluating commonsenseqa :  52%|██████████████████▊                 | 33/63 [36:10<33:13, 66.44s/it]Evaluating commonsenseqa :  54%|███████████████████▍                | 34/63 [37:17<32:09, 66.54s/it]Evaluating commonsenseqa :  56%|████████████████████                | 35/63 [38:23<31:05, 66.61s/it]Evaluating commonsenseqa :  57%|████████████████████▌               | 36/63 [39:30<29:59, 66.64s/it]Evaluating commonsenseqa :  59%|█████████████████████▏              | 37/63 [40:37<28:50, 66.57s/it]Evaluating commonsenseqa :  60%|█████████████████████▋              | 38/63 [41:43<27:43, 66.54s/it]Evaluating commonsenseqa :  62%|██████████████████████▎             | 39/63 [42:50<26:37, 66.58s/it]Evaluating commonsenseqa :  63%|██████████████████████▊             | 40/63 [43:56<25:28, 66.47s/it]Evaluating commonsenseqa :  65%|███████████████████████▍            | 41/63 [45:02<24:22, 66.47s/it]Evaluating commonsenseqa :  67%|████████████████████████            | 42/63 [46:09<23:14, 66.41s/it]Evaluating commonsenseqa :  68%|████████████████████████▌           | 43/63 [47:05<21:08, 63.45s/it]Evaluating commonsenseqa :  70%|█████████████████████████▏          | 44/63 [48:12<20:23, 64.42s/it]Evaluating commonsenseqa :  71%|█████████████████████████▋          | 45/63 [49:18<19:27, 64.87s/it]Evaluating commonsenseqa :  73%|██████████████████████████▎         | 46/63 [50:25<18:32, 65.44s/it]Evaluating commonsenseqa :  75%|██████████████████████████▊         | 47/63 [51:31<17:32, 65.76s/it]Evaluating commonsenseqa :  76%|███████████████████████████▍        | 48/63 [52:37<16:29, 65.95s/it]Evaluating commonsenseqa :  78%|████████████████████████████        | 49/63 [53:43<15:23, 65.96s/it]Evaluating commonsenseqa :  79%|████████████████████████████▌       | 50/63 [54:36<13:26, 62.07s/it]Evaluating commonsenseqa :  81%|█████████████████████████████▏      | 51/63 [55:43<12:42, 63.52s/it]Evaluating commonsenseqa :  83%|█████████████████████████████▋      | 52/63 [56:50<11:48, 64.42s/it]Evaluating commonsenseqa :  84%|██████████████████████████████▎     | 53/63 [57:56<10:49, 64.94s/it]Evaluating commonsenseqa :  86%|██████████████████████████████▊     | 54/63 [59:03<09:48, 65.42s/it]Evaluating commonsenseqa :  87%|█████████████████████████████▋    | 55/63 [1:00:10<08:47, 65.89s/it]Evaluating commonsenseqa :  89%|██████████████████████████████▏   | 56/63 [1:01:16<07:42, 66.06s/it]Evaluating commonsenseqa :  90%|██████████████████████████████▊   | 57/63 [1:02:23<06:37, 66.22s/it]Evaluating commonsenseqa :  92%|███████████████████████████████▎  | 58/63 [1:03:29<05:31, 66.29s/it]Evaluating commonsenseqa :  94%|███████████████████████████████▊  | 59/63 [1:04:36<04:26, 66.54s/it]Evaluating commonsenseqa :  95%|████████████████████████████████▍ | 60/63 [1:05:42<03:19, 66.43s/it]Evaluating commonsenseqa :  97%|████████████████████████████████▉ | 61/63 [1:06:49<02:12, 66.45s/it]Evaluating commonsenseqa :  98%|█████████████████████████████████▍| 62/63 [1:07:55<01:06, 66.29s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:08:34<00:00, 58.31s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:08:34<00:00, 65.32s/it]
name: commonsenseqa | avg. gen lenth: 417.104 | time: 4115.514319658279s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-29 08:26:32,017] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 08:26:32,095] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 08:26:32,097] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 08:26:32,097] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 8
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                        | 0/9733 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████| 9733/9733 [00:00<00:00, 297542.74it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:06<00:06,  6.51s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:06<00:06,  6.87s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.05s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  3.92s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.30s/it]
[2023-08-29 08:26:42,274] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.04s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.47s/it]
[2023-08-29 08:26:42,566] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:09<00:09,  9.18s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.37s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.77s/it]
 > number of parameters: 6738415616
[2023-08-29 08:26:43,189] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:11<00:00,  4.91s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:11<00:00,  5.55s/it]
[2023-08-29 08:26:44,775] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 08:26:45,386] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-29 08:26:45,388] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-29 08:26:45,388] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-29 08:26:45,388] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9a4c96f640>
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-29 08:26:45,389] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-29 08:26:45,390] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-29 08:26:45,391] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-29 08:26:45,391] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                             | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: 1. The question asks where fabric is cut to order.
2. Cutting fabric to order means taking a specific measurement and then cutting the fabric according to the customer's requirement.
3. Looking at the given options, a tailor shop is the only place that deals with the custom cutting of fabric to fit a specific clothing design or customer's specifications.
4. Although curtains, clothing stores, sewing rooms, and hardware stores may have some relation to fabric, none of them uniquely involve the specified action of cutting fabric to order.
5. Therefore, the answer is B: tailor shop.
So the final answer is B: tailor shop

Input: Where are you if your reading magazines while waiting for a vehicle on rails? Choices:  A: vegetables B: market C: doctor D: train station E: bookstore
Output: 1. The question is essentially a scenario which asks for your location based on your activity.
2. The activity is "reading magazines while waiting for a vehicle on rails".
3. A "vehicle on rails" implies modes of transportation that moves on tracks such as trains, trams or subways. 
4. Out of the provided options; vegetables, market, doctor, train station, and bookstore, the place where one can wait for a vehicle on rails is the 'train station'. 
5. Therefore, if you're reading magazines while waiting for a train, the most probable place you can be is at a train station.
6. Hence, the answer is D: train station.
So the final answer is D: train station

Input: What would need oil to be used? Choices:  A: ground B: human  body C: repair shop D: combustion engines E: service station
Output: Step 1: Consider each option in relation to the practical use of oil.
Step 2: Oil is not typically used in association with ground (option A), so we can eliminate this option.
Step 3: While some oils are used for human body (option B) in terms of massage and skincare, the term "oil" in this context typically refers to a type of fuel or lubricant, which would not apply to the human body. So, we can eliminate this option.
Step 4: A repair shop (option C) might use oil as part of repairs on certain equipment or vehicles, but the shop itself doesn't need oil to operate. It further relies on a wide range of tools and materials other than oil. Therefore, we can eliminate this option.
Step 5: Combustion engines (option D) do indeed require oil for lubrication and cooling to operate effectively. This makes this a strong potential answer.
Step 6: A service station (option E) might sell oil, but the station itself does not require oil to function. The service station relies on many resources including electricity, water among others for its operation, and is not dependent on oil alone. Therefore, this option can be eliminated.
Step 7: Upon evaluating all options, combustion engines (option D) are the only choice that require oil as a necessary component to their function. Hence, the answer is D: combustion engines.
So the final answer is D: combustion engines

Input: What is person probably feeling that plans on stopping being married to their spouse? Choices:  A: detachment B: bankruptcy C: sad D: fights E: wrong
Output: 1. The question entails understanding the emotional state of a person who is contemplating ending their marriage.
2. The choices provided are: detachment, bankruptcy, sad, fights, and wrong. It's necessary to eliminate options which are not emotions, like "bankruptcy" and "fights".
3. "Wrong" is a feeling in some circumstances but in this context, it doesn't seem to fit as well as the remaining options, "detachment" and "sad".
4. While "sad" is indeed an emotion that could be associated with this situation, the act of deciding to end a marriage often comes with a feeling of emotional distance or disconnection from one's spouse, which is essentially what "detachment" means.
5. Therefore, the person is probably feeling "detachment", as they are planning to distance themselves from their spouse by divorcing them.
So the final answer is A: detachment

Input: What could you use to store a clock? Choices:  A: shelf B: own bedroom C: desk D: wall E: car
Output: 1. The question asks where you could place or store a clock.
2. The clue here is that we are looking for a place where you can place objects, and ideally such that the object could be viewed or used easily. 
3. Looking at the choices, all of them could realistically serve as a place to store a clock, but some options may not be suitable for all types of clocks.
4. "Own bedroom" is a broad term - it doesn't specify where in the bedroom one could place a clock. Therefore, this option is rather vague in this context.
5. "Car" could be a possible answer if one thinks of small, portable clocks. However, it's not an ideal place to store a clock in general considering the movement and vibration.
6. Similarly, using a "desk" to store a clock is possible, but it might take up useful space where you might be working.
7. A "wall" is a common place to hang or mount a wall clock. But not all types of clocks, especially table clocks, can be placed on a wall.
8. This brings us to the "shelf". A shelf is usually a place where objects are kept for display or easy access. It's a place where pretty much any kind of clock, be it desk clock, a small wall clock, or a grandfather clock, could possibly be kept.
9. Therefore, considering the mentioned points, the best answer is option A: shelf.
So the final answer is A: shelf

Input: The person put on lotion, what did they want? Choices:  A: fresh smell B: good credit C: smooth skin D: fresh produce E: headache
Output: 1. Firstly, we have to identify the purpose of putting on lotion. 
2. Creams, lotions and moisturizers are skincare products usually used for hydrating skin and improving its texture.
3. So, the person putting on lotion might want a positive effect on their skin. 
4. Looking at the options, 'fresh smell' could be a possibility because some lotions have a fragrant smell. However, the primary reason people use lotion is not for its smell. 
5. 'Good credit' is irrelevant. Lotion can't affect personal finance or credit report.
6. 'Fresh produce' is also unrelated to the context.
7. 'Headache' has no relationship with lotion application.
8. Therefore, the only remaining and most plausible answer is C: smooth skin.
So the final answer is C: smooth skin

Input: They burned the record, they were trying to do what to history? Choices:  A: compact disc B: tape C: rewrite D: play music E: erase
Output: 1. The question emphasizes that a record was burned, thus, removing any content that was on it. 
2. This can be interpreted as an action to destroy or annihilate that particular piece of history the record holds since burning a record will result in its total destruction.
3. To understand the question better one needs to relate this action to the choices given which pertains to 'history'. 
4. Choices A: "compact disc", B: "tape", and D: "play music" are related to the physical act of dealing with a record or music and aren't particularly related to "history".
5. Choice C: "rewrite" typically means to write something again with changes or improvements. But in this context, the action given was the destruction of the record and not creating something new from it.
6. This leaves us with Choice E: "erase" because when you burn a record, you essentially erase its content or history.
7. Therefore, the answer is E: erase.
So the final answer is E: erase

Input: She sure didn't have a green thumb, every time she thought she was making grow something it would what? Choices:  A: growth B: flowering C: ground D: die E: plants
Output: 1. The problem indicates that "she" allegedly does not have a "green thumb", which is a phrase used to refer to someone who is good at growing plants.
2. It also refers that every time she tries to make something grow, the opposite happens, indicating a negative outcome to her efforts.
3. The options given are growth, flowering, ground, die, and plants.
4. If she doesn't have a green thumb and fails in her growing attempts, the plants would not grow, nor flower, nor become plants, and "ground" is not a logical result.
5. As per the given context, "die" matches the negative outcome the phrase is trying to convey.
6. So, the answer is D: die.
So the final answer is D: die

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s10-rTrue-m4096
Evaluating commonsenseqa :   2%|▌                                  | 1/63 [01:04<1:06:30, 64.36s/it]Evaluating commonsenseqa :   3%|█                                  | 2/63 [02:07<1:04:40, 63.61s/it]Evaluating commonsenseqa :   5%|█▋                                 | 3/63 [03:11<1:03:35, 63.59s/it]Evaluating commonsenseqa :   6%|██▏                                | 4/63 [04:14<1:02:25, 63.49s/it]Evaluating commonsenseqa :   8%|██▊                                | 5/63 [05:17<1:01:24, 63.53s/it]Evaluating commonsenseqa :  10%|███▎                               | 6/63 [06:20<1:00:06, 63.28s/it]Evaluating commonsenseqa :  11%|████                                 | 7/63 [07:24<59:03, 63.28s/it]Evaluating commonsenseqa :  13%|████▋                                | 8/63 [08:27<58:03, 63.34s/it]Evaluating commonsenseqa :  14%|█████▎                               | 9/63 [09:30<56:52, 63.20s/it]Evaluating commonsenseqa :  16%|█████▋                              | 10/63 [10:33<55:54, 63.30s/it]Evaluating commonsenseqa :  17%|██████▎                             | 11/63 [11:37<54:48, 63.24s/it]Evaluating commonsenseqa :  19%|██████▊                             | 12/63 [12:40<53:43, 63.21s/it]Evaluating commonsenseqa :  21%|███████▍                            | 13/63 [13:43<52:45, 63.32s/it]Evaluating commonsenseqa :  22%|████████                            | 14/63 [14:46<51:40, 63.27s/it]Evaluating commonsenseqa :  24%|████████▌                           | 15/63 [15:50<50:40, 63.35s/it]Evaluating commonsenseqa :  25%|█████████▏                          | 16/63 [16:53<49:34, 63.29s/it]Evaluating commonsenseqa :  27%|█████████▋                          | 17/63 [17:56<48:23, 63.12s/it]Evaluating commonsenseqa :  29%|██████████▎                         | 18/63 [18:59<47:16, 63.03s/it]Evaluating commonsenseqa :  30%|██████████▊                         | 19/63 [20:02<46:12, 63.01s/it]Evaluating commonsenseqa :  32%|███████████▍                        | 20/63 [21:04<45:08, 62.98s/it]Evaluating commonsenseqa :  33%|████████████                        | 21/63 [22:08<44:10, 63.10s/it]Evaluating commonsenseqa :  35%|████████████▌                       | 22/63 [23:11<43:09, 63.16s/it]Evaluating commonsenseqa :  37%|█████████████▏                      | 23/63 [24:14<42:08, 63.20s/it]Evaluating commonsenseqa :  38%|█████████████▋                      | 24/63 [25:18<41:10, 63.35s/it]Evaluating commonsenseqa :  40%|██████████████▎                     | 25/63 [26:21<40:01, 63.19s/it]Evaluating commonsenseqa :  41%|██████████████▊                     | 26/63 [27:24<38:52, 63.03s/it]Evaluating commonsenseqa :  43%|███████████████▍                    | 27/63 [28:27<37:53, 63.14s/it]Evaluating commonsenseqa :  44%|████████████████                    | 28/63 [29:30<36:52, 63.21s/it]Evaluating commonsenseqa :  46%|████████████████▌                   | 29/63 [30:34<35:50, 63.26s/it]Evaluating commonsenseqa :  48%|█████████████████▏                  | 30/63 [31:38<34:52, 63.41s/it]Evaluating commonsenseqa :  49%|█████████████████▋                  | 31/63 [32:42<33:55, 63.60s/it]Evaluating commonsenseqa :  51%|██████████████████▎                 | 32/63 [33:44<32:43, 63.35s/it]Evaluating commonsenseqa :  52%|██████████████████▊                 | 33/63 [34:48<31:42, 63.40s/it]Evaluating commonsenseqa :  54%|███████████████████▍                | 34/63 [35:51<30:35, 63.28s/it]Evaluating commonsenseqa :  56%|████████████████████                | 35/63 [36:54<29:33, 63.36s/it]Evaluating commonsenseqa :  57%|████████████████████▌               | 36/63 [37:58<28:33, 63.46s/it]Evaluating commonsenseqa :  59%|█████████████████████▏              | 37/63 [39:01<27:26, 63.34s/it]Evaluating commonsenseqa :  60%|█████████████████████▋              | 38/63 [40:04<26:19, 63.20s/it]Evaluating commonsenseqa :  62%|██████████████████████▎             | 39/63 [41:07<25:14, 63.12s/it]Evaluating commonsenseqa :  63%|██████████████████████▊             | 40/63 [42:11<24:17, 63.36s/it]Evaluating commonsenseqa :  65%|███████████████████████▍            | 41/63 [43:14<23:12, 63.28s/it]Evaluating commonsenseqa :  67%|████████████████████████            | 42/63 [44:17<22:07, 63.22s/it]Evaluating commonsenseqa :  68%|████████████████████████▌           | 43/63 [45:21<21:06, 63.31s/it]Evaluating commonsenseqa :  70%|█████████████████████████▏          | 44/63 [46:23<20:00, 63.18s/it]Evaluating commonsenseqa :  71%|█████████████████████████▋          | 45/63 [47:27<18:58, 63.24s/it]Evaluating commonsenseqa :  73%|██████████████████████████▎         | 46/63 [48:30<17:54, 63.22s/it]Evaluating commonsenseqa :  75%|██████████████████████████▊         | 47/63 [49:33<16:51, 63.24s/it]Evaluating commonsenseqa :  76%|███████████████████████████▍        | 48/63 [50:36<15:48, 63.21s/it]Evaluating commonsenseqa :  78%|████████████████████████████        | 49/63 [51:40<14:46, 63.34s/it]Evaluating commonsenseqa :  79%|████████████████████████████▌       | 50/63 [52:43<13:43, 63.31s/it]Evaluating commonsenseqa :  81%|█████████████████████████████▏      | 51/63 [53:48<12:43, 63.64s/it]Evaluating commonsenseqa :  83%|█████████████████████████████▋      | 52/63 [54:51<11:40, 63.65s/it]Evaluating commonsenseqa :  84%|██████████████████████████████▎     | 53/63 [55:54<10:34, 63.45s/it]Evaluating commonsenseqa :  86%|██████████████████████████████▊     | 54/63 [56:57<09:29, 63.28s/it]Evaluating commonsenseqa :  87%|███████████████████████████████▍    | 55/63 [58:01<08:26, 63.29s/it]Evaluating commonsenseqa :  89%|████████████████████████████████    | 56/63 [59:04<07:22, 63.20s/it]Evaluating commonsenseqa :  90%|██████████████████████████████▊   | 57/63 [1:00:06<06:18, 63.10s/it]Evaluating commonsenseqa :  92%|███████████████████████████████▎  | 58/63 [1:01:10<05:16, 63.24s/it]Evaluating commonsenseqa :  94%|███████████████████████████████▊  | 59/63 [1:02:14<04:13, 63.34s/it]Evaluating commonsenseqa :  95%|████████████████████████████████▍ | 60/63 [1:03:18<03:10, 63.58s/it]Evaluating commonsenseqa :  97%|████████████████████████████████▉ | 61/63 [1:04:21<02:06, 63.34s/it]Evaluating commonsenseqa :  98%|█████████████████████████████████▍| 62/63 [1:05:24<01:03, 63.36s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:06:01<00:00, 55.51s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:06:01<00:00, 62.88s/it]
name: commonsenseqa | avg. gen lenth: 433.144 | time: 3962.193336725235s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-29 09:32:58,544] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 09:32:58,552] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 09:32:58,561] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 09:32:58,572] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 9
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i9-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                        | 0/9732 [00:00<?, ?it/s]Loading data: 100%|█████████████████████████████████████████| 9732/9732 [00:00<00:00, 266223.38it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:06<00:06,  6.30s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:06<00:06,  6.66s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:06<00:06,  6.78s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:06<00:06,  6.80s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  3.85s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.22s/it]
[2023-08-29 09:33:08,724] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.00s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.41s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.04s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.43s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.02s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:08<00:00,  4.44s/it]
 > number of parameters: 6738415616
[2023-08-29 09:33:09,006] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 09:33:09,082] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 09:33:09,099] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 09:33:09,668] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-29 09:33:09,669] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-29 09:33:09,669] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-29 09:33:09,669] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-29 09:33:09,669] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-29 09:33:09,669] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-29 09:33:09,669] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-29 09:33:09,669] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-29 09:33:09,669] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-29 09:33:09,669] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1f8fe63640>
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-29 09:33:09,670] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-29 09:33:09,671] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-29 09:33:09,671] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-29 09:33:09,671] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-29 09:33:09,671] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-29 09:33:09,671] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-29 09:33:09,671] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-29 09:33:09,671] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-29 09:33:09,671] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-29 09:33:09,671] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                             | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: 1. Since we are discussing fabric cutting to order, it seems logical to cross out the options that do not typically involve selling fabric or making custom fabric items. This includes the options E: hardware store as it deals more with tools and home improvement goods and A: curtains, as this is a product rather than a type of seller.
2. Although options C: clothing store and D: sewing room might initially seem plausible, upon further consideration, both are not the most suitable options. Clothing stores usually sell ready-made clothes, not fabric that is cut to order. A sewing room, on the other hand, is more of a place in a home or business where sewing is done rather than a type of seller.
3. This leaves us with option B: tailor shop. Tailors often provide fabric and cut it to order for making custom clothes. Therefore, the best answer is B: Tailor shop, where fabric is cut to your personal requirements.
So the final answer is B: tailor shop

Input: Where are you if your reading magazines while waiting for a vehicle on rails? Choices:  A: vegetables B: market C: doctor D: train station E: bookstore
Output: 1. The question gives us the context that you're waiting for a vehicle on rails. 
2. We can eliminate options A, B, and C since it's unlikely to wait for a vehicle on rails at a place that sells vegetables, a general market, or a doctor's clinic.
3. Option E, a bookstore, seems unlikely because, even though you can read magazines there, it's not a usual place to wait for a vehicle on rails too.
4. Therefore, the most logical place to be waiting for a vehicle on rails would be at a station specified for such vehicles.
5. In considering the available options, option D, a train station, fits the given description the best — you can read magazines while you wait, and it's a place where you typically wait for a vehicle on rails, i.e., a train.
6. Therefore, the answer is D: train station.
So the final answer is D: train station

Input: What would need oil to be used? Choices:  A: ground B: human  body C: repair shop D: combustion engines E: service station
Output: 1. Start by analyzing each choice and determine its relationship with oil.
2. Choice A is the ground. Generally, oil isn't applied to the ground in most instances. Therefore A cannot be the answer.
3. Choice B is the human body. Although there are body oils formulated for use on the skin, this is entirely different from the oil used in industries or machinery. Therefore B is not the correct answer.
4. Choice C is a repair shop. However, this is rather generic. A repair shop might use oil but it is not necessary. Therefore, C cannot be the answer.
5. Choice D is combustion engines. These types of engines require oil as a lubricant for their numerous parts, hence allowing the process of combustion to occur in an efficient and smooth manner. Therefore, it can be said that oil is indeed necessary for combustion engines.
6. Lastly, choice E-service station. A service station may handle oil but isn't necessarily reliant on oil for any particular function.
7. Based on these analyses, the correct answer is D: combustion engines.
So the final answer is D: combustion engines

Input: What is person probably feeling that plans on stopping being married to their spouse? Choices:  A: detachment B: bankruptcy C: sad D: fights E: wrong
Output: 1. The question asks for the emotion or state a person likely feels when planning to end their marriage.
2. This is likely a complex emotional situation, and all of the choices could potentially be relevant to different aspects of a divorce. 
3. However, 'bankruptcy', 'fights', and 'wrong' are specific conditions or actions, not feelings.
4. Out of 'detachment' and'sad', both could be feelings experienced during a divorce. 
5. However, considering the fact that this person is planning on ending the marriage, it implies they have already emotionally distanced themselves from their spouse.
6. Taking all these points into account, the most probable answer then is A: detachment.
So the final answer is A: detachment

Input: What could you use to store a clock? Choices:  A: shelf B: own bedroom C: desk D: wall E: car
Output: 1. The question asks where one could store a clock.
2. We can eliminate choice B: own bedroom, because it's not specific enough. A bedroom is a space that contains various objects and not a specific place to store a clock.
3. We can also eliminate choice E: car, as a car is also not the ideal place to store a clock since we generally don't keep clocks in cars.
4. Choice D: wall, is not completely illogical but we typically hang a clock on a wall and not store it there.
5. Choice C: desk, is possible since people often have desk clocks. However, the desk is not only for storing the clock. It serves mainly other purposes like reading, writing, working on a computer, etc.
6. This leaves us with choice A: shelf. A shelf is a surface in a home or office where items can be stored or displayed. A clock could certainly be stored on a shelf. So, the answer is A: shelf.
So the final answer is A: shelf

Input: The person put on lotion, what did they want? Choices:  A: fresh smell B: good credit C: smooth skin D: fresh produce E: headache
Output: 1. The question tells us that the person put on lotion.
2. People typically use lotion for its moisturizing effects. 
3. Lotion is designed to improve the feel of your skin, making it smoother to touch.
4. Accordingly, fresh smell (option A) might be a welcome secondary effect of applying lotion, but it is not the primary reason. 
5. Good credit (option B) has no connection to applying lotion, so we can exclude this option.
6. Fresh produce (option D) is also unrelated to the application of skin products.
7. A headache (option E) is not connected to or the result of using lotion.
8. Thus, the most logical reason someone would use lotion is to improve the texture of their skin, making it smooth (option C).
So the final answer is C: smooth skin

Input: They burned the record, they were trying to do what to history? Choices:  A: compact disc B: tape C: rewrite D: play music E: erase
Output: 1. The question is asking what these people were trying to do to history by burning the record.
2. Burning a record is equivalent to destroying evidence or eradicating information.
3. Therefore, the act may indicate an intention to remove or get rid of that part of data, information, or history.
4. If we look at the choices, only "erase" fits the context of eliminating something.
5. Other choices like compact disc, tape, rewrite and play music don't fit into the context of obliterating or removing information.
6. "Rewrite" may sound plausible but it implies changing or modifying already existing information, not destroying it.
7.We can therefore conclude from this that they were trying to "erase" history. Thus, the answer is E: erase.
So the final answer is E: erase

Input: She sure didn't have a green thumb, every time she thought she was making grow something it would what? Choices:  A: growth B: flowering C: ground D: die E: plants
Output: 1. The question is about the expression "have a green thumb", which is often used to describe someone's ability to cultivate and grow plants successfully.
2. From the statement "She sure didn't have a green thumb", we understand this lady was unsuccessful in growing plants. 
3. Then the text says "every time she thought she was making grow something it would...". This implies some negative outcome because of the previous statement that she was unsuccessful in growing plants.
4. Going through the options, 'growth' and 'flowering' are positive outcomes, so they can be ruled out. The option 'ground' doesn't make sense in this context.
5. 'Plants' can be ruled out because it doesn't make sense to say "every time she thought she was making grow something it would plants".
6. So, the only option left is 'die', and it fits the context of the sentence perfectly, indicating the unsuccessful attempts to grow plants. Hence, the answer is D: die.
So the final answer is D: die

Input: After a long night out the drunken man lost consciousness, he showed a sign of sickness right before passing out, what was it? Choices:  A: dream B: vomiting C: panic D: cancer E: blurred vision
Output: 1. The question is asking to determine the sign of sickness a man showed before passing out.
2. This requires an understanding of typical symptoms or signs that indicate an individual might be unwell or about to lose consciousness.
3. By analyzing the available choices, we can conclude that:

   - 'A: Dream' is an unlikely option, as dreams aren't a physical sign of sickness and usually occur once a person is already unconscious or asleep.
   - 'C: Panic' might be a psychological symptom in some circumstances, but it is not necessarily associated with physical illness or losing consciousness.
   - 'D: Cancer' is a disease that doesn't have instant or immediate signs particularly in relation to losing consciousness after a night out due to drinking.
   - 'E: Blurred Vision': Although this can be a symptom of many conditions, it still doesn't sufficiently represent a sign of illness before passing out specifically from drinking.
   
5. Based on these eliminations, 'B: Vomiting' is the most plausible answer. It's a common symptom of drinking excessively and can occur before a person loses consciousness.
So the final answer is B: vomiting

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i9-s10-rTrue-m4096
Evaluating commonsenseqa :   2%|▌                                  | 1/63 [01:00<1:02:35, 60.57s/it]Evaluating commonsenseqa :   3%|█                                  | 2/63 [01:59<1:00:29, 59.51s/it]Evaluating commonsenseqa :   5%|█▊                                   | 3/63 [02:58<59:26, 59.44s/it]Evaluating commonsenseqa :   6%|██▎                                  | 4/63 [03:57<58:18, 59.29s/it]Evaluating commonsenseqa :   8%|██▉                                  | 5/63 [04:56<57:10, 59.15s/it]Evaluating commonsenseqa :  10%|███▌                                 | 6/63 [05:56<56:17, 59.25s/it]Evaluating commonsenseqa :  11%|████                                 | 7/63 [06:55<55:11, 59.14s/it]Evaluating commonsenseqa :  13%|████▋                                | 8/63 [07:53<54:09, 59.09s/it]Evaluating commonsenseqa :  14%|█████▎                               | 9/63 [08:52<53:08, 59.05s/it]Evaluating commonsenseqa :  16%|█████▋                              | 10/63 [09:51<52:06, 58.99s/it]Evaluating commonsenseqa :  17%|██████▎                             | 11/63 [10:51<51:19, 59.22s/it]Evaluating commonsenseqa :  19%|██████▊                             | 12/63 [11:51<50:25, 59.33s/it]Evaluating commonsenseqa :  21%|███████▍                            | 13/63 [12:25<43:10, 51.82s/it]Evaluating commonsenseqa :  22%|████████                            | 14/63 [13:25<44:15, 54.19s/it]Evaluating commonsenseqa :  24%|████████▌                           | 15/63 [14:25<44:45, 55.95s/it]Evaluating commonsenseqa :  25%|█████████▏                          | 16/63 [15:24<44:41, 57.05s/it]Evaluating commonsenseqa :  27%|█████████▋                          | 17/63 [16:24<44:16, 57.74s/it]Evaluating commonsenseqa :  29%|██████████▎                         | 18/63 [17:23<43:36, 58.15s/it]Evaluating commonsenseqa :  30%|██████████▊                         | 19/63 [18:22<42:51, 58.44s/it]Evaluating commonsenseqa :  32%|███████████▍                        | 20/63 [19:21<42:04, 58.71s/it]Evaluating commonsenseqa :  33%|████████████                        | 21/63 [20:21<41:16, 58.96s/it]Evaluating commonsenseqa :  35%|████████████▌                       | 22/63 [21:20<40:23, 59.11s/it]Evaluating commonsenseqa :  37%|█████████████▏                      | 23/63 [22:20<39:24, 59.12s/it]Evaluating commonsenseqa :  38%|█████████████▋                      | 24/63 [23:19<38:29, 59.21s/it]Evaluating commonsenseqa :  40%|██████████████▎                     | 25/63 [24:18<37:28, 59.17s/it]Evaluating commonsenseqa :  41%|██████████████▊                     | 26/63 [25:18<36:37, 59.39s/it]Evaluating commonsenseqa :  43%|███████████████▍                    | 27/63 [26:17<35:35, 59.31s/it]Evaluating commonsenseqa :  44%|████████████████                    | 28/63 [27:16<34:33, 59.25s/it]Evaluating commonsenseqa :  46%|████████████████▌                   | 29/63 [28:16<33:37, 59.35s/it]Evaluating commonsenseqa :  48%|█████████████████▏                  | 30/63 [29:15<32:37, 59.32s/it]Evaluating commonsenseqa :  49%|█████████████████▋                  | 31/63 [30:14<31:36, 59.26s/it]Evaluating commonsenseqa :  51%|██████████████████▎                 | 32/63 [31:07<29:37, 57.34s/it]Evaluating commonsenseqa :  52%|██████████████████▊                 | 33/63 [32:06<28:59, 57.98s/it]Evaluating commonsenseqa :  54%|███████████████████▍                | 34/63 [32:54<26:32, 54.91s/it]Evaluating commonsenseqa :  56%|████████████████████                | 35/63 [33:54<26:15, 56.29s/it]Evaluating commonsenseqa :  57%|████████████████████▌               | 36/63 [34:53<25:46, 57.29s/it]Evaluating commonsenseqa :  59%|█████████████████████▏              | 37/63 [35:53<25:04, 57.86s/it]Evaluating commonsenseqa :  60%|█████████████████████▋              | 38/63 [36:51<24:14, 58.18s/it]Evaluating commonsenseqa :  62%|██████████████████████▎             | 39/63 [37:51<23:23, 58.49s/it]Evaluating commonsenseqa :  63%|██████████████████████▊             | 40/63 [38:51<22:35, 58.93s/it]Evaluating commonsenseqa :  65%|███████████████████████▍            | 41/63 [39:50<21:37, 58.96s/it]Evaluating commonsenseqa :  67%|████████████████████████            | 42/63 [40:49<20:41, 59.10s/it]Evaluating commonsenseqa :  68%|████████████████████████▌           | 43/63 [41:48<19:42, 59.12s/it]Evaluating commonsenseqa :  70%|█████████████████████████▏          | 44/63 [42:47<18:41, 59.04s/it]Evaluating commonsenseqa :  71%|█████████████████████████▋          | 45/63 [43:46<17:42, 59.05s/it]Evaluating commonsenseqa :  73%|██████████████████████████▎         | 46/63 [44:45<16:44, 59.08s/it]Evaluating commonsenseqa :  75%|██████████████████████████▊         | 47/63 [45:44<15:44, 59.05s/it]Evaluating commonsenseqa :  76%|███████████████████████████▍        | 48/63 [46:43<14:46, 59.08s/it]Evaluating commonsenseqa :  78%|████████████████████████████        | 49/63 [47:43<13:47, 59.08s/it]Evaluating commonsenseqa :  79%|████████████████████████████▌       | 50/63 [48:42<12:48, 59.15s/it]Evaluating commonsenseqa :  81%|█████████████████████████████▏      | 51/63 [49:41<11:49, 59.16s/it]Evaluating commonsenseqa :  83%|█████████████████████████████▋      | 52/63 [50:40<10:49, 59.07s/it]Evaluating commonsenseqa :  84%|██████████████████████████████▎     | 53/63 [51:39<09:51, 59.11s/it]Evaluating commonsenseqa :  86%|██████████████████████████████▊     | 54/63 [52:39<08:53, 59.30s/it]Evaluating commonsenseqa :  87%|███████████████████████████████▍    | 55/63 [53:38<07:55, 59.39s/it]Evaluating commonsenseqa :  89%|████████████████████████████████    | 56/63 [54:37<06:54, 59.28s/it]Evaluating commonsenseqa :  90%|████████████████████████████████▌   | 57/63 [55:37<05:55, 59.25s/it]Evaluating commonsenseqa :  92%|█████████████████████████████████▏  | 58/63 [56:25<04:40, 56.07s/it]Evaluating commonsenseqa :  94%|█████████████████████████████████▋  | 59/63 [57:24<03:47, 56.91s/it]Evaluating commonsenseqa :  95%|██████████████████████████████████▎ | 60/63 [58:23<02:52, 57.61s/it]Evaluating commonsenseqa :  97%|██████████████████████████████████▊ | 61/63 [59:23<01:56, 58.14s/it]Evaluating commonsenseqa :  98%|█████████████████████████████████▍| 62/63 [1:00:22<00:58, 58.44s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:00:58<00:00, 51.66s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:00:58<00:00, 58.07s/it]
name: commonsenseqa | avg. gen lenth: 446.012 | time: 3658.9307799339294s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-29 10:35:24,638] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 10:35:24,696] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 10:35:24,699] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 10:35:24,718] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s10-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 1
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s10-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                        | 0/9740 [00:00<?, ?it/s]Loading data: 100%|████████████████████████████████████████| 9740/9740 [00:00<00:00, 1256884.62it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:06<00:06,  6.92s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.10s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.11s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.15s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.14s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.55s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.15s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.60s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.19s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.62s/it]
[2023-08-29 10:35:35,110] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.22s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.66s/it]
 > number of parameters: 6738415616
[2023-08-29 10:35:35,167] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 10:35:35,223] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 10:35:35,299] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 10:35:35,871] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-29 10:35:35,873] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-29 10:35:35,874] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-29 10:35:35,874] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-29 10:35:35,874] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-29 10:35:35,874] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-29 10:35:35,874] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-29 10:35:35,874] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-29 10:35:35,874] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-29 10:35:35,874] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-29 10:35:35,874] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd0c1fd3640>
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-29 10:35:35,875] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-29 10:35:35,876] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-29 10:35:35,876] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                             | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: B: tailor shop

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s10-rFalse-m4096
Evaluating commonsenseqa :   2%|▌                                 | 1/63 [01:43<1:47:23, 103.92s/it]Evaluating commonsenseqa :   3%|█                                 | 2/63 [03:41<1:53:43, 111.87s/it]Evaluating commonsenseqa :   5%|█▌                                | 3/63 [05:38<1:54:04, 114.08s/it]Evaluating commonsenseqa :   6%|██▏                               | 4/63 [07:32<1:52:15, 114.17s/it]Evaluating commonsenseqa :   8%|██▋                               | 5/63 [09:26<1:50:16, 114.08s/it]Evaluating commonsenseqa :  10%|███▏                              | 6/63 [11:20<1:48:30, 114.23s/it]Evaluating commonsenseqa :  11%|███▊                              | 7/63 [13:14<1:46:32, 114.16s/it]Evaluating commonsenseqa :  13%|████▎                             | 8/63 [15:06<1:43:52, 113.31s/it]Evaluating commonsenseqa :  14%|█████                              | 9/63 [15:43<1:20:29, 89.44s/it]Evaluating commonsenseqa :  16%|█████▍                            | 10/63 [17:40<1:26:27, 97.88s/it]Evaluating commonsenseqa :  17%|█████▊                           | 11/63 [19:34<1:29:10, 102.90s/it]Evaluating commonsenseqa :  19%|██████▍                           | 12/63 [21:01<1:23:23, 98.11s/it]Evaluating commonsenseqa :  21%|██████▊                          | 13/63 [22:55<1:25:49, 103.00s/it]Evaluating commonsenseqa :  22%|███████▌                          | 14/63 [23:49<1:11:53, 88.03s/it]Evaluating commonsenseqa :  24%|████████                          | 15/63 [25:44<1:17:02, 96.31s/it]Evaluating commonsenseqa :  25%|████████▍                        | 16/63 [27:38<1:19:36, 101.63s/it]Evaluating commonsenseqa :  27%|████████▉                        | 17/63 [29:31<1:20:28, 104.96s/it]Evaluating commonsenseqa :  29%|█████████▍                       | 18/63 [31:27<1:21:13, 108.30s/it]Evaluating commonsenseqa :  30%|█████████▉                       | 19/63 [32:48<1:13:24, 100.10s/it]Evaluating commonsenseqa :  32%|██████████▊                       | 20/63 [34:21<1:10:10, 97.92s/it]Evaluating commonsenseqa :  33%|███████████                      | 21/63 [36:15<1:11:52, 102.68s/it]Evaluating commonsenseqa :  35%|███████████▌                     | 22/63 [38:09<1:12:29, 106.09s/it]Evaluating commonsenseqa :  37%|████████████                     | 23/63 [40:04<1:12:33, 108.84s/it]Evaluating commonsenseqa :  38%|████████████▌                    | 24/63 [41:59<1:11:58, 110.72s/it]Evaluating commonsenseqa :  40%|█████████████                    | 25/63 [43:51<1:10:28, 111.27s/it]Evaluating commonsenseqa :  41%|██████████████▊                     | 26/63 [44:40<57:01, 92.46s/it]Evaluating commonsenseqa :  43%|███████████████▍                    | 27/63 [46:32<59:02, 98.40s/it]Evaluating commonsenseqa :  44%|████████████████                    | 28/63 [47:35<51:12, 87.80s/it]Evaluating commonsenseqa :  46%|████████████████▌                   | 29/63 [48:31<44:14, 78.07s/it]Evaluating commonsenseqa :  48%|█████████████████▏                  | 30/63 [50:25<48:54, 88.91s/it]Evaluating commonsenseqa :  49%|█████████████████▋                  | 31/63 [52:13<50:27, 94.61s/it]Evaluating commonsenseqa :  51%|█████████████████▊                 | 32/63 [54:06<51:41, 100.06s/it]Evaluating commonsenseqa :  52%|██████████████████▊                 | 33/63 [55:15<45:25, 90.84s/it]Evaluating commonsenseqa :  54%|███████████████████▍                | 34/63 [57:07<46:58, 97.17s/it]Evaluating commonsenseqa :  56%|███████████████████▍               | 35/63 [58:59<47:25, 101.63s/it]Evaluating commonsenseqa :  57%|██████████████████▊              | 36/63 [1:00:53<47:26, 105.42s/it]Evaluating commonsenseqa :  59%|███████████████████▉              | 37/63 [1:02:01<40:47, 94.13s/it]Evaluating commonsenseqa :  60%|████████████████████▌             | 38/63 [1:03:17<37:00, 88.81s/it]Evaluating commonsenseqa :  62%|█████████████████████             | 39/63 [1:05:01<37:19, 93.33s/it]Evaluating commonsenseqa :  63%|█████████████████████▌            | 40/63 [1:06:56<38:12, 99.66s/it]Evaluating commonsenseqa :  65%|█████████████████████▍           | 41/63 [1:08:48<37:55, 103.44s/it]Evaluating commonsenseqa :  67%|██████████████████████           | 42/63 [1:10:44<37:33, 107.31s/it]Evaluating commonsenseqa :  68%|██████████████████████▌          | 43/63 [1:12:39<36:30, 109.55s/it]Evaluating commonsenseqa :  70%|███████████████████████          | 44/63 [1:14:32<35:01, 110.59s/it]Evaluating commonsenseqa :  71%|███████████████████████▌         | 45/63 [1:16:27<33:31, 111.76s/it]Evaluating commonsenseqa :  73%|████████████████████████         | 46/63 [1:18:11<31:02, 109.57s/it]Evaluating commonsenseqa :  75%|████████████████████████▌        | 47/63 [1:20:06<29:38, 111.16s/it]Evaluating commonsenseqa :  76%|█████████████████████████▉        | 48/63 [1:20:57<23:15, 93.02s/it]Evaluating commonsenseqa :  78%|██████████████████████████▍       | 49/63 [1:22:12<20:29, 87.82s/it]Evaluating commonsenseqa :  79%|██████████████████████████▉       | 50/63 [1:24:07<20:46, 95.85s/it]Evaluating commonsenseqa :  81%|██████████████████████████▋      | 51/63 [1:25:59<20:08, 100.74s/it]Evaluating commonsenseqa :  83%|████████████████████████████      | 52/63 [1:26:46<15:30, 84.61s/it]Evaluating commonsenseqa :  84%|████████████████████████████▌     | 53/63 [1:28:42<15:38, 93.89s/it]Evaluating commonsenseqa :  86%|████████████████████████████▎    | 54/63 [1:30:37<15:04, 100.48s/it]Evaluating commonsenseqa :  87%|████████████████████████████▊    | 55/63 [1:32:31<13:55, 104.47s/it]Evaluating commonsenseqa :  89%|█████████████████████████████▎   | 56/63 [1:34:23<12:26, 106.62s/it]Evaluating commonsenseqa :  90%|█████████████████████████████▊   | 57/63 [1:36:19<10:56, 109.43s/it]Evaluating commonsenseqa :  92%|██████████████████████████████▍  | 58/63 [1:38:15<09:17, 111.54s/it]Evaluating commonsenseqa :  94%|██████████████████████████████▉  | 59/63 [1:40:09<07:29, 112.30s/it]Evaluating commonsenseqa :  95%|███████████████████████████████▍ | 60/63 [1:42:07<05:41, 113.99s/it]Evaluating commonsenseqa :  97%|███████████████████████████████▉ | 61/63 [1:43:58<03:46, 113.09s/it]Evaluating commonsenseqa :  98%|████████████████████████████████▍| 62/63 [1:45:52<01:53, 113.38s/it]Evaluating commonsenseqa : 100%|██████████████████████████████████| 63/63 [1:46:54<00:00, 97.96s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:46:54<00:00, 101.82s/it]
name: commonsenseqa | avg. gen lenth: 283.928 | time: 6415.066477298737s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 4 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-29 12:27:08,931] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 12:27:08,937] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 12:27:08,979] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 12:27:08,998] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s10-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 2
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s10-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                        | 0/9739 [00:00<?, ?it/s]Loading data: 100%|████████████████████████████████████████| 9739/9739 [00:00<00:00, 1163471.66it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:06<00:06,  6.96s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.01s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.07s/it]Loading checkpoint shards:  50%|███████████████████                   | 1/2 [00:07<00:07,  7.09s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.11s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.54s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.09s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.53s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.13s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.57s/it]
[2023-08-29 12:27:19,361] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 12:27:19,380] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 12:27:19,428] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.18s/it]Loading checkpoint shards: 100%|██████████████████████████████████████| 2/2 [00:09<00:00,  4.61s/it]
 > number of parameters: 6738415616
[2023-08-29 12:27:19,631] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-29 12:27:20,172] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-29 12:27:20,177] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-29 12:27:20,178] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-29 12:27:20,178] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-29 12:27:20,178] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-29 12:27:20,178] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-29 12:27:20,179] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-29 12:27:20,179] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-29 12:27:20,179] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-29 12:27:20,179] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-29 12:27:20,179] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-29 12:27:20,179] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc1fa047610>
[2023-08-29 12:27:20,179] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-29 12:27:20,179] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-29 12:27:20,180] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-29 12:27:20,180] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-29 12:27:20,180] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-29 12:27:20,180] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-29 12:27:20,180] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-29 12:27:20,180] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-29 12:27:20,180] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-29 12:27:20,180] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-29 12:27:20,180] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-29 12:27:20,180] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-29 12:27:20,180] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-29 12:27:20,180] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-29 12:27:20,180] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-29 12:27:20,180] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-29 12:27:20,180] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-29 12:27:20,181] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-29 12:27:20,181] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-29 12:27:20,181] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-29 12:27:20,181] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-29 12:27:20,181] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-29 12:27:20,181] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-29 12:27:20,181] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-29 12:27:20,181] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-29 12:27:20,181] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-29 12:27:20,181] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-29 12:27:20,181] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-29 12:27:20,181] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-29 12:27:20,182] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-29 12:27:20,182] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-29 12:27:20,182] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-29 12:27:20,182] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-29 12:27:20,182] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-29 12:27:20,182] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-29 12:27:20,182] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-29 12:27:20,182] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-29 12:27:20,182] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-29 12:27:20,182] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-29 12:27:20,182] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-29 12:27:20,183] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-29 12:27:20,183] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-29 12:27:20,183] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-29 12:27:20,183] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-29 12:27:20,183] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-29 12:27:20,183] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-29 12:27:20,183] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-29 12:27:20,183] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-29 12:27:20,183] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-29 12:27:20,183] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-29 12:27:20,183] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-29 12:27:20,183] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-29 12:27:20,183] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-29 12:27:20,183] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-29 12:27:20,183] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-29 12:27:20,184] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-29 12:27:20,184] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-29 12:27:20,184] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-29 12:27:20,184] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-29 12:27:20,184] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                             | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: B: tailor shop

Input: Where are you if your reading magazines while waiting for a vehicle on rails? Choices:  A: vegetables B: market C: doctor D: train station E: bookstore
Output: D: train station

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s10-rFalse-m4096
Evaluating commonsenseqa :   2%|▌                                    | 1/63 [00:24<24:52, 24.08s/it]Evaluating commonsenseqa :   3%|█                                  | 2/63 [02:16<1:17:29, 76.22s/it]Evaluating commonsenseqa :   5%|█▋                                 | 3/63 [03:03<1:02:42, 62.71s/it]Evaluating commonsenseqa :   6%|██▏                                | 4/63 [04:58<1:22:06, 83.49s/it]Evaluating commonsenseqa :   8%|██▊                                | 5/63 [06:17<1:19:08, 81.87s/it]Evaluating commonsenseqa :  10%|███▎                               | 6/63 [08:11<1:28:06, 92.75s/it]Evaluating commonsenseqa :  11%|███▉                               | 7/63 [09:35<1:23:53, 89.88s/it]Evaluating commonsenseqa :  13%|████▍                              | 8/63 [11:28<1:29:12, 97.33s/it]Evaluating commonsenseqa :  14%|████▊                             | 9/63 [13:21<1:31:53, 102.10s/it]Evaluating commonsenseqa :  16%|█████▏                           | 10/63 [15:16<1:33:44, 106.13s/it]Evaluating commonsenseqa :  17%|█████▊                           | 11/63 [17:03<1:32:12, 106.39s/it]Evaluating commonsenseqa :  19%|██████▎                          | 12/63 [18:56<1:32:01, 108.27s/it]Evaluating commonsenseqa :  21%|███████                           | 13/63 [19:57<1:18:28, 94.18s/it]Evaluating commonsenseqa :  22%|███████▌                          | 14/63 [21:49<1:21:06, 99.32s/it]Evaluating commonsenseqa :  24%|████████                          | 15/63 [22:22<1:03:30, 79.39s/it]Evaluating commonsenseqa :  25%|████████▋                         | 16/63 [24:18<1:10:58, 90.60s/it]Evaluating commonsenseqa :  27%|█████████▏                        | 17/63 [26:12<1:14:43, 97.46s/it]Evaluating commonsenseqa :  29%|█████████▍                       | 18/63 [28:05<1:16:32, 102.05s/it]Evaluating commonsenseqa :  30%|██████████▎                       | 19/63 [28:49<1:02:12, 84.82s/it]Evaluating commonsenseqa :  32%|██████████▊                       | 20/63 [30:43<1:07:02, 93.55s/it]Evaluating commonsenseqa :  33%|███████████▎                      | 21/63 [32:20<1:06:15, 94.65s/it]