torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i0-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 0
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:26:57,991] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:26:57,996] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:26:58,010] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:26:58,025] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:04,347] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:04,363] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:04,369] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:27:04,919] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:11,188] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:11,314] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:11,314] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:27:11,942] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:18,248] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:18,743] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:18,756] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:18,793] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:25,539] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:25,557] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:26,053] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:26,054] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:32,406] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:32,407] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:32,912] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:32,913] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:39,331] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:39,343] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:27:39,806] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:39,932] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:46,257] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:46,281] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:27:46,799] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:46,835] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:27:53,168] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:53,169] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:27:53,182] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:27:53,692] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:00,111] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:28:00,629] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:00,630] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:00,632] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:07,052] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:07,053] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:07,053] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:28:07,581] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:13,988] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:14,506] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:14,507] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:14,514] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:20,914] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:20,952] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:21,415] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:21,450] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:27,858] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:27,867] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:28,371] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:28,377] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:34,779] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:35,316] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:35,318] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:35,342] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:41,708] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:41,708] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:42,228] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:42,228] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:48,522] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:48,652] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:28:49,082] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:49,175] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:28:55,482] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:55,607] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:55,607] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:28:55,607] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s1-rFalse --seed 1 --max-prompt-length 4096 --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:02,535] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:29:03,044] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:03,066] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:03,067] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:09,406] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:09,516] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:09,516] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:29:10,038] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:16,475] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:16,476] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:29:16,978] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:16,998] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:23,382] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:23,382] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:23,401] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:23,953] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:30,226] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:30,335] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:30,335] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:29:30,848] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:37,271] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:37,802] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:37,804] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:37,810] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:44,214] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:44,231] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:44,710] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:44,723] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:51,173] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:51,173] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:51,678] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:51,695] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:29:58,136] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:58,145] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:29:58,657] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:29:58,658] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:05,042] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:05,064] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:05,083] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:05,681] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:11,969] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:12,485] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:12,485] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:12,494] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:18,897] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:18,918] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:30:19,415] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:19,425] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:25,869] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:25,869] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:25,881] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:26,388] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:32,807] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:32,808] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:33,251] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:33,334] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:39,738] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:39,738] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:39,739] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:30:40,263] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...Answers already exist, exiting...

Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i6-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:46,689] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:46,705] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:30:47,212] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:47,224] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i7-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:30:53,540] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:30:54,186] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:54,186] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:30:54,186] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
Answers already exist, exiting...
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:31:00,581] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:31:00,582] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:31:00,582] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2023-08-30 00:31:01,112] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i8-s10-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 8
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s10-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9733 [00:00<?, ?it/s]Loading data: 100%|| 9733/9733 [00:00<00:00, 864384.70it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.22s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.33s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.78s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.77s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.14s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.61s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.20s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.67s/it]
[2023-08-30 00:31:11,486] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 00:31:11,597] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.35s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.86s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.36s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.87s/it]
[2023-08-30 00:31:12,005] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-30 00:31:12,018] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 00:31:12,580] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 00:31:12,581] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 00:31:12,581] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 00:31:12,581] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 00:31:12,581] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 00:31:12,581] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd3fbec52d0>
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 00:31:12,582] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 00:31:12,583] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 00:31:12,583] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: B: tailor shop

Input: Where are you if your reading magazines while waiting for a vehicle on rails? Choices:  A: vegetables B: market C: doctor D: train station E: bookstore
Output: D: train station

Input: What would need oil to be used? Choices:  A: ground B: human  body C: repair shop D: combustion engines E: service station
Output: D: combustion engines

Input: What is person probably feeling that plans on stopping being married to their spouse? Choices:  A: detachment B: bankruptcy C: sad D: fights E: wrong
Output: A: detachment

Input: What could you use to store a clock? Choices:  A: shelf B: own bedroom C: desk D: wall E: car
Output: A: shelf

Input: The person put on lotion, what did they want? Choices:  A: fresh smell B: good credit C: smooth skin D: fresh produce E: headache
Output: C: smooth skin

Input: They burned the record, they were trying to do what to history? Choices:  A: compact disc B: tape C: rewrite D: play music E: erase
Output: E: erase

Input: She sure didn't have a green thumb, every time she thought she was making grow something it would what? Choices:  A: growth B: flowering C: ground D: die E: plants
Output: D: die

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i8-s10-rFalse-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [02:08<1:44:44, 128.25s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [04:12<1:40:56, 126.18s/it]Evaluating commonsenseqa :   6%|                                                                  | 3/50 [05:08<1:13:25, 93.74s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [07:13<1:21:33, 106.39s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [09:19<1:24:56, 113.25s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [11:26<1:26:24, 117.84s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [12:35<1:13:08, 102.05s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [14:43<1:17:17, 110.42s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [16:51<1:19:05, 115.75s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [18:58<1:19:29, 119.24s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [21:03<1:18:42, 121.09s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [23:09<1:17:40, 122.64s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [25:15<1:16:08, 123.47s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [27:24<1:15:04, 125.12s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [29:30<1:13:08, 125.38s/it]Evaluating commonsenseqa :  32%|                                                | 16/50 [30:20<58:15, 102.81s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [32:29<1:00:48, 110.57s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [34:34<1:01:16, 114.89s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [36:39<1:01:03, 118.17s/it]Evaluating commonsenseqa :  40%|                                          | 20/50 [38:05<54:15, 108.51s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [40:11<54:51, 113.49s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [42:15<54:32, 116.89s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [44:24<54:08, 120.30s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [46:31<53:03, 122.45s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [48:39<51:41, 124.05s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [50:46<49:56, 124.84s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [52:52<48:01, 125.27s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [55:00<46:12, 126.00s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [57:06<44:10, 126.21s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [59:16<42:24, 127.23s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:01:22<40:13, 127.03s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:03:26<37:46, 125.93s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:05:31<35:35, 125.60s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:07:36<33:28, 125.55s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:09:45<31:36, 126.43s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:11:51<29:30, 126.47s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:13:58<27:26, 126.67s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:16:06<25:23, 126.95s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:18:13<23:18, 127.14s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:20:22<21:14, 127.50s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:22:30<19:10, 127.82s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:24:39<17:05, 128.17s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:26:10<13:38, 116.87s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:28:18<12:01, 120.33s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:30:23<10:08, 121.76s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:32:33<08:16, 124.07s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:34:35<06:10, 123.54s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:36:40<04:08, 124.06s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:38:15<01:55, 115.32s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:40:24<00:00, 119.36s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:40:24<00:00, 120.49s/it]
name: commonsenseqa | avg. gen lenth: 272.64 | time: 6024.883975982666s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s10-rFalse --seed 10 --max-prompt-length 4096 --num-in-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 02:11:44,346] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 02:11:44,523] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 02:11:44,608] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 02:11:44,730] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i9-s10-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 9
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i9-s10-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9732 [00:00<?, ?it/s]Loading data: 100%|| 9732/9732 [00:00<00:00, 817638.49it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.15s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.26s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.33s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.40s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.24s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.68s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.29s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.74s/it]
[2023-08-30 02:11:55,401] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.32s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.77s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.32s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.78s/it]
[2023-08-30 02:11:55,523] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 02:11:55,548] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-30 02:11:55,575] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 02:11:56,150] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 02:11:56,152] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0ca2ab92d0>
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 02:11:56,152] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 02:11:56,153] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 02:11:56,154] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 02:11:56,154] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Fabric is cut to order at what type of seller? Choices:  A: curtains B: tailor shop C: clothing store D: sewing room E: hardware store
Output: B: tailor shop

Input: Where are you if your reading magazines while waiting for a vehicle on rails? Choices:  A: vegetables B: market C: doctor D: train station E: bookstore
Output: D: train station

Input: What would need oil to be used? Choices:  A: ground B: human  body C: repair shop D: combustion engines E: service station
Output: D: combustion engines

Input: What is person probably feeling that plans on stopping being married to their spouse? Choices:  A: detachment B: bankruptcy C: sad D: fights E: wrong
Output: A: detachment

Input: What could you use to store a clock? Choices:  A: shelf B: own bedroom C: desk D: wall E: car
Output: A: shelf

Input: The person put on lotion, what did they want? Choices:  A: fresh smell B: good credit C: smooth skin D: fresh produce E: headache
Output: C: smooth skin

Input: They burned the record, they were trying to do what to history? Choices:  A: compact disc B: tape C: rewrite D: play music E: erase
Output: E: erase

Input: She sure didn't have a green thumb, every time she thought she was making grow something it would what? Choices:  A: growth B: flowering C: ground D: die E: plants
Output: D: die

Input: After a long night out the drunken man lost consciousness, he showed a sign of sickness right before passing out, what was it? Choices:  A: dream B: vomiting C: panic D: cancer E: blurred vision
Output: B: vomiting

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i9-s10-rFalse-m4096
Evaluating commonsenseqa :   2%|                                                                       | 1/50 [00:46<37:44, 46.21s/it]Evaluating commonsenseqa :   4%|                                                                    | 2/50 [02:50<1:13:38, 92.06s/it]Evaluating commonsenseqa :   6%|                                                                  | 3/50 [04:20<1:11:24, 91.17s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [06:30<1:21:32, 106.36s/it]Evaluating commonsenseqa :  10%|                                                                | 5/50 [07:58<1:14:52, 99.84s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [10:02<1:19:20, 108.18s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [12:05<1:20:59, 113.02s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [14:12<1:22:15, 117.52s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [16:19<1:22:10, 120.25s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [18:23<1:21:01, 121.53s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [20:30<1:20:02, 123.14s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [22:37<1:18:46, 124.39s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [24:44<1:17:13, 125.23s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [26:49<1:15:03, 125.08s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [28:58<1:13:34, 126.14s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [31:04<1:11:35, 126.34s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [33:10<1:09:23, 126.15s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [35:16<1:07:11, 125.98s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [37:22<1:05:05, 125.97s/it]Evaluating commonsenseqa :  40%|                                         | 20/50 [39:28<1:03:05, 126.18s/it]Evaluating commonsenseqa :  42%|                                        | 21/50 [41:34<1:00:57, 126.13s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [43:41<58:54, 126.23s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [45:47<56:49, 126.29s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [47:23<50:48, 117.27s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [49:28<49:46, 119.47s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [51:34<48:35, 121.50s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [53:38<46:50, 122.19s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [55:44<45:14, 123.40s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [57:49<43:21, 123.88s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [59:53<41:19, 123.95s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:01:58<39:18, 124.13s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:04:03<37:20, 124.45s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:06:07<35:09, 124.12s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:08:14<33:21, 125.09s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:10:21<31:24, 125.61s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:12:22<29:00, 124.30s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:14:26<26:55, 124.31s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:16:29<24:46, 123.89s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:18:36<22:51, 124.67s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:20:43<20:56, 125.62s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:22:46<18:41, 124.58s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:24:49<16:34, 124.30s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:26:57<14:36, 125.25s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:29:01<12:30, 125.01s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:30:18<09:12, 110.42s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:32:25<07:41, 115.41s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:34:30<05:55, 118.43s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:36:36<04:01, 120.56s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:38:43<02:02, 122.74s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:40:50<00:00, 123.75s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:40:50<00:00, 121.00s/it]
name: commonsenseqa | avg. gen lenth: 266.952 | time: 6050.371901988983s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 03:52:53,240] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 03:52:53,292] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 03:52:53,307] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 03:52:53,312] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i1-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 1
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9740 [00:00<?, ?it/s]Loading data: 100%|| 9740/9740 [00:00<00:00, 1244896.42it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.95s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.05s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.21s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.64s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.12s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.56s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.18s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.60s/it]
[2023-08-30 03:53:03,677] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-30 03:53:03,723] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.25s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.69s/it]
[2023-08-30 03:53:03,978] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.51s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.98s/it]
[2023-08-30 03:53:04,611] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 03:53:05,162] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 03:53:05,163] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 03:53:05,163] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f01bd7c1300>
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 03:53:05,164] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 03:53:05,165] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 03:53:05,165] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. Let's analyze each answer choice in the context of the sentence.
2. The sentence says that the machine was'very intricate'. It means the machine has complex and detailed features.
3. First, see Option A: 'box'. A box is a simple container and it does not convey the intricate mechanisms or complexity of a machine.
4. Option D: 'wash dishes' does not fit the context as it's related to a very specific function and does not effectively describe the complexity of a machine.
5. Option C: 'appliance'. This usually refers to a device, like a refrigerator or a toaster, doing practical work, but the level of intricacy or complexity in the usual sense of the term 'appliance' is not as high as the context demands.
6. Option E: 'implement'. An implement is a tool or instrument, which might be intricate, but this word is typically used for hand-held things and wouldn't usually describe a complex machine.
7. Option B: 'apparatus'. An apparatus is often used to refer to a complex instrument or machine with a particular purpose. 
8. Therefore, considering all the option meanings and their contextual relevance, we can conclude that the best choice to complete the sentence is B: 'apparatus' as it signifies the necessary complexity and intricacy the sentence demands.
So the final answer is B: apparatus

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i1-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [02:07<1:44:29, 127.95s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [04:15<1:42:23, 128.00s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [06:22<1:39:53, 127.53s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [08:30<1:37:44, 127.50s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [10:35<1:35:05, 126.78s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [12:41<1:32:47, 126.53s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [14:48<1:30:35, 126.41s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [16:56<1:28:51, 126.93s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [19:02<1:26:36, 126.75s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [21:10<1:24:40, 127.01s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [23:16<1:22:28, 126.89s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [25:22<1:20:14, 126.70s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [27:32<1:18:42, 127.62s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [29:38<1:16:17, 127.14s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [31:44<1:13:58, 126.80s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [33:51<1:11:47, 126.68s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [35:57<1:09:32, 126.44s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [38:04<1:07:31, 126.60s/it]Evaluating commonsenseqa :  38%|                                          | 19/50 [40:02<1:04:07, 124.12s/it]Evaluating commonsenseqa :  40%|                                         | 20/50 [42:07<1:02:17, 124.57s/it]Evaluating commonsenseqa :  42%|                                        | 21/50 [44:13<1:00:17, 124.75s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [46:19<58:26, 125.24s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [48:25<56:28, 125.49s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [50:32<54:33, 125.91s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [52:37<52:23, 125.75s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [54:46<50:39, 126.66s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [56:51<48:22, 126.21s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [58:57<46:11, 125.95s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [1:01:04<44:15, 126.48s/it]Evaluating commonsenseqa :  60%|                           | 30/50 [1:03:11<42:09, 126.50s/it]Evaluating commonsenseqa :  62%|                          | 31/50 [1:05:19<40:13, 127.02s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:07:26<38:03, 126.85s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:09:32<35:54, 126.73s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:11:38<33:46, 126.63s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:13:44<31:34, 126.30s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:15:40<28:44, 123.19s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:17:49<27:05, 125.00s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:19:56<25:05, 125.46s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:22:02<23:04, 125.86s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:24:13<21:13, 127.38s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:26:21<19:06, 127.42s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:28:27<16:56, 127.11s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:30:35<14:51, 127.40s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:32:42<12:42, 127.13s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:34:48<10:33, 126.78s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:36:56<08:28, 127.05s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:39:03<06:21, 127.06s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:41:08<04:13, 126.70s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:43:14<02:06, 126.36s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:45:22<00:00, 126.73s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:45:22<00:00, 126.44s/it]
name: commonsenseqa | avg. gen lenth: 390.652 | time: 6322.509834051132s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 05:38:37,184] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 05:38:37,187] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 05:38:37,190] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 05:38:37,205] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i2-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 2
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9739 [00:00<?, ?it/s]Loading data: 100%|| 9739/9739 [00:00<00:00, 667849.17it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.75s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.36s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.38s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.39s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  3.99s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.40s/it]
[2023-08-30 05:38:47,477] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.27s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.73s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.29s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.75s/it]
 > number of parameters: 6738415616
[2023-08-30 05:38:48,078] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 05:38:48,140] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.40s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.85s/it]
[2023-08-30 05:38:48,329] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 05:38:48,955] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 05:38:48,957] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 05:38:48,957] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 05:38:48,957] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 05:38:48,957] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 05:38:48,957] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f86c33bd2a0>
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 05:38:48,958] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 05:38:48,959] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 05:38:48,960] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. The key to answering this question correctly is understanding the context and meaning of the question. The sentence is describing a machine that is said to be very intricate.
2. "Intricate" means that something is complicated or detailed. So, the word that follows should reasonably describe a detailed or complex machine.
3. We can now evaluate each of the multiple-choice options in light of this reasoning. In this case, the options are A: box, B: apparatus, C: appliance, D: wash dishes, and E: implement.
4. Option A, box, is unlikely because a box is a simple, generally not intricate item.
5. Option C, appliance, generally refers to devices for particular uses in the home, like a dishwasher. However, appliances aren't necessarily intricate or complex.
6. Option D, wash dishes, is not correct because it is not a noun but a verb phrase and doesn't follow grammatically after "an."
7. Option E, implement, usually refers to a tool or piece of equipment, but doesn't inherently suggest something intricate.
8. This leaves us with option B: apparatus, which is a technical term for a complex device or piece of equipment. It fits best with the description of the machine being intricate.

Therefore, the correct answer is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: Step 1: Identify the question and understand what it's asking. The question is "Where do you get petrol?"
Step 2: Consider the context of the question. It's asking for a location, so we can exclude any answers that are not places.
Step 3: Eliminate answer choice A (burn hot) because it isn't a place where you can get petrol
Step 4: Eliminate answer choice B (fuel tank) because while petrol can be stored there, it's not the place where you obtain it from.
Step 5: Eliminate answer choice C (burn hot) again because it isn't a place where you can get petrol.
Step 6: Eliminate answer choice D (car) because similar to a fuel tank, a car may store petrol but it's not the place where you get it from.
Step 7: By process of elimination, you're left with answer E (gas station) which is indeed a place where you can get petrol. 
Step 8: So, the answer is E: gas station.
So the final answer is E: gas station

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i2-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [01:58<1:36:26, 118.08s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [03:55<1:34:00, 117.51s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [05:51<1:31:38, 116.99s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [07:47<1:29:18, 116.50s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [09:44<1:27:30, 116.67s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [11:42<1:25:49, 117.03s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [13:39<1:23:56, 117.12s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [15:28<1:20:09, 114.51s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [17:24<1:18:43, 115.20s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [19:23<1:17:25, 116.14s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [21:23<1:16:16, 117.35s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [23:19<1:14:09, 117.08s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [25:00<1:09:03, 111.98s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [26:57<1:08:15, 113.76s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [28:56<1:07:10, 115.16s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [30:52<1:05:22, 115.36s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [32:47<1:03:30, 115.48s/it]Evaluating commonsenseqa :  36%|                                            | 18/50 [34:43<1:01:38, 115.59s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [36:38<59:37, 115.41s/it]Evaluating commonsenseqa :  40%|                                          | 20/50 [38:35<57:52, 115.74s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [40:32<56:10, 116.21s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [42:28<54:11, 116.11s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [44:24<52:16, 116.15s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [46:21<50:25, 116.36s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [48:00<46:15, 111.02s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [49:55<44:56, 112.35s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [51:51<43:27, 113.35s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [53:48<42:01, 114.64s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [55:47<40:34, 115.93s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [57:46<38:56, 116.80s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [59:42<36:52, 116.42s/it]Evaluating commonsenseqa :  64%|                        | 32/50 [1:01:38<34:53, 116.28s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:03:35<33:03, 116.69s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:05:34<31:15, 117.25s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:07:32<29:22, 117.53s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:09:30<27:25, 117.55s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:11:28<25:30, 117.72s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:13:25<23:30, 117.51s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:15:21<21:29, 117.27s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:17:17<19:28, 116.85s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:19:15<17:32, 116.99s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:21:11<15:33, 116.69s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:23:05<13:31, 115.88s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:25:02<11:37, 116.29s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:26:57<09:40, 116.03s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:28:55<07:45, 116.41s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:30:50<05:48, 116.21s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:32:48<03:53, 116.56s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:34:44<01:56, 116.36s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:36:40<00:00, 116.34s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:36:40<00:00, 116.01s/it]
name: commonsenseqa | avg. gen lenth: 361.584 | time: 5800.766259670258s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 07:15:45,595] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 07:15:45,595] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 07:15:45,599] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 07:15:45,612] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i3-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 3
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i3-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9738 [00:00<?, ?it/s]Loading data: 100%|| 9738/9738 [00:00<00:00, 590301.37it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.83s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.89s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.92s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.15s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.05s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.46s/it]
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.08s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.50s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.11s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.53s/it]
[2023-08-30 07:15:55,936] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 07:15:56,015] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.15s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.60s/it]
 > number of parameters: 6738415616
[2023-08-30 07:15:56,083] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 07:15:56,215] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 07:15:56,823] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 07:15:56,825] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f92b9bc52d0>
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 07:15:56,825] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 07:15:56,826] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 07:15:56,826] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. To answer this question, it is necessary to understand the context of a complex, detailed machine. 
2. Equally important is to know the meanings of the provided choices, which are: a box, an apparatus, an appliance, washing dishes, and an implement.
3. Let's go through the choices by their definition:
   - A box is simply a container, which doesn't describe something intricate.
   - An apparatus encompasses a set of machines or equipment designed for a particular function, which could be intricate.
   - An appliance refers to a device or piece of equipment designed to perform a specific task, typically a household task. The definition does not imply intricacy.
   - "Wash dishes" is an action, not a descriptor for a complex machine.
   - An implement is a tool, utensil, or other piece of equipment, which usually refers to simpler tools, not intricate machines.
4. Keeping the given context and definitions in mind, it is clear that the word 'apparatus' best fits the description of a machine that is'very intricate'.
5. Therefore, the correct answer is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: 1. The question asks for the location where you typically obtain petrol.
2. Petrol is a type of fuel used to run cars and other automobiles.
3. Choices A and C, "burn hot," are incorrect because they are not locations nor related to obtaining petrol.
4. Choice B, "fuel tank," is the part of the car where petrol is stored and not the place where it is procured.
5. Choice D, "car," is also incorrect because we don't get petrol from the car itself; instead, we add petrol to the car.
6. Therefore, the correct answer is choice E, "gas station," because this is the location where you can buy and get petrol for your car.
So the final answer is E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: 1. First, we consider the fact that a television is a piece of technology that needs to be plugged into an electricity source to be used.
2. A television is also typically used indoors due to the need to protect it from weather and theft.
3. With this in mind, we can eliminate option A: 'cabinet' because it is unlikely to be used there, and 'woods' because a television is typically not used outdoors.
4. We are left with 'house', 'apartment' and 'bedroom'. All these places can have electricity and are inside a building.
5. However, considering the word 'night' in the question suggests the location is likely to be a place used during the night.
6. While 'house' and 'apartment' could be correct, they are not specific enough  as they consist of many rooms.
7. 'Bedroom' is the most likely answer because it is common to watch television in a bedroom at night. Hence, the answer is D: bedroom.
So the final answer is D: bedroom

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i3-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [01:53<1:32:44, 113.57s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [03:44<1:29:23, 111.74s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [05:35<1:27:15, 111.40s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [07:26<1:25:29, 111.51s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [08:55<1:17:23, 103.18s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [10:44<1:17:13, 105.31s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [12:34<1:16:34, 106.84s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [14:24<1:15:25, 107.76s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [16:13<1:14:01, 108.34s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [18:03<1:12:34, 108.87s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [19:54<1:11:06, 109.40s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [21:46<1:09:41, 110.04s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [23:36<1:07:54, 110.11s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [25:25<1:05:53, 109.82s/it]Evaluating commonsenseqa :  30%|                                                | 15/50 [27:16<1:04:20, 110.29s/it]Evaluating commonsenseqa :  32%|                                               | 16/50 [29:05<1:02:16, 109.89s/it]Evaluating commonsenseqa :  34%|                                             | 17/50 [30:55<1:00:27, 109.91s/it]Evaluating commonsenseqa :  36%|                                             | 18/50 [32:44<58:26, 109.59s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [34:36<56:54, 110.14s/it]Evaluating commonsenseqa :  40%|                                          | 20/50 [36:26<55:02, 110.09s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [38:16<53:15, 110.17s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [40:06<51:28, 110.29s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [41:56<49:29, 109.99s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [43:45<47:36, 109.88s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [45:35<45:47, 109.91s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [47:25<43:54, 109.76s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [49:16<42:17, 110.31s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [51:07<40:31, 110.54s/it]Evaluating commonsenseqa :  58%|                             | 29/50 [52:57<38:37, 110.36s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [54:47<36:40, 110.02s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [56:37<34:51, 110.08s/it]Evaluating commonsenseqa :  64%|                         | 32/50 [58:27<33:00, 110.02s/it]Evaluating commonsenseqa :  66%|                       | 33/50 [1:00:19<31:19, 110.57s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [1:02:08<29:22, 110.17s/it]Evaluating commonsenseqa :  70%|                    | 35/50 [1:03:59<27:36, 110.41s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:05:47<25:37, 109.80s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:07:38<23:53, 110.26s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:09:29<22:02, 110.24s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:11:19<20:11, 110.16s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:13:09<18:21, 110.17s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:14:58<16:27, 109.73s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:16:49<14:42, 110.27s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:18:38<12:49, 109.99s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:20:28<11:00, 110.02s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:22:19<09:10, 110.08s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:24:10<07:21, 110.47s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:26:00<05:30, 110.19s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:27:51<03:40, 110.48s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:29:41<01:50, 110.40s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:31:31<00:00, 110.22s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:31:31<00:00, 109.83s/it]
name: commonsenseqa | avg. gen lenth: 400.16 | time: 5491.700274467468s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 08:47:58,210] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 08:47:58,220] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 08:47:58,296] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 08:47:58,312] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i4-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 4
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i4-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9737 [00:00<?, ?it/s]Loading data: 100%|| 9737/9737 [00:00<00:00, 477096.51it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.93s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.31s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.25s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.35s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.21s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.62s/it]
[2023-08-30 08:48:08,905] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.30s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.74s/it]
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.39s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.82s/it]
[2023-08-30 08:48:09,235] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.42s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.86s/it]
[2023-08-30 08:48:09,354] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-30 08:48:09,521] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 08:48:10,075] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 08:48:10,076] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb473ecd2d0>
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 08:48:10,077] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 08:48:10,078] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 08:48:10,078] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. In this question, you need to identify the best word to complete the sentence.
2. The sentence is talking about an intricate machine, which means it is complex or complicated.
3. Now, look at the options given - A: box, B: apparatus, C: appliance, D: wash dishes, E: implement. 
4. The word "box" usually refers to a simple container, not something intricate or complex.
5. "Apparatus" refers to a complex machine or device, which fits with the description in the question.
6. "Appliance" refers to a device or piece of equipment designed to perform a specific task, usually a domestic one. The context of the sentence does not suggest a domestic usage.
7. "Wash dishes" is a verb phrase, not a noun, therefore it can't be the answer because we are describing a noun. 
8. "Implement" would imply a tool or instrument for a specific task, which does not necessarily need to be intricate or complex.
9. Therefore, the best fit from the given options to complete the sentence is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: 1. The question is asking about the location where one can obtain petrol. 
2. Petrol is a type of fuel that is used to run engines.
3. By going over the options given, we can eliminate options that do not fit the context of the question.
4. Options A and C, "burn hot," are incorrect because this response suggests a state of matter, not a location.
5. Option B, "fuel tank", while related to petrol, is where the petrol is stored in a car, not where it is originally obtained.
6. Option D, "car", is incorrect because cars use petrol, they do not provide it.
7. This leaves us with the last option, E, "gas station".
8. A gas station is a location where fuel, including petrol, is sold and distributed to customers for their vehicles. 
9. Therefore, the answer is E: gas station.
So the final answer is E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: 1. A television is usually used in places where people live or work, as this tool is typically used for entertainment or informative purposes.
2. The options provided in the question are cabinet, house, apartment, bedroom, and woods. 
3. A cabinet is not a suitable location for using a television as it is a storage unit and does not usually provide enough space to watch television comfortably.
4. The woods is also not a good option as it typically lacks the necessary resources such as electricity and proper protective shielding for the television.
5. The remaining options are house, apartment, and bedroom. The question indicates that the television is used at night, suggesting the place should be somewhere people often reside at night.
6. Although people reside both in houses and apartments, they are too broad and do not specify a particular area within them where a television might be used at night. A television can be in many rooms within a house or an apartment.
7. Considering all this, the most suitable answer among the provided options would be a bedroom as it specifies a location within a residential structure where people commonly use televisions at night. Hence, the answer is D: bedroom.
So the final answer is D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: 1. The question asks about the location of getting knowledge which is described as being "expensive."
2. If we consider the options, gaining knowledge from a book, field, meeting, or class doesn't typically have a high financial cost associated with it.
3. However, attending a university typically involves substantial financial expenses in terms of tuition, living expenses, etc.
4. The term "expensive" is a key term in this question because it differentiates university from the other options.
5. So, by considering the distinctive "expensive" feature, we can conclude that the location is the university. Hence, the answer is A: university.
So the final answer is A: university

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i4-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                    | 1/50 [01:44<1:25:10, 104.30s/it]Evaluating commonsenseqa :   4%|                                                                   | 2/50 [03:26<1:22:40, 103.34s/it]Evaluating commonsenseqa :   6%|                                                                 | 3/50 [05:09<1:20:36, 102.91s/it]Evaluating commonsenseqa :   8%|                                                                | 4/50 [06:51<1:18:35, 102.51s/it]Evaluating commonsenseqa :  10%|                                                               | 5/50 [08:33<1:16:48, 102.42s/it]Evaluating commonsenseqa :  12%|                                                             | 6/50 [10:16<1:15:15, 102.63s/it]Evaluating commonsenseqa :  14%|                                                            | 7/50 [11:59<1:13:40, 102.80s/it]Evaluating commonsenseqa :  16%|                                                          | 8/50 [13:42<1:12:03, 102.93s/it]Evaluating commonsenseqa :  18%|                                                         | 9/50 [15:25<1:10:20, 102.93s/it]Evaluating commonsenseqa :  20%|                                                       | 10/50 [17:07<1:08:22, 102.57s/it]Evaluating commonsenseqa :  22%|                                                     | 11/50 [18:50<1:06:38, 102.54s/it]Evaluating commonsenseqa :  24%|                                                    | 12/50 [20:32<1:04:53, 102.47s/it]Evaluating commonsenseqa :  26%|                                                   | 13/50 [22:14<1:03:02, 102.22s/it]Evaluating commonsenseqa :  28%|                                                 | 14/50 [23:56<1:01:18, 102.18s/it]Evaluating commonsenseqa :  30%|                                                 | 15/50 [25:37<59:32, 102.07s/it]Evaluating commonsenseqa :  32%|                                                | 16/50 [27:20<57:56, 102.26s/it]Evaluating commonsenseqa :  34%|                                              | 17/50 [29:02<56:09, 102.12s/it]Evaluating commonsenseqa :  36%|                                             | 18/50 [30:45<54:34, 102.33s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [32:27<52:54, 102.40s/it]Evaluating commonsenseqa :  40%|                                          | 20/50 [34:10<51:14, 102.50s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [35:52<49:24, 102.21s/it]Evaluating commonsenseqa :  44%|                                       | 22/50 [37:35<47:53, 102.62s/it]Evaluating commonsenseqa :  46%|                                      | 23/50 [39:19<46:17, 102.87s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [41:01<44:31, 102.74s/it]Evaluating commonsenseqa :  50%|                                   | 25/50 [42:39<42:11, 101.25s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [44:21<40:33, 101.41s/it]Evaluating commonsenseqa :  54%|                                | 27/50 [46:04<39:05, 101.97s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [47:24<35:00, 95.49s/it]Evaluating commonsenseqa :  58%|                              | 29/50 [49:07<34:10, 97.66s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [50:50<33:06, 99.31s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [52:32<31:42, 100.11s/it]Evaluating commonsenseqa :  64%|                         | 32/50 [54:14<30:11, 100.66s/it]Evaluating commonsenseqa :  66%|                        | 33/50 [55:59<28:50, 101.81s/it]Evaluating commonsenseqa :  68%|                      | 34/50 [57:42<27:14, 102.16s/it]Evaluating commonsenseqa :  70%|                     | 35/50 [59:23<25:31, 102.08s/it]Evaluating commonsenseqa :  72%|                   | 36/50 [1:01:07<23:56, 102.60s/it]Evaluating commonsenseqa :  74%|                  | 37/50 [1:02:49<22:12, 102.49s/it]Evaluating commonsenseqa :  76%|                | 38/50 [1:04:32<20:31, 102.59s/it]Evaluating commonsenseqa :  78%|               | 39/50 [1:06:17<18:54, 103.11s/it]Evaluating commonsenseqa :  80%|             | 40/50 [1:07:59<17:09, 102.98s/it]Evaluating commonsenseqa :  82%|            | 41/50 [1:09:41<15:23, 102.62s/it]Evaluating commonsenseqa :  84%|           | 42/50 [1:11:24<13:42, 102.77s/it]Evaluating commonsenseqa :  86%|         | 43/50 [1:13:06<11:57, 102.50s/it]Evaluating commonsenseqa :  88%|        | 44/50 [1:14:37<09:54, 99.11s/it]Evaluating commonsenseqa :  90%|       | 45/50 [1:16:20<08:21, 100.30s/it]Evaluating commonsenseqa :  92%|     | 46/50 [1:18:03<06:43, 100.95s/it]Evaluating commonsenseqa :  94%|    | 47/50 [1:19:45<05:03, 101.26s/it]Evaluating commonsenseqa :  96%|  | 48/50 [1:21:28<03:23, 101.91s/it]Evaluating commonsenseqa :  98%| | 49/50 [1:23:11<01:42, 102.21s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:24:53<00:00, 102.25s/it]Evaluating commonsenseqa : 100%|| 50/50 [1:24:53<00:00, 101.88s/it]
name: commonsenseqa | avg. gen lenth: 398.152 | time: 5094.448546171188s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2113 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-out-domain 0 --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --batch-size 5 --data-dir /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-in-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 10:14:31,018] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 10:14:31,031] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 10:14:31,049] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 10:14:31,068] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/in-domain/i5-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 5
  num_out_domain ............... 0
  out_domain_data_name ......... None
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i5-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                                                            | 0/9736 [00:00<?, ?it/s]Loading data: 100%|| 9736/9736 [00:00<00:00, 392251.59it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.82s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.85s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:06<00:06,  6.90s/it]Loading checkpoint shards:  50%|                                     | 1/2 [00:07<00:07,  7.11s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.43s/it]
Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.06s/it]Loading checkpoint shards: 100%|| 2/2 [00:08<00:00,  4.48s/it]
[2023-08-30 10:14:41,454] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.11s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.52s/it]
 > number of parameters: 6738415616
[2023-08-30 10:14:41,556] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.12s/it]Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.57s/it]
[2023-08-30 10:14:41,625] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 10:14:41,676] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 10:14:42,261] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 10:14:42,262] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 10:14:42,262] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 10:14:42,262] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 10:14:42,262] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 10:14:42,262] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe9befc12d0>
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 10:14:42,263] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 10:14:42,264] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 10:14:42,264] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                                                                 | 0/50 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: The machine was very intricate, it was quite an what? Choices:  A: box B: apparatus C: appliance D: wash dishes E: implement
Output: 1. The sentence talks about a machine being very intricate.
2. The missing word would need to be something that can be described as being complex or intricate. 
3. An 'intricate box' doesn't make much sense as a box is often simple in its design. Hence option A is incorrect.
4. Similarly, 'intricate wash dishes' does not fit the given context. Hence option D is incorrect.
5. Option E, 'implement' often refers to tools or equipment for specific purposes, which can be intricate but it is less often used to describe machinery. Hence it might not be the best fit.
6. Although appliances can be intricate, they are generally things like a kitchen or household devices, which may not be the best fit for the general term of'machine'. Therefore, option C might not be the best fit.
7. That leaves 'apparatus'. An apparatus is a technical device or machinery, and can indeed be intricate, fitting the sentence context well.
8. Therefore, the answer is B: apparatus.
So the final answer is B: apparatus

Input: Where do you get petrol? Choices:  A: burn hot B: fuel tank C: burn hot D: car E: gas station
Output: 1. The question is asking where petrol is obtained from or where we can usually get petrol. 
2. Option A: burn hot, does not pertain to the question since it does not have a location where petrol can be obtained. It seems more related to the process of burning or combustion.
3. Option B: fuel tank, while it is a place where petrol is stored, it is not typically where individuals go to obtain petrol. Therefore, this option is skipped.
4. Option C: burn hot, as it is the same as option A and it does not provide an answer related to where petrol is typically obtained. 
5. Option D: car; petrol is used by a car and is stored in its fuel tank for operation, but it is not typically the place where individuals go to obtain or purchase petrol. 
6. Option E: gas station; this is the correct answer as this is the typical place where individuals go to obtain or purchase petrol for their vehicles.
So the final answer is E: gas station

Input: Where might a television used at night be? Choices:  A: cabinet B: house C: apartment D: bedroom E: woods
Output: Step 1: Consider the options. The question asks where a television, used at night, might be.
Step 2: Option A: Cabinet is not a valid option since televisions are typically stored in cabinets when not in use, not when they are being watched. 
Step 3: Option B: House is a valid option as televisions are typically found in houses. But it's quite broad and does not specify a particular location within a house where a television would be used at night.
Step 4: Option C: Apartment is also a valid option like a house. But the same reasoning as option B applies here - it does not specify a particular place where a television would usually be used at night.
Step 5: Option D: Bedroom is a valid answer. People often watch TV in their bedrooms before going to sleep at night, so this choice is more specific than just a house or an apartment.
Step 6: Option E: Woods is not a valid option. Televisions require a power source and strong signal connection to broadcast, which are typically not available in the woods.
Step 7: Among all the options, bedroom is the most specific one which answers the question of where a television might be used at night. Therefore, the answer is D: bedroom.
So the final answer is D: bedroom

Input: The knowledge was expensive to get, where was it being gained? Choices:  A: university B: book C: field D: meeting E: class
Output: 1. The question asks about where an expensive knowledge acquisition process takes place.
2. We start by examining each of the choices.
3. Choice B: Knowledge acquired from books can be expensive, but not necessarily as it can also be freely accessible through libraries or online sources. So, it might not be the correct answer.
4. Choice C: Getting knowledge through field or practical experience is not necessarily expensive, it depends more on the nature of the job or project. So, it's less likely to be the right answer.
5. Choice D: Obtaining knowledge via a meeting is not usually thought of as an expensive process, as meetings can often be attended for free or at a low cost. Therefore, it rules out the choice.
6. Choice E: While knowledge can be expensive to gain in classes, this could be any type of class, not necessarily one that comes with a high cost.
7. Choice A: Universities are commonly referred to as places where one gains knowledge, and it is typically an expensive process due to tuition fees, living expenses, etc. Thus, it fits best with the descriptor 'expensive.'
8. Therefore, the answer is A: University.
So the final answer is A: university

Input: Where are small grapes likely to be found? Choices:  A: lunch box B: food store C: wine country D: kitchen E: raisins
Output: 1. This question is asking for a place where small grapes are typically found.
2. If we look at choice A: lunch box, we can eliminate this option because any type of fruit, not specifically small grapes, could be in a lunch box.  
3. Choice B: food store is a broader option as it could contain all kinds of food, not specifically small grapes.
4. Choice D: kitchen, a place where all types of food are prepared and not solely devoted for small grapes.
5. Choice E: raisins. While grapes are indeed used to make raisins, the question asks for a place where one would find small grapes, not their byproducts.
6. The answer is therefore choice C: wine country. This is because small grapes are often used in making certain types of wine, and vineyards in wine countries are known to cultivate and harvest small grapes specifically for this purpose.
So the final answer is C: wine country

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/in-domain/i5-s20-rTrue-m4096
Evaluating commonsenseqa :   2%|                                                                     | 1/50 [01:32<1:15:29, 92.44s/it]Evaluating commonsenseqa :   4%|                                                                    | 2/50 [03:03<1:13:10, 91.47s/it]Evaluating commonsenseqa :   6%|                                                                  | 3/50 [04:34<1:11:37, 91.44s/it]Evaluating commonsenseqa :   8%|                                                                 | 4/50 [06:06<1:10:05, 91.43s/it]Evaluating commonsenseqa :  10%|                                                                | 5/50 [07:37<1:08:33, 91.40s/it]Evaluating commonsenseqa :  12%|                                                              | 6/50 [09:08<1:06:56, 91.29s/it]Evaluating commonsenseqa :  14%|                                                             | 7/50 [10:40<1:05:42, 91.68s/it]Evaluating commonsenseqa :  16%|                                                           | 8/50 [12:13<1:04:21, 91.94s/it]Evaluating commonsenseqa :  18%|                                                          | 9/50 [13:45<1:02:44, 91.82s/it]Evaluating commonsenseqa :  20%|                                                        | 10/50 [15:16<1:01:03, 91.59s/it]Evaluating commonsenseqa :  22%|                                                        | 11/50 [16:46<59:21, 91.32s/it]Evaluating commonsenseqa :  24%|                                                      | 12/50 [18:18<57:49, 91.29s/it]Evaluating commonsenseqa :  26%|                                                     | 13/50 [19:48<56:12, 91.15s/it]Evaluating commonsenseqa :  28%|                                                   | 14/50 [21:20<54:48, 91.34s/it]Evaluating commonsenseqa :  30%|                                                  | 15/50 [22:52<53:26, 91.61s/it]Evaluating commonsenseqa :  32%|                                                 | 16/50 [24:24<51:56, 91.67s/it]Evaluating commonsenseqa :  34%|                                               | 17/50 [25:55<50:16, 91.40s/it]Evaluating commonsenseqa :  36%|                                              | 18/50 [27:26<48:41, 91.28s/it]Evaluating commonsenseqa :  38%|                                            | 19/50 [28:57<47:04, 91.12s/it]Evaluating commonsenseqa :  40%|                                           | 20/50 [30:27<45:30, 91.01s/it]Evaluating commonsenseqa :  42%|                                         | 21/50 [31:58<43:51, 90.75s/it]Evaluating commonsenseqa :  44%|                                        | 22/50 [33:30<42:31, 91.11s/it]Evaluating commonsenseqa :  46%|                                       | 23/50 [35:00<40:58, 91.04s/it]Evaluating commonsenseqa :  48%|                                     | 24/50 [36:31<39:26, 91.03s/it]Evaluating commonsenseqa :  50%|                                    | 25/50 [38:02<37:56, 91.04s/it]Evaluating commonsenseqa :  52%|                                  | 26/50 [39:34<36:26, 91.10s/it]Evaluating commonsenseqa :  54%|                                 | 27/50 [41:05<34:56, 91.16s/it]Evaluating commonsenseqa :  56%|                               | 28/50 [42:37<33:33, 91.51s/it]Evaluating commonsenseqa :  58%|                              | 29/50 [44:08<31:58, 91.36s/it]Evaluating commonsenseqa :  60%|                            | 30/50 [45:39<30:23, 91.15s/it]Evaluating commonsenseqa :  62%|                           | 31/50 [47:10<28:50, 91.10s/it]Evaluating commonsenseqa :  64%|                          | 32/50 [48:41<27:19, 91.10s/it]