WORLD_SIZE=1
MASTER_ADDR=icgpu05
WORLD_SIZE=1
WORLD_SIZE=1
MASTER_ADDR=icgpu05
MASTER_ADDR=icgpu05
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13644 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 5 5 7 True False 4096 10
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13646 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 13 13 15 True False 4096 10
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13645 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 9 9 11 True False 4096 10
[2023-09-09 17:19:11,842] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-09 17:19:11,842] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-09 17:19:11,842] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
using world size: 1
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 5
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o5-tmathqa-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample ......speed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 1340355.46it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s].......... 1
  world_size ................... 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 13
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tmathqa-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 9
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o9-tmathqa-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 679676.52it/s]
Num instances: 1000
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 507133.65it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 400371.55it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.86s/it]
 > number of parameters: 6738415616
[2023-09-09 17:19:23,948] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-09 17:19:24,150] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-09 17:19:24,152] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554b5f82430>
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-09 17:19:24,154] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: "we assume that 80 is 100 % assume'x'is value we looking for here, 80 = 100 % and x = 12 % therefore, 80 / x = 100 % / 12 % 80 / x = 8.33 x = 9.6 b"
So the final answer is b

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o1-tmathqa-s1-rTrue-m4096
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s][2023-09-09 17:19:25,622] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-09 17:19:25,624] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-09 17:19:25,624] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-09 17:19:25,624] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-09 17:19:25,624] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-09 17:19:25,624] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-09 17:19:25,624] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-09 17:19:25,624] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554b5f111f0>
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-09 17:19:25,626] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s][2023-09-09 17:19:25,641] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-09 17:19:25,643] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba15f280>
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-09 17:19:25,645] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: "we assume that 80 is 100 % assume'x'is value we looking for here, 80 = 100 % and x = 12 % therefore, 80 / x = 100 % / 12 % 80 / x = 8.33 x = 9.6 b"
So the final answer is b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: a : b = 92000 : 20000 = 92 : 20 = 46 : 10 = 23 : 5 answer : c
So the final answer is c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: "actual cost = $ 60 sp = actual cost + mark up = actual cost + 20 % sp = 60 * 100 / 80 on sale sp = 80 / 100 ( 60 * 100 / 80 ) = 60 gross profit = $ 0 answer is a"
So the final answer is a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: "apparently it's q = x ^ 2 - x ^ 3 instead of q = x ^ 2 - q ^ 3. in this case : the x - intercept is the value ( s ) of x for q = 0. 0 = x ^ 2 - x ^ 3 ; 0 = x ^ 2 ( 1 - x ) ; x = 0 or x = 1. answer : c."
So the final answer is c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: "official solution : the problem asks for the amount of weekly sales it takes for angelo and isabella to earn the same amount of money. you can write an equation that sets angelo ’ s and isabella ’ s weekly earnings equal to each other, with x representing weekly sales. weekly earnings for each salesperson equal base salary plus commission. so angelo ’ s earnings are 550 + ( 0.08 ) ( x – 3,000 ), and isabella ’ s are 0.10 x. set up the equation and solve : 550 + ( 0.08 ) ( x – 3,000 ) = 0.10 x distribute the 0.08 : 550 + 0.08 x – 240 = 0.10 x combine terms and subtract 0.08 x from both sides : 310 = 0.02 x divide both sides by 0.02 : 15,500 = x your answer is b."
So the final answer is b

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o5-tmathqa-s1-rTrue-m4096
############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: "we assume that 80 is 100 % assume'x'is value we looking for here, 80 = 100 % and x = 12 % therefore, 80 / x = 100 % / 12 % 80 / x = 8.33 x = 9.6 b"
So the final answer is b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: a : b = 92000 : 20000 = 92 : 20 = 46 : 10 = 23 : 5 answer : c
So the final answer is c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: "actual cost = $ 60 sp = actual cost + mark up = actual cost + 20 % sp = 60 * 100 / 80 on sale sp = 80 / 100 ( 60 * 100 / 80 ) = 60 gross profit = $ 0 answer is a"
So the final answer is a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: "apparently it's q = x ^ 2 - x ^ 3 instead of q = x ^ 2 - q ^ 3. in this case : the x - intercept is the value ( s ) of x for q = 0. 0 = x ^ 2 - x ^ 3 ; 0 = x ^ 2 ( 1 - x ) ; x = 0 or x = 1. answer : c."
So the final answer is c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: "official solution : the problem asks for the amount of weekly sales it takes for angelo and isabella to earn the same amount of money. you can write an equation that sets angelo ’ s and isabella ’ s weekly earnings equal to each other, with x representing weekly sales. weekly earnings for each salesperson equal base salary plus commission. so angelo ’ s earnings are 550 + ( 0.08 ) ( x – 3,000 ), and isabella ’ s are 0.10 x. set up the equation and solve : 550 + ( 0.08 ) ( x – 3,000 ) = 0.10 x distribute the 0.08 : 550 + 0.08 x – 240 = 0.10 x combine terms and subtract 0.08 x from both sides : 310 = 0.02 x divide both sides by 0.02 : 15,500 = x your answer is b."
So the final answer is b

Input: find the total number of prime factors in the expression ( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 a ) 26, b ) 22, c ) 25, d ) 30, e ) 29
Output: "( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = ( 2 x 2 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = 2 ^ 11 x 2 ^ 11 x 7 ^ 5 x 11 ^ 3 = 2 ^ 22 x 7 ^ 5 x 11 ^ 3 total number of prime factors = ( 22 + 5 + 3 ) = 30. answer is d."
So the final answer is d

Input: the length of the bridge, which a train 140 m long and traveling at 45 km / hr can cross in 30 sec is? a ) 235, b ) 240, c ) 245, d ) 250, e ) 255
Output: "speed = 45 * 5 / 18 = 25 / 2 m / sec. time = 30 sec let the length of bridge be x meters. then, ( 140 + x ) / 30 = 25 / 2 x = 235 m. answer : option a"
So the final answer is a

Input: 360 metres long yard, 31 trees are palnted at equal distances, one tree being at each end of the yard. what is the distance between 2 consecutive trees a ) 10, b ) 12, c ) 14, d ) 16, e ) 17
Output: "31 trees have 30 gaps between them, required distance ( 360 / 30 ) = 12 b"
So the final answer is b

Input: a vessel of capacity 45 litres is fully filled with pure milk. nine litres of milk is removed from the vessel and replaced with water. nine litres of the solution thus formed is removed and replaced with water. find the quantity of pure milk in the final milk solution? a ) 28.8, b ) 72.9, c ) 38.3, d ) 78.3, e ) 79.3
Output: "explanation : let the initial quantity of milk in vessel be t litres. let us say y litres of the mixture is taken out and replaced by water for n times, alternatively. quantity of milk finally in the vessel is then given by [ ( t - y ) / t ] ^ n * t for the given problem, t = 45, y = 9 and n = 2. hence, quantity of milk finally in the vessel = [ ( 45 - 9 ) / 45 ] ^ 2 ( 45 ) = 28.8 litres. answer : option a"
So the final answer is a

Input: a certain telescope increases the visual range at a particular location from 90 kilometers to 150 kilometers. by what percent is the visual range increased by using the telescope? a ) 30 %, b ) 33 1 / 2 %, c ) 40 %, d ) 60 %, e ) 66 2 / 3 %
Output: "original visual range = 90 km new visual range = 150 km percent increase in the visual range by using the telescope = ( 150 - 90 ) / 90 * 100 % = 2 / 3 * 100 % = 66.67 % answer e"
So the final answer is e

Input: a bag contains 6 black and 3 white balls. one ball is drawn at random. what is the probability that the ball drawn is white? a ) 3 / 4, b ) 1 / 3, c ) 1 / 7, d ) 1 / 8, e ) 4 / 3
Output: "let number of balls = ( 6 + 3 ) = 9. number of white balls = 3. p ( drawing a white ball ) = 3 / 9 = 1 / 3. option b."
So the final answer is b

Input: rajat, vikas and abhishek are submitting questions in the ratio 7 : 3 : 2. if total number of questions submitted by them is 24. find the number of questions submitted by vikas. a ) 3, b ) 4, c ) 5, d ) 6, e ) 7
Output: explanation : number of questions submitted by vikas = ( 24 * 3 ) / ( 7 + 3 + 2 ) = 6 answer : d
So the final answer is d

Input: at a certain restaurant, the ratio of the number of cooks to the number of waiters is 3 to 8. when 12 more waiters are hired, the ratio of the number of cooks to the number of waiters changes to 1 to 4. how many cooks does the restaurant have? a ) 4, b ) 6, c ) 9, d ) 12, e ) 15
Output: originally there were 3 k cooks and 8 k waiters. the new ratio is 1 : 4 which equals 3 : 12. 12 k = 8 k + 12 k = 3 there are 9 cooks. the answer is c.
So the final answer is c

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tmathqa-s1-rTrue-m4096
############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: "we assume that 80 is 100 % assume'x'is value we looking for here, 80 = 100 % and x = 12 % therefore, 80 / x = 100 % / 12 % 80 / x = 8.33 x = 9.6 b"
So the final answer is b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: a : b = 92000 : 20000 = 92 : 20 = 46 : 10 = 23 : 5 answer : c
So the final answer is c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: "actual cost = $ 60 sp = actual cost + mark up = actual cost + 20 % sp = 60 * 100 / 80 on sale sp = 80 / 100 ( 60 * 100 / 80 ) = 60 gross profit = $ 0 answer is a"
So the final answer is a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: "apparently it's q = x ^ 2 - x ^ 3 instead of q = x ^ 2 - q ^ 3. in this case : the x - intercept is the value ( s ) of x for q = 0. 0 = x ^ 2 - x ^ 3 ; 0 = x ^ 2 ( 1 - x ) ; x = 0 or x = 1. answer : c."
So the final answer is c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: "official solution : the problem asks for the amount of weekly sales it takes for angelo and isabella to earn the same amount of money. you can write an equation that sets angelo ’ s and isabella ’ s weekly earnings equal to each other, with x representing weekly sales. weekly earnings for each salesperson equal base salary plus commission. so angelo ’ s earnings are 550 + ( 0.08 ) ( x – 3,000 ), and isabella ’ s are 0.10 x. set up the equation and solve : 550 + ( 0.08 ) ( x – 3,000 ) = 0.10 x distribute the 0.08 : 550 + 0.08 x – 240 = 0.10 x combine terms and subtract 0.08 x from both sides : 310 = 0.02 x divide both sides by 0.02 : 15,500 = x your answer is b."
So the final answer is b

Input: find the total number of prime factors in the expression ( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 a ) 26, b ) 22, c ) 25, d ) 30, e ) 29
Output: "( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = ( 2 x 2 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = 2 ^ 11 x 2 ^ 11 x 7 ^ 5 x 11 ^ 3 = 2 ^ 22 x 7 ^ 5 x 11 ^ 3 total number of prime factors = ( 22 + 5 + 3 ) = 30. answer is d."
So the final answer is d

Input: the length of the bridge, which a train 140 m long and traveling at 45 km / hr can cross in 30 sec is? a ) 235, b ) 240, c ) 245, d ) 250, e ) 255
Output: "speed = 45 * 5 / 18 = 25 / 2 m / sec. time = 30 sec let the length of bridge be x meters. then, ( 140 + x ) / 30 = 25 / 2 x = 235 m. answer : option a"
So the final answer is a

Input: 360 metres long yard, 31 trees are palnted at equal distances, one tree being at each end of the yard. what is the distance between 2 consecutive trees a ) 10, b ) 12, c ) 14, d ) 16, e ) 17
Output: "31 trees have 30 gaps between them, required distance ( 360 / 30 ) = 12 b"
So the final answer is b

Input: a vessel of capacity 45 litres is fully filled with pure milk. nine litres of milk is removed from the vessel and replaced with water. nine litres of the solution thus formed is removed and replaced with water. find the quantity of pure milk in the final milk solution? a ) 28.8, b ) 72.9, c ) 38.3, d ) 78.3, e ) 79.3
Output: "explanation : let the initial quantity of milk in vessel be t litres. let us say y litres of the mixture is taken out and replaced by water for n times, alternatively. quantity of milk finally in the vessel is then given by [ ( t - y ) / t ] ^ n * t for the given problem, t = 45, y = 9 and n = 2. hence, quantity of milk finally in the vessel = [ ( 45 - 9 ) / 45 ] ^ 2 ( 45 ) = 28.8 litres. answer : option a"
So the final answer is a

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o9-tmathqa-s1-rTrue-m4096
Evaluating commonsenseqa :   1%|          | 1/100 [02:03<3:23:56, 123.60s/it]Evaluating commonsenseqa :   1%|          | 1/100 [02:38<4:21:59, 158.78s/it]Evaluating commonsenseqa :   1%|          | 1/100 [03:30<5:47:36, 210.67s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [04:05<3:20:38, 122.84s/it]                                                                              Evaluating commonsenseqa :   2%|▏         | 2/100 [05:14<4:16:39, 157.14s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [06:09<3:18:53, 123.02s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [06:57<5:40:42, 208.60s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [07:47<4:11:02, 155.28s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [08:10<3:15:34, 122.23s/it]                                                                                Evaluating commonsenseqa :   5%|▌         | 5/100 [10:11<3:12:50, 121.80s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [10:23<4:08:48, 155.50s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [10:25<5:36:25, 208.10s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [12:13<3:11:14, 122.07s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [13:00<4:07:01, 156.01s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [13:49<5:30:18, 206.45s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [14:16<3:09:42, 122.40s/it]                                                                                Evaluating commonsenseqa :   6%|▌         | 6/100 [15:37<4:05:08, 156.48s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [16:20<3:08:07, 122.69s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [17:14<5:26:17, 206.08s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [18:15<4:03:14, 156.93s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [18:26<3:07:42, 123.76s/it]                                                                                Evaluating commonsenseqa :  10%|█         | 10/100 [20:29<3:05:18, 123.54s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [20:44<5:24:47, 207.31s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [20:52<4:00:29, 156.84s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [22:33<3:03:43, 123.86s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [23:33<3:59:50, 158.14s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [24:11<5:21:00, 207.10s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [24:36<3:01:17, 123.61s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [26:10<3:56:37, 157.75s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [26:40<2:59:16, 123.64s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [27:37<5:17:06, 206.81s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [28:44<2:57:23, 123.76s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [28:48<3:54:14, 157.92s/it]                                                                                Evaluating commonsenseqa :  15%|█▌        | 15/100 [30:48<2:55:24, 123.82s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [31:07<5:15:21, 207.93s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [31:24<3:50:52, 157.42s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [32:52<2:53:16, 123.77s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [34:03<3:48:39, 157.69s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [34:35<5:11:46, 207.85s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [34:57<2:51:38, 124.08s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [36:42<3:46:29, 158.02s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [37:05<2:51:18, 125.35s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [38:05<5:09:11, 208.44s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [39:08<2:48:08, 124.55s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [39:19<3:43:34, 157.81s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [41:14<2:46:53, 125.17s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [41:22<5:00:59, 205.22s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [41:55<3:40:25, 157.45s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [43:18<2:44:24, 124.87s/it]                                                                                Evaluating commonsenseqa :  17%|█▋        | 17/100 [44:31<3:37:05, 156.94s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [44:45<4:56:15, 204.31s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [45:21<2:41:34, 124.28s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [47:12<3:36:09, 158.17s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [47:26<2:39:43, 124.47s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [48:07<4:51:51, 203.62s/it]                                                                                 Evaluating commonsenseqa :  24%|██▍       | 24/100 [49:31<2:37:49, 124.60s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [49:46<3:31:48, 156.90s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [51:31<4:48:45, 203.83s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [51:35<2:35:33, 124.44s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [52:23<3:29:13, 156.91s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [53:38<2:32:51, 123.94s/it]                                                                                 Evaluating commonsenseqa :  16%|█▌        | 16/100 [54:58<4:46:41, 204.78s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [55:03<3:27:48, 157.82s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [55:39<2:29:45, 123.08s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [57:40<3:24:49, 157.56s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [57:43<2:28:00, 123.34s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [58:20<4:42:13, 204.02s/it]                                                                                   Evaluating commonsenseqa :  29%|██▉       | 29/100 [59:46<2:25:55, 123.31s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:00:19<3:22:44, 157.98s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [1:01:47<4:39:54, 204.82s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:01:50<2:24:07, 123.54s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:02:57<3:20:09, 158.02s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [1:03:56<2:22:51, 124.22s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [1:05:12<4:36:36, 204.89s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:05:33<3:16:42, 157.37s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [1:05:59<2:20:09, 123.67s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [1:08:01<2:17:40, 123.29s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:08:15<3:15:54, 158.84s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [1:08:39<4:33:53, 205.42s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [1:10:03<2:15:09, 122.88s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:10:50<3:11:45, 157.61s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:12:04<2:12:30, 122.31s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [1:12:07<4:31:48, 206.44s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:13:31<3:10:11, 158.49s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [1:14:06<2:10:33, 122.40s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:15:37<4:29:34, 207.37s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:16:09<3:07:29, 158.45s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [1:16:12<2:09:23, 123.23s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [1:18:13<2:06:45, 122.66s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:18:49<3:05:31, 159.02s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:19:02<4:25:17, 206.72s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [1:20:15<2:04:28, 122.43s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [1:21:27<3:02:29, 158.69s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [1:22:19<2:02:52, 122.87s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:22:27<4:21:07, 206.15s/it]                                                                                     Evaluating commonsenseqa :  32%|███▏      | 32/100 [1:24:02<2:58:34, 157.56s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [1:24:22<2:01:02, 123.09s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:25:55<4:18:25, 206.73s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:26:25<1:58:54, 123.01s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [1:26:39<2:55:42, 157.34s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:28:29<1:57:04, 123.23s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [1:29:12<2:51:46, 156.16s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:29:25<4:16:02, 207.60s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:30:31<1:54:47, 122.99s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:31:51<2:49:54, 156.84s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:32:34<1:52:41, 122.93s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:32:51<4:12:01, 207.15s/it]                                                                                     Evaluating commonsenseqa :  36%|███▌      | 36/100 [1:34:30<2:47:59, 157.49s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [1:34:36<1:50:18, 122.56s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:36:20<4:09:12, 207.68s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:36:37<1:47:52, 122.12s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [1:37:09<2:45:46, 157.88s/it]                                                                                     Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:38:41<1:46:17, 122.64s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [1:39:45<2:42:36, 157.37s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:39:46<4:05:24, 207.39s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:40:44<1:44:22, 122.79s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [1:42:24<2:40:30, 157.88s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [1:42:46<1:42:12, 122.65s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:43:17<4:03:02, 208.32s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [1:44:48<1:40:01, 122.48s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [1:44:59<2:37:06, 157.10s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [1:46:46<3:59:59, 208.68s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:46:52<1:38:18, 122.88s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [1:47:33<2:33:25, 156.02s/it]                                                                                       Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:48:55<1:36:20, 123.00s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:50:08<2:30:39, 155.86s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [1:50:21<3:58:27, 210.40s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:50:59<1:34:22, 123.10s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:52:45<2:28:21, 156.17s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:53:01<1:32:14, 122.99s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [1:53:50<3:54:29, 209.99s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:55:04<1:30:06, 122.88s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:55:20<2:25:34, 155.97s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:57:06<1:27:55, 122.68s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [1:57:18<3:50:18, 209.36s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:58:01<2:24:09, 157.26s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:59:09<1:25:52, 122.68s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [2:00:42<2:22:38, 158.49s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [2:00:46<3:46:19, 208.92s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [2:01:11<1:23:38, 122.40s/it]                                                                                       Evaluating commonsenseqa :  60%|██████    | 60/100 [2:03:13<1:21:29, 122.24s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [2:03:20<2:19:53, 158.37s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [2:04:13<3:42:20, 208.44s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [2:05:17<1:19:53, 122.92s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [2:05:59<2:17:19, 158.45s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [2:07:18<1:17:29, 122.35s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [2:07:41<3:38:34, 208.16s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [2:08:37<2:14:32, 158.29s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [2:09:21<1:15:32, 122.49s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [2:11:09<3:35:03, 208.12s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [2:11:14<2:11:37, 157.95s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [2:11:27<1:14:05, 123.50s/it]                                                                                       Evaluating commonsenseqa :  65%|██████▌   | 65/100 [2:13:29<1:11:48, 123.10s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [2:13:50<2:08:28, 157.31s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [2:14:34<3:30:51, 207.39s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [2:15:30<1:09:20, 122.38s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [2:16:30<2:06:32, 158.17s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [2:17:34<1:07:38, 122.97s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [2:18:03<3:27:54, 207.91s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [2:19:06<2:03:31, 157.70s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [2:19:36<1:05:27, 122.74s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [2:21:28<3:23:31, 206.97s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [2:21:39<1:03:29, 122.88s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [2:21:44<2:00:50, 157.62s/it]                                                                                       Evaluating commonsenseqa :  70%|███████   | 70/100 [2:23:42<1:01:26, 122.87s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [2:24:20<1:57:58, 157.31s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [2:24:55<3:19:54, 206.80s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [2:25:47<59:43, 123.58s/it]  Evaluating commonsenseqa :  56%|█████▌    | 56/100 [2:26:57<1:55:04, 156.92s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [2:27:52<57:48, 123.87s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [2:28:28<3:18:22, 208.81s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [2:29:32<1:52:11, 156.55s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [2:29:57<55:49, 124.06s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [2:31:59<3:15:26, 209.40s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [2:32:02<53:55, 124.46s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [2:32:12<1:50:13, 157.46s/it]                                                                                       Evaluating commonsenseqa :  75%|███████▌  | 75/100 [2:34:03<51:29, 123.56s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [2:34:50<1:47:42, 157.63s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [2:35:25<3:11:10, 208.55s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [2:36:05<49:13, 123.06s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [2:37:28<1:45:11, 157.79s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [2:38:09<47:14, 123.26s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [2:38:50<3:06:44, 207.48s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [2:40:05<1:42:28, 157.65s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [2:40:13<45:15, 123.42s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [2:42:15<43:02, 122.99s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [2:42:15<3:02:27, 206.56s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [2:42:45<1:40:09, 158.13s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [2:44:19<41:05, 123.28s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [2:45:24<1:37:46, 158.56s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [2:45:42<2:59:05, 206.64s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [2:46:21<38:59, 123.11s/it]                                                                                         Evaluating commonsenseqa :  64%|██████▍   | 64/100 [2:47:59<1:34:25, 157.38s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [2:48:25<36:59, 123.28s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [2:49:10<2:56:07, 207.21s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [2:50:29<34:57, 123.41s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [2:50:37<1:31:59, 157.69s/it]                                                                                         Evaluating commonsenseqa :  84%|████████▍ | 84/100 [2:52:30<32:42, 122.65s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [2:52:37<2:52:33, 207.07s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [2:53:12<1:28:54, 156.90s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [2:54:31<30:35, 122.35s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [2:55:49<1:26:15, 156.84s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [2:56:02<2:48:35, 206.43s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [2:56:32<28:26, 121.89s/it]                                                                                         Evaluating commonsenseqa :  68%|██████▊   | 68/100 [2:58:28<1:24:01, 157.56s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [2:58:38<26:39, 123.06s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [2:59:27<2:44:55, 206.15s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [3:00:42<24:40, 123.37s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [3:01:07<1:21:36, 157.96s/it]                                                                                         Evaluating commonsenseqa :  89%|████████▉ | 89/100 [3:02:46<22:38, 123.49s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [3:03:00<2:43:00, 208.09s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [3:03:47<1:19:14, 158.49s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [3:04:49<20:32, 123.30s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [3:06:23<1:16:18, 157.87s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [3:06:28<2:39:34, 208.15s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [3:06:55<18:37, 124.12s/it]                                                                                         Evaluating commonsenseqa :  92%|█████████▏| 92/100 [3:08:55<16:24, 123.06s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [3:09:01<1:13:43, 157.99s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [3:09:12<2:26:01, 194.71s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [3:11:01<14:26, 123.72s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [3:11:41<1:11:17, 158.43s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [3:12:36<2:24:58, 197.70s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [3:13:05<12:24, 124.02s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [3:14:20<1:08:46, 158.71s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [3:15:08<10:17, 123.58s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [3:16:07<2:24:33, 201.72s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [3:16:57<1:05:52, 158.11s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [3:17:11<08:14, 123.57s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [3:19:15<06:10, 123.52s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [3:19:32<1:02:54, 157.26s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [3:19:37<2:22:50, 204.06s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [3:21:18<04:07, 123.56s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [3:22:10<1:00:18, 157.34s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [3:23:04<2:20:08, 205.09s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [3:23:21<02:03, 123.20s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [3:24:48<57:50, 157.73s/it]  Evaluating commonsenseqa : 100%|██████████| 100/100 [3:25:28<00:00, 124.51s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [3:25:28<00:00, 123.29s/it]
name: commonsenseqa | avg. gen lenth: 378.508 | time: 12331.607967376709s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13646 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 15 13 15 True False 4096 10
[2023-09-09 20:45:04,789] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 15
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o15-tmathqa-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 372865.30it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.79s/it]
 > number of parameters: 6738415616
[2023-09-09 20:45:17,993] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-09 20:45:18,200] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-09 20:45:18,202] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-09 20:45:18,202] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-09 20:45:18,202] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-09 20:45:18,202] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-09 20:45:18,202] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554b5f11250>
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-09 20:45:18,203] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-09 20:45:18,204] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-09 20:45:18,204] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: "we assume that 80 is 100 % assume'x'is value we looking for here, 80 = 100 % and x = 12 % therefore, 80 / x = 100 % / 12 % 80 / x = 8.33 x = 9.6 b"
So the final answer is b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: a : b = 92000 : 20000 = 92 : 20 = 46 : 10 = 23 : 5 answer : c
So the final answer is c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: "actual cost = $ 60 sp = actual cost + mark up = actual cost + 20 % sp = 60 * 100 / 80 on sale sp = 80 / 100 ( 60 * 100 / 80 ) = 60 gross profit = $ 0 answer is a"
So the final answer is a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: "apparently it's q = x ^ 2 - x ^ 3 instead of q = x ^ 2 - q ^ 3. in this case : the x - intercept is the value ( s ) of x for q = 0. 0 = x ^ 2 - x ^ 3 ; 0 = x ^ 2 ( 1 - x ) ; x = 0 or x = 1. answer : c."
So the final answer is c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: "official solution : the problem asks for the amount of weekly sales it takes for angelo and isabella to earn the same amount of money. you can write an equation that sets angelo ’ s and isabella ’ s weekly earnings equal to each other, with x representing weekly sales. weekly earnings for each salesperson equal base salary plus commission. so angelo ’ s earnings are 550 + ( 0.08 ) ( x – 3,000 ), and isabella ’ s are 0.10 x. set up the equation and solve : 550 + ( 0.08 ) ( x – 3,000 ) = 0.10 x distribute the 0.08 : 550 + 0.08 x – 240 = 0.10 x combine terms and subtract 0.08 x from both sides : 310 = 0.02 x divide both sides by 0.02 : 15,500 = x your answer is b."
So the final answer is b

Input: find the total number of prime factors in the expression ( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 a ) 26, b ) 22, c ) 25, d ) 30, e ) 29
Output: "( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = ( 2 x 2 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = 2 ^ 11 x 2 ^ 11 x 7 ^ 5 x 11 ^ 3 = 2 ^ 22 x 7 ^ 5 x 11 ^ 3 total number of prime factors = ( 22 + 5 + 3 ) = 30. answer is d."
So the final answer is d

Input: the length of the bridge, which a train 140 m long and traveling at 45 km / hr can cross in 30 sec is? a ) 235, b ) 240, c ) 245, d ) 250, e ) 255
Output: "speed = 45 * 5 / 18 = 25 / 2 m / sec. time = 30 sec let the length of bridge be x meters. then, ( 140 + x ) / 30 = 25 / 2 x = 235 m. answer : option a"
So the final answer is a

Input: 360 metres long yard, 31 trees are palnted at equal distances, one tree being at each end of the yard. what is the distance between 2 consecutive trees a ) 10, b ) 12, c ) 14, d ) 16, e ) 17
Output: "31 trees have 30 gaps between them, required distance ( 360 / 30 ) = 12 b"
So the final answer is b

Input: a vessel of capacity 45 litres is fully filled with pure milk. nine litres of milk is removed from the vessel and replaced with water. nine litres of the solution thus formed is removed and replaced with water. find the quantity of pure milk in the final milk solution? a ) 28.8, b ) 72.9, c ) 38.3, d ) 78.3, e ) 79.3
Output: "explanation : let the initial quantity of milk in vessel be t litres. let us say y litres of the mixture is taken out and replaced by water for n times, alternatively. quantity of milk finally in the vessel is then given by [ ( t - y ) / t ] ^ n * t for the given problem, t = 45, y = 9 and n = 2. hence, quantity of milk finally in the vessel = [ ( 45 - 9 ) / 45 ] ^ 2 ( 45 ) = 28.8 litres. answer : option a"
So the final answer is a

Input: a certain telescope increases the visual range at a particular location from 90 kilometers to 150 kilometers. by what percent is the visual range increased by using the telescope? a ) 30 %, b ) 33 1 / 2 %, c ) 40 %, d ) 60 %, e ) 66 2 / 3 %
Output: "original visual range = 90 km new visual range = 150 km percent increase in the visual range by using the telescope = ( 150 - 90 ) / 90 * 100 % = 2 / 3 * 100 % = 66.67 % answer e"
So the final answer is e

Input: a bag contains 6 black and 3 white balls. one ball is drawn at random. what is the probability that the ball drawn is white? a ) 3 / 4, b ) 1 / 3, c ) 1 / 7, d ) 1 / 8, e ) 4 / 3
Output: "let number of balls = ( 6 + 3 ) = 9. number of white balls = 3. p ( drawing a white ball ) = 3 / 9 = 1 / 3. option b."
So the final answer is b

Input: rajat, vikas and abhishek are submitting questions in the ratio 7 : 3 : 2. if total number of questions submitted by them is 24. find the number of questions submitted by vikas. a ) 3, b ) 4, c ) 5, d ) 6, e ) 7
Output: explanation : number of questions submitted by vikas = ( 24 * 3 ) / ( 7 + 3 + 2 ) = 6 answer : d
So the final answer is d

Input: at a certain restaurant, the ratio of the number of cooks to the number of waiters is 3 to 8. when 12 more waiters are hired, the ratio of the number of cooks to the number of waiters changes to 1 to 4. how many cooks does the restaurant have? a ) 4, b ) 6, c ) 9, d ) 12, e ) 15
Output: originally there were 3 k cooks and 8 k waiters. the new ratio is 1 : 4 which equals 3 : 12. 12 k = 8 k + 12 k = 3 there are 9 cooks. the answer is c.
So the final answer is c

Input: if 7 ^ k + 7 ^ k = ( 7 ^ 9 ) ^ ( 7 ^ 9 ) - 7 ^ k, then k =? a ) 11 / 3, b ) 11 / 2, c ) 242, d ) 3 ^ 10, e ) 7 ^ 11 - 1
Output: "7 ^ k + 7 ^ k = ( 7 ^ 9 ) ^ 7 ^ 9 - 7 ^ k 7 * ( 7 ^ k ) = 7 ^ ( 49 * 7 ^ 9 ) = 7 ^ ( 7 ^ 2 * 7 ^ 9 ) = 7 ^ ( 7 ^ 11 ) 7 ^ k + 1 = 7 ^ ( 7 ^ 11 ) so k + 1 = 7 ^ 11 so k = 7 ^ 11 - 1 answer is e"
So the final answer is e

Input: average of 10 matches is 32, how many runs one should should score to increase his average by 4 runs. a ) 70, b ) 76, c ) 78, d ) 80, e ) 88
Output: "explanation : average after 11 innings should be 36 so, required score = ( 11 * 36 ) - ( 10 * 32 ) = 396 - 320 = 76 answer : option b"
So the final answer is b

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o15-tmathqa-s1-rTrue-m4096
Evaluating commonsenseqa :  60%|██████    | 60/100 [3:26:28<2:16:27, 204.68s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [3:27:25<55:07, 157.50s/it]Evaluating commonsenseqa :   1%|          | 1/100 [01:52<3:06:16, 112.89s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [03:42<3:01:16, 110.99s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [3:29:53<2:13:10, 204.88s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [3:30:05<52:45, 158.25s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [05:32<2:58:21, 110.32s/it]                                                                                           Evaluating commonsenseqa :  81%|████████  | 81/100 [3:32:46<50:22, 159.08s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [07:22<2:56:53, 110.55s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [3:33:23<2:10:43, 206.40s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [09:14<2:55:50, 111.06s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [3:35:26<47:46, 159.27s/it]                                                                                           Evaluating commonsenseqa :  63%|██████▎   | 63/100 [3:36:54<2:08:05, 207.71s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [11:04<2:53:23, 110.68s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [3:38:04<45:01, 158.89s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [12:55<2:51:40, 110.76s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [3:40:21<2:04:28, 207.45s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [14:45<2:49:31, 110.56s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [3:40:41<42:11, 158.24s/it]                                                                                           Evaluating commonsenseqa :   9%|▉         | 9/100 [16:36<2:47:29, 110.43s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [3:43:19<39:31, 158.11s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [3:43:47<2:00:46, 207.04s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [18:27<2:46:00, 110.68s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [3:45:54<36:43, 157.38s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [20:17<2:43:52, 110.48s/it]                                                                                           Evaluating commonsenseqa :  66%|██████▌   | 66/100 [3:47:16<1:57:41, 207.70s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [22:08<2:42:09, 110.56s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [3:48:35<34:17, 158.30s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [23:58<2:40:06, 110.42s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [3:50:41<1:53:40, 206.69s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [3:51:13<31:39, 158.31s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [25:48<2:38:12, 110.38s/it]        Evaluating commonsenseqa :  15%|█▌        | 15/100 [27:40<2:36:56, 110.78s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [3:53:56<29:16, 159.65s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [3:54:08<1:50:21, 206.91s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [29:32<2:35:45, 111.26s/it]                                                                                           Evaluating commonsenseqa :  90%|█████████ | 90/100 [3:56:36<26:36, 159.66s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [31:22<2:33:26, 110.92s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [3:57:35<1:46:56, 206.98s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [33:14<2:31:53, 111.14s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [3:59:17<24:01, 160.11s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [35:04<2:29:31, 110.76s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [4:00:59<1:43:01, 206.05s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [4:01:55<21:15, 159.43s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [36:54<2:27:22, 110.53s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [4:04:28<1:39:56, 206.79s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [4:04:36<18:40, 160.00s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [38:45<2:25:39, 110.62s/it]                                                                                           Evaluating commonsenseqa :  22%|██▏       | 22/100 [40:35<2:23:52, 110.67s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [4:07:15<15:58, 159.82s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [4:07:49<1:35:48, 205.30s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [42:25<2:21:36, 110.34s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [4:09:55<13:18, 159.64s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [44:18<2:20:43, 111.09s/it]                                                                                           Evaluating commonsenseqa :  73%|███████▎  | 73/100 [4:11:17<1:32:37, 205.83s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [46:08<2:18:37, 110.89s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [4:12:33<10:37, 159.35s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [47:58<2:16:30, 110.68s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [4:14:40<1:28:56, 205.24s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [4:15:12<07:57, 159.09s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [49:50<2:14:51, 110.85s/it]                                                                                             Evaluating commonsenseqa :  28%|██▊       | 28/100 [51:39<2:12:37, 110.52s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [4:17:50<05:17, 158.83s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [4:18:09<1:25:55, 206.24s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [53:31<2:11:00, 110.71s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [4:20:28<02:38, 158.71s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [55:21<2:09:04, 110.63s/it]        Evaluating commonsenseqa :  76%|███████▌  | 76/100 [4:21:35<1:22:31, 206.32s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [57:12<2:07:18, 110.71s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [4:23:07<00:00, 158.80s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [4:23:07<00:00, 157.88s/it]
name: commonsenseqa | avg. gen lenth: 334.438 | time: 15790.025553703308s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13645 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 11 9 11 True False 4096 10
[2023-09-09 21:42:44,592] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 11
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o11-tmathqa-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 449810.25it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.72s/it]
 > number of parameters: 6738415616
[2023-09-09 21:42:57,697] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-09 21:42:57,911] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-09 21:42:57,913] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-09 21:42:57,913] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-09 21:42:57,913] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-09 21:42:57,913] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-09 21:42:57,913] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-09 21:42:57,913] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-09 21:42:57,913] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-09 21:42:57,913] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-09 21:42:57,913] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-09 21:42:57,913] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3be250>
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-09 21:42:57,914] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-09 21:42:57,915] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-09 21:42:57,915] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: "we assume that 80 is 100 % assume'x'is value we looking for here, 80 = 100 % and x = 12 % therefore, 80 / x = 100 % / 12 % 80 / x = 8.33 x = 9.6 b"
So the final answer is b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: a : b = 92000 : 20000 = 92 : 20 = 46 : 10 = 23 : 5 answer : c
So the final answer is c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: "actual cost = $ 60 sp = actual cost + mark up = actual cost + 20 % sp = 60 * 100 / 80 on sale sp = 80 / 100 ( 60 * 100 / 80 ) = 60 gross profit = $ 0 answer is a"
So the final answer is a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: "apparently it's q = x ^ 2 - x ^ 3 instead of q = x ^ 2 - q ^ 3. in this case : the x - intercept is the value ( s ) of x for q = 0. 0 = x ^ 2 - x ^ 3 ; 0 = x ^ 2 ( 1 - x ) ; x = 0 or x = 1. answer : c."
So the final answer is c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: "official solution : the problem asks for the amount of weekly sales it takes for angelo and isabella to earn the same amount of money. you can write an equation that sets angelo ’ s and isabella ’ s weekly earnings equal to each other, with x representing weekly sales. weekly earnings for each salesperson equal base salary plus commission. so angelo ’ s earnings are 550 + ( 0.08 ) ( x – 3,000 ), and isabella ’ s are 0.10 x. set up the equation and solve : 550 + ( 0.08 ) ( x – 3,000 ) = 0.10 x distribute the 0.08 : 550 + 0.08 x – 240 = 0.10 x combine terms and subtract 0.08 x from both sides : 310 = 0.02 x divide both sides by 0.02 : 15,500 = x your answer is b."
So the final answer is b

Input: find the total number of prime factors in the expression ( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 a ) 26, b ) 22, c ) 25, d ) 30, e ) 29
Output: "( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = ( 2 x 2 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = 2 ^ 11 x 2 ^ 11 x 7 ^ 5 x 11 ^ 3 = 2 ^ 22 x 7 ^ 5 x 11 ^ 3 total number of prime factors = ( 22 + 5 + 3 ) = 30. answer is d."
So the final answer is d

Input: the length of the bridge, which a train 140 m long and traveling at 45 km / hr can cross in 30 sec is? a ) 235, b ) 240, c ) 245, d ) 250, e ) 255
Output: "speed = 45 * 5 / 18 = 25 / 2 m / sec. time = 30 sec let the length of bridge be x meters. then, ( 140 + x ) / 30 = 25 / 2 x = 235 m. answer : option a"
So the final answer is a

Input: 360 metres long yard, 31 trees are palnted at equal distances, one tree being at each end of the yard. what is the distance between 2 consecutive trees a ) 10, b ) 12, c ) 14, d ) 16, e ) 17
Output: "31 trees have 30 gaps between them, required distance ( 360 / 30 ) = 12 b"
So the final answer is b

Input: a vessel of capacity 45 litres is fully filled with pure milk. nine litres of milk is removed from the vessel and replaced with water. nine litres of the solution thus formed is removed and replaced with water. find the quantity of pure milk in the final milk solution? a ) 28.8, b ) 72.9, c ) 38.3, d ) 78.3, e ) 79.3
Output: "explanation : let the initial quantity of milk in vessel be t litres. let us say y litres of the mixture is taken out and replaced by water for n times, alternatively. quantity of milk finally in the vessel is then given by [ ( t - y ) / t ] ^ n * t for the given problem, t = 45, y = 9 and n = 2. hence, quantity of milk finally in the vessel = [ ( 45 - 9 ) / 45 ] ^ 2 ( 45 ) = 28.8 litres. answer : option a"
So the final answer is a

Input: a certain telescope increases the visual range at a particular location from 90 kilometers to 150 kilometers. by what percent is the visual range increased by using the telescope? a ) 30 %, b ) 33 1 / 2 %, c ) 40 %, d ) 60 %, e ) 66 2 / 3 %
Output: "original visual range = 90 km new visual range = 150 km percent increase in the visual range by using the telescope = ( 150 - 90 ) / 90 * 100 % = 2 / 3 * 100 % = 66.67 % answer e"
So the final answer is e

Input: a bag contains 6 black and 3 white balls. one ball is drawn at random. what is the probability that the ball drawn is white? a ) 3 / 4, b ) 1 / 3, c ) 1 / 7, d ) 1 / 8, e ) 4 / 3
Output: "let number of balls = ( 6 + 3 ) = 9. number of white balls = 3. p ( drawing a white ball ) = 3 / 9 = 1 / 3. option b."
So the final answer is b

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o11-tmathqa-s1-rTrue-m4096
Evaluating commonsenseqa :  32%|███▏      | 32/100 [59:05<2:06:21, 111.49s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [4:25:02<1:19:06, 206.39s/it]Evaluating commonsenseqa :   1%|          | 1/100 [02:21<3:53:33, 141.55s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [1:00:55<2:04:01, 111.07s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [04:40<3:49:07, 140.29s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [4:28:36<1:16:27, 208.53s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [1:02:46<2:02:08, 111.04s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:04:36<1:59:54, 110.68s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [06:59<3:45:48, 139.68s/it]                                                                                             Evaluating commonsenseqa :  79%|███████▉  | 79/100 [4:32:02<1:12:48, 208.01s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [1:06:27<1:58:01, 110.65s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [09:19<3:43:22, 139.61s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [1:08:17<1:56:11, 110.66s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [11:37<3:40:15, 139.11s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [4:35:32<1:09:27, 208.40s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [1:10:09<1:54:33, 110.86s/it]    Evaluating commonsenseqa :   6%|▌         | 6/100 [13:58<3:38:49, 139.68s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [1:11:58<1:52:19, 110.49s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [4:38:54<1:05:27, 206.71s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [1:13:51<1:51:11, 111.19s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [16:16<3:35:42, 139.16s/it]                                                                                             Evaluating commonsenseqa :  41%|████      | 41/100 [1:15:40<1:48:39, 110.50s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [18:37<3:34:19, 139.78s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [4:42:26<1:02:25, 208.07s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:17:30<1:46:39, 110.34s/it]  Evaluating commonsenseqa :   9%|▉         | 9/100 [20:58<3:32:40, 140.23s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:19:22<1:45:21, 110.90s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [4:45:59<59:24, 209.70s/it]  Evaluating commonsenseqa :  10%|█         | 10/100 [23:17<3:29:28, 139.65s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:21:12<1:43:18, 110.68s/it]                                                                                             Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:23:04<1:41:42, 110.96s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [25:36<3:26:51, 139.45s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [4:49:24<55:32, 208.27s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [1:24:56<1:40:15, 111.39s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [27:57<3:25:15, 139.94s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:26:47<1:38:04, 111.02s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [4:52:54<52:11, 208.77s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [30:18<3:23:37, 140.43s/it]          Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:28:38<1:36:16, 111.09s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [32:39<3:21:29, 140.58s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:30:28<1:34:09, 110.78s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [4:56:22<48:41, 208.66s/it]                                                                                             Evaluating commonsenseqa :  50%|█████     | 50/100 [1:32:21<1:32:49, 111.40s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [34:59<3:18:58, 140.45s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [4:59:54<45:24, 209.59s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [1:34:11<1:30:45, 111.13s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [37:19<3:16:22, 140.26s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:36:02<1:28:52, 111.08s/it]                                                                                               Evaluating commonsenseqa :  17%|█▋        | 17/100 [39:39<3:13:50, 140.12s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [5:03:22<41:49, 209.11s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:37:52<1:26:50, 110.85s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [42:01<3:12:16, 140.69s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:39:44<1:25:02, 110.93s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [5:06:46<38:04, 207.67s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:41:35<1:23:14, 110.99s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [44:20<3:09:13, 140.16s/it]            Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:43:26<1:21:22, 110.96s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [5:10:15<34:38, 207.84s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [46:42<3:07:50, 140.88s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:45:16<1:19:25, 110.81s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [49:03<3:05:13, 140.68s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:47:08<1:17:48, 111.15s/it]  Evaluating commonsenseqa :  91%|█████████ | 91/100 [5:13:43<31:12, 208.05s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [1:49:00<1:16:02, 111.27s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [51:23<3:02:46, 140.60s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [1:50:49<1:13:45, 110.63s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [5:17:08<27:37, 207.17s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [53:40<2:59:06, 139.56s/it]                                                                                               Evaluating commonsenseqa :  61%|██████    | 61/100 [1:52:39<1:11:53, 110.61s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [55:59<2:56:20, 139.22s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [1:54:30<1:10:00, 110.55s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [5:20:36<24:12, 207.46s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [58:17<2:53:48, 139.05s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [1:56:21<1:08:16, 110.71s/it]                                                                                               Evaluating commonsenseqa :  94%|█████████▍| 94/100 [5:24:01<20:39, 206.56s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [1:58:12<1:06:32, 110.91s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:00:38<2:52:02, 139.49s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [2:00:03<1:04:41, 110.91s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:02:56<2:49:14, 139.10s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [5:27:27<17:12, 206.50s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [2:01:52<1:02:33, 110.40s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:05:19<2:48:19, 140.27s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [2:03:43<1:00:50, 110.62s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [5:30:59<13:52, 208.00s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:07:37<2:45:14, 139.63s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [2:05:32<58:45, 110.17s/it]                                                                                                 Evaluating commonsenseqa :  69%|██████▉   | 69/100 [2:07:24<57:11, 110.69s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:09:58<2:43:23, 140.05s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [5:34:26<10:23, 207.69s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [2:09:15<55:21, 110.72s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [1:12:18<2:40:56, 139.95s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [2:11:06<53:30, 110.69s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [5:37:58<06:57, 208.98s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [1:14:39<2:39:02, 140.33s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [2:12:58<51:50, 111.08s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [1:17:02<2:37:37, 141.16s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [2:14:51<50:12, 111.57s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [5:41:20<03:27, 207.08s/it]                                                                                               Evaluating commonsenseqa :  74%|███████▍  | 74/100 [2:16:44<48:37, 112.23s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [1:19:22<2:34:56, 140.86s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [2:18:35<46:36, 111.88s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [5:44:41<00:00, 205.24s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [5:44:41<00:00, 206.82s/it]
name: commonsenseqa | avg. gen lenth: 319.352 | time: 20683.68094921112s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13644 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 7 5 7 True False 4096 10
[2023-09-09 23:04:14,460] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 7
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o7-tmathqa-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 588204.94it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.15s/it]
 > number of parameters: 6738415616
[2023-09-09 23:04:28,305] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-09 23:04:28,504] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-09 23:04:28,506] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-09 23:04:28,506] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-09 23:04:28,506] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-09 23:04:28,506] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-09 23:04:28,506] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-09 23:04:28,506] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-09 23:04:28,506] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-09 23:04:28,506] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-09 23:04:28,506] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-09 23:04:28,506] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-09 23:04:28,506] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0280>
[2023-09-09 23:04:28,506] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-09 23:04:28,506] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-09 23:04:28,506] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-09 23:04:28,506] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-09 23:04:28,507] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-09 23:04:28,508] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-09 23:04:28,508] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-09 23:04:28,508] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-09 23:04:28,508] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-09 23:04:28,508] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-09 23:04:28,508] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-09 23:04:28,508] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-09 23:04:28,508] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-09 23:04:28,508] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-09 23:04:28,508] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-09 23:04:28,508] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-09 23:04:28,508] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: "we assume that 80 is 100 % assume'x'is value we looking for here, 80 = 100 % and x = 12 % therefore, 80 / x = 100 % / 12 % 80 / x = 8.33 x = 9.6 b"
So the final answer is b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: a : b = 92000 : 20000 = 92 : 20 = 46 : 10 = 23 : 5 answer : c
So the final answer is c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: "actual cost = $ 60 sp = actual cost + mark up = actual cost + 20 % sp = 60 * 100 / 80 on sale sp = 80 / 100 ( 60 * 100 / 80 ) = 60 gross profit = $ 0 answer is a"
So the final answer is a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: "apparently it's q = x ^ 2 - x ^ 3 instead of q = x ^ 2 - q ^ 3. in this case : the x - intercept is the value ( s ) of x for q = 0. 0 = x ^ 2 - x ^ 3 ; 0 = x ^ 2 ( 1 - x ) ; x = 0 or x = 1. answer : c."
So the final answer is c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: "official solution : the problem asks for the amount of weekly sales it takes for angelo and isabella to earn the same amount of money. you can write an equation that sets angelo ’ s and isabella ’ s weekly earnings equal to each other, with x representing weekly sales. weekly earnings for each salesperson equal base salary plus commission. so angelo ’ s earnings are 550 + ( 0.08 ) ( x – 3,000 ), and isabella ’ s are 0.10 x. set up the equation and solve : 550 + ( 0.08 ) ( x – 3,000 ) = 0.10 x distribute the 0.08 : 550 + 0.08 x – 240 = 0.10 x combine terms and subtract 0.08 x from both sides : 310 = 0.02 x divide both sides by 0.02 : 15,500 = x your answer is b."
So the final answer is b

Input: find the total number of prime factors in the expression ( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 a ) 26, b ) 22, c ) 25, d ) 30, e ) 29
Output: "( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = ( 2 x 2 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = 2 ^ 11 x 2 ^ 11 x 7 ^ 5 x 11 ^ 3 = 2 ^ 22 x 7 ^ 5 x 11 ^ 3 total number of prime factors = ( 22 + 5 + 3 ) = 30. answer is d."
So the final answer is d

Input: the length of the bridge, which a train 140 m long and traveling at 45 km / hr can cross in 30 sec is? a ) 235, b ) 240, c ) 245, d ) 250, e ) 255
Output: "speed = 45 * 5 / 18 = 25 / 2 m / sec. time = 30 sec let the length of bridge be x meters. then, ( 140 + x ) / 30 = 25 / 2 x = 235 m. answer : option a"
So the final answer is a

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o7-tmathqa-s1-rTrue-m4096
Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:21:42<2:32:03, 140.37s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [2:20:25<44:27, 111.13s/it]                                                                                               Evaluating commonsenseqa :  36%|███▌      | 36/100 [1:24:01<2:29:19, 139.99s/it]Evaluating commonsenseqa :   1%|          | 1/100 [02:59<4:56:36, 179.77s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [2:22:15<42:32, 111.00s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [1:26:24<2:28:02, 141.00s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [2:24:08<40:52, 111.46s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [06:00<4:54:09, 180.10s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [2:26:00<39:06, 111.74s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [1:28:48<2:26:38, 141.91s/it]        Evaluating commonsenseqa :  80%|████████  | 80/100 [2:27:52<37:16, 111.82s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [08:58<4:49:44, 179.22s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [1:31:11<2:24:36, 142.23s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [2:29:44<35:21, 111.64s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [11:57<4:47:04, 179.42s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [1:33:32<2:21:55, 141.93s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [2:31:34<33:23, 111.33s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [2:33:25<31:31, 111.28s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [1:35:49<2:18:03, 140.41s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [14:55<4:42:43, 178.56s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [2:35:16<29:38, 111.16s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:38:09<2:15:35, 140.26s/it]                                                                                                 Evaluating commonsenseqa :   6%|▌         | 6/100 [17:56<4:41:21, 179.59s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [2:37:06<27:43, 110.89s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:40:29<2:13:14, 140.26s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [2:38:56<25:47, 110.56s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [20:55<4:38:10, 179.47s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:42:48<2:10:28, 139.79s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [2:40:50<24:10, 111.54s/it]                                                                                                 Evaluating commonsenseqa :  88%|████████▊ | 88/100 [2:42:40<22:14, 111.18s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:45:08<2:08:15, 139.92s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [23:53<4:34:23, 178.95s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [2:44:33<20:27, 111.59s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [1:47:30<2:06:16, 140.31s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [26:50<4:30:12, 178.16s/it]                 Evaluating commonsenseqa :  90%|█████████ | 90/100 [2:46:24<18:35, 111.51s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:49:46<2:02:56, 139.19s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [2:48:16<16:43, 111.50s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [29:50<4:28:29, 178.99s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:52:08<2:01:28, 140.16s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [2:50:05<14:46, 110.84s/it]                                                                                                 Evaluating commonsenseqa :  93%|█████████▎| 93/100 [2:51:57<12:57, 111.02s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [32:46<4:24:05, 178.04s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:54:25<1:58:16, 139.14s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [2:53:48<11:06, 111.01s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [1:56:43<1:55:40, 138.81s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [35:43<4:20:38, 177.71s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [2:55:40<09:16, 111.33s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [1:59:03<1:53:31, 139.01s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [2:57:30<07:24, 111.20s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [38:46<4:19:43, 179.12s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [2:01:23<1:51:22, 139.23s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [2:59:20<05:32, 110.78s/it]                                                                                                 Evaluating commonsenseqa :  14%|█▍        | 14/100 [41:40<4:14:48, 177.77s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [3:01:10<03:41, 110.55s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [2:03:43<1:49:23, 139.64s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [3:03:03<01:51, 111.21s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [2:06:03<1:47:02, 139.62s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [44:40<4:12:40, 178.36s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [3:04:56<00:00, 111.76s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [3:04:56<00:00, 110.97s/it]
name: commonsenseqa | avg. gen lenth: 372.937 | time: 11099.444944858551s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13646 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 13 13 15 True False 4096 10
[2023-09-09 23:50:23,137] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 13
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tmathqa-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 603612.44it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.80s/it]
 > number of parameters: 6738415616
[2023-09-09 23:50:39,099] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-09 23:50:39,305] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-09 23:50:39,307] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-09 23:50:39,307] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-09 23:50:39,307] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-09 23:50:39,307] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-09 23:50:39,307] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-09 23:50:39,307] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-09 23:50:39,307] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-09 23:50:39,307] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-09 23:50:39,307] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-09 23:50:39,307] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3be280>
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-09 23:50:39,308] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-09 23:50:39,309] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-09 23:50:39,309] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: b

Input: find the total number of prime factors in the expression ( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 a ) 26, b ) 22, c ) 25, d ) 30, e ) 29
Output: d

Input: the length of the bridge, which a train 140 m long and traveling at 45 km / hr can cross in 30 sec is? a ) 235, b ) 240, c ) 245, d ) 250, e ) 255
Output: a

Input: 360 metres long yard, 31 trees are palnted at equal distances, one tree being at each end of the yard. what is the distance between 2 consecutive trees a ) 10, b ) 12, c ) 14, d ) 16, e ) 17
Output: b

Input: a vessel of capacity 45 litres is fully filled with pure milk. nine litres of milk is removed from the vessel and replaced with water. nine litres of the solution thus formed is removed and replaced with water. find the quantity of pure milk in the final milk solution? a ) 28.8, b ) 72.9, c ) 38.3, d ) 78.3, e ) 79.3
Output: a

Input: a certain telescope increases the visual range at a particular location from 90 kilometers to 150 kilometers. by what percent is the visual range increased by using the telescope? a ) 30 %, b ) 33 1 / 2 %, c ) 40 %, d ) 60 %, e ) 66 2 / 3 %
Output: e

Input: a bag contains 6 black and 3 white balls. one ball is drawn at random. what is the probability that the ball drawn is white? a ) 3 / 4, b ) 1 / 3, c ) 1 / 7, d ) 1 / 8, e ) 4 / 3
Output: b

Input: rajat, vikas and abhishek are submitting questions in the ratio 7 : 3 : 2. if total number of questions submitted by them is 24. find the number of questions submitted by vikas. a ) 3, b ) 4, c ) 5, d ) 6, e ) 7
Output: d

Input: at a certain restaurant, the ratio of the number of cooks to the number of waiters is 3 to 8. when 12 more waiters are hired, the ratio of the number of cooks to the number of waiters changes to 1 to 4. how many cooks does the restaurant have? a ) 4, b ) 6, c ) 9, d ) 12, e ) 15
Output: c

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tmathqa-s1-rFalse-m4096
Evaluating commonsenseqa :  55%|█████▌    | 55/100 [2:08:24<1:44:59, 139.99s/it]    Evaluating commonsenseqa :  16%|█▌        | 16/100 [47:42<4:11:14, 179.45s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [2:10:41<1:42:01, 139.14s/it]Evaluating commonsenseqa :   1%|          | 1/100 [03:23<5:35:35, 203.39s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [50:43<4:09:02, 180.03s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [2:12:57<1:39:06, 138.30s/it]    Evaluating commonsenseqa :   2%|▏         | 2/100 [06:47<5:32:58, 203.86s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [2:15:17<1:37:12, 138.86s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [53:47<4:07:39, 181.21s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [2:17:35<1:34:36, 138.44s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [10:08<5:27:27, 202.55s/it]                   Evaluating commonsenseqa :  19%|█▉        | 19/100 [56:45<4:03:06, 180.08s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [2:19:52<1:32:03, 138.08s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [13:28<5:22:22, 201.49s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [59:49<4:01:44, 181.30s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [2:22:13<1:30:20, 138.99s/it]                                                                                                   Evaluating commonsenseqa :   5%|▌         | 5/100 [16:16<4:59:44, 189.31s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [1:02:45<3:56:36, 179.70s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [2:24:32<1:28:01, 138.98s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [2:26:53<1:26:05, 139.60s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:05:42<3:52:39, 178.97s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [19:42<5:05:30, 195.00s/it]                                                                                                   Evaluating commonsenseqa :  64%|██████▍   | 64/100 [2:29:11<1:23:24, 139.00s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:08:42<3:49:55, 179.17s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [23:07<5:07:41, 198.51s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [2:31:29<1:21:02, 138.93s/it]                                                                                                   Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:11:40<3:46:34, 178.88s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [2:33:47<1:18:35, 138.69s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [26:28<5:05:15, 199.08s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [2:36:05<1:16:09, 138.46s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:14:37<3:42:42, 178.16s/it]                                                                                                   Evaluating commonsenseqa :   9%|▉         | 9/100 [29:47<5:02:12, 199.26s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [2:38:23<1:13:44, 138.26s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:17:35<3:39:47, 178.21s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [2:40:44<1:11:51, 139.07s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [33:08<4:59:21, 199.58s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:20:33<3:36:45, 178.16s/it]            Evaluating commonsenseqa :  70%|███████   | 70/100 [2:43:05<1:09:47, 139.60s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [36:32<4:58:07, 200.99s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:23:32<3:34:11, 178.50s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [2:45:24<1:07:22, 139.40s/it]                                                                                                   Evaluating commonsenseqa :  12%|█▏        | 12/100 [39:54<4:55:16, 201.32s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [2:47:45<1:05:13, 139.78s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:26:30<3:31:00, 178.31s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [2:50:04<1:02:51, 139.69s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [42:28<4:31:06, 186.97s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:29:30<3:28:44, 178.93s/it]                                                                                                 Evaluating commonsenseqa :  74%|███████▍  | 74/100 [2:52:25<1:00:40, 140.02s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [45:49<4:34:16, 191.35s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [1:32:31<3:26:21, 179.45s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [2:54:45<58:17, 139.90s/it]  Evaluating commonsenseqa :  15%|█▌        | 15/100 [49:06<4:33:32, 193.08s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [1:35:29<3:22:55, 179.05s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [2:57:02<55:41, 139.21s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [2:59:23<53:32, 139.66s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [1:38:28<3:19:41, 178.83s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [52:26<4:32:59, 194.99s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [3:01:43<51:18, 139.95s/it]                                                                                                 Evaluating commonsenseqa :  34%|███▍      | 34/100 [1:41:22<3:15:18, 177.55s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [55:50<4:33:33, 197.76s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [3:04:02<48:53, 139.67s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:44:22<3:13:06, 178.26s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [3:06:25<46:49, 140.50s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [59:09<4:30:41, 198.07s/it]                Evaluating commonsenseqa :  81%|████████  | 81/100 [3:08:45<44:24, 140.26s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [1:47:21<3:10:18, 178.41s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [1:02:29<4:28:22, 198.79s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [3:11:02<41:51, 139.54s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [1:50:20<3:07:30, 178.58s/it]          Evaluating commonsenseqa :  83%|████████▎ | 83/100 [3:13:20<39:19, 138.80s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [1:05:48<4:25:00, 198.75s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [1:53:24<3:06:17, 180.28s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [3:15:37<36:54, 138.41s/it]                                                                                                   Evaluating commonsenseqa :  21%|██        | 21/100 [1:09:09<4:22:32, 199.40s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [3:17:57<34:40, 138.73s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [1:56:26<3:03:55, 180.91s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:12:28<4:19:12, 199.39s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [3:20:15<32:20, 138.62s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [1:59:25<3:00:18, 180.32s/it]                                                                                                   Evaluating commonsenseqa :  87%|████████▋ | 87/100 [3:22:38<30:17, 139.82s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:15:50<4:16:39, 199.99s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [2:02:20<2:55:45, 178.74s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [3:24:56<27:51, 139.30s/it]                                                                                                   Evaluating commonsenseqa :  42%|████▏     | 42/100 [2:05:16<2:51:47, 177.71s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:19:09<4:13:10, 199.88s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [3:27:15<25:33, 139.40s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [3:29:35<23:15, 139.59s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [2:08:16<2:49:35, 178.51s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:22:29<4:09:57, 199.96s/it]                                                                                                   Evaluating commonsenseqa :  91%|█████████ | 91/100 [3:31:57<21:02, 140.25s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [2:11:15<2:46:50, 178.76s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:25:52<4:07:34, 200.73s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [3:34:15<18:37, 139.65s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [2:14:14<2:43:49, 178.71s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [3:36:38<16:22, 140.42s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:29:10<4:03:19, 199.99s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [2:17:19<2:42:26, 180.50s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [3:38:57<14:01, 140.19s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:32:34<4:01:23, 201.15s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [3:41:15<11:37, 139.47s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [2:20:14<2:38:09, 179.05s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [3:43:32<09:15, 138.88s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:36:00<3:59:33, 202.44s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [2:23:12<2:34:46, 178.59s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [8:09:08<00:00, 291.34s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [8:09:08<00:00, 293.49s/it]
name: commonsenseqa | avg. gen lenth: 282.71 | time: 29349.99131679535s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu04 --master_port 13643 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 3 1 3 True False 4096 10
[2023-09-10 01:28:42,589] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 3
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o3-tmathqa-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 945472.78it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.43s/it]
 > number of parameters: 6738415616
[2023-09-10 01:28:54,857] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-10 01:28:55,065] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-10 01:28:55,067] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554b5f85430>
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-10 01:28:55,067] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-10 01:28:55,068] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-10 01:28:55,069] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-10 01:28:55,069] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-10 01:28:55,069] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-10 01:28:55,069] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-10 01:28:55,069] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: "we assume that 80 is 100 % assume'x'is value we looking for here, 80 = 100 % and x = 12 % therefore, 80 / x = 100 % / 12 % 80 / x = 8.33 x = 9.6 b"
So the final answer is b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: a : b = 92000 : 20000 = 92 : 20 = 46 : 10 = 23 : 5 answer : c
So the final answer is c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: "actual cost = $ 60 sp = actual cost + mark up = actual cost + 20 % sp = 60 * 100 / 80 on sale sp = 80 / 100 ( 60 * 100 / 80 ) = 60 gross profit = $ 0 answer is a"
So the final answer is a

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o3-tmathqa-s1-rTrue-m4096
Evaluating commonsenseqa :  30%|███       | 30/100 [1:39:22<3:56:04, 202.35s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [2:26:12<2:32:05, 178.94s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [3:48:11<04:38, 139.04s/it]                                                                              Evaluating commonsenseqa :  31%|███       | 31/100 [1:42:42<3:51:57, 201.71s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [3:50:32<02:19, 139.72s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [2:29:12<2:29:23, 179.28s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [3:52:53<00:00, 140.10s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [3:52:53<00:00, 139.74s/it]
name: commonsenseqa | avg. gen lenth: 359.313 | time: 13976.313434123993s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13645 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 9 9 11 True False 4096 10
[2023-09-10 01:36:00,753] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 9
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o9-tmathqa-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 761683.73it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.41s/it]
 > number of parameters: 6738415616
[2023-09-10 01:36:15,105] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-10 01:36:15,315] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-10 01:36:15,317] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-10 01:36:15,317] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-10 01:36:15,317] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-10 01:36:15,317] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-10 01:36:15,317] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-10 01:36:15,317] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-10 01:36:15,317] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-10 01:36:15,317] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-10 01:36:15,317] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-10 01:36:15,317] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-10 01:36:15,317] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0250>
[2023-09-10 01:36:15,317] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-10 01:36:15,317] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-10 01:36:15,317] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-10 01:36:15,318] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-10 01:36:15,319] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-10 01:36:15,319] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-10 01:36:15,319] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-10 01:36:15,319] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-10 01:36:15,319] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-10 01:36:15,319] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-10 01:36:15,319] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-10 01:36:15,319] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-10 01:36:15,319] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-10 01:36:15,319] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-10 01:36:15,319] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-10 01:36:15,319] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-10 01:36:15,319] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: b

Input: find the total number of prime factors in the expression ( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 a ) 26, b ) 22, c ) 25, d ) 30, e ) 29
Output: d

Input: the length of the bridge, which a train 140 m long and traveling at 45 km / hr can cross in 30 sec is? a ) 235, b ) 240, c ) 245, d ) 250, e ) 255
Output: a

Input: 360 metres long yard, 31 trees are palnted at equal distances, one tree being at each end of the yard. what is the distance between 2 consecutive trees a ) 10, b ) 12, c ) 14, d ) 16, e ) 17
Output: b

Input: a vessel of capacity 45 litres is fully filled with pure milk. nine litres of milk is removed from the vessel and replaced with water. nine litres of the solution thus formed is removed and replaced with water. find the quantity of pure milk in the final milk solution? a ) 28.8, b ) 72.9, c ) 38.3, d ) 78.3, e ) 79.3
Output: a

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o9-tmathqa-s1-rFalse-m4096
Evaluating commonsenseqa :  51%|█████     | 51/100 [2:32:12<2:26:43, 179.65s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [1:46:05<3:48:57, 202.03s/it]                                                                                Evaluating commonsenseqa :  33%|███▎      | 33/100 [1:47:56<3:15:16, 174.88s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [2:35:12<2:23:41, 179.61s/it]Evaluating commonsenseqa :   1%|          | 1/100 [03:51<6:22:20, 231.72s/it]                                                                                Evaluating commonsenseqa :  34%|███▍      | 34/100 [1:51:14<3:20:04, 181.88s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [2:38:11<2:20:37, 179.53s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [07:42<6:17:18, 231.01s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:54:36<3:23:35, 187.93s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [2:41:13<2:18:14, 180.33s/it]                                                                                Evaluating commonsenseqa :   3%|▎         | 3/100 [11:32<6:12:42, 230.54s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [1:57:54<3:23:30, 190.78s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [2:44:10<2:14:30, 179.34s/it]                                                                                Evaluating commonsenseqa :  56%|█████▌    | 56/100 [2:47:06<2:10:39, 178.17s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [15:26<6:11:14, 232.03s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [2:01:13<3:22:55, 193.26s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [2:50:03<2:07:30, 177.92s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [2:04:31<3:21:20, 194.85s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [19:17<6:06:45, 231.64s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [2:53:05<2:05:23, 179.13s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [2:07:55<3:20:37, 197.34s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [23:09<6:03:02, 231.73s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [2:56:06<2:02:52, 179.83s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [2:11:16<3:18:24, 198.41s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [27:02<5:59:51, 232.16s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [2:59:05<1:59:33, 179.34s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [2:14:40<3:16:53, 200.22s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [3:02:04<1:56:37, 179.42s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [30:57<5:57:13, 232.98s/it]                                                                                Evaluating commonsenseqa :  42%|████▏     | 42/100 [2:17:59<3:13:13, 199.89s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [3:05:03<1:53:29, 179.21s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [34:46<5:51:46, 231.94s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [2:21:21<3:10:35, 200.63s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [3:08:03<1:50:45, 179.60s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [38:42<5:49:49, 233.21s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [2:24:39<3:06:27, 199.77s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [3:11:04<1:47:57, 179.92s/it]                                                                                 Evaluating commonsenseqa :  65%|██████▌   | 65/100 [3:14:03<1:44:44, 179.55s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [2:27:58<3:02:46, 199.39s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [42:33<5:44:33, 232.29s/it]                                                                                   Evaluating commonsenseqa :  66%|██████▌   | 66/100 [3:17:00<1:41:24, 178.96s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [2:31:20<3:00:09, 200.17s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [46:24<5:40:25, 232.11s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [3:20:02<1:38:56, 179.88s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [2:34:37<2:56:02, 199.30s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [50:17<5:36:56, 232.37s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [3:23:01<1:35:44, 179.53s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [2:37:57<2:52:55, 199.52s/it]                                                                                   Evaluating commonsenseqa :  14%|█▍        | 14/100 [54:08<5:32:26, 231.93s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [3:25:57<1:32:12, 178.48s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [2:41:15<2:49:15, 199.12s/it]                                                                                     Evaluating commonsenseqa :  70%|███████   | 70/100 [3:29:00<1:29:55, 179.86s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [57:58<5:27:51, 231.43s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [2:44:36<2:46:16, 199.52s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [3:31:59<1:26:42, 179.38s/it]                                                                                     Evaluating commonsenseqa :  16%|█▌        | 16/100 [1:01:51<5:24:32, 231.82s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [2:47:55<2:42:52, 199.45s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [3:34:58<1:23:42, 179.38s/it]                                                                                     Evaluating commonsenseqa :  52%|█████▏    | 52/100 [2:51:14<2:39:24, 199.27s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [1:05:43<5:20:37, 231.77s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [3:38:01<1:21:12, 180.45s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [2:54:32<2:35:45, 198.84s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [3:40:59<1:17:55, 179.81s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [1:09:37<5:17:36, 232.39s/it]                                                                                     Evaluating commonsenseqa :  75%|███████▌  | 75/100 [3:44:00<1:15:00, 180.02s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [2:57:54<2:33:21, 200.04s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [1:13:29<5:13:37, 232.32s/it]                                                                                     Evaluating commonsenseqa :  76%|███████▌  | 76/100 [3:46:55<1:11:23, 178.49s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [3:01:14<2:30:01, 200.03s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [1:17:20<5:09:20, 232.00s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [3:49:52<1:08:14, 178.03s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [3:04:38<2:27:29, 201.13s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [3:52:55<1:05:54, 179.76s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [1:21:15<5:06:36, 232.86s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [3:08:01<2:24:32, 201.69s/it]                                                                                     Evaluating commonsenseqa :  79%|███████▉  | 79/100 [3:55:54<1:02:48, 179.44s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:25:08<5:02:51, 232.96s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [3:11:09<2:18:20, 197.62s/it]                                                                                       Evaluating commonsenseqa :  80%|████████  | 80/100 [3:58:57<1:00:11, 180.58s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [3:13:48<2:07:08, 186.07s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:28:59<4:57:59, 232.19s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [4:01:55<56:55, 179.74s/it]                                                                                         Evaluating commonsenseqa :  60%|██████    | 60/100 [3:17:09<2:06:59, 190.50s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:32:50<4:53:40, 231.85s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [4:04:57<54:09, 180.55s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [3:20:34<2:06:31, 194.65s/it]                                                                                       Evaluating commonsenseqa :  83%|████████▎ | 83/100 [4:07:55<50:55, 179.73s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:36:39<4:48:45, 231.01s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [3:23:51<2:03:52, 195.59s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [4:10:54<47:51, 179.47s/it]                                                                                       Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:40:28<4:44:29, 230.66s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [3:27:15<2:02:06, 198.00s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [4:13:54<44:55, 179.71s/it]                                                                                       Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:44:22<4:41:31, 231.39s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [3:30:37<1:59:27, 199.11s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [4:16:52<41:48, 179.20s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [4:19:55<39:01, 180.15s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:48:10<4:36:42, 230.59s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [3:33:54<1:55:54, 198.70s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [4:22:52<35:51, 179.33s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [3:37:17<1:53:19, 199.97s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:51:57<4:31:38, 229.56s/it]                                                                                       Evaluating commonsenseqa :  89%|████████▉ | 89/100 [4:25:54<33:00, 180.04s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [3:40:41<1:50:34, 201.04s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:55:51<4:29:20, 230.86s/it]                                                                                       Evaluating commonsenseqa :  90%|█████████ | 90/100 [4:28:55<30:04, 180.47s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [3:44:00<1:46:50, 200.32s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [1:59:40<4:24:37, 230.10s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [4:32:01<27:17, 181.91s/it]                                                                                       Evaluating commonsenseqa :  69%|██████▉   | 69/100 [3:47:19<1:43:25, 200.17s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [4:34:59<24:05, 180.72s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [2:03:24<4:18:55, 228.46s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [3:50:46<1:41:03, 202.13s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [4:37:57<20:59, 179.88s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [2:07:13<4:15:20, 228.67s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [3:54:10<1:37:58, 202.72s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [4:40:59<18:04, 180.75s/it]                                                                                         Evaluating commonsenseqa :  34%|███▍      | 34/100 [2:11:07<4:13:10, 230.16s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [3:57:33<1:34:37, 202.77s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [4:43:55<14:55, 179.16s/it]                                                                                         Evaluating commonsenseqa :  35%|███▌      | 35/100 [2:14:56<4:08:56, 229.80s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [4:46:53<11:55, 178.97s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [4:00:56<1:31:16, 202.84s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [4:49:52<08:56, 178.84s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [4:04:17<1:27:41, 202.37s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [2:18:47<4:05:23, 230.05s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [4:52:51<05:57, 178.96s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [4:07:42<1:24:33, 202.93s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [2:22:41<4:02:46, 231.22s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [4:55:51<02:59, 179.30s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [4:11:03<1:21:00, 202.54s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [2:26:33<3:59:24, 231.68s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [4:58:47<00:00, 178.36s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [4:58:47<00:00, 179.28s/it]
name: commonsenseqa | avg. gen lenth: 324.003 | time: 17929.82593512535s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13644 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 5 5 7 True False 4096 10
[2023-09-10 04:03:24,758] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 5
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o5-tmathqa-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 935578.55it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.76s/it]
 > number of parameters: 6738415616
[2023-09-10 04:03:37,714] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-10 04:03:37,918] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-10 04:03:37,919] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3be280>
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-10 04:03:37,920] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-10 04:03:37,921] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-10 04:03:37,922] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: b

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o5-tmathqa-s1-rFalse-m4096
Evaluating commonsenseqa :  77%|███████▋  | 77/100 [4:14:24<1:17:23, 201.88s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [2:30:28<3:56:31, 232.65s/it]                                                                                         Evaluating commonsenseqa :   1%|          | 1/100 [04:21<7:11:59, 261.81s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [4:17:46<1:14:03, 201.97s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [2:32:41<3:22:45, 202.76s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [4:21:05<1:10:26, 201.25s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [08:41<7:05:48, 260.70s/it]         Evaluating commonsenseqa :  41%|████      | 41/100 [2:36:32<3:27:38, 211.17s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [4:24:25<1:06:53, 200.68s/it]                                                                                         Evaluating commonsenseqa :  42%|████▏     | 42/100 [2:40:22<3:29:30, 216.74s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [13:03<7:02:13, 261.17s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [4:27:46<1:03:35, 200.84s/it]                                                                                         Evaluating commonsenseqa :  43%|████▎     | 43/100 [2:44:13<3:30:07, 221.18s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [17:26<6:59:06, 261.94s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [4:31:12<1:00:41, 202.32s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [2:48:05<3:29:17, 224.23s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [4:34:31<57:04, 201.41s/it]  Evaluating commonsenseqa :   5%|▌         | 5/100 [21:48<6:54:33, 261.83s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [4:37:20<51:08, 191.79s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [2:51:57<3:27:47, 226.67s/it]                                                                                           Evaluating commonsenseqa :   6%|▌         | 6/100 [26:05<6:48:01, 260.44s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [4:40:40<48:31, 194.13s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [2:55:51<3:25:56, 228.83s/it]                                                                                           Evaluating commonsenseqa :   7%|▋         | 7/100 [30:27<6:44:08, 260.73s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [4:44:02<45:53, 196.67s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [2:59:46<3:23:39, 230.55s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [4:46:36<39:48, 183.71s/it]                                                                                           Evaluating commonsenseqa :   8%|▊         | 8/100 [34:51<6:41:35, 261.91s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [4:48:32<32:41, 163.49s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [3:03:39<3:20:29, 231.34s/it]                                                                                           Evaluating commonsenseqa :  89%|████████▉ | 89/100 [4:51:50<31:50, 173.67s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [39:08<6:34:42, 260.25s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [3:07:40<3:19:03, 234.18s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [4:53:49<26:12, 157.23s/it]                                                                                           Evaluating commonsenseqa :  10%|█         | 10/100 [43:31<6:31:49, 261.22s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [4:57:09<25:32, 170.26s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [3:11:35<3:15:23, 234.47s/it]                                                                                           Evaluating commonsenseqa :  92%|█████████▏| 92/100 [5:00:29<23:52, 179.11s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [47:55<6:28:29, 261.90s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [3:15:25<3:10:30, 233.27s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [5:03:53<21:45, 186.49s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [3:19:19<3:06:43, 233.41s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [52:15<6:23:11, 261.27s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [5:07:15<19:08, 191.39s/it]                                                                                           Evaluating commonsenseqa :  53%|█████▎    | 53/100 [3:23:13<3:02:54, 233.51s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [56:38<6:19:49, 261.95s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [5:10:36<16:10, 194.18s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [3:27:07<2:59:09, 233.68s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [5:13:56<13:03, 195.97s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [1:00:58<6:14:29, 261.27s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [3:31:01<2:55:19, 233.76s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [5:17:18<09:52, 197.56s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [1:05:14<6:08:04, 259.81s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [3:34:51<2:50:34, 232.61s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [5:20:42<06:39, 199.61s/it]                                                                                             Evaluating commonsenseqa :  16%|█▌        | 16/100 [1:09:33<6:03:10, 259.41s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [5:24:07<03:21, 201.11s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [3:38:38<2:45:35, 231.07s/it]                                                                                             Evaluating commonsenseqa :  17%|█▋        | 17/100 [1:13:51<5:58:23, 259.08s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [5:27:26<00:00, 200.68s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [5:27:26<00:00, 196.47s/it]
name: commonsenseqa | avg. gen lenth: 237.755 | time: 19648.491233348846s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13646 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 15 13 15 True False 4096 10
[2023-09-10 05:18:14,983] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 15
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o15-tmathqa-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 584318.46it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.09s/it]
 > number of parameters: 6738415616
[2023-09-10 05:18:28,610] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-10 05:18:28,814] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-10 05:18:28,816] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-10 05:18:28,816] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-10 05:18:28,816] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-10 05:18:28,816] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-10 05:18:28,816] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0280>
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-10 05:18:28,817] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-10 05:18:28,818] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-10 05:18:28,818] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: b

Input: find the total number of prime factors in the expression ( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 a ) 26, b ) 22, c ) 25, d ) 30, e ) 29
Output: d

Input: the length of the bridge, which a train 140 m long and traveling at 45 km / hr can cross in 30 sec is? a ) 235, b ) 240, c ) 245, d ) 250, e ) 255
Output: a

Input: 360 metres long yard, 31 trees are palnted at equal distances, one tree being at each end of the yard. what is the distance between 2 consecutive trees a ) 10, b ) 12, c ) 14, d ) 16, e ) 17
Output: b

Input: a vessel of capacity 45 litres is fully filled with pure milk. nine litres of milk is removed from the vessel and replaced with water. nine litres of the solution thus formed is removed and replaced with water. find the quantity of pure milk in the final milk solution? a ) 28.8, b ) 72.9, c ) 38.3, d ) 78.3, e ) 79.3
Output: a

Input: a certain telescope increases the visual range at a particular location from 90 kilometers to 150 kilometers. by what percent is the visual range increased by using the telescope? a ) 30 %, b ) 33 1 / 2 %, c ) 40 %, d ) 60 %, e ) 66 2 / 3 %
Output: e

Input: a bag contains 6 black and 3 white balls. one ball is drawn at random. what is the probability that the ball drawn is white? a ) 3 / 4, b ) 1 / 3, c ) 1 / 7, d ) 1 / 8, e ) 4 / 3
Output: b

Input: rajat, vikas and abhishek are submitting questions in the ratio 7 : 3 : 2. if total number of questions submitted by them is 24. find the number of questions submitted by vikas. a ) 3, b ) 4, c ) 5, d ) 6, e ) 7
Output: d

Input: at a certain restaurant, the ratio of the number of cooks to the number of waiters is 3 to 8. when 12 more waiters are hired, the ratio of the number of cooks to the number of waiters changes to 1 to 4. how many cooks does the restaurant have? a ) 4, b ) 6, c ) 9, d ) 12, e ) 15
Output: c

Input: if 7 ^ k + 7 ^ k = ( 7 ^ 9 ) ^ ( 7 ^ 9 ) - 7 ^ k, then k =? a ) 11 / 3, b ) 11 / 2, c ) 242, d ) 3 ^ 10, e ) 7 ^ 11 - 1
Output: e

Input: average of 10 matches is 32, how many runs one should should score to increase his average by 4 runs. a ) 70, b ) 76, c ) 78, d ) 80, e ) 88
Output: b

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o15-tmathqa-s1-rFalse-m4096
Evaluating commonsenseqa :  58%|█████▊    | 58/100 [3:42:33<2:42:27, 232.08s/it]                                                                                             Evaluating commonsenseqa :   1%|          | 1/100 [03:11<5:15:26, 191.18s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [1:18:08<5:53:14, 258.47s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [3:46:21<2:37:57, 231.15s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [06:19<5:09:29, 189.49s/it]             Evaluating commonsenseqa :  60%|██████    | 60/100 [3:49:44<2:28:19, 222.49s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [1:22:29<5:50:02, 259.29s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [09:28<5:05:58, 189.26s/it]                                                                                             Evaluating commonsenseqa :  61%|██████    | 61/100 [3:53:34<2:26:12, 224.94s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [1:26:47<5:45:11, 258.89s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [12:39<5:04:06, 190.07s/it]                                                                                             Evaluating commonsenseqa :  62%|██████▏   | 62/100 [3:57:31<2:24:35, 228.29s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [15:46<4:58:48, 188.72s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [1:31:06<5:40:42, 258.76s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [18:55<4:56:15, 189.11s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [4:01:21<2:21:16, 229.09s/it]                                                                                             Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:35:28<5:37:52, 259.90s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [22:05<4:53:20, 189.26s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [4:05:12<2:17:44, 229.58s/it]                                                                                             Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:39:49<5:34:03, 260.30s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [25:18<4:52:02, 190.46s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [4:09:01<2:13:48, 229.39s/it]                                                                                             Evaluating commonsenseqa :   9%|▉         | 9/100 [28:25<4:47:09, 189.33s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [4:11:09<1:52:44, 198.95s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:44:10<5:29:45, 260.33s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [31:38<4:45:41, 190.46s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [4:15:03<1:55:13, 209.51s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:48:07<5:16:37, 253.30s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [34:48<4:42:11, 190.24s/it]                                                                                               Evaluating commonsenseqa :  68%|██████▊   | 68/100 [4:18:57<1:55:35, 216.75s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:52:26<5:14:31, 255.02s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [38:02<4:40:40, 191.37s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [4:22:49<1:54:23, 221.40s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [41:10<4:36:05, 190.41s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:56:41<5:10:16, 255.03s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:58:04<4:04:14, 203.54s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [44:20<4:32:53, 190.39s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [4:26:43<1:52:36, 225.21s/it]                                                                                               Evaluating commonsenseqa :  71%|███████   | 71/100 [4:28:55<1:35:18, 197.20s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [47:30<4:29:18, 190.10s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [2:02:25<4:21:04, 220.62s/it]                                                                                               Evaluating commonsenseqa :  72%|███████▏  | 72/100 [4:32:48<1:37:01, 207.92s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [50:35<4:24:04, 188.63s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [2:06:48<4:32:14, 233.35s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [53:46<4:22:05, 189.46s/it]            Evaluating commonsenseqa :  73%|███████▎  | 73/100 [4:36:40<1:36:48, 215.14s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [2:11:10<4:38:13, 241.93s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [56:55<4:18:40, 189.27s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [4:40:29<1:35:02, 219.31s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [58:43<3:42:29, 164.80s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [2:15:29<4:40:02, 247.09s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [1:01:52<3:49:36, 172.21s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [4:44:19<1:32:40, 222.44s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [1:03:59<3:28:43, 158.53s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [2:19:50<4:40:38, 251.33s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [4:48:13<1:30:25, 226.08s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:06:14<3:16:57, 151.51s/it]                                                                                               Evaluating commonsenseqa :  34%|███▍      | 34/100 [2:23:42<4:30:11, 245.63s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:09:22<3:28:28, 162.45s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [4:52:04<1:27:14, 227.59s/it]                                                                                               Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:12:29<3:35:06, 169.83s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [2:27:58<4:29:20, 248.63s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [4:55:59<1:24:10, 229.55s/it]                                                                                               Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:15:38<3:39:31, 175.62s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [2:31:37<4:15:41, 239.72s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [4:59:49<1:20:28, 229.92s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:18:49<3:42:18, 180.25s/it]                                                                                                 Evaluating commonsenseqa :  37%|███▋      | 37/100 [2:36:02<4:19:36, 247.25s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [5:03:42<1:16:56, 230.82s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:22:00<3:43:02, 183.32s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [5:13:30<1:56:34, 259.06s/it]                                                                                                                                                                                                                                                                                                                                                                        Evaluating commonsenseqa :  74%|███████▍  | 74/100 [5:17:46<1:51:58, 258.41s/it]                                                                                                                                                                                                                                                                                   Evaluating commonsenseqa :  75%|███████▌  | 75/100 [5:22:05<1:47:39, 258.39s/it]                                                                                                                                                                                                                                                                                                                                                                              Evaluating commonsenseqa :  76%|███████▌  | 76/100 [5:26:25<1:43:31, 258.81s/it]                                                                                                                                                                                                                                                                                   Evaluating commonsenseqa :  77%|███████▋  | 77/100 [5:30:49<1:39:51, 260.48s/it]                                                                                                                                                                                                                                                                                     Evaluating commonsenseqa :  78%|███████▊  | 78/100 [5:35:08<1:35:19, 259.97s/it]                                                                                                                                                                                                                                                                                     Evaluating commonsenseqa :  79%|███████▉  | 79/100 [5:39:23<1:30:27, 258.43s/it]                                                                                                                                                                                                                                                                                                                                                                              Evaluating commonsenseqa :  80%|████████  | 80/100 [5:43:47<1:26:42, 260.11s/it]                                                                                                                                                                                          Evaluating commonsenseqa :  81%|████████  | 81/100 [5:48:00<1:21:46, 258.23s/it]                                                                                                                                                                                                                                                                                     Evaluating commonsenseqa :  82%|████████▏ | 82/100 [5:52:28<1:18:16, 260.90s/it]                                                                                                                                                                                                                                                                                                                                                                                Evaluating commonsenseqa :  92%|█████████▏| 92/100 [5:49:21<30:57, 232.17s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [3:23:08<3:42:55, 257.21s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [2:09:27<2:57:50, 187.21s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [5:53:15<27:09, 232.80s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [2:12:34<2:54:43, 187.20s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [3:27:36<3:41:22, 260.45s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [5:57:11<23:23, 233.86s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [2:15:41<2:51:32, 187.13s/it]        Evaluating commonsenseqa :  50%|█████     | 50/100 [3:31:55<3:36:31, 259.84s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [2:17:36<2:28:47, 165.33s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [6:01:06<19:31, 234.25s/it]                                                                                                   Evaluating commonsenseqa :  47%|████▋     | 47/100 [2:20:44<2:32:16, 172.38s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [3:36:21<3:33:53, 261.92s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [6:05:03<15:39, 234.99s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [2:23:54<2:33:59, 177.68s/it]        Evaluating commonsenseqa :  52%|█████▏    | 52/100 [3:40:44<3:29:48, 262.27s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [6:08:58<11:44, 234.98s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [2:27:02<2:33:36, 180.71s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [3:42:32<2:49:06, 215.88s/it]                                                                                                 Evaluating commonsenseqa :  50%|█████     | 50/100 [2:30:12<2:32:46, 183.33s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [6:12:47<07:46, 233.23s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [3:46:56<2:56:32, 230.27s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [2:33:17<2:30:17, 184.03s/it]      Evaluating commonsenseqa :  99%|█████████▉| 99/100 [6:16:41<03:53, 233.49s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [2:36:29<2:28:57, 186.20s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [3:51:20<3:00:12, 240.29s/it]                                                                                                 Evaluating commonsenseqa : 100%|██████████| 100/100 [6:20:34<00:00, 233.14s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [6:20:34<00:00, 228.34s/it]
name: commonsenseqa | avg. gen lenth: 221.908 | time: 22835.6094892025s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13645 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 11 9 11 True False 4096 10
[2023-09-10 07:56:55,886] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 11
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o11-tmathqa-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 684229.56it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.24s/it]
 > number of parameters: 6738415616
[2023-09-10 07:57:09,744] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-10 07:57:09,949] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-10 07:57:09,950] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-10 07:57:09,950] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-10 07:57:09,950] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-10 07:57:09,950] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-10 07:57:09,950] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0280>
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-10 07:57:09,951] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-10 07:57:09,952] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-10 07:57:09,952] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: b

Input: find the total number of prime factors in the expression ( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 a ) 26, b ) 22, c ) 25, d ) 30, e ) 29
Output: d

Input: the length of the bridge, which a train 140 m long and traveling at 45 km / hr can cross in 30 sec is? a ) 235, b ) 240, c ) 245, d ) 250, e ) 255
Output: a

Input: 360 metres long yard, 31 trees are palnted at equal distances, one tree being at each end of the yard. what is the distance between 2 consecutive trees a ) 10, b ) 12, c ) 14, d ) 16, e ) 17
Output: b

Input: a vessel of capacity 45 litres is fully filled with pure milk. nine litres of milk is removed from the vessel and replaced with water. nine litres of the solution thus formed is removed and replaced with water. find the quantity of pure milk in the final milk solution? a ) 28.8, b ) 72.9, c ) 38.3, d ) 78.3, e ) 79.3
Output: a

Input: a certain telescope increases the visual range at a particular location from 90 kilometers to 150 kilometers. by what percent is the visual range increased by using the telescope? a ) 30 %, b ) 33 1 / 2 %, c ) 40 %, d ) 60 %, e ) 66 2 / 3 %
Output: e

Input: a bag contains 6 black and 3 white balls. one ball is drawn at random. what is the probability that the ball drawn is white? a ) 3 / 4, b ) 1 / 3, c ) 1 / 7, d ) 1 / 8, e ) 4 / 3
Output: b

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o11-tmathqa-s1-rFalse-m4096
Evaluating commonsenseqa :  53%|█████▎    | 53/100 [2:39:41<2:27:14, 187.97s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [3:55:38<3:00:07, 245.62s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [2:41:29<2:05:46, 164.06s/it]Evaluating commonsenseqa :   1%|          | 1/100 [03:37<5:59:06, 217.64s/it]                   Evaluating commonsenseqa :  57%|█████▋    | 57/100 [3:59:19<2:50:47, 238.33s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [2:44:42<2:09:39, 172.87s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [07:14<5:55:11, 217.47s/it]                   Evaluating commonsenseqa :  56%|█████▌    | 56/100 [2:47:52<2:10:23, 177.80s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [4:03:34<2:50:21, 243.37s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [10:52<5:51:35, 217.48s/it]                                                                                                   Evaluating commonsenseqa :  57%|█████▋    | 57/100 [2:51:02<2:10:09, 181.61s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [4:07:54<2:49:47, 248.48s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [14:30<5:48:30, 217.82s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [2:54:14<2:09:12, 184.59s/it]                                                                                                   Evaluating commonsenseqa :   5%|▌         | 5/100 [18:09<5:45:18, 218.09s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [4:12:13<2:47:39, 251.48s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [2:57:24<2:07:16, 186.26s/it]                                                                                                   Evaluating commonsenseqa :  60%|██████    | 60/100 [2:59:59<1:57:57, 176.93s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [21:48<5:42:13, 218.44s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [4:16:33<2:45:05, 253.97s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [3:03:10<1:57:40, 181.03s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [25:26<5:38:05, 218.13s/it]                   Evaluating commonsenseqa :  62%|██████▏   | 62/100 [4:20:54<2:42:12, 256.13s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [3:06:18<1:56:05, 183.31s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [28:02<5:04:31, 198.60s/it]                                                                                                   Evaluating commonsenseqa :  63%|██████▎   | 63/100 [3:09:26<1:53:51, 184.64s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [31:41<5:10:35, 204.78s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [4:25:19<2:39:33, 258.74s/it]                                                                                                   Evaluating commonsenseqa :  64%|██████▍   | 64/100 [3:12:34<1:51:25, 185.70s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [35:18<5:12:56, 208.63s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [4:29:43<2:36:12, 260.36s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [3:15:42<1:48:44, 186.41s/it]                                                                                                   Evaluating commonsenseqa :  11%|█         | 11/100 [38:53<5:12:11, 210.47s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [4:33:14<2:23:20, 245.73s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [3:18:50<1:45:52, 186.83s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [7:10:10<00:00, 259.23s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [7:10:10<00:00, 258.11s/it]
name: commonsenseqa | avg. gen lenth: 304.731 | time: 25812.04271697998s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu04 --master_port 13643 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 1 1 3 True False 4096 10
[2023-09-10 08:39:15,395] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 1
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o1-tmathqa-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 1407202.43it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.85s/it]
 > number of parameters: 6738415616
[2023-09-10 08:39:26,423] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-10 08:39:26,625] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-10 08:39:26,626] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-10 08:39:26,626] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-10 08:39:26,626] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ad6d0250>
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-10 08:39:26,627] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-10 08:39:26,628] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-10 08:39:26,628] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: b

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o1-tmathqa-s1-rFalse-m4096
                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Evaluating commonsenseqa :   1%|          | 1/100 [05:01<8:17:34, 301.56s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [4:42:08<2:21:07, 256.58s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [3:28:21<1:37:47, 189.26s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [49:45<5:08:40, 215.36s/it]                                                                                Evaluating commonsenseqa :  68%|██████▊   | 68/100 [4:46:15<2:15:20, 253.77s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [3:31:28<1:34:14, 188.50s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [53:22<5:05:49, 215.87s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [3:34:33<1:30:37, 187.51s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [56:57<5:01:55, 215.66s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [4:50:40<2:12:50, 257.12s/it]                                                                                Evaluating commonsenseqa :  72%|███████▏  | 72/100 [3:37:44<1:28:01, 188.63s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [1:00:28<4:56:04, 214.03s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [4:55:00<2:08:56, 257.87s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [3:40:53<1:24:53, 188.64s/it]                                                                                Evaluating commonsenseqa :  18%|█▊        | 18/100 [1:04:03<4:53:16, 214.59s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [3:44:02<1:21:50, 188.87s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [4:59:22<2:05:21, 259.35s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [1:07:39<4:50:00, 214.82s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [3:47:14<1:19:01, 189.67s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [5:03:43<2:01:13, 259.77s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [1:11:22<4:49:46, 217.33s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [3:50:21<1:15:33, 188.88s/it]                                                                                Evaluating commonsenseqa :  73%|███████▎  | 73/100 [5:06:43<1:46:07, 235.84s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [3:53:33<1:12:48, 189.95s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [1:14:53<4:43:35, 215.38s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [5:11:06<1:45:46, 244.08s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [3:56:41<1:09:27, 189.42s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:18:28<4:39:53, 215.31s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [3:59:49<1:06:04, 188.76s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [5:15:30<1:44:06, 249.87s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:21:58<4:34:24, 213.82s/it]                                                                                Evaluating commonsenseqa :  80%|████████  | 80/100 [4:02:58<1:03:00, 189.03s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:24:46<4:13:08, 199.84s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [5:19:47<1:40:48, 252.00s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [4:06:10<1:00:04, 189.70s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:28:19<4:14:52, 203.89s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [5:24:05<1:37:18, 253.87s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [4:09:15<56:33, 188.54s/it]  Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:31:57<4:16:49, 208.24s/it]                                                                                 Evaluating commonsenseqa :  83%|████████▎ | 83/100 [4:12:23<53:21, 188.31s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [5:28:23<1:33:33, 255.14s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:35:31<4:15:23, 209.92s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [4:15:27<49:51, 186.98s/it]                                                                                 Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:38:40<4:04:17, 203.58s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [5:32:48<1:30:21, 258.15s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [4:18:38<47:02, 188.14s/it]                                                                                   Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:42:17<4:05:43, 207.66s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [4:21:44<43:46, 187.60s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [5:37:13<1:26:39, 259.98s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:45:56<4:06:12, 211.04s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [4:24:57<40:58, 189.13s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [5:41:37<1:22:45, 261.34s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [4:28:03<37:39, 188.31s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [1:49:30<4:03:40, 211.89s/it]                                                                                     Evaluating commonsenseqa :  82%|████████▏ | 82/100 [5:46:01<1:18:36, 262.04s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [4:31:13<34:36, 188.79s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [1:53:08<4:02:18, 213.81s/it]                                                                                     Evaluating commonsenseqa :  90%|█████████ | 90/100 [4:34:26<31:39, 190.00s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [1:56:44<3:59:29, 214.47s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [5:50:26<1:14:31, 263.03s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [4:37:33<28:22, 189.22s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [2:00:20<3:56:20, 214.85s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [5:54:51<1:10:17, 263.58s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [4:40:49<25:29, 191.20s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [2:04:01<3:54:50, 216.77s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [4:44:01<22:18, 191.22s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [5:59:17<1:06:03, 264.23s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [4:45:39<16:19, 163.29s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [2:07:39<3:51:42, 217.22s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [4:48:54<14:24, 172.95s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [6:03:46<1:02:02, 265.89s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [2:11:14<3:47:22, 216.54s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [4:52:07<11:55, 178.90s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [6:08:05<57:05, 263.53s/it]  Evaluating commonsenseqa :  38%|███▊      | 38/100 [2:14:51<3:43:42, 216.49s/it]                                                                                     Evaluating commonsenseqa :  97%|█████████▋| 97/100 [4:55:18<09:07, 182.42s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [2:18:25<3:39:33, 215.96s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [6:12:29<52:47, 263.96s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [4:58:30<06:10, 185.39s/it]                                                                                     Evaluating commonsenseqa :  40%|████      | 40/100 [2:22:07<3:37:40, 217.67s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [5:01:43<03:07, 187.73s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [6:16:50<48:11, 262.87s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [2:25:43<3:33:25, 217.05s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [5:04:53<00:00, 188.28s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [5:04:53<00:00, 182.93s/it]
name: commonsenseqa | avg. gen lenth: 232.306 | time: 18294.843970775604s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13646 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 13 13 15 True False 4096 10
[2023-09-10 10:23:30,592] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 13
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tmathqa-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 418879.98it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.94s/it]
 > number of parameters: 6738415616
[2023-09-10 10:23:44,123] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-10 10:23:44,337] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-10 10:23:44,339] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-10 10:23:44,339] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-10 10:23:44,339] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-10 10:23:44,339] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-10 10:23:44,339] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3be250>
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-10 10:23:44,340] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-10 10:23:44,341] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-10 10:23:44,341] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: in a recent head - to - head run - off election, 12000 absentee ballets were cast. 1 / 6 of the absentee ballets were thrown out and 3 / 5 of the remaining absentee ballets were cast for candidate a. how many absentee votes did candidate b receive? a ) 2,000, b ) 3,000, c ) 4,000, d ) 8,000, e ) 9,000
Output: 5 / 6 * 2 / 5 ( total absentee votes ) = 1 / 3 ( total votes ) = 1 / 3 * 12000 = 4000 answer is c
So the final answer is c

Input: the total age of a and b is 11 years more than the total age of b and c. c is how many years younger than a.? a ) 16, b ) 12, c ) 11, d ) 20, e ) 10
Output: "( a + b ) - ( b - c ) = 11 a - c = 11 answer is c"
So the final answer is c

Input: the scoring system in a certain football competition goes as follows : 3 points for victory, 1 point for a draw, and 0 points for defeat. each team plays 20 matches. if a team scored 14 points after 5 games, what is the least number of the remaining matches it has to win to reach the 40 - point mark by the end of the tournament? a ) 6, b ) 7, c ) 8, d ) 9, e ) 10
Output: "to get 40 points as end of season we need another 26 points or more from remaining 15 matches : option a = 6 * 3 + 9 * 1 = 27 hence option a - 6"
So the final answer is a

Input: how much water should be added to 10 liters of a 20 % - solution of alcohol to reduce the concentration of alcohol in the solution by 75 %? a ) 25 liters, b ) 27 liters, c ) 30 liters, d ) 32 liters, e ) 35 liters
Output: "let x ltr water to be added 2 ltr alcohol to be represented as ( 20 ( 1 - 3 / 4 ( new soln. = 10 + x ) ) ) 2 = 5 % * ( 10 + x ) - - - - - - - - > x = 30 answer : c"
So the final answer is c

Input: a goods train runs at the speed of 72 kmph and crosses a 250 m long platform in 36 seconds. what is the length of the goods train? a ) 470 m, b ) 240 m, c ) 260 m, d ) 270 m, e ) none of these
Output: "explanation : speed = [ 72 x ( 5 / 18 ) ] m / sec = 20 m / sec. time = 36 sec. let the length of the train be x metres. then, [ ( x + 250 ) / 36 ] = 20 = > x + 250 = 720 = > x = 470. answer : a"
So the final answer is a

Input: the volume of a cube is 1728 cc. find its surface. a ) 864, b ) 556, c ) 255, d ) 287, e ) 267
Output: "a 3 = 1728 = > a = 12 6 a 2 = 6 * 12 * 12 = 864 answer : a"
So the final answer is a

Input: a certain drink of type a is prepared by mixing 4 parts milk with 3 parts fruit juice. another drink of type b is prepared by mixing 4 parts of fruit juice and 3 parts of milk. how many liters of fruit juice must be added to 63 liters of drink a to convert it to drink b? a ) 7, b ) 14, c ) 21, d ) 28, e ) 35
Output: "in 63 liters of drink a, there are 36 liters of milk and 27 liters of juice. with 36 liters of milk, we need a total of 48 liters of juice to make drink b. we need to add 21 liters of juice. the answer is c."
So the final answer is c

Input: last week john spent 10 percent of his wages on recreation. this week, his wages are 10 percent less than last week ʼ s wages and he spent 40 percent of his wages on recreation. the amount he spends on recreation this week is what percent of the amount he spent on recreation last week a ) 180 %, b ) 360 %, c ) 200 %, d ) 220 %, e ) 250 %
Output: "say john's wages last week were $ 100, so he spent 0.10 * 100 = $ 10 on recreation ; this week's wages is 0.90 * 100 = $ 90, so he spends 0.4 * 90 = $ 36 on recreation ; 36 / 10 = 3.60, hence the amount he spends on recreation this week is 360 % of the amount he spent on recreation last week : 10 * 3.6 = 36 answer : b"
So the final answer is b

Input: the salary of a person was reduced by 15 %. by what percent should his reduced salary be raised so as to bring it at par with his original salary? a ) 10 %, b ) 17.6 %, c ) 13.7 %, d ) 15.1 %, e ) 12.3 %
Output: "let the original salary be $ 100 new salary = $ 85 increase on 85 = 15 increase on 100 = 15 / 85 * 100 = 17.6 % answer is b"
So the final answer is b

Input: if 20 men can build a water fountain 56 metres long in 7 days, what length of a similar water fountain can be built by 35 men in 3 days? a ) 40 m, b ) 42 m, c ) 47 m, d ) 49 m, e ) 50 m
Output: "explanation : let the required length be x metres more men, more length built ( direct proportion ) less days, less length built ( direct proportion ) men 20 : 35 days 7 : 3 : : 56 : x therefore ( 20 x 7 x x ) = ( 35 x 3 x 56 ) x = ( 35 x 3 x 56 ) / 140 = 42 hence, the required length is 42 m. answer : b"
So the final answer is b

Input: 12 men complete a work in 9 days. after they have worked for 6 days, 6 more men join them. how many days will they take to complete the remaining work? a ) 2 days, b ) 7 days, c ) 8 days, d ) 9 days, e ) 45 days
Output: 1 man's 1 day work = 1 / 108 12 men's 6 day's work = 1 / 9 * 6 = 2 / 3 remaining work = 1 - 2 / 3 = 1 / 3 18 men's 1 day work = 1 / 108 * 18 = 1 / 6 1 / 6 work is done by them in 1 day. 1 / 3 work is done by them in 6 * 1 / 3 = 2 days. answer : a
So the final answer is a

Input: a cheese factory sells its cheese in rectangular blocks. a normal block has a volume of 5 cubic feet. if a large block has twice the width, twice the depth, and the same length of a normal block, what is the volume of cheese in a large block in cubic feet? a ) 15, b ) 30, c ) 20, d ) 18, e ) 25
Output: "volume of cube = lbh = 5 new cube l, b, h are increases of l, 2 b, 2 h new volume of cube = l * 2 b * 2 h = 4 * lbh = 4 * 5 = 20 answer : c"
So the final answer is c

Input: two trains, one from howrah to patna and the other from patna to howrah, start simultaneously. after they meet, the trains reach their destinations after 64 hours and 25 hours respectively. the ratio of their speeds is? a ) 4 : 9, b ) 4 : 3, c ) 4 : 5, d ) 5 : 8, e ) 4 : 2
Output: "let us name the trains a and b. then, ( a's speed ) : ( b's speed ) = √ b : √ a = √ 25 : √ 64 = 5 : 8 answer : d"
So the final answer is d

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tmathqa-s10-rTrue-m4096
Evaluating commonsenseqa :  90%|█████████ | 90/100 [6:21:14<43:51, 263.18s/it]Evaluating commonsenseqa :   1%|          | 1/100 [02:12<3:38:26, 132.39s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [2:29:17<3:28:59, 216.20s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [04:21<3:33:25, 130.67s/it]       Evaluating commonsenseqa :  91%|█████████ | 91/100 [6:25:33<39:16, 261.89s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [2:32:53<3:25:14, 216.04s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [06:31<3:30:16, 130.06s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [08:40<3:27:44, 129.83s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [2:36:25<3:20:44, 215.09s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [6:30:00<35:08, 263.51s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [10:49<3:25:14, 129.62s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [12:57<3:22:00, 128.95s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [2:40:04<3:18:01, 216.04s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [6:34:25<30:47, 263.99s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [15:04<3:18:56, 128.35s/it]       Evaluating commonsenseqa :  46%|████▌     | 46/100 [2:43:44<3:15:37, 217.35s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [17:13<3:17:10, 128.59s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [6:38:39<26:05, 260.91s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [19:22<3:15:11, 128.70s/it]                                                                                       Evaluating commonsenseqa :  47%|████▋     | 47/100 [2:47:20<3:11:41, 217.01s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [21:31<3:12:59, 128.66s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [6:43:00<21:44, 260.91s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [23:38<3:10:16, 128.28s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [2:51:03<3:09:30, 218.66s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [25:44<3:07:01, 127.51s/it]    Evaluating commonsenseqa :  96%|█████████▌| 96/100 [6:47:24<17:27, 261.83s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [27:54<3:05:56, 128.23s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [2:54:40<3:05:34, 218.32s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [30:01<3:03:30, 128.03s/it]    Evaluating commonsenseqa :  50%|█████     | 50/100 [2:58:14<3:00:39, 216.80s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [6:51:46<13:06, 262.02s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [32:10<3:01:31, 128.13s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [34:18<2:59:23, 128.14s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [3:01:46<2:55:57, 215.47s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [6:56:07<08:43, 261.66s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [36:28<2:57:59, 128.67s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [38:38<2:56:22, 129.06s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [3:05:18<2:51:35, 214.49s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [7:00:26<04:20, 261.00s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [40:47<2:54:15, 129.08s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [3:08:55<2:48:28, 215.07s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [42:59<2:53:14, 129.93s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [7:04:51<00:00, 262.09s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [7:04:51<00:00, 254.91s/it]
name: commonsenseqa | avg. gen lenth: 220.245 | time: 25492.741103172302s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13644 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 7 5 7 True False 4096 10
[2023-09-10 11:08:37,465] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 7
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o7-tmathqa-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 873548.04it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Evaluating commonsenseqa :  21%|██        | 21/100 [45:07<2:50:11, 129.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.99s/it]
 > number of parameters: 6738415616
[2023-09-10 11:08:54,897] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-10 11:08:55,104] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-10 11:08:55,105] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c01f0>
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-10 11:08:55,106] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-10 11:08:55,107] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-10 11:08:55,107] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: b

Input: find the total number of prime factors in the expression ( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 a ) 26, b ) 22, c ) 25, d ) 30, e ) 29
Output: d

Input: the length of the bridge, which a train 140 m long and traveling at 45 km / hr can cross in 30 sec is? a ) 235, b ) 240, c ) 245, d ) 250, e ) 255
Output: a

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o7-tmathqa-s1-rFalse-m4096
Evaluating commonsenseqa :  54%|█████▍    | 54/100 [3:12:26<2:44:04, 214.02s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [47:13<2:46:55, 128.40s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [3:15:18<2:30:58, 201.31s/it]Evaluating commonsenseqa :   1%|          | 1/100 [04:07<6:48:25, 247.53s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [49:23<2:45:26, 128.91s/it]                                                                                       Evaluating commonsenseqa :  24%|██▍       | 24/100 [51:29<2:42:13, 128.07s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [3:18:51<2:30:09, 204.77s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [08:16<6:46:05, 248.63s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [53:37<2:39:56, 127.95s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [55:43<2:37:04, 127.36s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [3:22:23<2:28:21, 207.02s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [12:24<6:41:27, 248.33s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [57:51<2:35:05, 127.48s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [3:26:00<2:27:06, 210.16s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [59:58<2:32:58, 127.47s/it]                                                                                         Evaluating commonsenseqa :   4%|▍         | 4/100 [16:32<6:37:00, 248.13s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:02:05<2:30:40, 127.33s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [3:29:37<2:24:52, 212.00s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:04:13<2:28:53, 127.62s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [20:42<6:33:32, 248.56s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [1:06:20<2:26:18, 127.22s/it]  Evaluating commonsenseqa :  60%|██████    | 60/100 [3:33:17<2:23:05, 214.63s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [1:08:28<2:24:36, 127.60s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [24:49<6:28:50, 248.19s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [3:36:53<2:19:43, 214.96s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [1:10:38<2:23:18, 128.33s/it]                                                                                         Evaluating commonsenseqa :   7%|▋         | 7/100 [27:18<5:34:11, 215.61s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [1:12:45<2:20:47, 128.00s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [3:39:53<2:09:29, 204.46s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:14:53<2:18:27, 127.81s/it]                                                                                         Evaluating commonsenseqa :   8%|▊         | 8/100 [31:25<5:46:12, 225.79s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [3:43:17<2:05:57, 204.25s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [1:17:00<2:16:04, 127.58s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [1:19:09<2:14:28, 128.08s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [3:46:59<2:05:50, 209.72s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [35:31<5:52:06, 232.16s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [1:21:16<2:12:03, 127.80s/it]                                                                                         Evaluating commonsenseqa :  39%|███▉      | 39/100 [1:23:22<2:09:18, 127.19s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [3:50:36<2:03:36, 211.89s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [39:00<5:37:23, 224.92s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [1:25:31<2:07:50, 127.85s/it]                                                                                         Evaluating commonsenseqa :  41%|████      | 41/100 [1:27:38<2:05:24, 127.53s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [3:54:14<2:00:59, 213.52s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [43:06<5:43:16, 231.42s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:29:45<2:03:08, 127.38s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [3:57:48<1:57:31, 213.67s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [46:35<5:29:31, 224.67s/it]      Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:31:57<2:02:09, 128.58s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:34:05<1:59:54, 128.47s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [4:01:22<1:54:06, 213.95s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [50:42<5:35:19, 231.26s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:36:11<1:57:06, 127.75s/it]                                                                                         Evaluating commonsenseqa :  46%|████▌     | 46/100 [1:38:20<1:55:14, 128.06s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [4:05:01<1:51:16, 215.36s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [54:48<5:37:52, 235.72s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:40:25<1:52:33, 127.42s/it]                                                                                         Evaluating commonsenseqa :  70%|███████   | 70/100 [4:08:40<1:48:14, 216.47s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:42:34<1:50:46, 127.82s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [58:58<5:40:11, 240.14s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:44:39<1:47:54, 126.96s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [4:12:17<1:44:43, 216.67s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [1:46:45<1:45:29, 126.60s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [1:03:06<5:39:12, 242.29s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [1:48:51<1:43:09, 126.31s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [4:15:53<1:41:02, 216.53s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:50:59<1:41:30, 126.89s/it]                                                                                           Evaluating commonsenseqa :  17%|█▋        | 17/100 [1:07:12<5:36:56, 243.57s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [4:19:26<1:36:51, 215.23s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:53:07<1:39:47, 127.39s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:55:14<1:37:27, 127.13s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [4:23:01<1:33:14, 215.17s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [1:11:24<5:36:21, 246.12s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:57:20<1:35:01, 126.69s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:59:27<1:33:03, 126.90s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [4:26:33<1:29:16, 214.26s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [1:15:28<5:31:18, 245.42s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [2:01:33<1:30:47, 126.69s/it]                                                                                           Evaluating commonsenseqa :  76%|███████▌  | 76/100 [4:30:12<1:26:21, 215.90s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [2:03:41<1:28:50, 126.91s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [1:19:10<5:17:57, 238.47s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [2:05:49<1:27:05, 127.45s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [4:33:48<1:22:40, 215.68s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [2:07:59<1:25:22, 128.07s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [1:23:15<5:16:38, 240.48s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [2:10:06<1:23:01, 127.72s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [4:37:26<1:19:24, 216.59s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [2:12:12<1:20:34, 127.23s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:27:21<5:14:33, 241.97s/it]    Evaluating commonsenseqa :  63%|██████▎   | 63/100 [2:14:21<1:18:50, 127.86s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [4:40:59<1:15:22, 215.36s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [2:16:29<1:16:42, 127.84s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:31:32<5:13:54, 244.61s/it]                                                                                           Evaluating commonsenseqa :  80%|████████  | 80/100 [4:44:38<1:12:08, 216.41s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [2:18:36<1:14:29, 127.70s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [2:20:42<1:11:59, 127.05s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:35:39<5:10:46, 245.34s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [4:48:15<1:08:34, 216.56s/it]                                                                                           Evaluating commonsenseqa :  67%|██████▋   | 67/100 [2:22:49<1:09:50, 126.99s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:39:43<5:06:09, 244.93s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [2:24:55<1:07:32, 126.64s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [4:51:30<1:03:05, 210.29s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [2:27:03<1:05:46, 127.29s/it]                                                                                           Evaluating commonsenseqa :  83%|████████▎ | 83/100 [4:55:06<1:00:02, 211.89s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:43:49<5:02:47, 245.50s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [2:29:10<1:03:33, 127.11s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [2:31:16<1:01:17, 126.82s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [4:58:44<56:58, 213.65s/it]  Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:47:59<5:00:11, 246.74s/it]    Evaluating commonsenseqa :  72%|███████▏  | 72/100 [2:33:23<59:12, 126.88s/it]  Evaluating commonsenseqa :  73%|███████▎  | 73/100 [2:35:31<57:13, 127.18s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:50:33<4:22:49, 219.03s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [5:02:21<53:39, 214.64s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [2:37:37<54:56, 126.80s/it]                                                                                             Evaluating commonsenseqa :  86%|████████▌ | 86/100 [5:05:52<49:49, 213.55s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [2:39:46<53:08, 127.56s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:54:43<4:29:55, 228.11s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [2:41:54<51:03, 127.67s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [5:09:32<46:42, 215.56s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [2:44:00<48:44, 127.13s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:58:56<4:34:55, 235.65s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [2:46:08<46:42, 127.40s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [5:13:12<43:24, 217.05s/it]                                                                                             Evaluating commonsenseqa :  79%|███████▉  | 79/100 [2:48:15<44:31, 127.20s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [2:03:05<4:35:40, 239.72s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [5:16:48<39:42, 216.60s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [2:50:22<42:23, 127.17s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [2:07:17<4:35:51, 243.40s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [2:52:31<40:24, 127.59s/it]                                                                                             Evaluating commonsenseqa :  90%|█████████ | 90/100 [5:20:24<36:04, 216.43s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [2:54:36<38:03, 126.87s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [2:11:29<4:34:39, 245.96s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [2:56:42<35:51, 126.57s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [5:23:59<32:23, 215.98s/it]                                                                                             Evaluating commonsenseqa :  84%|████████▍ | 84/100 [2:58:47<33:40, 126.26s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [2:15:41<4:32:30, 247.74s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [3:00:53<31:32, 126.17s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [5:27:39<28:57, 217.16s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [3:03:01<29:32, 126.62s/it]                                                                                             Evaluating commonsenseqa :  93%|█████████▎| 93/100 [5:31:16<25:20, 217.16s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [2:19:49<4:28:33, 247.89s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [3:05:10<27:37, 127.48s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [3:07:18<25:30, 127.56s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [5:34:52<21:41, 216.89s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [2:23:58<4:24:32, 248.01s/it]    Evaluating commonsenseqa :  89%|████████▉ | 89/100 [3:09:25<23:20, 127.36s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [3:11:34<21:18, 127.80s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [5:38:29<18:03, 216.73s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [2:28:11<4:22:02, 249.57s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [3:13:41<19:09, 127.67s/it]                                                                                             Evaluating commonsenseqa :  96%|█████████▌| 96/100 [5:42:04<14:24, 216.20s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [3:15:50<17:03, 127.99s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [2:32:18<4:17:15, 248.96s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [3:17:55<14:50, 127.28s/it]                                                                                             Evaluating commonsenseqa :  97%|█████████▋| 97/100 [5:45:39<10:47, 215.88s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [3:20:04<12:45, 127.62s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [2:36:26<4:12:37, 248.49s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [3:22:12<10:39, 127.92s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [5:49:17<07:13, 216.55s/it]                                                                                             Evaluating commonsenseqa :  96%|█████████▌| 96/100 [3:24:20<08:31, 127.92s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [2:40:36<4:09:05, 249.10s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [5:52:50<03:35, 215.60s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [3:26:25<06:20, 126.97s/it]                                                                                               Evaluating commonsenseqa :  98%|█████████▊| 98/100 [3:28:32<04:14, 127.07s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [5:55:32<00:00, 199.35s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [5:55:32<00:00, 213.32s/it]
name: commonsenseqa | avg. gen lenth: 239.251 | time: 21333.6802175045s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13645 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 9 9 11 True False 4096 10
[2023-09-10 13:52:49,333] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 9
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o9-tmathqa-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 536085.91it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.52s/it]
 > number of parameters: 6738415616
[2023-09-10 13:53:03,830] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-10 13:53:04,037] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-10 13:53:04,039] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-10 13:53:04,039] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-10 13:53:04,039] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-10 13:53:04,039] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-10 13:53:04,039] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0280>
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-10 13:53:04,040] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-10 13:53:04,041] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-10 13:53:04,041] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: in a recent head - to - head run - off election, 12000 absentee ballets were cast. 1 / 6 of the absentee ballets were thrown out and 3 / 5 of the remaining absentee ballets were cast for candidate a. how many absentee votes did candidate b receive? a ) 2,000, b ) 3,000, c ) 4,000, d ) 8,000, e ) 9,000
Output: 5 / 6 * 2 / 5 ( total absentee votes ) = 1 / 3 ( total votes ) = 1 / 3 * 12000 = 4000 answer is c
So the final answer is c

Input: the total age of a and b is 11 years more than the total age of b and c. c is how many years younger than a.? a ) 16, b ) 12, c ) 11, d ) 20, e ) 10
Output: "( a + b ) - ( b - c ) = 11 a - c = 11 answer is c"
So the final answer is c

Input: the scoring system in a certain football competition goes as follows : 3 points for victory, 1 point for a draw, and 0 points for defeat. each team plays 20 matches. if a team scored 14 points after 5 games, what is the least number of the remaining matches it has to win to reach the 40 - point mark by the end of the tournament? a ) 6, b ) 7, c ) 8, d ) 9, e ) 10
Output: "to get 40 points as end of season we need another 26 points or more from remaining 15 matches : option a = 6 * 3 + 9 * 1 = 27 hence option a - 6"
So the final answer is a

Input: how much water should be added to 10 liters of a 20 % - solution of alcohol to reduce the concentration of alcohol in the solution by 75 %? a ) 25 liters, b ) 27 liters, c ) 30 liters, d ) 32 liters, e ) 35 liters
Output: "let x ltr water to be added 2 ltr alcohol to be represented as ( 20 ( 1 - 3 / 4 ( new soln. = 10 + x ) ) ) 2 = 5 % * ( 10 + x ) - - - - - - - - > x = 30 answer : c"
So the final answer is c

Input: a goods train runs at the speed of 72 kmph and crosses a 250 m long platform in 36 seconds. what is the length of the goods train? a ) 470 m, b ) 240 m, c ) 260 m, d ) 270 m, e ) none of these
Output: "explanation : speed = [ 72 x ( 5 / 18 ) ] m / sec = 20 m / sec. time = 36 sec. let the length of the train be x metres. then, [ ( x + 250 ) / 36 ] = 20 = > x + 250 = 720 = > x = 470. answer : a"
So the final answer is a

Input: the volume of a cube is 1728 cc. find its surface. a ) 864, b ) 556, c ) 255, d ) 287, e ) 267
Output: "a 3 = 1728 = > a = 12 6 a 2 = 6 * 12 * 12 = 864 answer : a"
So the final answer is a

Input: a certain drink of type a is prepared by mixing 4 parts milk with 3 parts fruit juice. another drink of type b is prepared by mixing 4 parts of fruit juice and 3 parts of milk. how many liters of fruit juice must be added to 63 liters of drink a to convert it to drink b? a ) 7, b ) 14, c ) 21, d ) 28, e ) 35
Output: "in 63 liters of drink a, there are 36 liters of milk and 27 liters of juice. with 36 liters of milk, we need a total of 48 liters of juice to make drink b. we need to add 21 liters of juice. the answer is c."
So the final answer is c

Input: last week john spent 10 percent of his wages on recreation. this week, his wages are 10 percent less than last week ʼ s wages and he spent 40 percent of his wages on recreation. the amount he spends on recreation this week is what percent of the amount he spent on recreation last week a ) 180 %, b ) 360 %, c ) 200 %, d ) 220 %, e ) 250 %
Output: "say john's wages last week were $ 100, so he spent 0.10 * 100 = $ 10 on recreation ; this week's wages is 0.90 * 100 = $ 90, so he spends 0.4 * 90 = $ 36 on recreation ; 36 / 10 = 3.60, hence the amount he spends on recreation this week is 360 % of the amount he spent on recreation last week : 10 * 3.6 = 36 answer : b"
So the final answer is b

Input: the salary of a person was reduced by 15 %. by what percent should his reduced salary be raised so as to bring it at par with his original salary? a ) 10 %, b ) 17.6 %, c ) 13.7 %, d ) 15.1 %, e ) 12.3 %
Output: "let the original salary be $ 100 new salary = $ 85 increase on 85 = 15 increase on 100 = 15 / 85 * 100 = 17.6 % answer is b"
So the final answer is b

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o9-tmathqa-s10-rTrue-m4096
Evaluating commonsenseqa :  41%|████      | 41/100 [2:44:43<4:04:17, 248.43s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [3:30:40<02:07, 127.11s/it]Evaluating commonsenseqa :   1%|          | 1/100 [02:56<4:51:52, 176.90s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [3:32:45<00:00, 126.55s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [3:32:45<00:00, 127.65s/it]
name: commonsenseqa | avg. gen lenth: 349.964 | time: 12767.991582155228s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13646 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 15 13 15 True False 4096 10
Evaluating commonsenseqa :  63%|██████▎   | 63/100 [5:17:47<3:05:29, 300.79s/it]or to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 15
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o15-tmathqa-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 370062.18it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.70s/it]
 > number of parameters: 6738415616
[2023-09-10 13:56:52,123] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-10 13:56:52,333] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-10 13:56:52,335] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3be280>
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-10 13:56:52,335] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-10 13:56:52,336] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-10 13:56:52,337] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-10 13:56:52,337] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-10 13:56:52,337] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: in a recent head - to - head run - off election, 12000 absentee ballets were cast. 1 / 6 of the absentee ballets were thrown out and 3 / 5 of the remaining absentee ballets were cast for candidate a. how many absentee votes did candidate b receive? a ) 2,000, b ) 3,000, c ) 4,000, d ) 8,000, e ) 9,000
Output: 5 / 6 * 2 / 5 ( total absentee votes ) = 1 / 3 ( total votes ) = 1 / 3 * 12000 = 4000 answer is c
So the final answer is c

Input: the total age of a and b is 11 years more than the total age of b and c. c is how many years younger than a.? a ) 16, b ) 12, c ) 11, d ) 20, e ) 10
Output: "( a + b ) - ( b - c ) = 11 a - c = 11 answer is c"
So the final answer is c

Input: the scoring system in a certain football competition goes as follows : 3 points for victory, 1 point for a draw, and 0 points for defeat. each team plays 20 matches. if a team scored 14 points after 5 games, what is the least number of the remaining matches it has to win to reach the 40 - point mark by the end of the tournament? a ) 6, b ) 7, c ) 8, d ) 9, e ) 10
Output: "to get 40 points as end of season we need another 26 points or more from remaining 15 matches : option a = 6 * 3 + 9 * 1 = 27 hence option a - 6"
So the final answer is a

Input: how much water should be added to 10 liters of a 20 % - solution of alcohol to reduce the concentration of alcohol in the solution by 75 %? a ) 25 liters, b ) 27 liters, c ) 30 liters, d ) 32 liters, e ) 35 liters
Output: "let x ltr water to be added 2 ltr alcohol to be represented as ( 20 ( 1 - 3 / 4 ( new soln. = 10 + x ) ) ) 2 = 5 % * ( 10 + x ) - - - - - - - - > x = 30 answer : c"
So the final answer is c

Input: a goods train runs at the speed of 72 kmph and crosses a 250 m long platform in 36 seconds. what is the length of the goods train? a ) 470 m, b ) 240 m, c ) 260 m, d ) 270 m, e ) none of these
Output: "explanation : speed = [ 72 x ( 5 / 18 ) ] m / sec = 20 m / sec. time = 36 sec. let the length of the train be x metres. then, [ ( x + 250 ) / 36 ] = 20 = > x + 250 = 720 = > x = 470. answer : a"
So the final answer is a

Input: the volume of a cube is 1728 cc. find its surface. a ) 864, b ) 556, c ) 255, d ) 287, e ) 267
Output: "a 3 = 1728 = > a = 12 6 a 2 = 6 * 12 * 12 = 864 answer : a"
So the final answer is a

Input: a certain drink of type a is prepared by mixing 4 parts milk with 3 parts fruit juice. another drink of type b is prepared by mixing 4 parts of fruit juice and 3 parts of milk. how many liters of fruit juice must be added to 63 liters of drink a to convert it to drink b? a ) 7, b ) 14, c ) 21, d ) 28, e ) 35
Output: "in 63 liters of drink a, there are 36 liters of milk and 27 liters of juice. with 36 liters of milk, we need a total of 48 liters of juice to make drink b. we need to add 21 liters of juice. the answer is c."
So the final answer is c

Input: last week john spent 10 percent of his wages on recreation. this week, his wages are 10 percent less than last week ʼ s wages and he spent 40 percent of his wages on recreation. the amount he spends on recreation this week is what percent of the amount he spent on recreation last week a ) 180 %, b ) 360 %, c ) 200 %, d ) 220 %, e ) 250 %
Output: "say john's wages last week were $ 100, so he spent 0.10 * 100 = $ 10 on recreation ; this week's wages is 0.90 * 100 = $ 90, so he spends 0.4 * 90 = $ 36 on recreation ; 36 / 10 = 3.60, hence the amount he spends on recreation this week is 360 % of the amount he spent on recreation last week : 10 * 3.6 = 36 answer : b"
So the final answer is b

Input: the salary of a person was reduced by 15 %. by what percent should his reduced salary be raised so as to bring it at par with his original salary? a ) 10 %, b ) 17.6 %, c ) 13.7 %, d ) 15.1 %, e ) 12.3 %
Output: "let the original salary be $ 100 new salary = $ 85 increase on 85 = 15 increase on 100 = 15 / 85 * 100 = 17.6 % answer is b"
So the final answer is b

Input: if 20 men can build a water fountain 56 metres long in 7 days, what length of a similar water fountain can be built by 35 men in 3 days? a ) 40 m, b ) 42 m, c ) 47 m, d ) 49 m, e ) 50 m
Output: "explanation : let the required length be x metres more men, more length built ( direct proportion ) less days, less length built ( direct proportion ) men 20 : 35 days 7 : 3 : : 56 : x therefore ( 20 x 7 x x ) = ( 35 x 3 x 56 ) x = ( 35 x 3 x 56 ) / 140 = 42 hence, the required length is 42 m. answer : b"
So the final answer is b

Input: 12 men complete a work in 9 days. after they have worked for 6 days, 6 more men join them. how many days will they take to complete the remaining work? a ) 2 days, b ) 7 days, c ) 8 days, d ) 9 days, e ) 45 days
Output: 1 man's 1 day work = 1 / 108 12 men's 6 day's work = 1 / 9 * 6 = 2 / 3 remaining work = 1 - 2 / 3 = 1 / 3 18 men's 1 day work = 1 / 108 * 18 = 1 / 6 1 / 6 work is done by them in 1 day. 1 / 3 work is done by them in 6 * 1 / 3 = 2 days. answer : a
So the final answer is a

Input: a cheese factory sells its cheese in rectangular blocks. a normal block has a volume of 5 cubic feet. if a large block has twice the width, twice the depth, and the same length of a normal block, what is the volume of cheese in a large block in cubic feet? a ) 15, b ) 30, c ) 20, d ) 18, e ) 25
Output: "volume of cube = lbh = 5 new cube l, b, h are increases of l, 2 b, 2 h new volume of cube = l * 2 b * 2 h = 4 * lbh = 4 * 5 = 20 answer : c"
So the final answer is c

Input: two trains, one from howrah to patna and the other from patna to howrah, start simultaneously. after they meet, the trains reach their destinations after 64 hours and 25 hours respectively. the ratio of their speeds is? a ) 4 : 9, b ) 4 : 3, c ) 4 : 5, d ) 5 : 8, e ) 4 : 2
Output: "let us name the trains a and b. then, ( a's speed ) : ( b's speed ) = √ b : √ a = √ 25 : √ 64 = 5 : 8 answer : d"
So the final answer is d

Input: john had a stock of 1400 books in his bookshop. he sold 62 on monday, 62 on tuesday, 60 on wednesday, 48 on thursday and 40 on friday. what percentage of the books were not sold? a ) 81.57 %, b ) 36.5 %, c ) 80.67 %, d ) 56.5 %, e ) 80.57 %
Output: "let n be the total number of books sold. hence n = 62 + 62 + 60 + 48 + 40 = 272 let m be the books not sold m = 1400 - n = 1400 - 272 = 1128 percentage books not sold / total number of books = 1128 / 1200 = 0.81 = 80.57 % correct answer e"
So the final answer is e

Input: a and b can do a piece of work in 18 days ; band c can do it in 24 days a and c can do it in 36 days. in how many days will a, band c finish it together? a ) 15 days, b ) 12 days, c ) 8 days, d ) 16 days, e ) 9 days
Output: sol. ( a + b )'s 1 day's work = ( 1 / 18 ) ( b + c )'s 1 day's work = ( 1 / 24 ) and ( a + c )'s 1 day's work = ( 1 / 36 ) adding, we get : 2 ( a + b + c )'s 1 day's work = ¬ ( 1 / 18 + 1 / 24 + 1 / 36 ) = 9 / 72 = 1 / 8 ( a + b + c )'s 1 day's work = 1 / 16 thus, a, band c together can finish the work in 16 days. ans : d
So the final answer is d

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o15-tmathqa-s10-rTrue-m4096
Evaluating commonsenseqa :  42%|████▏     | 42/100 [2:48:50<3:59:37, 247.89s/it]Evaluating commonsenseqa :   1%|          | 1/100 [01:47<2:57:50, 107.78s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [05:51<4:46:32, 175.44s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [03:35<2:55:50, 107.66s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [08:42<4:40:21, 173.42s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [2:52:58<3:55:42, 248.11s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [05:22<2:54:00, 107.63s/it]                                                                                               Evaluating commonsenseqa :   4%|▍         | 4/100 [07:08<2:50:55, 106.82s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [11:35<4:37:31, 173.46s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [08:57<2:50:28, 107.67s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [2:57:07<3:51:43, 248.28s/it]                                                                                               Evaluating commonsenseqa :   5%|▌         | 5/100 [14:30<4:35:33, 174.04s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [10:43<2:47:39, 107.01s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [12:31<2:46:15, 107.27s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [3:01:20<3:48:51, 249.66s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [17:21<4:30:48, 172.85s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [14:17<2:43:59, 106.95s/it]                                                                                               Evaluating commonsenseqa :   9%|▉         | 9/100 [16:04<2:42:04, 106.86s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [20:10<4:26:10, 171.73s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [3:05:29<3:44:34, 249.54s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [17:50<2:40:08, 106.76s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [23:01<4:22:38, 171.29s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [19:36<2:37:54, 106.45s/it]                                                                                               Evaluating commonsenseqa :  12%|█▏        | 12/100 [21:22<2:35:47, 106.22s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [3:09:41<3:41:02, 250.23s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [25:54<4:20:51, 172.00s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [23:06<2:33:25, 105.81s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [24:52<2:31:42, 105.85s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [28:45<4:17:11, 171.46s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [3:13:50<3:36:36, 249.94s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [26:39<2:30:15, 106.06s/it]            Evaluating commonsenseqa :  11%|█         | 11/100 [31:37<4:14:49, 171.79s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [28:25<2:28:24, 106.00s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [3:17:58<3:31:47, 249.17s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [30:12<2:27:05, 106.33s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [34:26<4:10:42, 170.93s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [32:00<2:25:57, 106.80s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [37:19<4:08:49, 171.60s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [33:47<2:24:30, 107.05s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [3:22:10<3:28:30, 250.20s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [35:36<2:23:30, 107.63s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [40:08<4:04:43, 170.74s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [37:23<2:21:18, 107.32s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [3:26:26<3:25:48, 252.01s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [42:59<4:01:51, 170.73s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [39:11<2:19:55, 107.64s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [3:28:06<2:45:00, 206.25s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [40:57<2:17:21, 107.03s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [45:51<3:59:42, 171.23s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [42:44<2:15:38, 107.09s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [3:32:19<2:52:35, 220.33s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [44:31<2:13:43, 106.98s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [48:43<3:57:05, 171.39s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [46:16<2:11:20, 106.49s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [51:33<3:53:56, 171.18s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [48:03<2:09:30, 106.44s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [3:36:28<2:55:23, 228.77s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [49:49<2:07:48, 106.51s/it]            Evaluating commonsenseqa :  19%|█▉        | 19/100 [54:24<3:50:46, 170.94s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [51:35<2:05:42, 106.23s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [3:40:42<2:57:16, 236.36s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [53:23<2:04:29, 106.71s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [57:19<3:49:42, 172.28s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [55:10<2:02:44, 106.73s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [1:00:12<3:46:56, 172.36s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [56:56<2:00:50, 106.63s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [3:44:54<2:56:49, 241.14s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [58:44<1:59:33, 107.07s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:03:03<3:43:32, 171.96s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [1:00:31<1:57:46, 107.06s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [3:49:03<2:54:33, 243.58s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:05:55<3:40:33, 171.87s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:02:17<1:55:42, 106.81s/it]                                                                                                 Evaluating commonsenseqa :  36%|███▌      | 36/100 [1:04:04<1:53:46, 106.66s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:08:44<3:36:46, 171.14s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [3:53:16<2:52:24, 246.29s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [1:05:52<1:52:23, 107.04s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [1:07:38<1:50:34, 107.01s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:11:36<3:34:24, 171.53s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [3:56:40<2:39:41, 233.69s/it]    Evaluating commonsenseqa :  39%|███▉      | 39/100 [1:09:25<1:48:37, 106.85s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:14:25<3:30:29, 170.67s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [1:11:11<1:46:28, 106.47s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [4:00:51<2:39:14, 238.86s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [1:12:58<1:44:50, 106.62s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:17:12<3:26:25, 169.67s/it]                                                                                                 Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:14:44<1:42:53, 106.44s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [4:04:08<2:27:02, 226.22s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:20:05<3:24:29, 170.41s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:16:31<1:41:23, 106.73s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:18:19<1:40:04, 107.23s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:22:53<3:20:50, 169.73s/it]          Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:20:06<1:38:07, 107.05s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [4:08:19<2:27:57, 233.63s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [1:21:56<1:37:01, 107.81s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:25:46<3:19:07, 170.68s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [4:11:22<2:14:51, 218.70s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:23:43<1:35:02, 107.59s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [1:28:36<3:16:15, 170.66s/it]          Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:25:29<1:33:02, 107.35s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:27:16<1:30:57, 107.00s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [4:15:32<2:16:43, 227.86s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [1:31:27<3:13:33, 170.78s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [1:29:03<1:29:21, 107.23s/it]      Evaluating commonsenseqa :  65%|██████▌   | 65/100 [4:18:19<2:02:17, 209.64s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [1:34:18<3:10:47, 170.86s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [1:30:50<1:27:18, 106.91s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:32:37<1:25:38, 107.06s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [1:37:10<3:08:14, 171.13s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [4:21:25<1:54:50, 202.65s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:34:25<1:24:11, 107.47s/it]      Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:36:12<1:22:09, 107.17s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:40:02<3:05:31, 171.26s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [4:25:34<1:59:03, 216.46s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:37:57<1:19:56, 106.60s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [1:42:49<3:01:28, 170.13s/it]          Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:39:44<1:18:12, 106.66s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:41:31<1:16:26, 106.66s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [4:29:45<2:00:59, 226.87s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [1:45:42<2:59:25, 170.89s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:43:17<1:14:41, 106.71s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [1:48:34<2:57:01, 171.31s/it]          Evaluating commonsenseqa :  59%|█████▉    | 59/100 [1:45:05<1:13:02, 106.88s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [4:33:55<2:00:44, 233.68s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [1:46:51<1:11:09, 106.74s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [1:51:26<2:54:19, 171.47s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [1:48:39<1:09:32, 106.99s/it]                                                                                                   Evaluating commonsenseqa :  70%|███████   | 70/100 [4:38:02<1:58:52, 237.74s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [1:50:24<1:07:31, 106.61s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [1:54:14<2:50:30, 170.51s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [1:52:12<1:05:52, 106.83s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [1:57:04<2:47:36, 170.46s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [1:53:58<1:04:01, 106.71s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [4:42:08<1:56:10, 240.35s/it]                                                                                                   Evaluating commonsenseqa :  65%|██████▌   | 65/100 [1:55:47<1:02:34, 107.28s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:59:56<2:44:58, 170.66s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [1:57:34<1:00:49, 107.33s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [4:46:19<1:53:36, 243.44s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [2:02:49<2:42:58, 171.55s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [1:59:22<59:02, 107.36s/it]      Evaluating commonsenseqa :  68%|██████▊   | 68/100 [2:01:09<57:14, 107.31s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [2:05:45<2:41:10, 172.69s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [2:02:27<50:59, 98.69s/it] Evaluating commonsenseqa :  73%|███████▎  | 73/100 [4:50:28<1:50:17, 245.10s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [2:04:14<50:32, 101.09s/it]      Evaluating commonsenseqa :  45%|████▌     | 45/100 [2:08:38<2:38:22, 172.77s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [2:06:00<49:30, 102.45s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [4:54:37<1:46:42, 246.26s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [2:11:33<2:36:20, 173.72s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [2:07:49<48:46, 104.51s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [2:09:34<47:05, 104.66s/it]    Evaluating commonsenseqa :  47%|████▋     | 47/100 [2:14:23<2:32:19, 172.45s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [4:58:50<1:43:27, 248.28s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [2:11:21<45:37, 105.27s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [2:13:07<43:59, 105.56s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [2:17:12<2:28:31, 171.37s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [5:02:48<1:38:08, 245.35s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [2:14:56<42:40, 106.70s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [2:20:03<2:25:33, 171.24s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [2:16:44<40:59, 106.96s/it]                                                                                                 Evaluating commonsenseqa :  78%|███████▊  | 78/100 [2:18:30<39:09, 106.78s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [5:06:57<1:34:27, 246.42s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [2:22:52<2:22:07, 170.55s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [2:20:17<37:22, 106.77s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [2:25:41<2:18:58, 170.18s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [2:22:03<35:30, 106.51s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [5:11:08<1:30:46, 247.59s/it]  Evaluating commonsenseqa :  81%|████████  | 81/100 [2:23:49<33:43, 106.48s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [2:28:29<2:15:43, 169.66s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [2:25:36<31:55, 106.41s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [5:15:18<1:26:54, 248.31s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [2:27:23<30:13, 106.66s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [2:31:20<2:13:12, 170.05s/it]                                                                                                   Evaluating commonsenseqa :  84%|████████▍ | 84/100 [2:29:12<28:36, 107.31s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [2:34:12<2:10:44, 170.53s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [2:30:58<26:46, 107.08s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [5:18:58<1:20:01, 240.07s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [2:32:45<24:56, 106.91s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [2:37:02<2:07:40, 170.24s/it]                                                                                                   Evaluating commonsenseqa :  87%|████████▋ | 87/100 [2:34:32<23:10, 106.99s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [5:23:16<1:17:38, 245.17s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [2:39:51<2:04:41, 170.03s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [2:36:20<21:27, 107.31s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [2:38:07<19:39, 107.24s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [2:42:40<2:01:35, 169.67s/it]      Evaluating commonsenseqa :  82%|████████▏ | 82/100 [5:27:23<1:13:44, 245.82s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [2:39:55<17:55, 107.58s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [2:41:43<16:07, 107.50s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [2:45:33<1:59:28, 170.67s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [2:43:30<14:19, 107.50s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [5:31:30<1:09:44, 246.15s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [2:48:25<1:56:49, 170.95s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [2:45:16<12:28, 106.97s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [2:47:01<10:38, 106.41s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [2:51:19<1:54:41, 172.04s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [5:35:39<1:05:55, 247.20s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [2:48:47<08:51, 106.27s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [2:54:08<1:51:16, 171.18s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [2:50:34<07:06, 106.52s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [5:39:49<1:01:59, 247.98s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [2:52:21<05:19, 106.57s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [2:56:59<1:48:16, 170.95s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [2:54:10<03:35, 107.52s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [2:55:58<01:47, 107.39s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [2:59:51<1:45:44, 171.47s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [5:44:04<58:20, 250.02s/it]  Evaluating commonsenseqa : 100%|██████████| 100/100 [2:57:43<00:00, 106.79s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:57:43<00:00, 106.63s/it]
name: commonsenseqa | avg. gen lenth: 373.464 | time: 10666.381003856659s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13646 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 13 13 15 True False 4096 10
[2023-09-10 16:54:45,509] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s10-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 13
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tmathqa-s10-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 593899.40it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.05s/it]
 > number of parameters: 6738415616
[2023-09-10 16:55:01,312] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-10 16:55:01,523] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-10 16:55:01,525] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-10 16:55:01,525] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-10 16:55:01,525] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-10 16:55:01,525] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-10 16:55:01,525] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-10 16:55:01,525] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-10 16:55:01,525] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-10 16:55:01,525] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-10 16:55:01,525] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3be280>
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-10 16:55:01,526] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-10 16:55:01,527] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-10 16:55:01,527] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: in a recent head - to - head run - off election, 12000 absentee ballets were cast. 1 / 6 of the absentee ballets were thrown out and 3 / 5 of the remaining absentee ballets were cast for candidate a. how many absentee votes did candidate b receive? a ) 2,000, b ) 3,000, c ) 4,000, d ) 8,000, e ) 9,000
Output: c

Input: the total age of a and b is 11 years more than the total age of b and c. c is how many years younger than a.? a ) 16, b ) 12, c ) 11, d ) 20, e ) 10
Output: c

Input: the scoring system in a certain football competition goes as follows : 3 points for victory, 1 point for a draw, and 0 points for defeat. each team plays 20 matches. if a team scored 14 points after 5 games, what is the least number of the remaining matches it has to win to reach the 40 - point mark by the end of the tournament? a ) 6, b ) 7, c ) 8, d ) 9, e ) 10
Output: a

Input: how much water should be added to 10 liters of a 20 % - solution of alcohol to reduce the concentration of alcohol in the solution by 75 %? a ) 25 liters, b ) 27 liters, c ) 30 liters, d ) 32 liters, e ) 35 liters
Output: c

Input: a goods train runs at the speed of 72 kmph and crosses a 250 m long platform in 36 seconds. what is the length of the goods train? a ) 470 m, b ) 240 m, c ) 260 m, d ) 270 m, e ) none of these
Output: a

Input: the volume of a cube is 1728 cc. find its surface. a ) 864, b ) 556, c ) 255, d ) 287, e ) 267
Output: a

Input: a certain drink of type a is prepared by mixing 4 parts milk with 3 parts fruit juice. another drink of type b is prepared by mixing 4 parts of fruit juice and 3 parts of milk. how many liters of fruit juice must be added to 63 liters of drink a to convert it to drink b? a ) 7, b ) 14, c ) 21, d ) 28, e ) 35
Output: c

Input: last week john spent 10 percent of his wages on recreation. this week, his wages are 10 percent less than last week ʼ s wages and he spent 40 percent of his wages on recreation. the amount he spends on recreation this week is what percent of the amount he spent on recreation last week a ) 180 %, b ) 360 %, c ) 200 %, d ) 220 %, e ) 250 %
Output: b

Input: the salary of a person was reduced by 15 %. by what percent should his reduced salary be raised so as to bring it at par with his original salary? a ) 10 %, b ) 17.6 %, c ) 13.7 %, d ) 15.1 %, e ) 12.3 %
Output: b

Input: if 20 men can build a water fountain 56 metres long in 7 days, what length of a similar water fountain can be built by 35 men in 3 days? a ) 40 m, b ) 42 m, c ) 47 m, d ) 49 m, e ) 50 m
Output: b

Input: 12 men complete a work in 9 days. after they have worked for 6 days, 6 more men join them. how many days will they take to complete the remaining work? a ) 2 days, b ) 7 days, c ) 8 days, d ) 9 days, e ) 45 days
Output: a

Input: a cheese factory sells its cheese in rectangular blocks. a normal block has a volume of 5 cubic feet. if a large block has twice the width, twice the depth, and the same length of a normal block, what is the volume of cheese in a large block in cubic feet? a ) 15, b ) 30, c ) 20, d ) 18, e ) 25
Output: c

Input: two trains, one from howrah to patna and the other from patna to howrah, start simultaneously. after they meet, the trains reach their destinations after 64 hours and 25 hours respectively. the ratio of their speeds is? a ) 4 : 9, b ) 4 : 3, c ) 4 : 5, d ) 5 : 8, e ) 4 : 2
Output: d

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tmathqa-s10-rFalse-m4096
Evaluating commonsenseqa :  64%|██████▍   | 64/100 [3:02:47<1:43:35, 172.65s/it]    Evaluating commonsenseqa :  87%|████████▋ | 87/100 [5:48:07<53:42, 247.90s/it]Evaluating commonsenseqa :   1%|          | 1/100 [03:19<5:28:47, 199.27s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [3:05:39<1:40:33, 172.38s/it]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Evaluating commonsenseqa :  88%|████████▊ | 88/100 [5:52:14<49:31, 247.66s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [3:08:30<1:37:28, 172.00s/it]b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 3
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o3-tmathqa-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 1084998.81it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.08s/it]
 > number of parameters: 6738415616
[2023-09-10 17:01:19,281] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-10 17:01:19,483] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-10 17:01:19,484] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-10 17:01:19,484] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554b5f85400>
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-10 17:01:19,485] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-10 17:01:19,486] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-10 17:01:19,486] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: a

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o3-tmathqa-s1-rFalse-m4096
                                                                                                                                                                                                                                                                                                                                                                Evaluating commonsenseqa :   1%|          | 1/100 [04:41<7:44:53, 281.75s/it]                                                                                                                                                                                                                                                                                Evaluating commonsenseqa :   2%|▏         | 2/100 [08:46<7:04:26, 259.86s/it]                                                                                                                                                                                                                                                                                               Evaluating commonsenseqa :   3%|▎         | 3/100 [13:29<7:17:06, 270.37s/it]                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Evaluating commonsenseqa :   4%|▍         | 4/100 [18:17<7:23:57, 277.48s/it]                                                                                                                                                                                                                                                                                    Evaluating commonsenseqa :   5%|▌         | 5/100 [21:40<6:37:01, 250.76s/it]                                                                                                                                                                                                                                                                                    Evaluating commonsenseqa :  75%|███████▌  | 75/100 [3:34:23<1:11:47, 172.31s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [33:17<4:59:17, 199.53s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [6:21:15<20:49, 249.93s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [3:37:16<1:09:00, 172.50s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [36:33<4:54:24, 198.48s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [3:40:08<1:06:03, 172.34s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [6:25:30<16:44, 251.16s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [39:48<4:49:42, 197.52s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [3:43:06<1:03:47, 173.97s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [43:05<4:45:49, 197.12s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [6:29:33<12:26, 248.85s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [3:45:59<1:00:53, 173.96s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [45:00<4:07:08, 172.42s/it]                                                                                Evaluating commonsenseqa :  80%|████████  | 80/100 [3:48:50<57:41, 173.07s/it]  Evaluating commonsenseqa :  98%|█████████▊| 98/100 [6:33:42<08:17, 248.97s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [48:18<4:15:15, 180.18s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [3:51:41<54:31, 172.20s/it]                                                                                 Evaluating commonsenseqa :  16%|█▌        | 16/100 [51:36<4:19:38, 185.46s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [6:37:48<04:07, 247.89s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [3:54:32<51:37, 172.09s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [54:55<4:22:17, 189.61s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [3:57:22<48:31, 171.28s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [6:42:01<00:00, 249.40s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [6:42:01<00:00, 241.21s/it]
name: commonsenseqa | avg. gen lenth: 208.918 | time: 24122.350596666336s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13644 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 5 5 7 True False 4096 10
[2023-09-10 17:51:03,859] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 5
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o5-tmathqa-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 801347.75it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.65s/it]
 > number of parameters: 6738415616
[2023-09-10 17:51:18,727] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-10 17:51:18,930] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-10 17:51:18,932] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-10 17:51:18,932] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-10 17:51:18,932] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-10 17:51:18,932] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-10 17:51:18,932] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0280>
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-10 17:51:18,933] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-10 17:51:18,934] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-10 17:51:18,934] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: in a recent head - to - head run - off election, 12000 absentee ballets were cast. 1 / 6 of the absentee ballets were thrown out and 3 / 5 of the remaining absentee ballets were cast for candidate a. how many absentee votes did candidate b receive? a ) 2,000, b ) 3,000, c ) 4,000, d ) 8,000, e ) 9,000
Output: 5 / 6 * 2 / 5 ( total absentee votes ) = 1 / 3 ( total votes ) = 1 / 3 * 12000 = 4000 answer is c
So the final answer is c

Input: the total age of a and b is 11 years more than the total age of b and c. c is how many years younger than a.? a ) 16, b ) 12, c ) 11, d ) 20, e ) 10
Output: "( a + b ) - ( b - c ) = 11 a - c = 11 answer is c"
So the final answer is c

Input: the scoring system in a certain football competition goes as follows : 3 points for victory, 1 point for a draw, and 0 points for defeat. each team plays 20 matches. if a team scored 14 points after 5 games, what is the least number of the remaining matches it has to win to reach the 40 - point mark by the end of the tournament? a ) 6, b ) 7, c ) 8, d ) 9, e ) 10
Output: "to get 40 points as end of season we need another 26 points or more from remaining 15 matches : option a = 6 * 3 + 9 * 1 = 27 hence option a - 6"
So the final answer is a

Input: how much water should be added to 10 liters of a 20 % - solution of alcohol to reduce the concentration of alcohol in the solution by 75 %? a ) 25 liters, b ) 27 liters, c ) 30 liters, d ) 32 liters, e ) 35 liters
Output: "let x ltr water to be added 2 ltr alcohol to be represented as ( 20 ( 1 - 3 / 4 ( new soln. = 10 + x ) ) ) 2 = 5 % * ( 10 + x ) - - - - - - - - > x = 30 answer : c"
So the final answer is c

Input: a goods train runs at the speed of 72 kmph and crosses a 250 m long platform in 36 seconds. what is the length of the goods train? a ) 470 m, b ) 240 m, c ) 260 m, d ) 270 m, e ) none of these
Output: "explanation : speed = [ 72 x ( 5 / 18 ) ] m / sec = 20 m / sec. time = 36 sec. let the length of the train be x metres. then, [ ( x + 250 ) / 36 ] = 20 = > x + 250 = 720 = > x = 470. answer : a"
So the final answer is a

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o5-tmathqa-s10-rTrue-m4096
Evaluating commonsenseqa :  18%|█▊        | 18/100 [58:09<4:20:49, 190.85s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [4:00:12<45:33, 170.83s/it]Evaluating commonsenseqa :   1%|          | 1/100 [03:49<6:19:01, 229.71s/it]     Evaluating commonsenseqa :  85%|████████▌ | 85/100 [4:03:03<42:44, 170.95s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [1:01:25<4:19:57, 192.56s/it]                                                                                   Evaluating commonsenseqa :   2%|▏         | 2/100 [07:37<6:13:36, 228.74s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [4:05:56<40:01, 171.56s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [1:04:44<4:19:21, 194.52s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [4:08:47<37:09, 171.53s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [11:22<6:06:38, 226.79s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [1:08:00<4:16:39, 194.93s/it]                                                                                     Evaluating commonsenseqa :  88%|████████▊ | 88/100 [4:11:40<34:22, 171.89s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:11:19<4:14:50, 196.04s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [15:06<6:01:31, 225.96s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [4:14:34<31:36, 172.38s/it]                                                                                     Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:14:37<4:12:23, 196.66s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [18:57<6:00:40, 227.79s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [4:17:26<28:42, 172.26s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:17:55<4:09:43, 197.15s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [4:20:19<25:52, 172.50s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [22:40<5:54:03, 225.99s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:21:09<4:05:17, 196.23s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [4:23:16<23:11, 173.92s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [26:30<5:52:23, 227.35s/it]     Evaluating commonsenseqa :  93%|█████████▎| 93/100 [4:26:07<20:11, 173.06s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:24:25<4:01:54, 196.14s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [30:25<5:52:04, 229.61s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [4:28:58<17:14, 172.36s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:27:40<3:58:01, 195.63s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [4:31:50<14:22, 172.47s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [34:12<5:47:26, 229.08s/it]     Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:30:59<3:56:00, 196.68s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [4:34:45<11:32, 173.01s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [37:58<5:42:12, 228.14s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:34:18<3:53:34, 197.39s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [4:37:34<08:35, 171.81s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:37:33<3:49:24, 196.64s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [41:43<5:36:42, 227.00s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [4:40:28<05:45, 172.51s/it]                                                                                     Evaluating commonsenseqa :  31%|███       | 31/100 [1:40:48<3:45:37, 196.20s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [4:43:19<02:52, 172.01s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [45:33<5:34:07, 227.81s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [1:44:04<3:42:18, 196.16s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [4:46:09<00:00, 171.55s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [4:46:09<00:00, 171.70s/it]
name: commonsenseqa | avg. gen lenth: 343.566 | time: 17171.68097305298s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13645 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 11 9 11 True False 4096 10
[2023-09-10 18:39:23,374] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 11
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o11-tmathqa-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 476552.07it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.45s/it]
 > number of parameters: 6738415616
[2023-09-10 18:39:39,991] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-10 18:39:40,219] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-10 18:39:40,221] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-10 18:39:40,221] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0280>
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-10 18:39:40,222] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-10 18:39:40,223] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-10 18:39:40,223] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: in a recent head - to - head run - off election, 12000 absentee ballets were cast. 1 / 6 of the absentee ballets were thrown out and 3 / 5 of the remaining absentee ballets were cast for candidate a. how many absentee votes did candidate b receive? a ) 2,000, b ) 3,000, c ) 4,000, d ) 8,000, e ) 9,000
Output: 5 / 6 * 2 / 5 ( total absentee votes ) = 1 / 3 ( total votes ) = 1 / 3 * 12000 = 4000 answer is c
So the final answer is c

Input: the total age of a and b is 11 years more than the total age of b and c. c is how many years younger than a.? a ) 16, b ) 12, c ) 11, d ) 20, e ) 10
Output: "( a + b ) - ( b - c ) = 11 a - c = 11 answer is c"
So the final answer is c

Input: the scoring system in a certain football competition goes as follows : 3 points for victory, 1 point for a draw, and 0 points for defeat. each team plays 20 matches. if a team scored 14 points after 5 games, what is the least number of the remaining matches it has to win to reach the 40 - point mark by the end of the tournament? a ) 6, b ) 7, c ) 8, d ) 9, e ) 10
Output: "to get 40 points as end of season we need another 26 points or more from remaining 15 matches : option a = 6 * 3 + 9 * 1 = 27 hence option a - 6"
So the final answer is a

Input: how much water should be added to 10 liters of a 20 % - solution of alcohol to reduce the concentration of alcohol in the solution by 75 %? a ) 25 liters, b ) 27 liters, c ) 30 liters, d ) 32 liters, e ) 35 liters
Output: "let x ltr water to be added 2 ltr alcohol to be represented as ( 20 ( 1 - 3 / 4 ( new soln. = 10 + x ) ) ) 2 = 5 % * ( 10 + x ) - - - - - - - - > x = 30 answer : c"
So the final answer is c

Input: a goods train runs at the speed of 72 kmph and crosses a 250 m long platform in 36 seconds. what is the length of the goods train? a ) 470 m, b ) 240 m, c ) 260 m, d ) 270 m, e ) none of these
Output: "explanation : speed = [ 72 x ( 5 / 18 ) ] m / sec = 20 m / sec. time = 36 sec. let the length of the train be x metres. then, [ ( x + 250 ) / 36 ] = 20 = > x + 250 = 720 = > x = 470. answer : a"
So the final answer is a

Input: the volume of a cube is 1728 cc. find its surface. a ) 864, b ) 556, c ) 255, d ) 287, e ) 267
Output: "a 3 = 1728 = > a = 12 6 a 2 = 6 * 12 * 12 = 864 answer : a"
So the final answer is a

Input: a certain drink of type a is prepared by mixing 4 parts milk with 3 parts fruit juice. another drink of type b is prepared by mixing 4 parts of fruit juice and 3 parts of milk. how many liters of fruit juice must be added to 63 liters of drink a to convert it to drink b? a ) 7, b ) 14, c ) 21, d ) 28, e ) 35
Output: "in 63 liters of drink a, there are 36 liters of milk and 27 liters of juice. with 36 liters of milk, we need a total of 48 liters of juice to make drink b. we need to add 21 liters of juice. the answer is c."
So the final answer is c

Input: last week john spent 10 percent of his wages on recreation. this week, his wages are 10 percent less than last week ʼ s wages and he spent 40 percent of his wages on recreation. the amount he spends on recreation this week is what percent of the amount he spent on recreation last week a ) 180 %, b ) 360 %, c ) 200 %, d ) 220 %, e ) 250 %
Output: "say john's wages last week were $ 100, so he spent 0.10 * 100 = $ 10 on recreation ; this week's wages is 0.90 * 100 = $ 90, so he spends 0.4 * 90 = $ 36 on recreation ; 36 / 10 = 3.60, hence the amount he spends on recreation this week is 360 % of the amount he spent on recreation last week : 10 * 3.6 = 36 answer : b"
So the final answer is b

Input: the salary of a person was reduced by 15 %. by what percent should his reduced salary be raised so as to bring it at par with his original salary? a ) 10 %, b ) 17.6 %, c ) 13.7 %, d ) 15.1 %, e ) 12.3 %
Output: "let the original salary be $ 100 new salary = $ 85 increase on 85 = 15 increase on 100 = 15 / 85 * 100 = 17.6 % answer is b"
So the final answer is b

Input: if 20 men can build a water fountain 56 metres long in 7 days, what length of a similar water fountain can be built by 35 men in 3 days? a ) 40 m, b ) 42 m, c ) 47 m, d ) 49 m, e ) 50 m
Output: "explanation : let the required length be x metres more men, more length built ( direct proportion ) less days, less length built ( direct proportion ) men 20 : 35 days 7 : 3 : : 56 : x therefore ( 20 x 7 x x ) = ( 35 x 3 x 56 ) x = ( 35 x 3 x 56 ) / 140 = 42 hence, the required length is 42 m. answer : b"
So the final answer is b

Input: 12 men complete a work in 9 days. after they have worked for 6 days, 6 more men join them. how many days will they take to complete the remaining work? a ) 2 days, b ) 7 days, c ) 8 days, d ) 9 days, e ) 45 days
Output: 1 man's 1 day work = 1 / 108 12 men's 6 day's work = 1 / 9 * 6 = 2 / 3 remaining work = 1 - 2 / 3 = 1 / 3 18 men's 1 day work = 1 / 108 * 18 = 1 / 6 1 / 6 work is done by them in 1 day. 1 / 3 work is done by them in 6 * 1 / 3 = 2 days. answer : a
So the final answer is a

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o11-tmathqa-s10-rTrue-m4096
Evaluating commonsenseqa :  13%|█▎        | 13/100 [49:23<5:31:34, 228.67s/it]Evaluating commonsenseqa :   1%|          | 1/100 [02:28<4:05:17, 148.66s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [1:47:24<3:40:16, 197.26s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [53:05<5:24:54, 226.69s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [04:56<4:01:55, 148.12s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [1:50:36<3:35:11, 195.63s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [07:20<3:56:41, 146.41s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [56:57<5:23:08, 228.10s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:53:51<3:31:46, 195.48s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [09:45<3:52:53, 145.56s/it]       Evaluating commonsenseqa :   5%|▌         | 5/100 [12:08<3:49:28, 144.93s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [1:00:39<5:16:59, 226.43s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [1:57:06<3:28:17, 195.28s/it]                                                                                       Evaluating commonsenseqa :   6%|▌         | 6/100 [14:33<3:46:58, 144.88s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [2:00:27<3:27:05, 197.22s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [1:04:24<5:12:42, 226.06s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [17:00<3:45:20, 145.38s/it]                                                                                       Evaluating commonsenseqa :  38%|███▊      | 38/100 [2:03:48<3:24:44, 198.13s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [19:24<3:42:28, 145.09s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [1:08:05<5:06:36, 224.35s/it]                                                                                       Evaluating commonsenseqa :   9%|▉         | 9/100 [21:51<3:40:47, 145.57s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [2:07:06<3:21:33, 198.25s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [1:11:51<5:03:46, 225.02s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [24:17<3:38:34, 145.72s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [2:10:24<3:18:00, 198.00s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [26:44<3:36:50, 146.19s/it]      Evaluating commonsenseqa :  20%|██        | 20/100 [1:15:46<5:03:40, 227.75s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [2:13:42<3:14:52, 198.18s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [29:08<3:33:37, 145.66s/it]                                                                                       Evaluating commonsenseqa :  21%|██        | 21/100 [1:19:33<4:59:48, 227.70s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [31:35<3:31:27, 145.83s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [2:17:01<3:11:36, 198.22s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [34:03<3:30:01, 146.53s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:23:18<4:55:03, 226.96s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [2:20:22<3:09:13, 199.18s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [36:27<3:26:43, 145.92s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [2:21:33<2:30:06, 160.84s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:27:08<4:52:13, 227.70s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [38:52<3:23:41, 145.49s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [2:24:49<2:37:07, 171.42s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [41:17<3:21:16, 145.50s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:30:52<4:47:15, 226.78s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [2:27:27<2:30:38, 167.38s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [43:42<3:18:37, 145.33s/it]                                                                                         Evaluating commonsenseqa :  47%|████▋     | 47/100 [2:30:43<2:35:12, 175.71s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [46:06<3:15:30, 144.82s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:34:38<4:43:03, 226.45s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [48:36<3:15:15, 146.45s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [2:34:00<2:37:48, 182.09s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:38:25<4:39:21, 226.51s/it]  Evaluating commonsenseqa :  21%|██        | 21/100 [51:02<3:12:34, 146.26s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [2:37:17<2:38:37, 186.61s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [53:27<3:09:40, 145.90s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:42:13<4:36:05, 226.92s/it]                                                                                         Evaluating commonsenseqa :  23%|██▎       | 23/100 [55:53<3:07:24, 146.03s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [2:40:33<2:37:48, 189.37s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:46:01<4:32:57, 227.46s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [58:18<3:04:36, 145.75s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [2:43:46<2:35:36, 190.55s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:00:43<3:01:54, 145.53s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:49:51<4:29:58, 228.14s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [2:47:03<2:33:56, 192.43s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:03:09<2:59:37, 145.64s/it]                                                                                         Evaluating commonsenseqa :  30%|███       | 30/100 [1:53:36<4:24:58, 227.12s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:05:36<2:57:38, 146.01s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [2:50:21<2:32:06, 194.18s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:07:59<2:54:04, 145.06s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [2:53:36<2:28:58, 194.32s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [1:57:21<4:20:37, 226.63s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:10:22<2:50:51, 144.39s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [2:56:55<2:26:48, 195.76s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [2:01:05<4:15:54, 225.80s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:12:47<2:48:46, 144.67s/it]  Evaluating commonsenseqa :  31%|███       | 31/100 [1:15:12<2:46:16, 144.58s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [3:00:12<2:23:52, 196.19s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [2:04:57<4:14:09, 227.60s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [1:17:36<2:43:47, 144.52s/it]                                                                                         Evaluating commonsenseqa :  57%|█████▋    | 57/100 [3:03:30<2:21:01, 196.77s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [1:20:01<2:41:42, 144.82s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [2:08:42<4:09:26, 226.77s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [3:06:53<2:18:58, 198.55s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [1:22:26<2:39:17, 144.81s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [2:12:25<4:04:39, 225.84s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:24:52<2:37:05, 145.00s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [3:10:11<2:15:38, 198.49s/it]                                                                                         Evaluating commonsenseqa :  36%|███▌      | 36/100 [1:27:15<2:34:11, 144.56s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [2:16:14<4:01:41, 226.58s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [3:12:52<2:04:52, 187.30s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [1:29:43<2:32:48, 145.54s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [3:16:08<2:03:23, 189.84s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [2:20:02<3:58:29, 227.14s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [1:32:10<2:30:54, 146.04s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [1:34:35<2:27:57, 145.53s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [3:19:25<2:01:38, 192.06s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [2:23:52<3:55:36, 228.01s/it]  Evaluating commonsenseqa :  40%|████      | 40/100 [1:36:59<2:25:18, 145.31s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [3:22:45<1:59:52, 194.38s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [2:27:40<3:51:50, 228.04s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [1:39:24<2:22:47, 145.20s/it]                                                                                           Evaluating commonsenseqa :  64%|██████▍   | 64/100 [3:26:05<1:57:42, 196.18s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:41:52<2:20:58, 145.83s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [2:31:28<3:47:50, 227.85s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:44:20<2:19:15, 146.58s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [3:29:21<1:54:25, 196.15s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:46:48<2:17:07, 146.92s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [2:35:12<3:42:52, 226.66s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [3:32:38<1:51:13, 196.27s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:49:14<2:14:37, 146.86s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [2:38:53<3:37:34, 225.08s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [3:35:58<1:48:35, 197.43s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [1:51:42<2:12:21, 147.07s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:54:09<2:09:46, 146.92s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [2:42:39<3:33:57, 225.23s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [3:39:16<1:45:22, 197.58s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:56:33<2:06:43, 146.21s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [3:42:36<1:42:21, 198.13s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [2:46:25<3:30:34, 225.62s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:58:59<2:04:13, 146.15s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [3:45:55<1:39:19, 198.65s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [2:01:24<2:01:24, 145.69s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [2:50:11<3:26:50, 225.64s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [2:03:48<1:58:34, 145.19s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [3:49:08<1:35:04, 196.71s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [2:54:02<3:24:42, 227.45s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [2:06:12<1:55:52, 144.85s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [3:52:25<1:31:53, 196.92s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [2:08:36<1:53:22, 144.73s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [2:57:50<3:21:03, 227.61s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [3:55:42<1:28:39, 197.03s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [2:11:04<1:51:44, 145.76s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [3:01:38<3:17:14, 227.59s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [2:13:28<1:48:55, 145.24s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [3:59:03<1:25:54, 198.24s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [2:15:55<1:46:47, 145.62s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [3:05:25<3:13:18, 227.43s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [3:56:25<3:47:122:52, 198.91s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [2:18:17<1:43:37, 144.60s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [2:20:43<1:41:29, 144.98s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [3:09:13<3:09:42, 227.66s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [4:05:45<1:19:52, 199.67s/it]                                                                                             Evaluating commonsenseqa :  59%|█████▉    | 59/100 [2:23:07<1:38:48, 144.61s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [4:09:06<1:16:39, 199.98s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [3:13:01<3:05:55, 227.66s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [2:25:32<1:36:26, 144.65s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [4:12:26<1:13:21, 200.05s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [2:27:57<1:34:10, 144.89s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [3:16:48<3:02:01, 227.52s/it]                                                                                             Evaluating commonsenseqa :  62%|██████▏   | 62/100 [2:30:23<1:31:55, 145.16s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [4:15:44<1:09:44, 199.28s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [3:20:33<2:57:29, 226.58s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [2:32:51<1:30:08, 146.19s/it]                                                                                             Evaluating commonsenseqa :  80%|████████  | 80/100 [4:18:59<1:06:00, 198.03s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [2:35:20<1:28:07, 146.86s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [3:24:20<2:53:53, 226.81s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [4:22:18<1:02:48, 198.32s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [2:37:46<1:25:28, 146.54s/it]                                                                                             Evaluating commonsenseqa :  55%|█████▌    | 55/100 [3:28:06<2:49:55, 226.56s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [2:40:12<1:23:01, 146.53s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [4:25:38<59:40, 198.89s/it]  Evaluating commonsenseqa :  67%|██████▋   | 67/100 [2:42:36<1:20:11, 145.82s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [3:31:57<2:47:05, 227.85s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [4:28:56<56:18, 198.74s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [2:45:01<1:17:32, 145.38s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [3:35:43<2:43:02, 227.50s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [2:47:28<1:15:28, 146.09s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [4:32:12<52:46, 197.90s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [2:49:56<1:13:19, 146.63s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [4:35:31<49:33, 198.21s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [3:39:34<2:39:57, 228.50s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [2:52:24<1:11:00, 146.92s/it]                                                                                             Evaluating commonsenseqa :  86%|████████▌ | 86/100 [4:38:48<46:08, 197.77s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [2:54:50<1:08:26, 146.65s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [3:43:23<2:36:11, 228.57s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [2:57:17<1:06:06, 146.89s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [4:42:06<42:53, 197.94s/it]                                                                                               Evaluating commonsenseqa :  60%|██████    | 60/100 [3:47:15<2:33:02, 229.56s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [2:59:43<1:03:27, 146.45s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [4:45:26<39:41, 198.47s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [3:02:10<1:01:05, 146.61s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [3:51:01<2:28:32, 228.52s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [4:48:46<36:27, 198.85s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [3:04:37<58:41, 146.75s/it]  Evaluating commonsenseqa :  62%|██████▏   | 62/100 [3:54:49<2:24:36, 228.34s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [3:07:04<56:16, 146.80s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [4:52:05<33:10, 199.07s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [3:09:32<54:00, 147.31s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [3:58:35<2:20:24, 227.70s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [4:55:24<29:50, 198.98s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [3:12:03<51:53, 148.26s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [4:02:25<2:17:00, 228.36s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [4:58:44<26:34, 199.28s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [3:14:30<49:18, 147.92s/it]                                                                                               Evaluating commonsenseqa :  93%|█████████▎| 93/100 [5:01:33<22:11, 190.18s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [3:16:55<46:33, 147.02s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [4:06:12<2:12:53, 227.83s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [5:03:25<16:39, 166.59s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [3:19:19<43:53, 146.31s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [4:10:02<2:09:37, 228.76s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [3:21:44<41:18, 145.77s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [5:06:40<14:36, 175.20s/it]                                                                                               Evaluating commonsenseqa :  84%|████████▍ | 84/100 [3:24:12<39:04, 146.51s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [5:09:54<12:03, 180.93s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [4:13:50<2:05:34, 228.31s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [3:26:38<36:36, 146.42s/it]                                                                                               Evaluating commonsenseqa :  97%|█████████▋| 97/100 [5:13:11<09:17, 185.75s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [3:29:02<33:58, 145.58s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [4:17:35<2:01:13, 227.29s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [3:31:28<31:32, 145.60s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [5:16:32<06:20, 190.14s/it]                                                                                               Evaluating commonsenseqa :  69%|██████▉   | 69/100 [4:21:23<1:57:37, 227.66s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [5:18:15<02:44, 164.06s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [3:33:55<29:12, 146.00s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [3:36:19<26:41, 145.61s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [4:25:10<1:53:46, 227.55s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [5:21:32<00:00, 174.02s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [5:21:32<00:00, 192.93s/it]
name: commonsenseqa | avg. gen lenth: 227.577 | time: 19294.219054222107s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13646 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 15 13 15 True False 4096 10
[2023-09-10 22:16:42,123] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s10-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 15
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o15-tmathqa-s10-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 544313.50it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.71s/it]
 > number of parameters: 6738415616
[2023-09-10 22:16:57,088] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-10 22:16:57,292] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-10 22:16:57,294] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0250>
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-10 22:16:57,294] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-10 22:16:57,295] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-10 22:16:57,296] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-10 22:16:57,296] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-10 22:16:57,296] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-10 22:16:57,296] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: in a recent head - to - head run - off election, 12000 absentee ballets were cast. 1 / 6 of the absentee ballets were thrown out and 3 / 5 of the remaining absentee ballets were cast for candidate a. how many absentee votes did candidate b receive? a ) 2,000, b ) 3,000, c ) 4,000, d ) 8,000, e ) 9,000
Output: c

Input: the total age of a and b is 11 years more than the total age of b and c. c is how many years younger than a.? a ) 16, b ) 12, c ) 11, d ) 20, e ) 10
Output: c

Input: the scoring system in a certain football competition goes as follows : 3 points for victory, 1 point for a draw, and 0 points for defeat. each team plays 20 matches. if a team scored 14 points after 5 games, what is the least number of the remaining matches it has to win to reach the 40 - point mark by the end of the tournament? a ) 6, b ) 7, c ) 8, d ) 9, e ) 10
Output: a

Input: how much water should be added to 10 liters of a 20 % - solution of alcohol to reduce the concentration of alcohol in the solution by 75 %? a ) 25 liters, b ) 27 liters, c ) 30 liters, d ) 32 liters, e ) 35 liters
Output: c

Input: a goods train runs at the speed of 72 kmph and crosses a 250 m long platform in 36 seconds. what is the length of the goods train? a ) 470 m, b ) 240 m, c ) 260 m, d ) 270 m, e ) none of these
Output: a

Input: the volume of a cube is 1728 cc. find its surface. a ) 864, b ) 556, c ) 255, d ) 287, e ) 267
Output: a

Input: a certain drink of type a is prepared by mixing 4 parts milk with 3 parts fruit juice. another drink of type b is prepared by mixing 4 parts of fruit juice and 3 parts of milk. how many liters of fruit juice must be added to 63 liters of drink a to convert it to drink b? a ) 7, b ) 14, c ) 21, d ) 28, e ) 35
Output: c

Input: last week john spent 10 percent of his wages on recreation. this week, his wages are 10 percent less than last week ʼ s wages and he spent 40 percent of his wages on recreation. the amount he spends on recreation this week is what percent of the amount he spent on recreation last week a ) 180 %, b ) 360 %, c ) 200 %, d ) 220 %, e ) 250 %
Output: b

Input: the salary of a person was reduced by 15 %. by what percent should his reduced salary be raised so as to bring it at par with his original salary? a ) 10 %, b ) 17.6 %, c ) 13.7 %, d ) 15.1 %, e ) 12.3 %
Output: b

Input: if 20 men can build a water fountain 56 metres long in 7 days, what length of a similar water fountain can be built by 35 men in 3 days? a ) 40 m, b ) 42 m, c ) 47 m, d ) 49 m, e ) 50 m
Output: b

Input: 12 men complete a work in 9 days. after they have worked for 6 days, 6 more men join them. how many days will they take to complete the remaining work? a ) 2 days, b ) 7 days, c ) 8 days, d ) 9 days, e ) 45 days
Output: a

Input: a cheese factory sells its cheese in rectangular blocks. a normal block has a volume of 5 cubic feet. if a large block has twice the width, twice the depth, and the same length of a normal block, what is the volume of cheese in a large block in cubic feet? a ) 15, b ) 30, c ) 20, d ) 18, e ) 25
Output: c

Input: two trains, one from howrah to patna and the other from patna to howrah, start simultaneously. after they meet, the trains reach their destinations after 64 hours and 25 hours respectively. the ratio of their speeds is? a ) 4 : 9, b ) 4 : 3, c ) 4 : 5, d ) 5 : 8, e ) 4 : 2
Output: d

Input: john had a stock of 1400 books in his bookshop. he sold 62 on monday, 62 on tuesday, 60 on wednesday, 48 on thursday and 40 on friday. what percentage of the books were not sold? a ) 81.57 %, b ) 36.5 %, c ) 80.67 %, d ) 56.5 %, e ) 80.57 %
Output: e

Input: a and b can do a piece of work in 18 days ; band c can do it in 24 days a and c can do it in 36 days. in how many days will a, band c finish it together? a ) 15 days, b ) 12 days, c ) 8 days, d ) 16 days, e ) 9 days
Output: d

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o15-tmathqa-s10-rFalse-m4096
                                                                                               Evaluating commonsenseqa :  90%|█████████ | 90/100 [3:38:46<24:20, 146.09s/it]Evaluating commonsenseqa :   1%|          | 1/100 [03:01<4:59:34, 181.57s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [4:28:59<1:50:04, 227.73s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [3:41:14<21:57, 146.39s/it]                                                                                                 Evaluating commonsenseqa :   2%|▏         | 2/100 [06:04<4:57:29, 182.13s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [3:43:42<19:36, 147.03s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [4:32:44<1:45:57, 227.06s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [3:46:05<17:00, 145.81s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [09:05<4:54:03, 181.89s/it]                                                                                                 Evaluating commonsenseqa :  73%|███████▎  | 73/100 [4:36:35<1:42:40, 228.19s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [3:48:30<14:34, 145.67s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [12:11<4:53:27, 183.41s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [3:50:59<12:13, 146.61s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [4:40:20<1:38:30, 227.33s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [15:16<4:51:14, 183.94s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [3:53:25<09:45, 146.47s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [18:18<4:47:10, 183.30s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [3:55:50<07:17, 145.83s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [4:44:11<1:35:11, 228.46s/it]                                                                                                 Evaluating commonsenseqa :  98%|█████████▊| 98/100 [3:58:16<04:51, 146.00s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [21:20<4:43:27, 182.87s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [4:47:55<1:30:47, 226.98s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [4:00:41<02:25, 145.62s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [24:22<4:40:10, 182.73s/it]                 Evaluating commonsenseqa : 100%|██████████| 100/100 [4:03:08<00:00, 146.12s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [4:03:08<00:00, 145.89s/it]
name: commonsenseqa | avg. gen lenth: 359.546 | time: 14590.88662147522s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13645 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 9 9 11 True False 4096 10
[2023-09-10 22:42:59,838] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s10-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 9
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o9-tmathqa-s10-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 730784.78it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [4:51:47<1:27:36, 228.53s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.89s/it]
 > number of parameters: 6738415616
[2023-09-10 22:43:15,179] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-10 22:43:15,386] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-10 22:43:15,387] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0280>
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-10 22:43:15,388] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-10 22:43:15,389] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-10 22:43:15,389] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: in a recent head - to - head run - off election, 12000 absentee ballets were cast. 1 / 6 of the absentee ballets were thrown out and 3 / 5 of the remaining absentee ballets were cast for candidate a. how many absentee votes did candidate b receive? a ) 2,000, b ) 3,000, c ) 4,000, d ) 8,000, e ) 9,000
Output: c

Input: the total age of a and b is 11 years more than the total age of b and c. c is how many years younger than a.? a ) 16, b ) 12, c ) 11, d ) 20, e ) 10
Output: c

Input: the scoring system in a certain football competition goes as follows : 3 points for victory, 1 point for a draw, and 0 points for defeat. each team plays 20 matches. if a team scored 14 points after 5 games, what is the least number of the remaining matches it has to win to reach the 40 - point mark by the end of the tournament? a ) 6, b ) 7, c ) 8, d ) 9, e ) 10
Output: a

Input: how much water should be added to 10 liters of a 20 % - solution of alcohol to reduce the concentration of alcohol in the solution by 75 %? a ) 25 liters, b ) 27 liters, c ) 30 liters, d ) 32 liters, e ) 35 liters
Output: c

Input: a goods train runs at the speed of 72 kmph and crosses a 250 m long platform in 36 seconds. what is the length of the goods train? a ) 470 m, b ) 240 m, c ) 260 m, d ) 270 m, e ) none of these
Output: a

Input: the volume of a cube is 1728 cc. find its surface. a ) 864, b ) 556, c ) 255, d ) 287, e ) 267
Output: a

Input: a certain drink of type a is prepared by mixing 4 parts milk with 3 parts fruit juice. another drink of type b is prepared by mixing 4 parts of fruit juice and 3 parts of milk. how many liters of fruit juice must be added to 63 liters of drink a to convert it to drink b? a ) 7, b ) 14, c ) 21, d ) 28, e ) 35
Output: c

Input: last week john spent 10 percent of his wages on recreation. this week, his wages are 10 percent less than last week ʼ s wages and he spent 40 percent of his wages on recreation. the amount he spends on recreation this week is what percent of the amount he spent on recreation last week a ) 180 %, b ) 360 %, c ) 200 %, d ) 220 %, e ) 250 %
Output: b

Input: the salary of a person was reduced by 15 %. by what percent should his reduced salary be raised so as to bring it at par with his original salary? a ) 10 %, b ) 17.6 %, c ) 13.7 %, d ) 15.1 %, e ) 12.3 %
Output: b

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o9-tmathqa-s10-rFalse-m4096
Evaluating commonsenseqa :   9%|▉         | 9/100 [26:53<4:22:09, 172.85s/it]                                                                                                 Evaluating commonsenseqa :  10%|█         | 10/100 [29:56<4:23:55, 175.95s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [4:55:41<1:24:22, 230.11s/it]Evaluating commonsenseqa :   1%|          | 1/100 [03:50<6:19:44, 230.15s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [32:59<4:24:04, 178.03s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [4:59:33<1:20:43, 230.66s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [07:37<6:12:44, 228.21s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [36:05<4:24:27, 180.31s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [11:22<6:06:34, 226.75s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [5:03:21<1:16:36, 229.83s/it]                                                                                                 Evaluating commonsenseqa :  13%|█▎        | 13/100 [39:08<4:22:37, 181.12s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [5:07:07<1:12:29, 228.91s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [15:16<6:07:49, 229.89s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [42:10<4:20:22, 181.66s/it]                                                                                                 Evaluating commonsenseqa :  15%|█▌        | 15/100 [45:13<4:17:45, 181.95s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [5:10:57<1:08:41, 228.99s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [19:09<6:05:26, 230.81s/it]                                                                                                 Evaluating commonsenseqa :  16%|█▌        | 16/100 [48:16<4:14:58, 182.13s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [5:14:40<1:04:22, 227.19s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [22:59<6:01:16, 230.60s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [51:19<4:12:28, 182.51s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [25:41<5:22:47, 208.25s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [5:18:29<1:00:47, 227.94s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [54:19<4:08:13, 181.63s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [29:25<5:26:50, 213.16s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [5:22:18<57:02, 228.16s/it]  Evaluating commonsenseqa :  19%|█▉        | 19/100 [57:19<4:04:33, 181.16s/it]                Evaluating commonsenseqa :   9%|▉         | 9/100 [33:09<5:28:22, 216.51s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [1:00:22<4:02:32, 181.91s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [5:26:03<53:02, 227.33s/it]                                                                                                   Evaluating commonsenseqa :  10%|█         | 10/100 [36:56<5:29:57, 219.98s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [1:03:27<4:00:37, 182.75s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [5:29:51<49:14, 227.29s/it]                                                                                                   Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:06:31<3:58:03, 183.12s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [40:42<5:28:47, 221.66s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [5:33:39<45:33, 227.79s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:09:33<3:54:33, 182.77s/it]                                                                                                   Evaluating commonsenseqa :  12%|█▏        | 12/100 [44:30<5:27:58, 223.62s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [5:37:25<41:38, 227.13s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:12:04<3:39:34, 173.35s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [48:17<5:25:32, 224.51s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:15:04<3:38:54, 175.13s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [5:41:15<38:01, 228.12s/it]  Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:18:07<3:39:06, 177.66s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [52:07<5:24:13, 226.21s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [5:45:04<34:13, 228.21s/it]                                                                                                   Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:21:08<3:37:27, 178.73s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [55:53<5:20:33, 226.28s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [5:48:53<30:27, 228.48s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:24:12<3:36:15, 180.21s/it]                                                                                                 Evaluating commonsenseqa :  16%|█▌        | 16/100 [59:32<5:13:40, 224.06s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:26:43<3:23:00, 171.55s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [5:52:40<26:36, 228.01s/it]                                                                                                 Evaluating commonsenseqa :  17%|█▋        | 17/100 [1:03:18<5:10:45, 224.65s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:29:52<3:25:55, 176.51s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [5:56:28<22:48, 228.09s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [1:32:52<3:24:21, 177.70s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [1:07:01<5:06:19, 224.14s/it]                                                                                                 Evaluating commonsenseqa :  95%|█████████▌| 95/100 [6:00:21<19:06, 229.40s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [1:35:55<3:23:12, 179.29s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [1:10:47<5:03:33, 224.86s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [6:04:11<15:18, 229.60s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [1:38:58<3:21:30, 180.46s/it]          Evaluating commonsenseqa :  20%|██        | 20/100 [1:14:37<5:01:34, 226.18s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [1:41:55<3:17:25, 179.48s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [6:07:54<11:23, 227.77s/it]                                                                                                   Evaluating commonsenseqa :  21%|██        | 21/100 [1:18:24<4:58:22, 226.61s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:44:55<3:14:26, 179.49s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [6:11:40<07:34, 227.11s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [1:47:55<3:11:40, 179.69s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:22:16<4:56:34, 228.13s/it]            Evaluating commonsenseqa :  99%|█████████▉| 99/100 [6:15:24<03:46, 226.15s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [1:50:56<3:09:08, 180.13s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:26:06<4:53:24, 228.63s/it]                                                                                                   Evaluating commonsenseqa : 100%|██████████| 100/100 [6:19:13<00:00, 227.06s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [6:19:13<00:00, 227.53s/it]
name: commonsenseqa | avg. gen lenth: 338.859 | time: 22755.118641138077s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13644 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 7 5 7 True False 4096 10
[2023-09-11 00:10:38,859] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 7
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o7-tmathqa-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 689262.35it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.62s/it]
 > number of parameters: 6738415616
[2023-09-11 00:10:53,594] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-11 00:10:53,798] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-11 00:10:53,800] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-11 00:10:53,800] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 00:10:53,800] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 00:10:53,800] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-11 00:10:53,800] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-11 00:10:53,800] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 00:10:53,800] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-11 00:10:53,800] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 00:10:53,800] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 00:10:53,800] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3be280>
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-11 00:10:53,801] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-11 00:10:53,802] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-11 00:10:53,802] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-11 00:10:53,802] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-11 00:10:53,802] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-11 00:10:53,802] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-11 00:10:53,802] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-11 00:10:53,802] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-11 00:10:53,802] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 00:10:53,802] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-11 00:10:53,802] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-11 00:10:53,802] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-11 00:10:53,802] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-11 00:10:53,802] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-11 00:10:53,802] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-11 00:10:53,802] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 00:10:53,802] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-11 00:10:53,802] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: in a recent head - to - head run - off election, 12000 absentee ballets were cast. 1 / 6 of the absentee ballets were thrown out and 3 / 5 of the remaining absentee ballets were cast for candidate a. how many absentee votes did candidate b receive? a ) 2,000, b ) 3,000, c ) 4,000, d ) 8,000, e ) 9,000
Output: 5 / 6 * 2 / 5 ( total absentee votes ) = 1 / 3 ( total votes ) = 1 / 3 * 12000 = 4000 answer is c
So the final answer is c

Input: the total age of a and b is 11 years more than the total age of b and c. c is how many years younger than a.? a ) 16, b ) 12, c ) 11, d ) 20, e ) 10
Output: "( a + b ) - ( b - c ) = 11 a - c = 11 answer is c"
So the final answer is c

Input: the scoring system in a certain football competition goes as follows : 3 points for victory, 1 point for a draw, and 0 points for defeat. each team plays 20 matches. if a team scored 14 points after 5 games, what is the least number of the remaining matches it has to win to reach the 40 - point mark by the end of the tournament? a ) 6, b ) 7, c ) 8, d ) 9, e ) 10
Output: "to get 40 points as end of season we need another 26 points or more from remaining 15 matches : option a = 6 * 3 + 9 * 1 = 27 hence option a - 6"
So the final answer is a

Input: how much water should be added to 10 liters of a 20 % - solution of alcohol to reduce the concentration of alcohol in the solution by 75 %? a ) 25 liters, b ) 27 liters, c ) 30 liters, d ) 32 liters, e ) 35 liters
Output: "let x ltr water to be added 2 ltr alcohol to be represented as ( 20 ( 1 - 3 / 4 ( new soln. = 10 + x ) ) ) 2 = 5 % * ( 10 + x ) - - - - - - - - > x = 30 answer : c"
So the final answer is c

Input: a goods train runs at the speed of 72 kmph and crosses a 250 m long platform in 36 seconds. what is the length of the goods train? a ) 470 m, b ) 240 m, c ) 260 m, d ) 270 m, e ) none of these
Output: "explanation : speed = [ 72 x ( 5 / 18 ) ] m / sec = 20 m / sec. time = 36 sec. let the length of the train be x metres. then, [ ( x + 250 ) / 36 ] = 20 = > x + 250 = 720 = > x = 470. answer : a"
So the final answer is a

Input: the volume of a cube is 1728 cc. find its surface. a ) 864, b ) 556, c ) 255, d ) 287, e ) 267
Output: "a 3 = 1728 = > a = 12 6 a 2 = 6 * 12 * 12 = 864 answer : a"
So the final answer is a

Input: a certain drink of type a is prepared by mixing 4 parts milk with 3 parts fruit juice. another drink of type b is prepared by mixing 4 parts of fruit juice and 3 parts of milk. how many liters of fruit juice must be added to 63 liters of drink a to convert it to drink b? a ) 7, b ) 14, c ) 21, d ) 28, e ) 35
Output: "in 63 liters of drink a, there are 36 liters of milk and 27 liters of juice. with 36 liters of milk, we need a total of 48 liters of juice to make drink b. we need to add 21 liters of juice. the answer is c."
So the final answer is c

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o7-tmathqa-s10-rTrue-m4096
Evaluating commonsenseqa :  38%|███▊      | 38/100 [1:53:57<3:06:24, 180.39s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:29:53<4:48:57, 228.13s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [1:56:59<3:03:40, 180.66s/it]Evaluating commonsenseqa :   1%|          | 1/100 [03:23<5:35:27, 203.31s/it]                                                                                                   Evaluating commonsenseqa :  40%|████      | 40/100 [1:59:55<2:59:32, 179.55s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:33:39<4:44:27, 227.57s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [06:43<5:28:57, 201.40s/it]                                                                                                   Evaluating commonsenseqa :  41%|████      | 41/100 [2:02:59<2:57:46, 180.79s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:37:09<4:34:07, 222.27s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [10:01<5:23:25, 200.06s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [2:06:04<2:55:57, 182.03s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:41:00<4:33:28, 224.77s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [13:22<5:20:41, 200.43s/it]                                                                                                   Evaluating commonsenseqa :  43%|████▎     | 43/100 [2:09:05<2:52:31, 181.60s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [16:44<5:18:14, 201.00s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:44:46<4:30:22, 225.31s/it]                                                                                                   Evaluating commonsenseqa :  44%|████▍     | 44/100 [2:12:07<2:49:38, 181.77s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [20:06<5:15:01, 201.07s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:48:37<4:28:28, 226.88s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [2:15:09<2:46:49, 182.00s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [7:31:50<00:00, 275.16s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [7:31:50<00:00, 271.11s/it]
name: commonsenseqa | avg. gen lenth: 215.303 | time: 27111.712695360184s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu04 --master_port 13643 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 1 1 3 True False 4096 10
[2023-09-11 00:33:16,994] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 1
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o1-tmathqa-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 1257400.53it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.44s/it]
 > number of parameters: 6738415616
[2023-09-11 00:33:29,239] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-11 00:33:29,444] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-11 00:33:29,446] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554b5f83430>
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-11 00:33:29,446] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-11 00:33:29,447] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-11 00:33:29,448] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-11 00:33:29,448] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-11 00:33:29,448] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 00:33:29,448] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-11 00:33:29,448] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: in a recent head - to - head run - off election, 12000 absentee ballets were cast. 1 / 6 of the absentee ballets were thrown out and 3 / 5 of the remaining absentee ballets were cast for candidate a. how many absentee votes did candidate b receive? a ) 2,000, b ) 3,000, c ) 4,000, d ) 8,000, e ) 9,000
Output: 5 / 6 * 2 / 5 ( total absentee votes ) = 1 / 3 ( total votes ) = 1 / 3 * 12000 = 4000 answer is c
So the final answer is c

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o1-tmathqa-s10-rTrue-m4096
                                                                                                                                                                                                                                                                  Evaluating commonsenseqa :   1%|          | 1/100 [02:55<4:50:04, 175.80s/it]                                                                                                                                                                                                                                                                                                                                                                                                                                             Evaluating commonsenseqa :   2%|▏         | 2/100 [07:44<6:35:29, 242.13s/it]                                                                                                                                                                                                                                                                     Evaluating commonsenseqa :   3%|▎         | 3/100 [12:35<7:07:30, 264.44s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [2:03:50<4:14:47, 228.17s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [2:30:18<2:31:40, 182.01s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [36:53<4:58:15, 201.07s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [2:33:21<2:28:51, 182.27s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [2:07:35<4:09:51, 227.15s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [40:13<4:54:03, 200.50s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [2:36:20<2:25:02, 181.31s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [43:30<4:49:21, 199.55s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [2:11:19<4:05:12, 226.34s/it]                                                                                Evaluating commonsenseqa :  53%|█████▎    | 53/100 [2:39:26<2:23:02, 182.60s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [46:50<4:46:21, 199.79s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [2:15:09<4:02:25, 227.27s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [2:42:26<2:19:27, 181.90s/it]                                                                                Evaluating commonsenseqa :  15%|█▌        | 15/100 [50:16<4:45:28, 201.51s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [2:18:56<3:58:36, 227.25s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [2:45:30<2:16:47, 182.38s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [53:38<4:42:13, 201.59s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [2:48:32<2:13:39, 182.27s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [2:22:43<3:54:51, 227.29s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [56:56<4:37:39, 200.72s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [2:51:32<2:10:12, 181.70s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [2:26:31<3:51:20, 227.56s/it]                                                                                Evaluating commonsenseqa :  18%|█▊        | 18/100 [1:00:16<4:33:55, 200.44s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [2:54:33<2:07:03, 181.51s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [2:30:20<3:47:58, 227.98s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [1:03:33<4:29:18, 199.49s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [2:57:36<2:04:13, 181.79s/it]                                                                                Evaluating commonsenseqa :  41%|████      | 41/100 [2:34:07<3:43:51, 227.65s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [3:00:39<2:01:33, 182.33s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [1:06:55<4:27:02, 200.29s/it]                                                                                 Evaluating commonsenseqa :  61%|██████    | 61/100 [3:03:40<1:58:18, 182.01s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [1:10:11<4:21:53, 198.90s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [2:37:55<3:40:12, 227.80s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [3:06:46<1:56:00, 183.17s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:13:30<4:18:41, 198.99s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [2:41:41<3:35:43, 227.08s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [3:09:49<1:52:50, 182.99s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:16:53<4:16:36, 199.96s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [2:45:26<3:31:34, 226.69s/it]                                                                                   Evaluating commonsenseqa :  64%|██████▍   | 64/100 [3:12:51<1:49:39, 182.77s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:20:10<4:12:23, 199.25s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [2:49:16<3:28:29, 227.45s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [3:15:51<1:46:05, 181.88s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:23:30<4:09:26, 199.55s/it]                                                                                     Evaluating commonsenseqa :  66%|██████▌   | 66/100 [3:18:55<1:43:28, 182.62s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [2:52:59<3:23:32, 226.17s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:26:51<4:06:22, 199.76s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [3:21:57<1:40:13, 182.22s/it]                                                                                     Evaluating commonsenseqa :  47%|████▋     | 47/100 [2:56:47<3:20:21, 226.82s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:30:08<4:02:10, 199.04s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [3:24:59<1:37:10, 182.20s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [3:00:29<3:15:20, 225.40s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:33:28<3:59:22, 199.48s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [3:28:00<1:33:58, 181.89s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [3:04:14<3:11:22, 225.15s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:36:49<3:56:27, 199.83s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [3:30:59<1:30:35, 181.20s/it]                                                                                     Evaluating commonsenseqa :  71%|███████   | 71/100 [3:34:02<1:27:42, 181.47s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:40:07<3:52:23, 199.19s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [3:08:05<3:09:03, 226.87s/it]                                                                                     Evaluating commonsenseqa :  72%|███████▏  | 72/100 [3:37:05<1:24:59, 182.12s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [1:43:25<3:48:34, 198.75s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [3:11:51<3:05:04, 226.62s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [3:40:07<1:21:58, 182.18s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [3:13:51<2:35:48, 194.77s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [1:46:46<3:46:05, 199.50s/it]                                                                                     Evaluating commonsenseqa :  74%|███████▍  | 74/100 [3:43:09<1:18:53, 182.07s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [3:17:39<2:40:21, 204.70s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [1:50:08<3:43:48, 200.42s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [3:46:16<1:16:26, 183.44s/it]                                                                                     Evaluating commonsenseqa :  34%|███▍      | 34/100 [1:53:28<3:40:02, 200.04s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [3:21:31<2:43:12, 212.88s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [3:49:18<1:13:11, 183.00s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:56:51<3:37:52, 201.11s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [3:25:16<2:42:26, 216.59s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [3:52:19<1:09:53, 182.33s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [2:00:08<3:33:10, 199.86s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [3:29:02<2:40:49, 219.32s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [3:55:21<1:06:52, 182.40s/it]                                                                                     Evaluating commonsenseqa :  37%|███▋      | 37/100 [2:03:28<3:29:52, 199.88s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [3:58:25<1:03:56, 182.70s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [3:32:49<2:38:52, 221.69s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [2:06:56<3:28:56, 202.20s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [4:01:26<1:00:46, 182.32s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [3:36:35<2:36:05, 222.99s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [2:10:17<3:25:12, 201.84s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [4:04:27<57:33, 181.76s/it]                                                                                         Evaluating commonsenseqa :  59%|█████▉    | 59/100 [3:39:59<2:28:27, 217.25s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [4:07:29<54:35, 181.96s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [2:13:38<3:21:42, 201.72s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [3:43:46<2:26:43, 220.08s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [4:10:31<51:32, 181.90s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [2:16:55<3:16:53, 200.22s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [4:13:36<48:47, 182.97s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [3:47:36<2:24:59, 223.05s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [2:20:15<3:13:38, 200.32s/it]                                                                                       Evaluating commonsenseqa :  85%|████████▌ | 85/100 [4:16:42<45:57, 183.85s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [2:23:08<3:02:29, 192.10s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [3:50:53<2:16:15, 215.15s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [4:19:38<42:21, 181.56s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [2:26:29<3:01:39, 194.63s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [3:54:40<2:15:02, 218.99s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [4:22:40<39:22, 181.73s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [2:29:53<3:01:05, 197.56s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [3:58:30<2:13:22, 222.30s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [4:25:39<36:10, 180.91s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [2:33:14<2:58:43, 198.58s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [4:02:19<2:10:46, 224.19s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [4:28:44<33:21, 181.95s/it]                                                                                       Evaluating commonsenseqa :  47%|████▋     | 47/100 [2:36:37<2:56:37, 199.96s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [4:31:44<30:13, 181.37s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [4:06:04<2:07:10, 224.42s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [2:40:00<2:54:07, 200.91s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [4:34:48<27:19, 182.18s/it]                                                                                       Evaluating commonsenseqa :  67%|██████▋   | 67/100 [4:09:55<2:04:33, 226.46s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [2:43:22<2:50:53, 201.06s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [4:37:49<24:15, 181.92s/it]                                                                                       Evaluating commonsenseqa :  68%|██████▊   | 68/100 [4:13:45<2:01:21, 227.54s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [2:46:44<2:47:55, 201.51s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [4:40:51<21:12, 181.79s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [4:17:25<1:56:21, 225.23s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [4:43:53<18:10, 181.80s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [2:50:05<2:44:17, 201.17s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [4:46:52<15:05, 181.13s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [4:21:00<1:51:06, 222.22s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [2:53:29<2:41:45, 202.20s/it]                                                                                         Evaluating commonsenseqa :  96%|█████████▌| 96/100 [4:49:56<12:07, 181.91s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [2:56:53<2:38:45, 202.67s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [4:24:41<1:47:07, 221.64s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [4:52:58<09:05, 181.99s/it]                                                                                         Evaluating commonsenseqa :  54%|█████▍    | 54/100 [3:00:15<2:35:14, 202.50s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [4:28:30<1:44:28, 223.87s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [4:55:44<05:54, 177.11s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [3:03:36<2:31:25, 201.90s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [4:32:19<1:41:24, 225.36s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [4:58:46<02:58, 178.75s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [3:06:58<2:28:13, 202.12s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [5:01:48<00:00, 179.74s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [5:01:48<00:00, 181.09s/it]
name: commonsenseqa | avg. gen lenth: 229.476 | time: 18110.65595817566s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13646 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 13 13 15 True False 4096 10
[2023-09-11 03:18:57,451] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 13
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tmathqa-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 382947.94it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.97s/it]
 > number of parameters: 6738415616
[2023-09-11 03:19:10,996] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-11 03:19:11,215] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-11 03:19:11,217] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-11 03:19:11,217] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 03:19:11,217] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 03:19:11,217] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-11 03:19:11,217] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3be280>
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-11 03:19:11,218] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 03:19:11,219] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-11 03:19:11,219] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: a man cycling along the road noticed that every 18 minutes a bus overtakes him and every 6 minutes he meets an oncoming bus. if all buses and the cyclist move at a constant speed, what is the time interval between consecutive buses? a ) 5 minutes, b ) 6 minutes, c ) 8 minutes, d ) 9 minutes, e ) 10 minutes
Output: let's say the distance between the buses is d. we want to determine interval = \ frac { d } { b }, where b is the speed of bus. let the speed of cyclist be c. every 18 minutes a bus overtakes cyclist : \ frac { d } { b - c } = 18, d = 18 b - 18 c ; every 6 minutes cyclist meets an oncoming bus : \ frac { d } { b + c } = 6, d = 6 b + 6 c ; d = 18 b - 18 c = 6 b + 6 c, - - > b = 2 c, - - > d = 18 b - 9 b = 9 b. interval = \ frac { d } { b } = \ frac { 9 b } { b } = 9 answer : d ( 9 minutes ).
So the final answer is d

Input: an athlete runs 200 metres race in 24 seconds. what is his speed? a ) 20 km / hr, b ) 30 km / hr, c ) 25 km / hr, d ) 35 km / hr, e ) 40 km / hr
Output: speed = dist / time = 200 / 24 200 / 24 * 18 / 5 = 40 * 3 / 4 = 30 km / hr answer b
So the final answer is b

Input: two numbers are in the ratio 3 : 4. if the sum of numbers is 63, find the numbers. a ) 27 and 36., b ) 25 and 30., c ) 23 and 44., d ) 63 and 12., e ) 12 and 36.
Output: "sum of the terms of the ratio = 3 + 4 = 7 sum of numbers = 63 therefore, first number = 3 / 7 × 63 = 27 second number = 4 / 7 × 63 = 36 therefore, the two numbers are 27 and 36. answer is a"
So the final answer is a

Input: a company wants to spend equal amounts of money for the purchase of two types of computer printers costing $ 375 and $ 150 per unit, respectively. what is the fewest number of computer printers that the company can purchase? a ) 3, b ) 4, c ) 5, d ) 6, e ) 7
Output: the smallest amount that the company can spend is the lcm of 375 and 150, which is 750 for each, which is total 1500. the number of 1 st type of computers which costing $ 375 = 750 / 375 = 2. the number of 2 nd type of computers which costing $ 150 = 750 / 150 = 5. total = 2 + 5 = 7 answer is e.
So the final answer is e

Input: a certain music store stocks 800 cellos and 600 violas. of these instruments, there are 80 cello - viola pairs, such that a cello and a viola were both made with wood from the same tree ( each tree can make at most one viola and one cello, so there are no pairs other than these 90 ). if one viola and one cello are chosen at random, what is the probability that the two instruments are made with wood from the same tree? a ) 3 / 16,000, b ) 1 / 8,100, c ) 1 / 600, d ) 1 / 90, e ) 2 / 45
Output: "solution provided by stanford 2012 is correct : 80 / 800 choosing one of the cellos which has a pair viola, 1 / 600 choosing the viola which is the pair of chosen cello - - > p = 80 / 800 * 1 / 600 = 1 / 6,000. answer : c."
So the final answer is c

Input: what is the difference between local value & face value of 7 in the numeral 65793? a ) 693, b ) 656, c ) 691, d ) 9890, e ) 10000
Output: ( local value of 7 ) - ( face value of 7 ) = ( 700 - 7 ) = 693 a
So the final answer is a

Input: in an office in singapore there are 60 % female employees. 50 % of all the male employees are computer literate. if there are total 62 % employees computer literate out of total 1100 employees, then the no. of female employees who are computer literate? a ) 462, b ) 674, c ) 672, d ) 960, e ) none
Output: "solution : total employees, = 1100 female employees, 60 % of 1100. = ( 60 * 1100 ) / 100 = 660. then male employees, = 440 50 % of male are computer literate, = 220 male computer literate. 62 % of total employees are computer literate, = ( 62 * 1100 ) / 100 = 682 computer literate. thus, female computer literate = 682 - 220 = 462. answer : option a"
So the final answer is a

Input: the cost price of 16 articles is the same as the selling price of 12 articles. find the loss / profit percentages. a ) 30 %, b ) 32.50 %, c ) 33 1 / 3 %, d ) 40 %, e ) 50 %
Output: "explanation : the gain is 4 out of 12 articles. therefore, gain percentage = 4 x 100 / 12 = 100 / 3 = 33 1 / 3 % answer : option c"
So the final answer is c

Input: mike drives his new corvette from san francisco to las vegas, a journey of 640 miles. he drives the first half of the trip at an average rate of 80 miles per hour, but has to slow down for the second half of his journey. if the second half of the trip takes him 200 percent longer than the first half, what is his average rate q in miles per hour for the entire trip? a ) q = 26.7, b ) q = 30.0, c ) q = 40.0, d ) q = 53.3, e ) q = 60.0
Output: "veritas prepofficial solution correct answer : c using the formula : time = distance / rate, we find that mike takes 4 hours to cover the first 320 miles of his trip. since the 2 nd 320 miles take 200 % longer than the first, it takes mike 8 hours longer, or 12 hours. ( note : 200 % longer than the first half is not 200 % of the first half. ) the overall time is 4 hours + 12 hours or 16 hours. since the definition of average rate = total distance traveled / total time of travel, mike's average rate = 640 / 16 or 40 miles per hour. answer choice c is correct."
So the final answer is c

Input: lisa and robert have taken the same number of photos on their school trip. lisa has taken 3 times as many photos as claire and robert has taken 20 more photos than claire. how many photos has claire taken? a ) 6, b ) 8, c ) 10, d ) 12, e ) 14
Output: "l = r l = 3 c r = c + 20 3 c = c + 20 c = 10 the answer is c."
So the final answer is c

Input: meena wrote all the numbers from 1 to 69,999 inclusive. how many digits did she write in total? a ) 278,889, b ) 308,889, c ) 338,889, d ) 368,889, e ) 398,889
Output: "1 - 9 = > 1 * 9 digits 10 - 99 = > 2 * 90 = 180 ( numbers between 10 - 99 is 90 where each has 2 digits ) 100 - 999 = > 3 * 900 = 2700 1000 - 9999 = > 4 * 9000 = 36,000 10000 - 69999 = > 5 * 60,000 = 300,000 the answer is 338,889 the answer is c."
So the final answer is c

Input: a pipe takes a hours to fill the tank. but because of a leakage it took 2 times of its original time. find the time taken by the leakage to empty the tank a ) 50 min, b ) 60 min, c ) 90 min, d ) 80 min, e ) 120 min
Output: "pipe a can do a work 60 min. lets leakage time is x ; then 1 / 60 - 1 / x = 1 / 120 x = 120 min answer : e"
So the final answer is e

Input: if x is an integer such that 0 < x < 7, 0 < x < 15, 5 > x > – 1, 3 > x > 0, and x + 2 < 4, then x is a ) 1, b ) 2, c ) 3, d ) 4, e ) 5
Output: "0 < x < 7, 0 < x < 15, 5 > x > – 1 3 > x > 0 x < 2 from above : 0 < x < 2 - - > x = 1. answer : a."
So the final answer is a

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tmathqa-s20-rTrue-m4096
Evaluating commonsenseqa :  74%|███████▍  | 74/100 [4:36:07<1:38:01, 226.19s/it]                                                                                         Evaluating commonsenseqa :   1%|          | 1/100 [01:56<3:12:34, 116.71s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [3:10:18<2:24:20, 201.40s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [03:51<3:08:22, 115.33s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [4:39:54<1:34:23, 226.52s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [3:13:37<2:20:31, 200.75s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [05:46<3:06:45, 115.52s/it]         Evaluating commonsenseqa :   4%|▍         | 4/100 [07:39<3:03:04, 114.42s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [4:43:40<1:30:36, 226.51s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [3:16:58<2:17:08, 200.69s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [09:34<3:01:10, 114.43s/it]                                                                                         Evaluating commonsenseqa :   6%|▌         | 6/100 [11:27<2:59:00, 114.26s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [4:47:24<1:26:30, 225.68s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [3:20:25<2:15:03, 202.59s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [13:21<2:56:36, 113.94s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [15:15<2:54:47, 114.00s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [4:51:14<1:23:14, 227.04s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [3:23:48<2:11:49, 202.82s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [17:09<2:52:51, 113.97s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [3:27:11<2:08:20, 202.65s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [4:54:55<1:18:44, 224.97s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [19:02<2:50:41, 113.79s/it]                                                                                         Evaluating commonsenseqa :  11%|█         | 11/100 [20:56<2:48:40, 113.72s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [3:30:29<2:04:12, 201.41s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [4:58:42<1:15:12, 225.64s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [22:51<2:47:16, 114.05s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [24:46<2:46:03, 114.52s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [3:33:52<2:01:07, 201.86s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [5:02:26<1:11:19, 225.23s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [26:40<2:43:48, 114.29s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [28:35<2:42:09, 114.47s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [3:37:19<1:58:35, 203.31s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [5:06:13<1:07:42, 225.71s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [30:29<2:40:16, 114.48s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [32:23<2:38:00, 114.22s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [3:40:43<1:55:25, 203.70s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [5:08:38<57:05, 201.52s/it]                                                                                             Evaluating commonsenseqa :  18%|█▊        | 18/100 [34:17<2:35:58, 114.13s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [3:44:07<1:52:07, 203.85s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [36:11<2:34:13, 114.25s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [5:12:22<55:31, 208.19s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [38:05<2:31:56, 113.95s/it]                                                                                           Evaluating commonsenseqa :  68%|██████▊   | 68/100 [3:47:29<1:48:19, 203.10s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [39:58<2:29:42, 113.70s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [5:16:07<53:20, 213.35s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [41:53<2:28:15, 114.04s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [3:50:51<1:44:51, 202.94s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [3:28:58<4:22:10, 280.90s/it]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Evaluating commonsenseqa :  45%|████▌     | 45/100 [3:33:51<4:20:51, 284.57s/it]                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Evaluating commonsenseqa :  46%|████▌     | 46/100 [3:38:45<4:18:37, 287.36s/it]                                                                                                                                                                                                                                                                                                                                                                            Evaluating commonsenseqa :  47%|████▋     | 47/100 [3:43:36<4:14:52, 288.53s/it]                                                                                                                                                                                                                                                                                                                                                                                Evaluating commonsenseqa :  48%|████▊     | 48/100 [3:48:28<4:10:58, 289.59s/it]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Evaluating commonsenseqa :  49%|████▉     | 49/100 [3:53:26<4:08:07, 291.92s/it]                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Evaluating commonsenseqa :  50%|█████     | 50/100 [3:58:21<4:04:00, 292.81s/it]                                                                                                                                                                                                                                                                                                                                                                                            Evaluating commonsenseqa :  51%|█████     | 51/100 [4:03:09<3:58:00, 291.45s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [1:18:09<1:52:25, 114.32s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [5:54:18<18:58, 227.78s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [4:28:10<1:07:28, 202.43s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:20:05<1:50:48, 114.62s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:22:00<1:49:10, 114.92s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [5:58:04<15:09, 227.35s/it]                                                                                             Evaluating commonsenseqa :  81%|████████  | 81/100 [4:31:38<1:04:36, 204.02s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [5:59:22<09:07, 182.54s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:23:53<1:46:41, 114.32s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:25:46<1:44:20, 113.83s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [4:35:02<1:01:10, 203.91s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [6:03:13<06:33, 196.89s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [1:27:40<1:42:36, 114.00s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:29:34<1:40:46, 114.08s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [4:38:28<57:57, 204.59s/it]  Evaluating commonsenseqa :  99%|█████████▉| 99/100 [6:07:04<03:27, 207.15s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:31:30<1:39:10, 114.43s/it]                                                                                             Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:33:25<1:37:22, 114.55s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [4:41:49<54:19, 203.72s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [6:10:50<00:00, 212.83s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [6:10:50<00:00, 222.50s/it]
name: commonsenseqa | avg. gen lenth: 227.389 | time: 22251.775867700577s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13645 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 11 9 11 True False 4096 10
[2023-09-11 04:54:14,336] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s10-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 11
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o11-tmathqa-s10-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 665581.42it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.06s/it]
 > number of parameters: 6738415616
[2023-09-11 04:54:27,885] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-11 04:54:28,092] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-11 04:54:28,093] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0280>
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 04:54:28,094] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 04:54:28,095] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-11 04:54:28,095] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: in a recent head - to - head run - off election, 12000 absentee ballets were cast. 1 / 6 of the absentee ballets were thrown out and 3 / 5 of the remaining absentee ballets were cast for candidate a. how many absentee votes did candidate b receive? a ) 2,000, b ) 3,000, c ) 4,000, d ) 8,000, e ) 9,000
Output: c

Input: the total age of a and b is 11 years more than the total age of b and c. c is how many years younger than a.? a ) 16, b ) 12, c ) 11, d ) 20, e ) 10
Output: c

Input: the scoring system in a certain football competition goes as follows : 3 points for victory, 1 point for a draw, and 0 points for defeat. each team plays 20 matches. if a team scored 14 points after 5 games, what is the least number of the remaining matches it has to win to reach the 40 - point mark by the end of the tournament? a ) 6, b ) 7, c ) 8, d ) 9, e ) 10
Output: a

Input: how much water should be added to 10 liters of a 20 % - solution of alcohol to reduce the concentration of alcohol in the solution by 75 %? a ) 25 liters, b ) 27 liters, c ) 30 liters, d ) 32 liters, e ) 35 liters
Output: c

Input: a goods train runs at the speed of 72 kmph and crosses a 250 m long platform in 36 seconds. what is the length of the goods train? a ) 470 m, b ) 240 m, c ) 260 m, d ) 270 m, e ) none of these
Output: a

Input: the volume of a cube is 1728 cc. find its surface. a ) 864, b ) 556, c ) 255, d ) 287, e ) 267
Output: a

Input: a certain drink of type a is prepared by mixing 4 parts milk with 3 parts fruit juice. another drink of type b is prepared by mixing 4 parts of fruit juice and 3 parts of milk. how many liters of fruit juice must be added to 63 liters of drink a to convert it to drink b? a ) 7, b ) 14, c ) 21, d ) 28, e ) 35
Output: c

Input: last week john spent 10 percent of his wages on recreation. this week, his wages are 10 percent less than last week ʼ s wages and he spent 40 percent of his wages on recreation. the amount he spends on recreation this week is what percent of the amount he spent on recreation last week a ) 180 %, b ) 360 %, c ) 200 %, d ) 220 %, e ) 250 %
Output: b

Input: the salary of a person was reduced by 15 %. by what percent should his reduced salary be raised so as to bring it at par with his original salary? a ) 10 %, b ) 17.6 %, c ) 13.7 %, d ) 15.1 %, e ) 12.3 %
Output: b

Input: if 20 men can build a water fountain 56 metres long in 7 days, what length of a similar water fountain can be built by 35 men in 3 days? a ) 40 m, b ) 42 m, c ) 47 m, d ) 49 m, e ) 50 m
Output: b

Input: 12 men complete a work in 9 days. after they have worked for 6 days, 6 more men join them. how many days will they take to complete the remaining work? a ) 2 days, b ) 7 days, c ) 8 days, d ) 9 days, e ) 45 days
Output: a

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o11-tmathqa-s10-rFalse-m4096
Evaluating commonsenseqa :  50%|█████     | 50/100 [1:35:19<1:35:21, 114.43s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [4:45:16<51:08, 204.54s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [1:37:12<1:33:07, 114.03s/it]Evaluating commonsenseqa :   1%|          | 1/100 [03:29<5:45:06, 209.16s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:39:08<1:31:42, 114.64s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [4:48:38<47:31, 203.68s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:41:03<1:29:56, 114.82s/it]                                                                                             Evaluating commonsenseqa :   2%|▏         | 2/100 [07:00<5:43:29, 210.31s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:42:59<1:28:16, 115.15s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [4:52:00<44:03, 203.33s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:44:54<1:26:22, 115.16s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [10:35<5:43:21, 212.38s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:46:48<1:24:14, 114.89s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [4:55:22<40:34, 202.89s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:48:44<1:22:28, 115.08s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [14:11<5:42:31, 214.08s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [4:58:43<37:06, 202.38s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:50:38<1:20:19, 114.75s/it]                                                                                             Evaluating commonsenseqa :  59%|█████▉    | 59/100 [1:52:34<1:18:40, 115.14s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [17:46<5:39:13, 214.24s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [5:02:05<33:42, 202.22s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [1:54:28<1:16:28, 114.71s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [1:56:22<1:14:34, 114.72s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [21:20<5:35:25, 214.10s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [5:05:28<30:21, 202.42s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [1:58:21<1:13:18, 115.75s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [24:54<5:32:03, 214.23s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [2:00:15<1:11:09, 115.39s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [5:08:50<26:59, 202.47s/it]                                                                                             Evaluating commonsenseqa :  64%|██████▍   | 64/100 [2:02:11<1:09:20, 115.58s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [28:26<5:27:29, 213.59s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [5:12:19<23:50, 204.41s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [2:04:06<1:07:19, 115.42s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [2:06:01<1:05:18, 115.24s/it]                                                                                             Evaluating commonsenseqa :   9%|▉         | 9/100 [31:59<5:23:27, 213.27s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [5:15:38<20:15, 202.61s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [2:07:55<1:03:13, 114.97s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [2:09:50<1:01:17, 114.93s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [35:27<5:17:43, 211.81s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [5:19:02<16:55, 203.05s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [2:11:45<59:24, 115.00s/it]  Evaluating commonsenseqa :  70%|███████   | 70/100 [2:13:38<57:12, 114.41s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [5:22:24<13:31, 202.87s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [39:00<5:14:19, 211.90s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [2:15:32<55:14, 114.30s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [5:01:43<3:00:36, 292.89s/it]                                                                                                                                                                                                                                                                                                                                                                                    Evaluating commonsenseqa :  64%|██████▍   | 64/100 [5:06:38<2:56:04, 293.46s/it]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Evaluating commonsenseqa :  65%|██████▌   | 65/100 [5:11:30<2:50:58, 293.11s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [2:26:59<43:50, 114.37s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [5:35:52<00:00, 202.10s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [5:35:52<00:00, 201.52s/it]
name: commonsenseqa | avg. gen lenth: 348.451 | time: 20154.367483377457s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13644 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 5 5 7 True False 4096 10
[2023-09-11 05:46:56,278] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s10-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 5
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o5-tmathqa-s10-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 962534.81it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.92s/it]
 > number of parameters: 6738415616
[2023-09-11 05:47:09,696] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-11 05:47:09,906] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-11 05:47:09,908] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-11 05:47:09,908] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 05:47:09,908] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 05:47:09,908] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-11 05:47:09,908] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0280>
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-11 05:47:09,909] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 05:47:09,910] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-11 05:47:09,910] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: in a recent head - to - head run - off election, 12000 absentee ballets were cast. 1 / 6 of the absentee ballets were thrown out and 3 / 5 of the remaining absentee ballets were cast for candidate a. how many absentee votes did candidate b receive? a ) 2,000, b ) 3,000, c ) 4,000, d ) 8,000, e ) 9,000
Output: c

Input: the total age of a and b is 11 years more than the total age of b and c. c is how many years younger than a.? a ) 16, b ) 12, c ) 11, d ) 20, e ) 10
Output: c

Input: the scoring system in a certain football competition goes as follows : 3 points for victory, 1 point for a draw, and 0 points for defeat. each team plays 20 matches. if a team scored 14 points after 5 games, what is the least number of the remaining matches it has to win to reach the 40 - point mark by the end of the tournament? a ) 6, b ) 7, c ) 8, d ) 9, e ) 10
Output: a

Input: how much water should be added to 10 liters of a 20 % - solution of alcohol to reduce the concentration of alcohol in the solution by 75 %? a ) 25 liters, b ) 27 liters, c ) 30 liters, d ) 32 liters, e ) 35 liters
Output: c

Input: a goods train runs at the speed of 72 kmph and crosses a 250 m long platform in 36 seconds. what is the length of the goods train? a ) 470 m, b ) 240 m, c ) 260 m, d ) 270 m, e ) none of these
Output: a

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o5-tmathqa-s10-rFalse-m4096
Evaluating commonsenseqa :  15%|█▌        | 15/100 [53:10<5:01:22, 212.74s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [2:28:55<42:03, 114.70s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [55:14<4:20:17, 185.92s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [2:30:48<39:59, 114.28s/it]Evaluating commonsenseqa :   1%|          | 1/100 [04:23<7:14:04, 263.08s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [2:32:41<37:59, 114.00s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [57:27<3:54:57, 169.85s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [2:34:35<36:06, 114.01s/it]                                                                                               Evaluating commonsenseqa :  18%|█▊        | 18/100 [1:00:59<4:09:48, 182.78s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [2:36:27<33:59, 113.29s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [08:44<7:08:09, 262.14s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [2:38:21<32:07, 113.38s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [1:04:32<4:18:56, 191.81s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [2:40:14<30:13, 113.34s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [13:08<7:04:50, 262.78s/it]               Evaluating commonsenseqa :  85%|████████▌ | 85/100 [2:42:09<28:26, 113.74s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [1:08:04<4:23:51, 197.89s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [2:44:02<26:29, 113.51s/it]                                                                                               Evaluating commonsenseqa :   4%|▍         | 4/100 [17:33<7:01:57, 263.73s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [2:45:55<24:34, 113.39s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [1:11:35<4:25:34, 201.70s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [2:47:48<22:38, 113.23s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [2:49:42<20:51, 113.74s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [22:01<7:00:22, 265.50s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:15:03<4:24:38, 203.58s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [2:51:35<18:54, 113.43s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [2:53:28<16:59, 113.29s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:18:35<4:24:32, 206.14s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [26:20<6:52:21, 263.21s/it]                                                                                               Evaluating commonsenseqa :  92%|█████████▏| 92/100 [2:55:22<15:08, 113.50s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:21:54<4:18:15, 203.89s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [2:57:17<13:17, 113.91s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [30:49<6:50:54, 265.10s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [2:59:09<11:20, 113.44s/it]                                                                                                 Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:25:26<4:18:12, 206.57s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [3:01:03<09:27, 113.56s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [3:02:56<07:33, 113.49s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [35:12<6:45:25, 264.40s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:28:58<4:16:44, 208.17s/it]Evaluating commonsenseqa :  97%|█████████▋| 97/100 [3:04:49<05:39, 113.24s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [3:06:44<03:47, 113.66s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [39:39<6:42:03, 265.09s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:32:29<4:14:10, 208.91s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [3:08:36<01:53, 113.15s/it]                                                                                                 Evaluating commonsenseqa : 100%|██████████| 100/100 [3:10:32<00:00, 114.17s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [3:10:32<00:00, 114.33s/it]
name: commonsenseqa | avg. gen lenth: 401.666 | time: 11435.657434225082s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13646 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 15 13 15 True False 4096 10
[2023-09-11 06:29:56,136] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 15
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o15-tmathqa-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 356105.67it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.63s/it]
 > number of parameters: 6738415616
[2023-09-11 06:30:09,183] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-11 06:30:09,390] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-11 06:30:09,392] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3be1f0>
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-11 06:30:09,392] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-11 06:30:09,393] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-11 06:30:09,394] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-11 06:30:09,394] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-11 06:30:09,394] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-11 06:30:09,394] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-11 06:30:09,394] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 06:30:09,394] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-11 06:30:09,394] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: a man cycling along the road noticed that every 18 minutes a bus overtakes him and every 6 minutes he meets an oncoming bus. if all buses and the cyclist move at a constant speed, what is the time interval between consecutive buses? a ) 5 minutes, b ) 6 minutes, c ) 8 minutes, d ) 9 minutes, e ) 10 minutes
Output: let's say the distance between the buses is d. we want to determine interval = \ frac { d } { b }, where b is the speed of bus. let the speed of cyclist be c. every 18 minutes a bus overtakes cyclist : \ frac { d } { b - c } = 18, d = 18 b - 18 c ; every 6 minutes cyclist meets an oncoming bus : \ frac { d } { b + c } = 6, d = 6 b + 6 c ; d = 18 b - 18 c = 6 b + 6 c, - - > b = 2 c, - - > d = 18 b - 9 b = 9 b. interval = \ frac { d } { b } = \ frac { 9 b } { b } = 9 answer : d ( 9 minutes ).
So the final answer is d

Input: an athlete runs 200 metres race in 24 seconds. what is his speed? a ) 20 km / hr, b ) 30 km / hr, c ) 25 km / hr, d ) 35 km / hr, e ) 40 km / hr
Output: speed = dist / time = 200 / 24 200 / 24 * 18 / 5 = 40 * 3 / 4 = 30 km / hr answer b
So the final answer is b

Input: two numbers are in the ratio 3 : 4. if the sum of numbers is 63, find the numbers. a ) 27 and 36., b ) 25 and 30., c ) 23 and 44., d ) 63 and 12., e ) 12 and 36.
Output: "sum of the terms of the ratio = 3 + 4 = 7 sum of numbers = 63 therefore, first number = 3 / 7 × 63 = 27 second number = 4 / 7 × 63 = 36 therefore, the two numbers are 27 and 36. answer is a"
So the final answer is a

Input: a company wants to spend equal amounts of money for the purchase of two types of computer printers costing $ 375 and $ 150 per unit, respectively. what is the fewest number of computer printers that the company can purchase? a ) 3, b ) 4, c ) 5, d ) 6, e ) 7
Output: the smallest amount that the company can spend is the lcm of 375 and 150, which is 750 for each, which is total 1500. the number of 1 st type of computers which costing $ 375 = 750 / 375 = 2. the number of 2 nd type of computers which costing $ 150 = 750 / 150 = 5. total = 2 + 5 = 7 answer is e.
So the final answer is e

Input: a certain music store stocks 800 cellos and 600 violas. of these instruments, there are 80 cello - viola pairs, such that a cello and a viola were both made with wood from the same tree ( each tree can make at most one viola and one cello, so there are no pairs other than these 90 ). if one viola and one cello are chosen at random, what is the probability that the two instruments are made with wood from the same tree? a ) 3 / 16,000, b ) 1 / 8,100, c ) 1 / 600, d ) 1 / 90, e ) 2 / 45
Output: "solution provided by stanford 2012 is correct : 80 / 800 choosing one of the cellos which has a pair viola, 1 / 600 choosing the viola which is the pair of chosen cello - - > p = 80 / 800 * 1 / 600 = 1 / 6,000. answer : c."
So the final answer is c

Input: what is the difference between local value & face value of 7 in the numeral 65793? a ) 693, b ) 656, c ) 691, d ) 9890, e ) 10000
Output: ( local value of 7 ) - ( face value of 7 ) = ( 700 - 7 ) = 693 a
So the final answer is a

Input: in an office in singapore there are 60 % female employees. 50 % of all the male employees are computer literate. if there are total 62 % employees computer literate out of total 1100 employees, then the no. of female employees who are computer literate? a ) 462, b ) 674, c ) 672, d ) 960, e ) none
Output: "solution : total employees, = 1100 female employees, 60 % of 1100. = ( 60 * 1100 ) / 100 = 660. then male employees, = 440 50 % of male are computer literate, = 220 male computer literate. 62 % of total employees are computer literate, = ( 62 * 1100 ) / 100 = 682 computer literate. thus, female computer literate = 682 - 220 = 462. answer : option a"
So the final answer is a

Input: the cost price of 16 articles is the same as the selling price of 12 articles. find the loss / profit percentages. a ) 30 %, b ) 32.50 %, c ) 33 1 / 3 %, d ) 40 %, e ) 50 %
Output: "explanation : the gain is 4 out of 12 articles. therefore, gain percentage = 4 x 100 / 12 = 100 / 3 = 33 1 / 3 % answer : option c"
So the final answer is c

Input: mike drives his new corvette from san francisco to las vegas, a journey of 640 miles. he drives the first half of the trip at an average rate of 80 miles per hour, but has to slow down for the second half of his journey. if the second half of the trip takes him 200 percent longer than the first half, what is his average rate q in miles per hour for the entire trip? a ) q = 26.7, b ) q = 30.0, c ) q = 40.0, d ) q = 53.3, e ) q = 60.0
Output: "veritas prepofficial solution correct answer : c using the formula : time = distance / rate, we find that mike takes 4 hours to cover the first 320 miles of his trip. since the 2 nd 320 miles take 200 % longer than the first, it takes mike 8 hours longer, or 12 hours. ( note : 200 % longer than the first half is not 200 % of the first half. ) the overall time is 4 hours + 12 hours or 16 hours. since the definition of average rate = total distance traveled / total time of travel, mike's average rate = 640 / 16 or 40 miles per hour. answer choice c is correct."
So the final answer is c

Input: lisa and robert have taken the same number of photos on their school trip. lisa has taken 3 times as many photos as claire and robert has taken 20 more photos than claire. how many photos has claire taken? a ) 6, b ) 8, c ) 10, d ) 12, e ) 14
Output: "l = r l = 3 c r = c + 20 3 c = c + 20 c = 10 the answer is c."
So the final answer is c

Input: meena wrote all the numbers from 1 to 69,999 inclusive. how many digits did she write in total? a ) 278,889, b ) 308,889, c ) 338,889, d ) 368,889, e ) 398,889
Output: "1 - 9 = > 1 * 9 digits 10 - 99 = > 2 * 90 = 180 ( numbers between 10 - 99 is 90 where each has 2 digits ) 100 - 999 = > 3 * 900 = 2700 1000 - 9999 = > 4 * 9000 = 36,000 10000 - 69999 = > 5 * 60,000 = 300,000 the answer is 338,889 the answer is c."
So the final answer is c

Input: a pipe takes a hours to fill the tank. but because of a leakage it took 2 times of its original time. find the time taken by the leakage to empty the tank a ) 50 min, b ) 60 min, c ) 90 min, d ) 80 min, e ) 120 min
Output: "pipe a can do a work 60 min. lets leakage time is x ; then 1 / 60 - 1 / x = 1 / 120 x = 120 min answer : e"
So the final answer is e

Input: if x is an integer such that 0 < x < 7, 0 < x < 15, 5 > x > – 1, 3 > x > 0, and x + 2 < 4, then x is a ) 1, b ) 2, c ) 3, d ) 4, e ) 5
Output: "0 < x < 7, 0 < x < 15, 5 > x > – 1 3 > x > 0 x < 2 from above : 0 < x < 2 - - > x = 1. answer : a."
So the final answer is a

Input: a and b put in rs. 200 and rs. 400 respectively into a business. a reinvests into the business his share of the first year's profit of rs. 210 where as b does not. in what ratio should they divide the second year's profit? a ) 39 : 40, b ) 39 : 49, c ) 39 : 42, d ) 39 : 47, e ) 17 : 20
Output: "explanation : 1 : 2 a = 2 / 3 * 210 = 140 340 : 400 17 : 20 answer : e"
So the final answer is e

Input: what least number must be subtracted from 427398 so that remaining no. is divisible by 10 a ) 3, b ) 5, c ) 6, d ) 7, e ) 8
Output: "explanation : on dividing 427398 by 10 we get the remainder 8, so 8 should be subtracted answer : option e"
So the final answer is e

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o15-tmathqa-s20-rTrue-m4096
Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:36:03<4:12:30, 210.42s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [43:26<6:19:57, 253.31s/it]Evaluating commonsenseqa :   1%|          | 1/100 [01:46<2:55:24, 106.31s/it]                                                                                                 Evaluating commonsenseqa :   2%|▏         | 2/100 [03:30<2:51:20, 104.90s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:39:24<4:05:32, 207.50s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [47:45<6:18:26, 255.13s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [05:14<2:48:55, 104.49s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [06:58<2:47:00, 104.38s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:42:56<4:03:45, 208.93s/it]                                                                                                 Evaluating commonsenseqa :   5%|▌         | 5/100 [08:41<2:44:46, 104.07s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [52:07<6:17:07, 257.13s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [10:27<2:43:46, 104.54s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [1:46:28<4:01:17, 209.82s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [12:12<2:42:22, 104.76s/it]                                                                                                 Evaluating commonsenseqa :  13%|█▎        | 13/100 [56:24<6:13:10, 257.37s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [13:58<2:41:02, 105.03s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [1:50:04<3:59:53, 211.67s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [15:42<2:39:07, 104.92s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [17:27<2:37:19, 104.88s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [1:00:41<6:08:18, 256.96s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [1:53:34<3:55:56, 211.29s/it]                                                                                                 Evaluating commonsenseqa :  11%|█         | 11/100 [19:11<2:35:10, 104.61s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [20:55<2:33:16, 104.50s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [1:57:10<3:54:01, 212.76s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [1:05:01<6:05:21, 257.91s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [22:40<2:31:43, 104.64s/it]              Evaluating commonsenseqa :  14%|█▍        | 14/100 [24:25<2:29:51, 104.55s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [2:00:41<3:49:52, 212.19s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [1:08:14<5:34:04, 238.62s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [26:10<2:28:32, 104.86s/it]                                                                                                 Evaluating commonsenseqa :  16%|█▌        | 16/100 [27:55<2:26:49, 104.87s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [2:04:14<3:46:32, 212.39s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [1:12:32<5:38:06, 244.41s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [29:39<2:24:38, 104.56s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [31:24<2:23:03, 104.67s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [2:07:48<3:43:21, 212.72s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [33:09<2:21:18, 104.68s/it]              Evaluating commonsenseqa :  18%|█▊        | 18/100 [1:16:55<5:41:43, 250.04s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [34:54<2:19:37, 104.72s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [2:11:17<3:38:55, 211.86s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [36:38<2:17:44, 104.61s/it]                                                                                                   Evaluating commonsenseqa :  19%|█▉        | 19/100 [1:21:19<5:43:08, 254.18s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [38:22<2:15:58, 104.60s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [2:14:52<3:36:07, 212.58s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [40:08<2:14:34, 104.86s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [41:55<2:13:36, 105.49s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [1:25:15<5:31:19, 248.50s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [2:18:28<3:33:40, 213.67s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [43:40<2:11:52, 105.50s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [45:25<2:09:56, 105.35s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [2:22:03<3:30:24, 213.97s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [1:29:37<5:32:45, 252.73s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [47:09<2:07:29, 104.79s/it]              Evaluating commonsenseqa :  28%|██▊       | 28/100 [48:53<2:05:38, 104.70s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [2:25:39<3:27:27, 214.61s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [50:39<2:04:04, 104.85s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:33:59<5:32:08, 255.50s/it]                                                                                                   Evaluating commonsenseqa :  30%|███       | 30/100 [52:23<2:02:05, 104.66s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [2:29:14<3:23:59, 214.73s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [54:07<2:00:03, 104.40s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:38:18<5:29:21, 256.65s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [55:51<1:58:20, 104.41s/it]                                                                                                   Evaluating commonsenseqa :  44%|████▍     | 44/100 [2:32:47<3:19:56, 214.22s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [57:36<1:56:45, 104.55s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [59:20<1:54:51, 104.42s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:42:41<5:27:23, 258.47s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [2:36:22<3:16:34, 214.44s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:01:05<1:53:08, 104.44s/it]                                                                                                   Evaluating commonsenseqa :  36%|███▌      | 36/100 [1:02:50<1:51:44, 104.75s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:47:00<5:23:20, 258.68s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [2:40:00<3:13:58, 215.52s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [1:04:34<1:49:38, 104.42s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [1:06:18<1:47:50, 104.37s/it]                                                                                                   Evaluating commonsenseqa :  47%|████▋     | 47/100 [2:43:39<3:11:14, 216.50s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [1:08:04<1:46:38, 104.90s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:51:22<5:20:01, 259.48s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [1:09:48<1:44:31, 104.52s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [1:11:31<1:42:28, 104.21s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [2:47:15<3:07:42, 216.59s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:55:45<5:17:01, 260.57s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [1:13:16<1:40:51, 104.34s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [1:15:00<1:39:06, 104.32s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [2:50:45<3:02:13, 214.38s/it]                                                                                                 Evaluating commonsenseqa :  44%|████▍     | 44/100 [1:16:44<1:37:14, 104.18s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [2:00:05<5:12:38, 260.53s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [1:18:29<1:35:43, 104.42s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [2:54:14<2:57:21, 212.82s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [1:20:13<1:34:01, 104.47s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [2:04:21<5:06:26, 258.97s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [1:21:58<1:32:17, 104.49s/it]      Evaluating commonsenseqa :  51%|█████     | 51/100 [2:57:45<2:53:28, 212.43s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [1:23:42<1:30:31, 104.45s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [1:25:26<1:28:36, 104.25s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [3:01:19<2:50:08, 212.69s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [2:08:49<5:05:26, 261.80s/it]                                                                                                   Evaluating commonsenseqa :  50%|█████     | 50/100 [1:27:10<1:26:51, 104.23s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [1:28:57<1:25:36, 104.84s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [3:04:54<2:47:18, 213.59s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [2:13:09<5:00:31, 261.33s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [1:30:41<1:23:45, 104.71s/it]                                                                                                   Evaluating commonsenseqa :  53%|█████▎    | 53/100 [1:32:26<1:22:07, 104.84s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [3:08:27<2:43:32, 213.31s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [1:34:10<1:20:11, 104.60s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [2:17:25<4:54:17, 259.67s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [1:35:55<1:18:32, 104.73s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [3:11:58<2:39:25, 212.57s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [1:37:40<1:16:48, 104.74s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [2:21:43<4:49:34, 259.32s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [1:39:25<1:15:07, 104.81s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [3:15:32<2:36:15, 213.08s/it]                                                                                                   Evaluating commonsenseqa :  58%|█████▊    | 58/100 [1:41:09<1:13:16, 104.67s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [1:42:54<1:11:37, 104.81s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [2:26:07<4:46:40, 260.62s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [3:19:08<2:33:18, 213.91s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [1:44:38<1:09:41, 104.55s/it]                                                                                                   Evaluating commonsenseqa :  61%|██████    | 61/100 [1:46:23<1:08:01, 104.65s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [3:22:43<2:30:05, 214.42s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [2:30:25<4:41:33, 259.90s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [1:48:10<1:06:40, 105.28s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [1:49:53<1:04:33, 104.69s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [3:26:18<2:26:33, 214.48s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [1:51:39<1:02:57, 104.93s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [2:34:44<4:36:54, 259.61s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [1:53:22<1:00:58, 104.54s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [3:29:54<2:23:19, 214.98s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [1:55:07<59:10, 104.43s/it]  Evaluating commonsenseqa :  37%|███▋      | 37/100 [2:39:01<4:31:42, 258.78s/it]          Evaluating commonsenseqa :  67%|██████▋   | 67/100 [1:56:51<57:23, 104.34s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [3:33:28<2:19:28, 214.59s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [1:58:36<55:44, 104.52s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [2:43:17<4:26:28, 257.88s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [2:00:19<53:52, 104.28s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [3:37:03<2:16:05, 214.89s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [2:02:04<52:07, 104.23s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [2:03:47<50:19, 104.11s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [2:47:30<4:20:47, 256.52s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [3:40:39<2:12:33, 214.96s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [2:05:32<48:38, 104.22s/it]s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [8:01:49<00:00, 289.10s/it]
name: commonsenseqa | avg. gen lenth: 266.907 | time: 28910.99286699295s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu04 --master_port 13643 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 3 1 3 True False 4096 10
[2023-09-11 08:35:28,073] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 3
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o3-tmathqa-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 980271.01it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.47s/it]
 > number of parameters: 6738415616
[2023-09-11 08:35:38,390] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-11 08:35:38,597] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-11 08:35:38,599] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-11 08:35:38,599] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 08:35:38,599] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 08:35:38,599] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-11 08:35:38,599] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-11 08:35:38,599] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 08:35:38,599] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-11 08:35:38,599] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3be250>
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-11 08:35:38,600] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 08:35:38,601] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-11 08:35:38,601] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: in a recent head - to - head run - off election, 12000 absentee ballets were cast. 1 / 6 of the absentee ballets were thrown out and 3 / 5 of the remaining absentee ballets were cast for candidate a. how many absentee votes did candidate b receive? a ) 2,000, b ) 3,000, c ) 4,000, d ) 8,000, e ) 9,000
Output: 5 / 6 * 2 / 5 ( total absentee votes ) = 1 / 3 ( total votes ) = 1 / 3 * 12000 = 4000 answer is c
So the final answer is c

Input: the total age of a and b is 11 years more than the total age of b and c. c is how many years younger than a.? a ) 16, b ) 12, c ) 11, d ) 20, e ) 10
Output: "( a + b ) - ( b - c ) = 11 a - c = 11 answer is c"
So the final answer is c

Input: the scoring system in a certain football competition goes as follows : 3 points for victory, 1 point for a draw, and 0 points for defeat. each team plays 20 matches. if a team scored 14 points after 5 games, what is the least number of the remaining matches it has to win to reach the 40 - point mark by the end of the tournament? a ) 6, b ) 7, c ) 8, d ) 9, e ) 10
Output: "to get 40 points as end of season we need another 26 points or more from remaining 15 matches : option a = 6 * 3 + 9 * 1 = 27 hence option a - 6"
So the final answer is a

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o3-tmathqa-s10-rTrue-m4096
                                                                                                                                                                                                                                                                                                                                                                                      Evaluating commonsenseqa :   1%|          | 1/100 [04:21<7:11:12, 261.34s/it]                                                                                                                                                                                                                                                                                                                                                                                      Evaluating commonsenseqa :   2%|▏         | 2/100 [08:40<7:04:31, 259.91s/it]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Evaluating commonsenseqa :   3%|▎         | 3/100 [13:00<7:00:07, 259.87s/it]                                                                                                                                                                                                                                                                                                                                                                                        Evaluating commonsenseqa :  68%|██████▊   | 68/100 [3:58:36<1:54:46, 215.19s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [2:23:01<31:28, 104.91s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [2:24:47<29:45, 105.04s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [3:09:07<4:01:38, 258.89s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [4:02:10<1:51:00, 214.85s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [2:26:33<28:08, 105.53s/it]                                                                                Evaluating commonsenseqa :  85%|████████▌ | 85/100 [2:28:20<26:27, 105.85s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [4:05:41<1:46:49, 213.65s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [2:30:06<24:42, 105.86s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [3:13:29<3:58:15, 259.92s/it]                                                                                Evaluating commonsenseqa :  87%|████████▋ | 87/100 [2:31:51<22:53, 105.66s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [4:09:10<1:42:38, 212.35s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [2:33:36<21:03, 105.31s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [3:17:52<3:54:49, 260.92s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [2:35:21<19:19, 105.44s/it]                                                                                Evaluating commonsenseqa :  72%|███████▏  | 72/100 [4:12:39<1:38:32, 211.18s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [2:37:05<17:30, 105.02s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [3:21:34<3:40:01, 249.08s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [2:38:51<15:47, 105.27s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [4:16:11<1:35:10, 211.51s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [2:40:37<14:03, 105.43s/it]Evaluating commonsenseqa :  93%|█████████▎| 93/100 [2:42:22<12:17, 105.36s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [3:25:58<3:39:50, 253.66s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [4:18:45<1:24:13, 194.37s/it]Evaluating commonsenseqa :  94%|█████████▍| 94/100 [2:44:07<10:30, 105.14s/it]                                                                                Evaluating commonsenseqa :  95%|█████████▌| 95/100 [2:45:52<08:45, 105.18s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [4:22:20<1:23:27, 200.31s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [3:30:18<3:37:10, 255.50s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [2:47:38<07:01, 105.40s/it]                                                                                 Evaluating commonsenseqa :  97%|█████████▋| 97/100 [2:49:22<05:15, 105.12s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [4:25:52<1:21:37, 204.05s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [2:51:09<03:30, 105.44s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [3:34:35<3:33:26, 256.14s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [2:52:54<01:45, 105.36s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [4:29:29<1:19:41, 207.88s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:54:40<00:00, 105.52s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [2:54:40<00:00, 104.80s/it]
name: commonsenseqa | avg. gen lenth: 401.167 | time: 10483.167836666107s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13646 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s20-rFalse --seed 20 --max-prompt-length 4096 --num-out-domain 13 13 15 True False 4096 10
[2023-09-11 09:24:57,751] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s20-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 13
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tmathqa-s20-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 592152.05it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.44s/it]
 > number of parameters: 6738415616
[2023-09-11 09:25:12,130] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-11 09:25:12,334] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-11 09:25:12,335] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c0280>
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-11 09:25:12,336] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 09:25:12,337] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-11 09:25:12,338] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: a man cycling along the road noticed that every 18 minutes a bus overtakes him and every 6 minutes he meets an oncoming bus. if all buses and the cyclist move at a constant speed, what is the time interval between consecutive buses? a ) 5 minutes, b ) 6 minutes, c ) 8 minutes, d ) 9 minutes, e ) 10 minutes
Output: d

Input: an athlete runs 200 metres race in 24 seconds. what is his speed? a ) 20 km / hr, b ) 30 km / hr, c ) 25 km / hr, d ) 35 km / hr, e ) 40 km / hr
Output: b

Input: two numbers are in the ratio 3 : 4. if the sum of numbers is 63, find the numbers. a ) 27 and 36., b ) 25 and 30., c ) 23 and 44., d ) 63 and 12., e ) 12 and 36.
Output: a

Input: a company wants to spend equal amounts of money for the purchase of two types of computer printers costing $ 375 and $ 150 per unit, respectively. what is the fewest number of computer printers that the company can purchase? a ) 3, b ) 4, c ) 5, d ) 6, e ) 7
Output: e

Input: a certain music store stocks 800 cellos and 600 violas. of these instruments, there are 80 cello - viola pairs, such that a cello and a viola were both made with wood from the same tree ( each tree can make at most one viola and one cello, so there are no pairs other than these 90 ). if one viola and one cello are chosen at random, what is the probability that the two instruments are made with wood from the same tree? a ) 3 / 16,000, b ) 1 / 8,100, c ) 1 / 600, d ) 1 / 90, e ) 2 / 45
Output: c

Input: what is the difference between local value & face value of 7 in the numeral 65793? a ) 693, b ) 656, c ) 691, d ) 9890, e ) 10000
Output: a

Input: in an office in singapore there are 60 % female employees. 50 % of all the male employees are computer literate. if there are total 62 % employees computer literate out of total 1100 employees, then the no. of female employees who are computer literate? a ) 462, b ) 674, c ) 672, d ) 960, e ) none
Output: a

Input: the cost price of 16 articles is the same as the selling price of 12 articles. find the loss / profit percentages. a ) 30 %, b ) 32.50 %, c ) 33 1 / 3 %, d ) 40 %, e ) 50 %
Output: c

Input: mike drives his new corvette from san francisco to las vegas, a journey of 640 miles. he drives the first half of the trip at an average rate of 80 miles per hour, but has to slow down for the second half of his journey. if the second half of the trip takes him 200 percent longer than the first half, what is his average rate q in miles per hour for the entire trip? a ) q = 26.7, b ) q = 30.0, c ) q = 40.0, d ) q = 53.3, e ) q = 60.0
Output: c

Input: lisa and robert have taken the same number of photos on their school trip. lisa has taken 3 times as many photos as claire and robert has taken 20 more photos than claire. how many photos has claire taken? a ) 6, b ) 8, c ) 10, d ) 12, e ) 14
Output: c

Input: meena wrote all the numbers from 1 to 69,999 inclusive. how many digits did she write in total? a ) 278,889, b ) 308,889, c ) 338,889, d ) 368,889, e ) 398,889
Output: c

Input: a pipe takes a hours to fill the tank. but because of a leakage it took 2 times of its original time. find the time taken by the leakage to empty the tank a ) 50 min, b ) 60 min, c ) 90 min, d ) 80 min, e ) 120 min
Output: e

Input: if x is an integer such that 0 < x < 7, 0 < x < 15, 5 > x > – 1, 3 > x > 0, and x + 2 < 4, then x is a ) 1, b ) 2, c ) 3, d ) 4, e ) 5
Output: a

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tmathqa-s20-rFalse-m4096
Evaluating commonsenseqa :  51%|█████     | 51/100 [3:38:57<3:30:32, 257.81s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [4:33:07<1:17:19, 210.87s/it]Evaluating commonsenseqa :   1%|          | 1/100 [03:14<5:21:35, 194.90s/it]     Evaluating commonsenseqa :  52%|█████▏    | 52/100 [3:43:12<3:25:40, 257.09s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [4:36:45<1:14:30, 212.88s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [06:30<5:19:15, 195.47s/it]                                                                                   Evaluating commonsenseqa :  53%|█████▎    | 53/100 [3:47:34<3:22:27, 258.46s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [4:40:17<1:10:53, 212.69s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [09:44<5:15:00, 194.85s/it]                                                                                     Evaluating commonsenseqa :   4%|▍         | 4/100 [13:00<5:12:27, 195.28s/it]Evaluating commonsenseqa :  81%|████████  | 81/100 [4:43:55<1:07:50, 214.24s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [3:51:53<3:18:08, 258.44s/it]                                                                                     Evaluating commonsenseqa :   5%|▌         | 5/100 [16:14<5:08:23, 194.78s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [4:47:25<1:03:56, 213.15s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [3:56:08<3:13:12, 257.60s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [18:48<4:43:18, 180.84s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [4:50:57<1:00:16, 212.72s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [22:04<4:48:05, 185.86s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [4:00:30<3:09:46, 258.79s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [4:54:28<56:32, 212.06s/it]                                                                                       Evaluating commonsenseqa :   8%|▊         | 8/100 [25:19<4:49:31, 188.82s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [4:04:46<3:04:51, 257.94s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [4:58:02<53:13, 212.89s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [28:31<4:47:35, 189.62s/it]                                                                                     Evaluating commonsenseqa :  86%|████████▌ | 86/100 [5:01:35<49:41, 212.94s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [4:09:06<3:01:03, 258.66s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [31:45<4:46:47, 191.20s/it]                                                                                     Evaluating commonsenseqa :  11%|█         | 11/100 [34:10<4:22:14, 176.79s/it]Evaluating commonsenseqa :  87%|████████▋ | 87/100 [5:05:11<46:17, 213.62s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [4:13:28<2:57:26, 259.67s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [37:26<4:28:08, 182.82s/it]Evaluating commonsenseqa :  88%|████████▊ | 88/100 [5:08:45<42:44, 213.73s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [4:17:50<2:53:36, 260.41s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [40:20<4:21:06, 180.07s/it]Evaluating commonsenseqa :  89%|████████▉ | 89/100 [5:12:18<39:08, 213.54s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [42:18<3:51:08, 161.26s/it]  Evaluating commonsenseqa :  61%|██████    | 61/100 [4:22:12<2:49:37, 260.97s/it]Evaluating commonsenseqa :  90%|█████████ | 90/100 [5:15:52<35:38, 213.81s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [45:28<4:00:47, 169.97s/it]                                                                                       Evaluating commonsenseqa :  62%|██████▏   | 62/100 [4:26:42<2:46:50, 263.43s/it]Evaluating commonsenseqa :  91%|█████████ | 91/100 [5:19:26<32:03, 213.69s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [48:41<4:07:51, 177.05s/it]                                                                                       Evaluating commonsenseqa :  17%|█▋        | 17/100 [51:58<4:13:13, 183.06s/it]Evaluating commonsenseqa :  92%|█████████▏| 92/100 [5:22:57<28:24, 213.12s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [4:31:04<2:42:13, 263.07s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [54:51<4:05:55, 179.95s/it]    Evaluating commonsenseqa :  93%|█████████▎| 93/100 [5:26:30<24:50, 213.00s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [4:35:27<2:37:45, 262.94s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [58:03<4:07:42, 183.48s/it]                                                                                       Evaluating commonsenseqa :  94%|█████████▍| 94/100 [5:30:03<21:18, 213.10s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [1:01:15<4:07:58, 185.98s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [4:39:52<2:33:45, 263.59s/it]Evaluating commonsenseqa :  95%|█████████▌| 95/100 [5:33:42<17:53, 214.69s/it]                                                                                       Evaluating commonsenseqa :  21%|██        | 21/100 [1:04:32<4:09:15, 189.31s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [4:44:11<2:28:41, 262.40s/it]Evaluating commonsenseqa :  96%|█████████▌| 96/100 [5:37:16<14:18, 214.62s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:07:12<3:54:56, 180.72s/it]                                                                                       Evaluating commonsenseqa :  97%|█████████▋| 97/100 [5:40:44<10:37, 212.61s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:10:26<3:56:53, 184.59s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [4:48:37<2:24:49, 263.32s/it]                                                                                       Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:13:36<3:55:45, 186.12s/it]Evaluating commonsenseqa :  98%|█████████▊| 98/100 [5:44:20<07:07, 213.68s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [4:53:00<2:20:25, 263.30s/it]                                                                                       Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:16:53<3:56:46, 189.42s/it]Evaluating commonsenseqa :  99%|█████████▉| 99/100 [5:47:56<03:34, 214.28s/it]Evaluating commonsenseqa :  69%|██████▉   | 69/100 [4:57:11<2:14:04, 259.51s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:20:06<3:54:57, 190.51s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [5:51:32<00:00, 214.91s/it]Evaluating commonsenseqa : 100%|██████████| 100/100 [5:51:32<00:00, 210.93s/it]
name: commonsenseqa | avg. gen lenth: 239.264 | time: 21094.55234646797s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13645 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 9 9 11 True False 4096 10
Evaluating commonsenseqa :  30%|███       | 30/100 [2:10:29<5:03:37, 260.26s/it]ccelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 9
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o9-tmathqa-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 467115.40it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.18s/it]
 > number of parameters: 6738415616
[2023-09-11 10:46:25,911] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-11 10:46:26,121] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-11 10:46:26,123] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-11 10:46:26,123] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-11 10:46:26,123] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-11 10:46:26,123] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-11 10:46:26,123] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3be280>
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-11 10:46:26,124] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-11 10:46:26,125] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-11 10:46:26,125] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: a man cycling along the road noticed that every 18 minutes a bus overtakes him and every 6 minutes he meets an oncoming bus. if all buses and the cyclist move at a constant speed, what is the time interval between consecutive buses? a ) 5 minutes, b ) 6 minutes, c ) 8 minutes, d ) 9 minutes, e ) 10 minutes
Output: let's say the distance between the buses is d. we want to determine interval = \ frac { d } { b }, where b is the speed of bus. let the speed of cyclist be c. every 18 minutes a bus overtakes cyclist : \ frac { d } { b - c } = 18, d = 18 b - 18 c ; every 6 minutes cyclist meets an oncoming bus : \ frac { d } { b + c } = 6, d = 6 b + 6 c ; d = 18 b - 18 c = 6 b + 6 c, - - > b = 2 c, - - > d = 18 b - 9 b = 9 b. interval = \ frac { d } { b } = \ frac { 9 b } { b } = 9 answer : d ( 9 minutes ).
So the final answer is d

Input: an athlete runs 200 metres race in 24 seconds. what is his speed? a ) 20 km / hr, b ) 30 km / hr, c ) 25 km / hr, d ) 35 km / hr, e ) 40 km / hr
Output: speed = dist / time = 200 / 24 200 / 24 * 18 / 5 = 40 * 3 / 4 = 30 km / hr answer b
So the final answer is b

Input: two numbers are in the ratio 3 : 4. if the sum of numbers is 63, find the numbers. a ) 27 and 36., b ) 25 and 30., c ) 23 and 44., d ) 63 and 12., e ) 12 and 36.
Output: "sum of the terms of the ratio = 3 + 4 = 7 sum of numbers = 63 therefore, first number = 3 / 7 × 63 = 27 second number = 4 / 7 × 63 = 36 therefore, the two numbers are 27 and 36. answer is a"
So the final answer is a

Input: a company wants to spend equal amounts of money for the purchase of two types of computer printers costing $ 375 and $ 150 per unit, respectively. what is the fewest number of computer printers that the company can purchase? a ) 3, b ) 4, c ) 5, d ) 6, e ) 7
Output: the smallest amount that the company can spend is the lcm of 375 and 150, which is 750 for each, which is total 1500. the number of 1 st type of computers which costing $ 375 = 750 / 375 = 2. the number of 2 nd type of computers which costing $ 150 = 750 / 150 = 5. total = 2 + 5 = 7 answer is e.
So the final answer is e

Input: a certain music store stocks 800 cellos and 600 violas. of these instruments, there are 80 cello - viola pairs, such that a cello and a viola were both made with wood from the same tree ( each tree can make at most one viola and one cello, so there are no pairs other than these 90 ). if one viola and one cello are chosen at random, what is the probability that the two instruments are made with wood from the same tree? a ) 3 / 16,000, b ) 1 / 8,100, c ) 1 / 600, d ) 1 / 90, e ) 2 / 45
Output: "solution provided by stanford 2012 is correct : 80 / 800 choosing one of the cellos which has a pair viola, 1 / 600 choosing the viola which is the pair of chosen cello - - > p = 80 / 800 * 1 / 600 = 1 / 6,000. answer : c."
So the final answer is c

Input: what is the difference between local value & face value of 7 in the numeral 65793? a ) 693, b ) 656, c ) 691, d ) 9890, e ) 10000
Output: ( local value of 7 ) - ( face value of 7 ) = ( 700 - 7 ) = 693 a
So the final answer is a

Input: in an office in singapore there are 60 % female employees. 50 % of all the male employees are computer literate. if there are total 62 % employees computer literate out of total 1100 employees, then the no. of female employees who are computer literate? a ) 462, b ) 674, c ) 672, d ) 960, e ) none
Output: "solution : total employees, = 1100 female employees, 60 % of 1100. = ( 60 * 1100 ) / 100 = 660. then male employees, = 440 50 % of male are computer literate, = 220 male computer literate. 62 % of total employees are computer literate, = ( 62 * 1100 ) / 100 = 682 computer literate. thus, female computer literate = 682 - 220 = 462. answer : option a"
So the final answer is a

Input: the cost price of 16 articles is the same as the selling price of 12 articles. find the loss / profit percentages. a ) 30 %, b ) 32.50 %, c ) 33 1 / 3 %, d ) 40 %, e ) 50 %
Output: "explanation : the gain is 4 out of 12 articles. therefore, gain percentage = 4 x 100 / 12 = 100 / 3 = 33 1 / 3 % answer : option c"
So the final answer is c

Input: mike drives his new corvette from san francisco to las vegas, a journey of 640 miles. he drives the first half of the trip at an average rate of 80 miles per hour, but has to slow down for the second half of his journey. if the second half of the trip takes him 200 percent longer than the first half, what is his average rate q in miles per hour for the entire trip? a ) q = 26.7, b ) q = 30.0, c ) q = 40.0, d ) q = 53.3, e ) q = 60.0
Output: "veritas prepofficial solution correct answer : c using the formula : time = distance / rate, we find that mike takes 4 hours to cover the first 320 miles of his trip. since the 2 nd 320 miles take 200 % longer than the first, it takes mike 8 hours longer, or 12 hours. ( note : 200 % longer than the first half is not 200 % of the first half. ) the overall time is 4 hours + 12 hours or 16 hours. since the definition of average rate = total distance traveled / total time of travel, mike's average rate = 640 / 16 or 40 miles per hour. answer choice c is correct."
So the final answer is c

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o9-tmathqa-s20-rTrue-m4096
Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:23:20<3:53:00, 191.51s/it]Evaluating commonsenseqa :  70%|███████   | 70/100 [5:01:29<2:09:38, 259.27s/it]Evaluating commonsenseqa :   1%|          | 1/100 [02:33<4:13:09, 153.43s/it]                                                                                       Evaluating commonsenseqa :   2%|▏         | 2/100 [05:05<4:09:42, 152.88s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:26:35<3:51:13, 192.69s/it]Evaluating commonsenseqa :  71%|███████   | 71/100 [5:05:52<2:05:49, 260.34s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [07:35<4:04:45, 151.40s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:29:47<3:47:46, 192.48s/it]  Evaluating commonsenseqa :   4%|▍         | 4/100 [10:07<4:02:33, 151.60s/it]Evaluating commonsenseqa :  72%|███████▏  | 72/100 [5:10:13<2:01:35, 260.55s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:32:59<3:44:11, 192.16s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [12:34<3:57:16, 149.86s/it]                                                                                         Evaluating commonsenseqa :  31%|███       | 31/100 [1:36:11<3:40:58, 192.15s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [15:03<3:54:13, 149.50s/it]Evaluating commonsenseqa :  73%|███████▎  | 73/100 [5:14:32<1:56:57, 259.89s/it]                                                                                         Evaluating commonsenseqa :   7%|▋         | 7/100 [17:31<3:50:56, 149.00s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [1:39:20<3:36:46, 191.27s/it]Evaluating commonsenseqa :  74%|███████▍  | 74/100 [5:18:53<1:52:46, 260.25s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [19:58<3:47:34, 148.42s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [1:42:29<3:33:01, 190.77s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [22:31<3:47:13, 149.82s/it]Evaluating commonsenseqa :  75%|███████▌  | 75/100 [5:23:14<1:48:36, 260.65s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [1:45:42<3:30:32, 191.41s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [24:59<3:44:10, 149.45s/it]                                                                                         Evaluating commonsenseqa :  11%|█         | 11/100 [27:26<3:40:40, 148.77s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [1:48:57<3:28:30, 192.47s/it]Evaluating commonsenseqa :  76%|███████▌  | 76/100 [5:27:35<1:44:16, 260.70s/it]Evaluating commonsenseqa :  37%|███▋      /100 [29:57<3:39:09, 149.43s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [1:52:12<3:26:01, 193.14s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [32:28<3:37:02, 149.68s/it]Evaluating commonsenseqa :  77%|███████▋  | 77/100 [5:31:58<1:40:09, 261.30s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [1:55:27<3:23:23, 193.70s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [34:56<3:34:11, 149.43s/it]Evaluating commonsenseqa :  78%|███████▊  | 78/100 [5:36:25<1:36:25, 262.99s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [37:22<3:30:04, 148.29s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [1:58:42<3:20:41, 194.22s/it]                                                                                         Evaluating commonsenseqa :  16%|█▌        | 16/100 [39:49<3:26:49, 147.74s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [2:01:57<3:17:26, 194.21s/it]Evaluating commonsenseqa :  79%|███████▉  | 79/100 [5:40:50<1:32:17, 263.69s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [42:15<3:23:50, 147.36s/it]                                                                                         Evaluating commonsenseqa :  40%|████      | 40/100 [2:05:13<3:14:54, 194.91s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [44:45<3:22:20, 148.05s/it]Evaluating commonsenseqa :  80%|████████  | 80/100 [5:44:07<1:21:11, 243.55s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [47:11<3:19:02, 147.44s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [2:08:27<3:11:19, 194.57s/it]                                                                                         Evaluating commonsenseqa :  81%|████████  | 81/100 [5:48:27<1:18:43, 248.59s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [49:39<3:16:53, 147.67s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [2:11:42<3:08:06, 194.60s/it]                                                                                           Evaluating commonsenseqa :  21%|██        | 21/100 [52:08<3:15:07, 148.20s/it]Evaluating commonsenseqa :  82%|████████▏ | 82/100 [5:52:48<1:15:43, 252.39s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [2:14:56<3:04:46, 194.49s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [54:35<3:11:53, 147.61s/it]                                                                                           Evaluating commonsenseqa :  44%|████▍     | 44/100 [2:18:11<3:01:37, 194.59s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [57:06<3:10:52, 148.74s/it]Evaluating commonsenseqa :  83%|████████▎ | 83/100 [5:57:14<1:12:38, 256.37s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [59:39<3:09:54, 149.92s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [2:21:24<2:57:57, 194.13s/it]                                                                                           Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:02:08<3:07:13, 149.78s/it]Evaluating commonsenseqa :  84%|████████▍ | 84/100 [6:01:37<1:08:54, 258.39s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [2:24:39<2:54:58, 194.42s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:04:36<3:03:56, 149.14s/it]    Evaluating commonsenseqa :  47%|████▋     | 47/100 [2:27:48<2:50:27, 192.97s/it]Evaluating commonsenseqa :  85%|████████▌ | 85/100 [6:05:53<1:04:27, 257.83s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:07:00<2:59:30, 147.54s/it]                                                                                           Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:09:28<2:57:19, 147.77s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [2:31:07<2:48:34, 194.51s/it]Evaluating commonsenseqa :  86%|████████▌ | 86/100 [6:10:18<1:00:37, 259.80s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:11:58<2:55:46, 148.54s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [2:34:23<2:45:42, 194.94s/it]                                                                                           Evaluating commonsenseqa :  30%|███       | 30/100 [1:14:27<2:53:29, 148.71s/it]