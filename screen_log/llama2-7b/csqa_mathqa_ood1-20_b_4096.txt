WORLD_SIZE=1
MASTER_ADDR=ia1
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10136 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o17-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 17 17 19 True False 4096 5
WORLD_SIZE=1
MASTER_ADDR=ia1
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10137 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 1 1 3 True False 4096 5
WORLD_SIZE=1
MASTER_ADDR=ia1
WORLD_SIZE=1
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10139 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 9 9 11 True False 4096 5
MASTER_ADDR=ia1
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10140 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 13 13 15 True False 4096 5
WORLD_SIZE=1
MASTER_ADDR=ia1
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10138 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 5 5 7 True False 4096 5
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2023-09-19 18:56:47,860] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
[2023-09-19 18:56:49,927] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o17-tmathqa-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 17
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o17-tmathqa-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
[2023-09-19 18:56:49,980] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:56:50,230] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:56:50,230] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 255075.48it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.33s/it]python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10139 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 11 9 11 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10140 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 15 13 15 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10138 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 7 5 7 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10137 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 3 1 3 True False 4096 5
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.94s/it]
 > number of parameters: 6738415616
[2023-09-19 18:57:00,676] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-09-19 18:57:00,676] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-19 18:57:01,004] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-19 18:57:01,005] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-09-19 18:57:01,005] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-19 18:57:01,005] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-19 18:57:01,005] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-09-19 18:57:01,005] [INFO] [config.py:971:print]   amp_params ................... False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fcae1a0ada0>
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   dump_state ................... False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 2048
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-19 18:57:01,006] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   pld_params ................... False
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   world_size ................... 1
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  True
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   zero_enabled ................. False
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-19 18:57:01,007] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0
[2023-09-19 18:57:01,007] [INFO] [config.py:957:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/200 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: "we assume that 80 is 100 % assume'x'is value we looking for here, 80 = 100 % and x = 12 % therefore, 80 / x = 100 % / 12 % 80 / x = 8.33 x = 9.6 b"
So the final answer is b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: a : b = 92000 : 20000 = 92 : 20 = 46 : 10 = 23 : 5 answer : c
So the final answer is c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: "actual cost = $ 60 sp = actual cost + mark up = actual cost + 20 % sp = 60 * 100 / 80 on sale sp = 80 / 100 ( 60 * 100 / 80 ) = 60 gross profit = $ 0 answer is a"
So the final answer is a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: "apparently it's q = x ^ 2 - x ^ 3 instead of q = x ^ 2 - q ^ 3. in this case : the x - intercept is the value ( s ) of x for q = 0. 0 = x ^ 2 - x ^ 3 ; 0 = x ^ 2 ( 1 - x ) ; x = 0 or x = 1. answer : c."
So the final answer is c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: "official solution : the problem asks for the amount of weekly sales it takes for angelo and isabella to earn the same amount of money. you can write an equation that sets angelo ’ s and isabella ’ s weekly earnings equal to each other, with x representing weekly sales. weekly earnings for each salesperson equal base salary plus commission. so angelo ’ s earnings are 550 + ( 0.08 ) ( x – 3,000 ), and isabella ’ s are 0.10 x. set up the equation and solve : 550 + ( 0.08 ) ( x – 3,000 ) = 0.10 x distribute the 0.08 : 550 + 0.08 x – 240 = 0.10 x combine terms and subtract 0.08 x from both sides : 310 = 0.02 x divide both sides by 0.02 : 15,500 = x your answer is b."
So the final answer is b

Input: find the total number of prime factors in the expression ( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 a ) 26, b ) 22, c ) 25, d ) 30, e ) 29
Output: "( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = ( 2 x 2 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = 2 ^ 11 x 2 ^ 11 x 7 ^ 5 x 11 ^ 3 = 2 ^ 22 x 7 ^ 5 x 11 ^ 3 total number of prime factors = ( 22 + 5 + 3 ) = 30. answer is d."
So the final answer is d

Input: the length of the bridge, which a train 140 m long and traveling at 45 km / hr can cross in 30 sec is? a ) 235, b ) 240, c ) 245, d ) 250, e ) 255
Output: "speed = 45 * 5 / 18 = 25 / 2 m / sec. time = 30 sec let the length of bridge be x meters. then, ( 140 + x ) / 30 = 25 / 2 x = 235 m. answer : option a"
So the final answer is a

Input: 360 metres long yard, 31 trees are palnted at equal distances, one tree being at each end of the yard. what is the distance between 2 consecutive trees a ) 10, b ) 12, c ) 14, d ) 16, e ) 17
Output: "31 trees have 30 gaps between them, required distance ( 360 / 30 ) = 12 b"
So the final answer is b

Input: a vessel of capacity 45 litres is fully filled with pure milk. nine litres of milk is removed from the vessel and replaced with water. nine litres of the solution thus formed is removed and replaced with water. find the quantity of pure milk in the final milk solution? a ) 28.8, b ) 72.9, c ) 38.3, d ) 78.3, e ) 79.3
Output: "explanation : let the initial quantity of milk in vessel be t litres. let us say y litres of the mixture is taken out and replaced by water for n times, alternatively. quantity of milk finally in the vessel is then given by [ ( t - y ) / t ] ^ n * t for the given problem, t = 45, y = 9 and n = 2. hence, quantity of milk finally in the vessel = [ ( 45 - 9 ) / 45 ] ^ 2 ( 45 ) = 28.8 litres. answer : option a"
So the final answer is a

Input: a certain telescope increases the visual range at a particular location from 90 kilometers to 150 kilometers. by what percent is the visual range increased by using the telescope? a ) 30 %, b ) 33 1 / 2 %, c ) 40 %, d ) 60 %, e ) 66 2 / 3 %
Output: "original visual range = 90 km new visual range = 150 km percent increase in the visual range by using the telescope = ( 150 - 90 ) / 90 * 100 % = 2 / 3 * 100 % = 66.67 % answer e"
So the final answer is e

Input: a bag contains 6 black and 3 white balls. one ball is drawn at random. what is the probability that the ball drawn is white? a ) 3 / 4, b ) 1 / 3, c ) 1 / 7, d ) 1 / 8, e ) 4 / 3
Output: "let number of balls = ( 6 + 3 ) = 9. number of white balls = 3. p ( drawing a white ball ) = 3 / 9 = 1 / 3. option b."
So the final answer is b

Input: rajat, vikas and abhishek are submitting questions in the ratio 7 : 3 : 2. if total number of questions submitted by them is 24. find the number of questions submitted by vikas. a ) 3, b ) 4, c ) 5, d ) 6, e ) 7
Output: explanation : number of questions submitted by vikas = ( 24 * 3 ) / ( 7 + 3 + 2 ) = 6 answer : d
So the final answer is d

Input: at a certain restaurant, the ratio of the number of cooks to the number of waiters is 3 to 8. when 12 more waiters are hired, the ratio of the number of cooks to the number of waiters changes to 1 to 4. how many cooks does the restaurant have? a ) 4, b ) 6, c ) 9, d ) 12, e ) 15
Output: originally there were 3 k cooks and 8 k waiters. the new ratio is 1 : 4 which equals 3 : 12. 12 k = 8 k + 12 k = 3 there are 9 cooks. the answer is c.
So the final answer is c

Input: if 7 ^ k + 7 ^ k = ( 7 ^ 9 ) ^ ( 7 ^ 9 ) - 7 ^ k, then k =? a ) 11 / 3, b ) 11 / 2, c ) 242, d ) 3 ^ 10, e ) 7 ^ 11 - 1
Output: "7 ^ k + 7 ^ k = ( 7 ^ 9 ) ^ 7 ^ 9 - 7 ^ k 7 * ( 7 ^ k ) = 7 ^ ( 49 * 7 ^ 9 ) = 7 ^ ( 7 ^ 2 * 7 ^ 9 ) = 7 ^ ( 7 ^ 11 ) 7 ^ k + 1 = 7 ^ ( 7 ^ 11 ) so k + 1 = 7 ^ 11 so k = 7 ^ 11 - 1 answer is e"
So the final answer is e

Input: average of 10 matches is 32, how many runs one should should score to increase his average by 4 runs. a ) 70, b ) 76, c ) 78, d ) 80, e ) 88
Output: "explanation : average after 11 innings should be 36 so, required score = ( 11 * 36 ) - ( 10 * 32 ) = 396 - 320 = 76 answer : option b"
So the final answer is b

Input: a ’ s speed is 20 / 19 times that of b. if a and b run a race, what part of the length of the race should a give b as a head start, so that the race ends in a dead heat? a ) 1 / 19, b ) 3 / 19, c ) 1 / 10, d ) 1 / 20, e ) 3 / 10
Output: "let d be the full distance. let x be the fraction of the distance that b runs. let v be the speed at which b runs. the time should be the same for both runners. time = d / ( 20 v / 19 ) = xd / v ( 19 / 20 ) * d / v = x * d / v x = 19 / 20 b should have a head start of 1 / 20 of the full distance. the answer is d."
So the final answer is d

Input: the length of a rectangular plot is 10 mtr more than its width. the cost of fencing the plot along its perimeter at the rate of rs. 6.5 mtr is rs. 2210. the perimeter of the plot is? a ) 126, b ) 156, c ) 340, d ) 321, e ) 260
Output: "sol. let width = x, length = ( 10 + x ) perimeter = 2 ( x + ( 10 + x ) ) = 2 ( 2 x = 10 ) & 2 ( 2 x + 10 ) * 6.5 = 2210 x = 80 required perimeter = 2 ( 80 + 90 ) = 340 c"
So the final answer is c

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o17-tmathqa-s1-rTrue-m4096
[2023-09-19 18:57:02,587] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:57:02,587] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:57:02,848] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:57:02,848] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
using world size: 1
Answers already exist, exiting...
Answers already exist, exiting...
using world size: 1using world size: 1

Answers already exist, exiting...Answers already exist, exiting...

python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10140 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 13 13 15 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10139 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 9 9 11 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10137 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 1 1 3 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10138 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 5 5 7 True False 4096 5
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2023-09-19 18:57:15,213] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:57:15,213] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:57:15,482] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:57:15,482] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10139 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 11 9 11 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10140 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 15 13 15 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10137 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 3 1 3 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10138 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 7 5 7 True False 4096 5
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2023-09-19 18:57:27,799] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:57:27,799] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:57:28,037] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:57:28,137] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
using world size: 1
Answers already exist, exiting...
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10140 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 13 13 15 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10139 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 9 9 11 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10137 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 1 1 3 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10138 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 5 5 7 True False 4096 5
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2023-09-19 18:57:40,417] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:57:40,417] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:57:40,736] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:57:40,736] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10139 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 11 9 11 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10140 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 15 13 15 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10137 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 3 1 3 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10138 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 7 5 7 True False 4096 5
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2023-09-19 18:57:52,910] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:57:52,910] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:57:53,368] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:57:53,368] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
using world size: 1
using world size: 1
Answers already exist, exiting...
Answers already exist, exiting...
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10140 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 13 13 15 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10139 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 9 9 11 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10138 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 5 5 7 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10137 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 1 1 3 True False 4096 5
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2023-09-19 18:58:05,477] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:58:05,531] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:58:05,907] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:58:05,942] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Evaluating commonsenseqa :   0%|          | 1/200 [01:07<3:43:24, 67.36s/it]using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s10-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 1
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o1-tmathqa-s10-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
using world size: 1
Answers already exist, exiting...
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 1015755.05it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10139 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 11 9 11 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10140 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 15 13 15 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10138 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 7 5 7 True False 4096 5
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.35s/it][2023-09-19 18:58:18,272] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:58:18,298] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:58:18,430] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.73s/it]
 > number of parameters: 6738415616
[2023-09-19 18:58:21,214] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-09-19 18:58:21,215] [INFO] [comm.py:637:init_distributed] cdb=None
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s10-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 7
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o7-tmathqa-s10-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
[2023-09-19 18:58:21,604] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-19 18:58:21,605] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-09-19 18:58:21,605] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   amp_params ................... False
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4f0c222d70>
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   dump_state ................... False
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-09-19 18:58:21,606] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 2048
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   pld_params ................... False
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   world_size ................... 1
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  True
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   zero_enabled ................. False
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-19 18:58:21,607] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0
[2023-09-19 18:58:21,607] [INFO] [config.py:957:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/200 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: in a recent head - to - head run - off election, 12000 absentee ballets were cast. 1 / 6 of the absentee ballets were thrown out and 3 / 5 of the remaining absentee ballets were cast for candidate a. how many absentee votes did candidate b receive? a ) 2,000, b ) 3,000, c ) 4,000, d ) 8,000, e ) 9,000
Output: c

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o1-tmathqa-s10-rFalse-m4096
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 569233.23it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10139 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 9 9 11 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10140 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 13 13 15 True False 4096 5
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2023-09-19 18:58:30,886] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:58:30,907] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.29s/it]using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.27s/it]
 > number of parameters: 6738415616
[2023-09-19 18:58:34,950] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-09-19 18:58:34,950] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-19 18:58:35,409] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-19 18:58:35,411] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   amp_params ................... False
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5970c22d70>
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-09-19 18:58:35,412] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   dump_state ................... False
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 2048
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-09-19 18:58:35,413] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   pld_params ................... False
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   world_size ................... 1
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  True
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   zero_enabled ................. False
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-19 18:58:35,414] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0
[2023-09-19 18:58:35,414] [INFO] [config.py:957:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/200 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: in a recent head - to - head run - off election, 12000 absentee ballets were cast. 1 / 6 of the absentee ballets were thrown out and 3 / 5 of the remaining absentee ballets were cast for candidate a. how many absentee votes did candidate b receive? a ) 2,000, b ) 3,000, c ) 4,000, d ) 8,000, e ) 9,000
Output: c

Input: the total age of a and b is 11 years more than the total age of b and c. c is how many years younger than a.? a ) 16, b ) 12, c ) 11, d ) 20, e ) 10
Output: c

Input: the scoring system in a certain football competition goes as follows : 3 points for victory, 1 point for a draw, and 0 points for defeat. each team plays 20 matches. if a team scored 14 points after 5 games, what is the least number of the remaining matches it has to win to reach the 40 - point mark by the end of the tournament? a ) 6, b ) 7, c ) 8, d ) 9, e ) 10
Output: a

Input: how much water should be added to 10 liters of a 20 % - solution of alcohol to reduce the concentration of alcohol in the solution by 75 %? a ) 25 liters, b ) 27 liters, c ) 30 liters, d ) 32 liters, e ) 35 liters
Output: c

Input: a goods train runs at the speed of 72 kmph and crosses a 250 m long platform in 36 seconds. what is the length of the goods train? a ) 470 m, b ) 240 m, c ) 260 m, d ) 270 m, e ) none of these
Output: a

Input: the volume of a cube is 1728 cc. find its surface. a ) 864, b ) 556, c ) 255, d ) 287, e ) 267
Output: a

Input: a certain drink of type a is prepared by mixing 4 parts milk with 3 parts fruit juice. another drink of type b is prepared by mixing 4 parts of fruit juice and 3 parts of milk. how many liters of fruit juice must be added to 63 liters of drink a to convert it to drink b? a ) 7, b ) 14, c ) 21, d ) 28, e ) 35
Output: c

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o7-tmathqa-s10-rFalse-m4096
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10140 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 15 13 15 True False 4096 5
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10139 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 11 9 11 True False 4096 5
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2023-09-19 18:58:43,315] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 18:58:43,365] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 11
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o11-tmathqa-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
using world size: 1
Answers already exist, exiting...
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 336915.36it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10140 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s20-rFalse --seed 20 --max-prompt-length 4096 --num-out-domain 13 13 15 True False 4096 5
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.20s/it][2023-09-19 18:58:55,899] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.67s/it]
 > number of parameters: 6738415616
[2023-09-19 18:58:58,715] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-09-19 18:58:58,715] [INFO] [comm.py:637:init_distributed] cdb=None
using world size: 1
Answers already exist, exiting...
[2023-09-19 18:58:59,123] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-19 18:58:59,126] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-09-19 18:58:59,126] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-19 18:58:59,126] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-19 18:58:59,126] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-09-19 18:58:59,126] [INFO] [config.py:971:print]   amp_params ................... False
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc590632da0>
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   dump_state ................... False
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-09-19 18:58:59,127] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 2048
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   pld_params ................... False
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-09-19 18:58:59,128] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-09-19 18:58:59,129] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-09-19 18:58:59,129] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-09-19 18:58:59,129] [INFO] [config.py:971:print]   world_size ................... 1
[2023-09-19 18:58:59,129] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  True
[2023-09-19 18:58:59,129] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-19 18:58:59,129] [INFO] [config.py:971:print]   zero_enabled ................. False
[2023-09-19 18:58:59,129] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-19 18:58:59,129] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0
[2023-09-19 18:58:59,129] [INFO] [config.py:957:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/200 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: a man cycling along the road noticed that every 18 minutes a bus overtakes him and every 6 minutes he meets an oncoming bus. if all buses and the cyclist move at a constant speed, what is the time interval between consecutive buses? a ) 5 minutes, b ) 6 minutes, c ) 8 minutes, d ) 9 minutes, e ) 10 minutes
Output: let's say the distance between the buses is d. we want to determine interval = \ frac { d } { b }, where b is the speed of bus. let the speed of cyclist be c. every 18 minutes a bus overtakes cyclist : \ frac { d } { b - c } = 18, d = 18 b - 18 c ; every 6 minutes cyclist meets an oncoming bus : \ frac { d } { b + c } = 6, d = 6 b + 6 c ; d = 18 b - 18 c = 6 b + 6 c, - - > b = 2 c, - - > d = 18 b - 9 b = 9 b. interval = \ frac { d } { b } = \ frac { 9 b } { b } = 9 answer : d ( 9 minutes ).
So the final answer is d

Input: an athlete runs 200 metres race in 24 seconds. what is his speed? a ) 20 km / hr, b ) 30 km / hr, c ) 25 km / hr, d ) 35 km / hr, e ) 40 km / hr
Output: speed = dist / time = 200 / 24 200 / 24 * 18 / 5 = 40 * 3 / 4 = 30 km / hr answer b
So the final answer is b

Input: two numbers are in the ratio 3 : 4. if the sum of numbers is 63, find the numbers. a ) 27 and 36., b ) 25 and 30., c ) 23 and 44., d ) 63 and 12., e ) 12 and 36.
Output: "sum of the terms of the ratio = 3 + 4 = 7 sum of numbers = 63 therefore, first number = 3 / 7 × 63 = 27 second number = 4 / 7 × 63 = 36 therefore, the two numbers are 27 and 36. answer is a"
So the final answer is a

Input: a company wants to spend equal amounts of money for the purchase of two types of computer printers costing $ 375 and $ 150 per unit, respectively. what is the fewest number of computer printers that the company can purchase? a ) 3, b ) 4, c ) 5, d ) 6, e ) 7
Output: the smallest amount that the company can spend is the lcm of 375 and 150, which is 750 for each, which is total 1500. the number of 1 st type of computers which costing $ 375 = 750 / 375 = 2. the number of 2 nd type of computers which costing $ 150 = 750 / 150 = 5. total = 2 + 5 = 7 answer is e.
So the final answer is e

Input: a certain music store stocks 800 cellos and 600 violas. of these instruments, there are 80 cello - viola pairs, such that a cello and a viola were both made with wood from the same tree ( each tree can make at most one viola and one cello, so there are no pairs other than these 90 ). if one viola and one cello are chosen at random, what is the probability that the two instruments are made with wood from the same tree? a ) 3 / 16,000, b ) 1 / 8,100, c ) 1 / 600, d ) 1 / 90, e ) 2 / 45
Output: "solution provided by stanford 2012 is correct : 80 / 800 choosing one of the cellos which has a pair viola, 1 / 600 choosing the viola which is the pair of chosen cello - - > p = 80 / 800 * 1 / 600 = 1 / 6,000. answer : c."
So the final answer is c

Input: what is the difference between local value & face value of 7 in the numeral 65793? a ) 693, b ) 656, c ) 691, d ) 9890, e ) 10000
Output: ( local value of 7 ) - ( face value of 7 ) = ( 700 - 7 ) = 693 a
So the final answer is a

Input: in an office in singapore there are 60 % female employees. 50 % of all the male employees are computer literate. if there are total 62 % employees computer literate out of total 1100 employees, then the no. of female employees who are computer literate? a ) 462, b ) 674, c ) 672, d ) 960, e ) none
Output: "solution : total employees, = 1100 female employees, 60 % of 1100. = ( 60 * 1100 ) / 100 = 660. then male employees, = 440 50 % of male are computer literate, = 220 male computer literate. 62 % of total employees are computer literate, = ( 62 * 1100 ) / 100 = 682 computer literate. thus, female computer literate = 682 - 220 = 462. answer : option a"
So the final answer is a

Input: the cost price of 16 articles is the same as the selling price of 12 articles. find the loss / profit percentages. a ) 30 %, b ) 32.50 %, c ) 33 1 / 3 %, d ) 40 %, e ) 50 %
Output: "explanation : the gain is 4 out of 12 articles. therefore, gain percentage = 4 x 100 / 12 = 100 / 3 = 33 1 / 3 % answer : option c"
So the final answer is c

Input: mike drives his new corvette from san francisco to las vegas, a journey of 640 miles. he drives the first half of the trip at an average rate of 80 miles per hour, but has to slow down for the second half of his journey. if the second half of the trip takes him 200 percent longer than the first half, what is his average rate q in miles per hour for the entire trip? a ) q = 26.7, b ) q = 30.0, c ) q = 40.0, d ) q = 53.3, e ) q = 60.0
Output: "veritas prepofficial solution correct answer : c using the formula : time = distance / rate, we find that mike takes 4 hours to cover the first 320 miles of his trip. since the 2 nd 320 miles take 200 % longer than the first, it takes mike 8 hours longer, or 12 hours. ( note : 200 % longer than the first half is not 200 % of the first half. ) the overall time is 4 hours + 12 hours or 16 hours. since the definition of average rate = total distance traveled / total time of travel, mike's average rate = 640 / 16 or 40 miles per hour. answer choice c is correct."
So the final answer is c

Input: lisa and robert have taken the same number of photos on their school trip. lisa has taken 3 times as many photos as claire and robert has taken 20 more photos than claire. how many photos has claire taken? a ) 6, b ) 8, c ) 10, d ) 12, e ) 14
Output: "l = r l = 3 c r = c + 20 3 c = c + 20 c = 10 the answer is c."
So the final answer is c

Input: meena wrote all the numbers from 1 to 69,999 inclusive. how many digits did she write in total? a ) 278,889, b ) 308,889, c ) 338,889, d ) 368,889, e ) 398,889
Output: "1 - 9 = > 1 * 9 digits 10 - 99 = > 2 * 90 = 180 ( numbers between 10 - 99 is 90 where each has 2 digits ) 100 - 999 = > 3 * 900 = 2700 1000 - 9999 = > 4 * 9000 = 36,000 10000 - 69999 = > 5 * 60,000 = 300,000 the answer is 338,889 the answer is c."
So the final answer is c

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o11-tmathqa-s20-rTrue-m4096
python -m torch.distributed.launch --use-env --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10140 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s20-rFalse --seed 20 --max-prompt-length 4096 --num-out-domain 15 13 15 True False 4096 5
/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2023-09-19 18:59:07,926] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s20-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 15
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o15-tmathqa-s20-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 372651.04it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Evaluating commonsenseqa :   1%|          | 2/200 [02:14<3:41:17, 67.06s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.19s/it]
 > number of parameters: 6738415616
[2023-09-19 18:59:24,205] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-09-19 18:59:24,206] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-19 18:59:24,649] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-19 18:59:24,652] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-09-19 18:59:24,652] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-19 18:59:24,652] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-19 18:59:24,653] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-09-19 18:59:24,653] [INFO] [config.py:971:print]   amp_params ................... False
[2023-09-19 18:59:24,653] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-19 18:59:24,653] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-09-19 18:59:24,653] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-09-19 18:59:24,653] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-09-19 18:59:24,653] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-09-19 18:59:24,653] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe8ed406d70>
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   dump_state ................... False
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-09-19 18:59:24,654] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 2048
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   pld_params ................... False
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-09-19 18:59:24,655] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-09-19 18:59:24,656] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-09-19 18:59:24,656] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-09-19 18:59:24,656] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-09-19 18:59:24,656] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-09-19 18:59:24,656] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-09-19 18:59:24,656] [INFO] [config.py:971:print]   world_size ................... 1
[2023-09-19 18:59:24,656] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  True
[2023-09-19 18:59:24,656] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-19 18:59:24,656] [INFO] [config.py:971:print]   zero_enabled ................. False
[2023-09-19 18:59:24,656] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-19 18:59:24,656] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0
[2023-09-19 18:59:24,656] [INFO] [config.py:957:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/200 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: a man cycling along the road noticed that every 18 minutes a bus overtakes him and every 6 minutes he meets an oncoming bus. if all buses and the cyclist move at a constant speed, what is the time interval between consecutive buses? a ) 5 minutes, b ) 6 minutes, c ) 8 minutes, d ) 9 minutes, e ) 10 minutes
Output: d

Input: an athlete runs 200 metres race in 24 seconds. what is his speed? a ) 20 km / hr, b ) 30 km / hr, c ) 25 km / hr, d ) 35 km / hr, e ) 40 km / hr
Output: b

Input: two numbers are in the ratio 3 : 4. if the sum of numbers is 63, find the numbers. a ) 27 and 36., b ) 25 and 30., c ) 23 and 44., d ) 63 and 12., e ) 12 and 36.
Output: a

Input: a company wants to spend equal amounts of money for the purchase of two types of computer printers costing $ 375 and $ 150 per unit, respectively. what is the fewest number of computer printers that the company can purchase? a ) 3, b ) 4, c ) 5, d ) 6, e ) 7
Output: e

Input: a certain music store stocks 800 cellos and 600 violas. of these instruments, there are 80 cello - viola pairs, such that a cello and a viola were both made with wood from the same tree ( each tree can make at most one viola and one cello, so there are no pairs other than these 90 ). if one viola and one cello are chosen at random, what is the probability that the two instruments are made with wood from the same tree? a ) 3 / 16,000, b ) 1 / 8,100, c ) 1 / 600, d ) 1 / 90, e ) 2 / 45
Output: c

Input: what is the difference between local value & face value of 7 in the numeral 65793? a ) 693, b ) 656, c ) 691, d ) 9890, e ) 10000
Output: a

Input: in an office in singapore there are 60 % female employees. 50 % of all the male employees are computer literate. if there are total 62 % employees computer literate out of total 1100 employees, then the no. of female employees who are computer literate? a ) 462, b ) 674, c ) 672, d ) 960, e ) none
Output: a

Input: the cost price of 16 articles is the same as the selling price of 12 articles. find the loss / profit percentages. a ) 30 %, b ) 32.50 %, c ) 33 1 / 3 %, d ) 40 %, e ) 50 %
Output: c

Input: mike drives his new corvette from san francisco to las vegas, a journey of 640 miles. he drives the first half of the trip at an average rate of 80 miles per hour, but has to slow down for the second half of his journey. if the second half of the trip takes him 200 percent longer than the first half, what is his average rate q in miles per hour for the entire trip? a ) q = 26.7, b ) q = 30.0, c ) q = 40.0, d ) q = 53.3, e ) q = 60.0
Output: c

Input: lisa and robert have taken the same number of photos on their school trip. lisa has taken 3 times as many photos as claire and robert has taken 20 more photos than claire. how many photos has claire taken? a ) 6, b ) 8, c ) 10, d ) 12, e ) 14
Output: c

Input: meena wrote all the numbers from 1 to 69,999 inclusive. how many digits did she write in total? a ) 278,889, b ) 308,889, c ) 338,889, d ) 368,889, e ) 398,889
Output: c

Input: a pipe takes a hours to fill the tank. but because of a leakage it took 2 times of its original time. find the time taken by the leakage to empty the tank a ) 50 min, b ) 60 min, c ) 90 min, d ) 80 min, e ) 120 min
Output: e

Input: if x is an integer such that 0 < x < 7, 0 < x < 15, 5 > x > – 1, 3 > x > 0, and x + 2 < 4, then x is a ) 1, b ) 2, c ) 3, d ) 4, e ) 5
Output: a

Input: a and b put in rs. 200 and rs. 400 respectively into a business. a reinvests into the business his share of the first year's profit of rs. 210 where as b does not. in what ratio should they divide the second year's profit? a ) 39 : 40, b ) 39 : 49, c ) 39 : 42, d ) 39 : 47, e ) 17 : 20
Output: e

Input: what least number must be subtracted from 427398 so that remaining no. is divisible by 10 a ) 3, b ) 5, c ) 6, d ) 7, e ) 8
Output: e

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o15-tmathqa-s20-rFalse-m4096
Evaluating commonsenseqa :   0%|          | 1/200 [00:35<1:58:21, 35.69s/it]Evaluating commonsenseqa :   2%|▏         | 3/200 [03:20<3:39:30, 66.86s/it]Evaluating commonsenseqa :   0%|          | 1/200 [01:25<4:43:32, 85.49s/it]