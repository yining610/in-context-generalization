WORLD_SIZE=1
MASTER_ADDR=ia1
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 1 1 3 5 7 9 True False 4096 5
WORLD_SIZE=1
MASTER_ADDR=ia1
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 11 11 13 15 17 19 True False 4096 5
[2023-09-22 09:23:50,693] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:23:50,693] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 3 1 3 5 7 9 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 13 11 13 15 17 19 True False 4096 5
[2023-09-22 09:24:03,379] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:24:03,379] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 15 11 13 15 17 19 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 5 1 3 5 7 9 True False 4096 5
[2023-09-22 09:24:15,822] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:24:15,879] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o17-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 17 11 13 15 17 19 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 7 1 3 5 7 9 True False 4096 5
[2023-09-22 09:24:28,400] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:24:28,413] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o19-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 19 11 13 15 17 19 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 9 1 3 5 7 9 True False 4096 5
[2023-09-22 09:24:40,941] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:24:40,941] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1using world size: 1

Answers already exist, exiting...
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o19-tmathqa-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 19
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o19-tmathqa-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 312462.93it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 1 1 3 5 7 9 True False 4096 5
[2023-09-22 09:24:53,488] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.70s/it]using world size: 1
Answers already exist, exiting...
Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.94s/it]
 > number of parameters: 6738415616
[2023-09-22 09:24:56,988] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-09-22 09:24:56,988] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-22 09:24:57,357] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-22 09:24:57,359] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-09-22 09:24:57,359] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-22 09:24:57,359] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-22 09:24:57,359] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-09-22 09:24:57,359] [INFO] [config.py:971:print]   amp_params ................... False
[2023-09-22 09:24:57,359] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-22 09:24:57,359] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-09-22 09:24:57,359] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-09-22 09:24:57,359] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-09-22 09:24:57,359] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-09-22 09:24:57,359] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1b6640eda0>
[2023-09-22 09:24:57,359] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   dump_state ................... False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 2048
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   pld_params ................... False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-09-22 09:24:57,360] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-09-22 09:24:57,361] [INFO] [config.py:971:print]   world_size ................... 1
[2023-09-22 09:24:57,361] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  True
[2023-09-22 09:24:57,361] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-22 09:24:57,361] [INFO] [config.py:971:print]   zero_enabled ................. False
[2023-09-22 09:24:57,361] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-22 09:24:57,361] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0
[2023-09-22 09:24:57,361] [INFO] [config.py:957:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/200 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: "we assume that 80 is 100 % assume'x'is value we looking for here, 80 = 100 % and x = 12 % therefore, 80 / x = 100 % / 12 % 80 / x = 8.33 x = 9.6 b"
So the final answer is b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: a : b = 92000 : 20000 = 92 : 20 = 46 : 10 = 23 : 5 answer : c
So the final answer is c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: "actual cost = $ 60 sp = actual cost + mark up = actual cost + 20 % sp = 60 * 100 / 80 on sale sp = 80 / 100 ( 60 * 100 / 80 ) = 60 gross profit = $ 0 answer is a"
So the final answer is a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: "apparently it's q = x ^ 2 - x ^ 3 instead of q = x ^ 2 - q ^ 3. in this case : the x - intercept is the value ( s ) of x for q = 0. 0 = x ^ 2 - x ^ 3 ; 0 = x ^ 2 ( 1 - x ) ; x = 0 or x = 1. answer : c."
So the final answer is c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: "official solution : the problem asks for the amount of weekly sales it takes for angelo and isabella to earn the same amount of money. you can write an equation that sets angelo ’ s and isabella ’ s weekly earnings equal to each other, with x representing weekly sales. weekly earnings for each salesperson equal base salary plus commission. so angelo ’ s earnings are 550 + ( 0.08 ) ( x – 3,000 ), and isabella ’ s are 0.10 x. set up the equation and solve : 550 + ( 0.08 ) ( x – 3,000 ) = 0.10 x distribute the 0.08 : 550 + 0.08 x – 240 = 0.10 x combine terms and subtract 0.08 x from both sides : 310 = 0.02 x divide both sides by 0.02 : 15,500 = x your answer is b."
So the final answer is b

Input: find the total number of prime factors in the expression ( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 a ) 26, b ) 22, c ) 25, d ) 30, e ) 29
Output: "( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = ( 2 x 2 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = 2 ^ 11 x 2 ^ 11 x 7 ^ 5 x 11 ^ 3 = 2 ^ 22 x 7 ^ 5 x 11 ^ 3 total number of prime factors = ( 22 + 5 + 3 ) = 30. answer is d."
So the final answer is d

Input: the length of the bridge, which a train 140 m long and traveling at 45 km / hr can cross in 30 sec is? a ) 235, b ) 240, c ) 245, d ) 250, e ) 255
Output: "speed = 45 * 5 / 18 = 25 / 2 m / sec. time = 30 sec let the length of bridge be x meters. then, ( 140 + x ) / 30 = 25 / 2 x = 235 m. answer : option a"
So the final answer is a

Input: 360 metres long yard, 31 trees are palnted at equal distances, one tree being at each end of the yard. what is the distance between 2 consecutive trees a ) 10, b ) 12, c ) 14, d ) 16, e ) 17
Output: "31 trees have 30 gaps between them, required distance ( 360 / 30 ) = 12 b"
So the final answer is b

Input: a vessel of capacity 45 litres is fully filled with pure milk. nine litres of milk is removed from the vessel and replaced with water. nine litres of the solution thus formed is removed and replaced with water. find the quantity of pure milk in the final milk solution? a ) 28.8, b ) 72.9, c ) 38.3, d ) 78.3, e ) 79.3
Output: "explanation : let the initial quantity of milk in vessel be t litres. let us say y litres of the mixture is taken out and replaced by water for n times, alternatively. quantity of milk finally in the vessel is then given by [ ( t - y ) / t ] ^ n * t for the given problem, t = 45, y = 9 and n = 2. hence, quantity of milk finally in the vessel = [ ( 45 - 9 ) / 45 ] ^ 2 ( 45 ) = 28.8 litres. answer : option a"
So the final answer is a

Input: a certain telescope increases the visual range at a particular location from 90 kilometers to 150 kilometers. by what percent is the visual range increased by using the telescope? a ) 30 %, b ) 33 1 / 2 %, c ) 40 %, d ) 60 %, e ) 66 2 / 3 %
Output: "original visual range = 90 km new visual range = 150 km percent increase in the visual range by using the telescope = ( 150 - 90 ) / 90 * 100 % = 2 / 3 * 100 % = 66.67 % answer e"
So the final answer is e

Input: a bag contains 6 black and 3 white balls. one ball is drawn at random. what is the probability that the ball drawn is white? a ) 3 / 4, b ) 1 / 3, c ) 1 / 7, d ) 1 / 8, e ) 4 / 3
Output: "let number of balls = ( 6 + 3 ) = 9. number of white balls = 3. p ( drawing a white ball ) = 3 / 9 = 1 / 3. option b."
So the final answer is b

Input: rajat, vikas and abhishek are submitting questions in the ratio 7 : 3 : 2. if total number of questions submitted by them is 24. find the number of questions submitted by vikas. a ) 3, b ) 4, c ) 5, d ) 6, e ) 7
Output: explanation : number of questions submitted by vikas = ( 24 * 3 ) / ( 7 + 3 + 2 ) = 6 answer : d
So the final answer is d

Input: at a certain restaurant, the ratio of the number of cooks to the number of waiters is 3 to 8. when 12 more waiters are hired, the ratio of the number of cooks to the number of waiters changes to 1 to 4. how many cooks does the restaurant have? a ) 4, b ) 6, c ) 9, d ) 12, e ) 15
Output: originally there were 3 k cooks and 8 k waiters. the new ratio is 1 : 4 which equals 3 : 12. 12 k = 8 k + 12 k = 3 there are 9 cooks. the answer is c.
So the final answer is c

Input: if 7 ^ k + 7 ^ k = ( 7 ^ 9 ) ^ ( 7 ^ 9 ) - 7 ^ k, then k =? a ) 11 / 3, b ) 11 / 2, c ) 242, d ) 3 ^ 10, e ) 7 ^ 11 - 1
Output: "7 ^ k + 7 ^ k = ( 7 ^ 9 ) ^ 7 ^ 9 - 7 ^ k 7 * ( 7 ^ k ) = 7 ^ ( 49 * 7 ^ 9 ) = 7 ^ ( 7 ^ 2 * 7 ^ 9 ) = 7 ^ ( 7 ^ 11 ) 7 ^ k + 1 = 7 ^ ( 7 ^ 11 ) so k + 1 = 7 ^ 11 so k = 7 ^ 11 - 1 answer is e"
So the final answer is e

Input: average of 10 matches is 32, how many runs one should should score to increase his average by 4 runs. a ) 70, b ) 76, c ) 78, d ) 80, e ) 88
Output: "explanation : average after 11 innings should be 36 so, required score = ( 11 * 36 ) - ( 10 * 32 ) = 396 - 320 = 76 answer : option b"
So the final answer is b

Input: a ’ s speed is 20 / 19 times that of b. if a and b run a race, what part of the length of the race should a give b as a head start, so that the race ends in a dead heat? a ) 1 / 19, b ) 3 / 19, c ) 1 / 10, d ) 1 / 20, e ) 3 / 10
Output: "let d be the full distance. let x be the fraction of the distance that b runs. let v be the speed at which b runs. the time should be the same for both runners. time = d / ( 20 v / 19 ) = xd / v ( 19 / 20 ) * d / v = x * d / v x = 19 / 20 b should have a head start of 1 / 20 of the full distance. the answer is d."
So the final answer is d

Input: the length of a rectangular plot is 10 mtr more than its width. the cost of fencing the plot along its perimeter at the rate of rs. 6.5 mtr is rs. 2210. the perimeter of the plot is? a ) 126, b ) 156, c ) 340, d ) 321, e ) 260
Output: "sol. let width = x, length = ( 10 + x ) perimeter = 2 ( x + ( 10 + x ) ) = 2 ( 2 x = 10 ) & 2 ( 2 x + 10 ) * 6.5 = 2210 x = 80 required perimeter = 2 ( 80 + 90 ) = 340 c"
So the final answer is c

Input: if n = 2 ^ 0.1 and n ^ b = 16, b must equal a ) 3 / 80, b ) 3 / 5, c ) 40, d ) 5 / 3, e ) 80 / 3
Output: 10 / 100 = 1 / 10 n = 2 ^ 1 / 10 n ^ b = 2 ^ 4 ( 2 ^ 1 / 10 ) ^ b = 2 ^ 4 b = 40 answer : c
So the final answer is c

Input: 150 ml of 30 % sulphuric acid was added to approximate 400 ml of 12 % sulphuric acid solution. find the approximate concentration c of the acid in the mixture? a ) 1 / 2, b ) 1 / 3, c ) 1 / 4, d ) 1 / 6, e ) 1 / 5
Output: "do not need any computation 30 % - - - - - - - - - - - 21 % - - - - - - - - - 12 % if volume of both sol. were equal the concentration c would be 21 % = 1 / 5, but 12 % is more than 3 times only possibility is 1 / 6 d"
So the final answer is d

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o19-tmathqa-s1-rTrue-m4096
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 3 1 3 5 7 9 True False 4096 5
Evaluating commonsenseqa :   0%|          | 0/200 [00:07<?, ?it/s]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 97, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 94, in main
    inference_main(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 90, in inference_main
    query_ids, response_ids, answers, indices = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 65, in run_model
    gen_out = model.generate(
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 1454, in generate
    return self.sample(
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 2568, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
[2023-09-22 09:25:05,554] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2985040) of binary: /home/ylu130/.conda/envs/ood/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-22_09:25:08
  host      : ia1.wse.jhu.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2985040)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 5 1 3 5 7 9 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 11 11 13 15 17 19 True False 4096 5
[2023-09-22 09:25:13,137] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:25:13,345] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 7 1 3 5 7 9 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 13 11 13 15 17 19 True False 4096 5
[2023-09-22 09:25:25,467] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:25:25,701] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 9 1 3 5 7 9 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 15 11 13 15 17 19 True False 4096 5
[2023-09-22 09:25:37,889] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:25:38,212] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 1 1 3 5 7 9 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o17-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 17 11 13 15 17 19 True False 4096 5
[2023-09-22 09:25:50,219] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:25:50,669] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 3 1 3 5 7 9 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o19-tmathqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 19 11 13 15 17 19 True False 4096 5
[2023-09-22 09:26:02,455] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:26:03,020] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 5 1 3 5 7 9 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 11 11 13 15 17 19 True False 4096 5
[2023-09-22 09:26:14,601] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:26:15,481] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 7 1 3 5 7 9 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 13 11 13 15 17 19 True False 4096 5
[2023-09-22 09:26:26,486] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:26:28,024] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 9 1 3 5 7 9 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 15 11 13 15 17 19 True False 4096 5
[2023-09-22 09:26:38,021] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:26:40,373] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 1 1 3 5 7 9 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o17-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 17 11 13 15 17 19 True False 4096 5
[2023-09-22 09:26:48,938] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
[2023-09-22 09:26:52,512] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o19-tmathqa-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 19 11 13 15 17 19 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 3 1 3 5 7 9 True False 4096 5
[2023-09-22 09:26:59,411] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:27:01,846] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o19-tmathqa-s10-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 19
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o19-tmathqa-s10-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 10
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 197799.70it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 5 1 3 5 7 9 True False 4096 5
Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.49s/it][2023-09-22 09:27:14,461] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.85s/it]
 > number of parameters: 6738415616
[2023-09-22 09:27:15,441] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-09-22 09:27:15,441] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-22 09:27:15,861] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-22 09:27:15,864] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-09-22 09:27:15,864] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-22 09:27:15,864] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-22 09:27:15,865] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-09-22 09:27:15,865] [INFO] [config.py:971:print]   amp_params ................... False
[2023-09-22 09:27:15,865] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-22 09:27:15,865] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-09-22 09:27:15,865] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-09-22 09:27:15,865] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-09-22 09:27:15,865] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-09-22 09:27:15,865] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f708280eef0>
[2023-09-22 09:27:15,865] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-09-22 09:27:15,865] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-22 09:27:15,865] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-09-22 09:27:15,865] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-09-22 09:27:15,865] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-22 09:27:15,865] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-09-22 09:27:15,865] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-09-22 09:27:15,865] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   dump_state ................... False
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 2048
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-22 09:27:15,866] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   pld_params ................... False
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   world_size ................... 1
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  True
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   zero_enabled ................. False
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-22 09:27:15,867] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0
[2023-09-22 09:27:15,867] [INFO] [config.py:957:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/200 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: in a recent head - to - head run - off election, 12000 absentee ballets were cast. 1 / 6 of the absentee ballets were thrown out and 3 / 5 of the remaining absentee ballets were cast for candidate a. how many absentee votes did candidate b receive? a ) 2,000, b ) 3,000, c ) 4,000, d ) 8,000, e ) 9,000
Output: 5 / 6 * 2 / 5 ( total absentee votes ) = 1 / 3 ( total votes ) = 1 / 3 * 12000 = 4000 answer is c
So the final answer is c

Input: the total age of a and b is 11 years more than the total age of b and c. c is how many years younger than a.? a ) 16, b ) 12, c ) 11, d ) 20, e ) 10
Output: "( a + b ) - ( b - c ) = 11 a - c = 11 answer is c"
So the final answer is c

Input: the scoring system in a certain football competition goes as follows : 3 points for victory, 1 point for a draw, and 0 points for defeat. each team plays 20 matches. if a team scored 14 points after 5 games, what is the least number of the remaining matches it has to win to reach the 40 - point mark by the end of the tournament? a ) 6, b ) 7, c ) 8, d ) 9, e ) 10
Output: "to get 40 points as end of season we need another 26 points or more from remaining 15 matches : option a = 6 * 3 + 9 * 1 = 27 hence option a - 6"
So the final answer is a

Input: how much water should be added to 10 liters of a 20 % - solution of alcohol to reduce the concentration of alcohol in the solution by 75 %? a ) 25 liters, b ) 27 liters, c ) 30 liters, d ) 32 liters, e ) 35 liters
Output: "let x ltr water to be added 2 ltr alcohol to be represented as ( 20 ( 1 - 3 / 4 ( new soln. = 10 + x ) ) ) 2 = 5 % * ( 10 + x ) - - - - - - - - > x = 30 answer : c"
So the final answer is c

Input: a goods train runs at the speed of 72 kmph and crosses a 250 m long platform in 36 seconds. what is the length of the goods train? a ) 470 m, b ) 240 m, c ) 260 m, d ) 270 m, e ) none of these
Output: "explanation : speed = [ 72 x ( 5 / 18 ) ] m / sec = 20 m / sec. time = 36 sec. let the length of the train be x metres. then, [ ( x + 250 ) / 36 ] = 20 = > x + 250 = 720 = > x = 470. answer : a"
So the final answer is a

Input: the volume of a cube is 1728 cc. find its surface. a ) 864, b ) 556, c ) 255, d ) 287, e ) 267
Output: "a 3 = 1728 = > a = 12 6 a 2 = 6 * 12 * 12 = 864 answer : a"
So the final answer is a

Input: a certain drink of type a is prepared by mixing 4 parts milk with 3 parts fruit juice. another drink of type b is prepared by mixing 4 parts of fruit juice and 3 parts of milk. how many liters of fruit juice must be added to 63 liters of drink a to convert it to drink b? a ) 7, b ) 14, c ) 21, d ) 28, e ) 35
Output: "in 63 liters of drink a, there are 36 liters of milk and 27 liters of juice. with 36 liters of milk, we need a total of 48 liters of juice to make drink b. we need to add 21 liters of juice. the answer is c."
So the final answer is c

Input: last week john spent 10 percent of his wages on recreation. this week, his wages are 10 percent less than last week ʼ s wages and he spent 40 percent of his wages on recreation. the amount he spends on recreation this week is what percent of the amount he spent on recreation last week a ) 180 %, b ) 360 %, c ) 200 %, d ) 220 %, e ) 250 %
Output: "say john's wages last week were $ 100, so he spent 0.10 * 100 = $ 10 on recreation ; this week's wages is 0.90 * 100 = $ 90, so he spends 0.4 * 90 = $ 36 on recreation ; 36 / 10 = 3.60, hence the amount he spends on recreation this week is 360 % of the amount he spent on recreation last week : 10 * 3.6 = 36 answer : b"
So the final answer is b

Input: the salary of a person was reduced by 15 %. by what percent should his reduced salary be raised so as to bring it at par with his original salary? a ) 10 %, b ) 17.6 %, c ) 13.7 %, d ) 15.1 %, e ) 12.3 %
Output: "let the original salary be $ 100 new salary = $ 85 increase on 85 = 15 increase on 100 = 15 / 85 * 100 = 17.6 % answer is b"
So the final answer is b

Input: if 20 men can build a water fountain 56 metres long in 7 days, what length of a similar water fountain can be built by 35 men in 3 days? a ) 40 m, b ) 42 m, c ) 47 m, d ) 49 m, e ) 50 m
Output: "explanation : let the required length be x metres more men, more length built ( direct proportion ) less days, less length built ( direct proportion ) men 20 : 35 days 7 : 3 : : 56 : x therefore ( 20 x 7 x x ) = ( 35 x 3 x 56 ) x = ( 35 x 3 x 56 ) / 140 = 42 hence, the required length is 42 m. answer : b"
So the final answer is b

Input: 12 men complete a work in 9 days. after they have worked for 6 days, 6 more men join them. how many days will they take to complete the remaining work? a ) 2 days, b ) 7 days, c ) 8 days, d ) 9 days, e ) 45 days
Output: 1 man's 1 day work = 1 / 108 12 men's 6 day's work = 1 / 9 * 6 = 2 / 3 remaining work = 1 - 2 / 3 = 1 / 3 18 men's 1 day work = 1 / 108 * 18 = 1 / 6 1 / 6 work is done by them in 1 day. 1 / 3 work is done by them in 6 * 1 / 3 = 2 days. answer : a
So the final answer is a

Input: a cheese factory sells its cheese in rectangular blocks. a normal block has a volume of 5 cubic feet. if a large block has twice the width, twice the depth, and the same length of a normal block, what is the volume of cheese in a large block in cubic feet? a ) 15, b ) 30, c ) 20, d ) 18, e ) 25
Output: "volume of cube = lbh = 5 new cube l, b, h are increases of l, 2 b, 2 h new volume of cube = l * 2 b * 2 h = 4 * lbh = 4 * 5 = 20 answer : c"
So the final answer is c

Input: two trains, one from howrah to patna and the other from patna to howrah, start simultaneously. after they meet, the trains reach their destinations after 64 hours and 25 hours respectively. the ratio of their speeds is? a ) 4 : 9, b ) 4 : 3, c ) 4 : 5, d ) 5 : 8, e ) 4 : 2
Output: "let us name the trains a and b. then, ( a's speed ) : ( b's speed ) = √ b : √ a = √ 25 : √ 64 = 5 : 8 answer : d"
So the final answer is d

Input: john had a stock of 1400 books in his bookshop. he sold 62 on monday, 62 on tuesday, 60 on wednesday, 48 on thursday and 40 on friday. what percentage of the books were not sold? a ) 81.57 %, b ) 36.5 %, c ) 80.67 %, d ) 56.5 %, e ) 80.57 %
Output: "let n be the total number of books sold. hence n = 62 + 62 + 60 + 48 + 40 = 272 let m be the books not sold m = 1400 - n = 1400 - 272 = 1128 percentage books not sold / total number of books = 1128 / 1200 = 0.81 = 80.57 % correct answer e"
So the final answer is e

Input: a and b can do a piece of work in 18 days ; band c can do it in 24 days a and c can do it in 36 days. in how many days will a, band c finish it together? a ) 15 days, b ) 12 days, c ) 8 days, d ) 16 days, e ) 9 days
Output: sol. ( a + b )'s 1 day's work = ( 1 / 18 ) ( b + c )'s 1 day's work = ( 1 / 24 ) and ( a + c )'s 1 day's work = ( 1 / 36 ) adding, we get : 2 ( a + b + c )'s 1 day's work = ¬ ( 1 / 18 + 1 / 24 + 1 / 36 ) = 9 / 72 = 1 / 8 ( a + b + c )'s 1 day's work = 1 / 16 thus, a, band c together can finish the work in 16 days. ans : d
So the final answer is d

Input: p, q and r have $ 9000 among themselves. r has two - thirds of the total amount with p and q. find the amount with r? a ) 2400, b ) 3600, c ) 3998, d ) 2539, e ) 1930
Output: "b 3600 let the amount with r be $ r r = 2 / 3 ( total amount with p and q ) r = 2 / 3 ( 9000 - r ) = > 3 r = 18000 - 2 r = > 5 r = 18000 = > r = 3600."
So the final answer is b

Input: the cost of the paint is rs. 40 per kg. if 1 kg of paint covers 20 sq. ft, how much will it cost to paint outside of a cube having 30 feet each side a ) rs. 962, b ) rs. 672, c ) rs. 546, d ) rs. 10800, e ) none of these
Output: "explanation : surface area of a cube = 6 x 30 ^ 2 = 5400 sq. ft quantity of paint required = ( 5400 / 20 ) = 270 kg cost of painting = 40 x 270 = rs. 10800 answer : d"
So the final answer is d

Input: a salesman's income consists of a commission and a base salary of $ 370 per week. over the past 5 weeks, his weekly income totals have been $ 406, $ 413, $ 420, $ 436 and $ 395. what must his average ( arithmetic mean ) commission be per week over the next two weeks so that his average weekly income is $ 500 over the 7 - week period? a ) $ 150, b ) $ 345, c ) $ 365, d ) $ 715, e ) $ 730
Output: "total weekly income over 5 weeks = $ 406 + $ 413 + $ 420 + $ 436 + $ 395 = $ 2070 for avg weekly income to be $ 500 over 7 weeks, we need total weekly income over 7 weeks = $ 3500 now, $ 3500 - $ 2070 = $ 1430 from this, we subtract base salary for 2 weeks i. e $ 370 * 2 = $ 740 therefore, commission = $ 1430 - $ 740 = $ 690 for 2 weeks avg weekly commission = $ 345 answer b"
So the final answer is b

Input: a 230 m long train running at the speed of 120 km / hr crosses another train running in opposite direction at the speed of 80 km / hr in 9 sec. what is the length of the other train? a ) 230, b ) 270, c ) 260, d ) 256, e ) 298
Output: "relative speed = 120 + 80 = 200 km / hr. = 200 * 5 / 18 = 500 / 9 m / sec. let the length of the other train be x m. then, ( x + 2340 ) / 9 = 500 / 9 = > x = 270. answer : b"
So the final answer is b

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o19-tmathqa-s10-rTrue-m4096
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 7 1 3 5 7 9 True False 4096 5
Evaluating commonsenseqa :   0%|          | 0/200 [00:07<?, ?it/s]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 97, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 94, in main
    inference_main(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 90, in inference_main
    query_ids, response_ids, answers, indices = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 65, in run_model
    gen_out = model.generate(
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 1454, in generate
    return self.sample(
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 2568, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
[2023-09-22 09:27:25,799] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2987701) of binary: /home/ylu130/.conda/envs/ood/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-22_09:27:27
  host      : ia1.wse.jhu.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2987701)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 11 11 13 15 17 19 True False 4096 5
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 9 1 3 5 7 9 True False 4096 5
[2023-09-22 09:27:31,212] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:27:33,920] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 13 11 13 15 17 19 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 1 1 3 5 7 9 True False 4096 5
[2023-09-22 09:27:42,221] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
[2023-09-22 09:27:46,003] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 3 1 3 5 7 9 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 15 11 13 15 17 19 True False 4096 5
[2023-09-22 09:27:52,994] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:27:55,362] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 5 1 3 5 7 9 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o17-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 17 11 13 15 17 19 True False 4096 5
[2023-09-22 09:28:03,892] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
[2023-09-22 09:28:07,449] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o19-tmathqa-s10-rFalse --seed 10 --max-prompt-length 4096 --num-out-domain 19 11 13 15 17 19 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 7 1 3 5 7 9 True False 4096 5
[2023-09-22 09:28:14,314] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:28:16,799] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 11 11 13 15 17 19 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 9 1 3 5 7 9 True False 4096 5
[2023-09-22 09:28:25,259] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
[2023-09-22 09:28:28,875] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s20-rFalse --seed 20 --max-prompt-length 4096 --num-out-domain 1 1 3 5 7 9 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 13 11 13 15 17 19 True False 4096 5
[2023-09-22 09:28:35,831] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:28:38,281] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s20-rFalse --seed 20 --max-prompt-length 4096 --num-out-domain 3 1 3 5 7 9 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 15 11 13 15 17 19 True False 4096 5
[2023-09-22 09:28:46,643] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
[2023-09-22 09:28:50,364] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o17-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 17 11 13 15 17 19 True False 4096 5
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s20-rFalse --seed 20 --max-prompt-length 4096 --num-out-domain 5 1 3 5 7 9 True False 4096 5
[2023-09-22 09:28:57,362] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:28:59,696] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o17-tmathqa-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 17
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o17-tmathqa-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 203768.04it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s20-rFalse --seed 20 --max-prompt-length 4096 --num-out-domain 7 1 3 5 7 9 True False 4096 5
Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.51s/it][2023-09-22 09:29:12,307] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.87s/it]
 > number of parameters: 6738415616
[2023-09-22 09:29:13,423] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-09-22 09:29:13,423] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-22 09:29:13,851] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-22 09:29:13,854] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-09-22 09:29:13,854] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-22 09:29:13,854] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-22 09:29:13,854] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-09-22 09:29:13,854] [INFO] [config.py:971:print]   amp_params ................... False
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc6d0a22da0>
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   dump_state ................... False
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-09-22 09:29:13,855] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 2048
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   pld_params ................... False
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-09-22 09:29:13,856] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-09-22 09:29:13,857] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-09-22 09:29:13,857] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-09-22 09:29:13,857] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-09-22 09:29:13,857] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-09-22 09:29:13,857] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-09-22 09:29:13,857] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-09-22 09:29:13,857] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-09-22 09:29:13,857] [INFO] [config.py:971:print]   world_size ................... 1
[2023-09-22 09:29:13,857] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  True
[2023-09-22 09:29:13,857] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-22 09:29:13,857] [INFO] [config.py:971:print]   zero_enabled ................. False
[2023-09-22 09:29:13,857] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-22 09:29:13,857] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0
[2023-09-22 09:29:13,857] [INFO] [config.py:957:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/200 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: a man cycling along the road noticed that every 18 minutes a bus overtakes him and every 6 minutes he meets an oncoming bus. if all buses and the cyclist move at a constant speed, what is the time interval between consecutive buses? a ) 5 minutes, b ) 6 minutes, c ) 8 minutes, d ) 9 minutes, e ) 10 minutes
Output: let's say the distance between the buses is d. we want to determine interval = \ frac { d } { b }, where b is the speed of bus. let the speed of cyclist be c. every 18 minutes a bus overtakes cyclist : \ frac { d } { b - c } = 18, d = 18 b - 18 c ; every 6 minutes cyclist meets an oncoming bus : \ frac { d } { b + c } = 6, d = 6 b + 6 c ; d = 18 b - 18 c = 6 b + 6 c, - - > b = 2 c, - - > d = 18 b - 9 b = 9 b. interval = \ frac { d } { b } = \ frac { 9 b } { b } = 9 answer : d ( 9 minutes ).
So the final answer is d

Input: an athlete runs 200 metres race in 24 seconds. what is his speed? a ) 20 km / hr, b ) 30 km / hr, c ) 25 km / hr, d ) 35 km / hr, e ) 40 km / hr
Output: speed = dist / time = 200 / 24 200 / 24 * 18 / 5 = 40 * 3 / 4 = 30 km / hr answer b
So the final answer is b

Input: two numbers are in the ratio 3 : 4. if the sum of numbers is 63, find the numbers. a ) 27 and 36., b ) 25 and 30., c ) 23 and 44., d ) 63 and 12., e ) 12 and 36.
Output: "sum of the terms of the ratio = 3 + 4 = 7 sum of numbers = 63 therefore, first number = 3 / 7 × 63 = 27 second number = 4 / 7 × 63 = 36 therefore, the two numbers are 27 and 36. answer is a"
So the final answer is a

Input: a company wants to spend equal amounts of money for the purchase of two types of computer printers costing $ 375 and $ 150 per unit, respectively. what is the fewest number of computer printers that the company can purchase? a ) 3, b ) 4, c ) 5, d ) 6, e ) 7
Output: the smallest amount that the company can spend is the lcm of 375 and 150, which is 750 for each, which is total 1500. the number of 1 st type of computers which costing $ 375 = 750 / 375 = 2. the number of 2 nd type of computers which costing $ 150 = 750 / 150 = 5. total = 2 + 5 = 7 answer is e.
So the final answer is e

Input: a certain music store stocks 800 cellos and 600 violas. of these instruments, there are 80 cello - viola pairs, such that a cello and a viola were both made with wood from the same tree ( each tree can make at most one viola and one cello, so there are no pairs other than these 90 ). if one viola and one cello are chosen at random, what is the probability that the two instruments are made with wood from the same tree? a ) 3 / 16,000, b ) 1 / 8,100, c ) 1 / 600, d ) 1 / 90, e ) 2 / 45
Output: "solution provided by stanford 2012 is correct : 80 / 800 choosing one of the cellos which has a pair viola, 1 / 600 choosing the viola which is the pair of chosen cello - - > p = 80 / 800 * 1 / 600 = 1 / 6,000. answer : c."
So the final answer is c

Input: what is the difference between local value & face value of 7 in the numeral 65793? a ) 693, b ) 656, c ) 691, d ) 9890, e ) 10000
Output: ( local value of 7 ) - ( face value of 7 ) = ( 700 - 7 ) = 693 a
So the final answer is a

Input: in an office in singapore there are 60 % female employees. 50 % of all the male employees are computer literate. if there are total 62 % employees computer literate out of total 1100 employees, then the no. of female employees who are computer literate? a ) 462, b ) 674, c ) 672, d ) 960, e ) none
Output: "solution : total employees, = 1100 female employees, 60 % of 1100. = ( 60 * 1100 ) / 100 = 660. then male employees, = 440 50 % of male are computer literate, = 220 male computer literate. 62 % of total employees are computer literate, = ( 62 * 1100 ) / 100 = 682 computer literate. thus, female computer literate = 682 - 220 = 462. answer : option a"
So the final answer is a

Input: the cost price of 16 articles is the same as the selling price of 12 articles. find the loss / profit percentages. a ) 30 %, b ) 32.50 %, c ) 33 1 / 3 %, d ) 40 %, e ) 50 %
Output: "explanation : the gain is 4 out of 12 articles. therefore, gain percentage = 4 x 100 / 12 = 100 / 3 = 33 1 / 3 % answer : option c"
So the final answer is c

Input: mike drives his new corvette from san francisco to las vegas, a journey of 640 miles. he drives the first half of the trip at an average rate of 80 miles per hour, but has to slow down for the second half of his journey. if the second half of the trip takes him 200 percent longer than the first half, what is his average rate q in miles per hour for the entire trip? a ) q = 26.7, b ) q = 30.0, c ) q = 40.0, d ) q = 53.3, e ) q = 60.0
Output: "veritas prepofficial solution correct answer : c using the formula : time = distance / rate, we find that mike takes 4 hours to cover the first 320 miles of his trip. since the 2 nd 320 miles take 200 % longer than the first, it takes mike 8 hours longer, or 12 hours. ( note : 200 % longer than the first half is not 200 % of the first half. ) the overall time is 4 hours + 12 hours or 16 hours. since the definition of average rate = total distance traveled / total time of travel, mike's average rate = 640 / 16 or 40 miles per hour. answer choice c is correct."
So the final answer is c

Input: lisa and robert have taken the same number of photos on their school trip. lisa has taken 3 times as many photos as claire and robert has taken 20 more photos than claire. how many photos has claire taken? a ) 6, b ) 8, c ) 10, d ) 12, e ) 14
Output: "l = r l = 3 c r = c + 20 3 c = c + 20 c = 10 the answer is c."
So the final answer is c

Input: meena wrote all the numbers from 1 to 69,999 inclusive. how many digits did she write in total? a ) 278,889, b ) 308,889, c ) 338,889, d ) 368,889, e ) 398,889
Output: "1 - 9 = > 1 * 9 digits 10 - 99 = > 2 * 90 = 180 ( numbers between 10 - 99 is 90 where each has 2 digits ) 100 - 999 = > 3 * 900 = 2700 1000 - 9999 = > 4 * 9000 = 36,000 10000 - 69999 = > 5 * 60,000 = 300,000 the answer is 338,889 the answer is c."
So the final answer is c

Input: a pipe takes a hours to fill the tank. but because of a leakage it took 2 times of its original time. find the time taken by the leakage to empty the tank a ) 50 min, b ) 60 min, c ) 90 min, d ) 80 min, e ) 120 min
Output: "pipe a can do a work 60 min. lets leakage time is x ; then 1 / 60 - 1 / x = 1 / 120 x = 120 min answer : e"
So the final answer is e

Input: if x is an integer such that 0 < x < 7, 0 < x < 15, 5 > x > – 1, 3 > x > 0, and x + 2 < 4, then x is a ) 1, b ) 2, c ) 3, d ) 4, e ) 5
Output: "0 < x < 7, 0 < x < 15, 5 > x > – 1 3 > x > 0 x < 2 from above : 0 < x < 2 - - > x = 1. answer : a."
So the final answer is a

Input: a and b put in rs. 200 and rs. 400 respectively into a business. a reinvests into the business his share of the first year's profit of rs. 210 where as b does not. in what ratio should they divide the second year's profit? a ) 39 : 40, b ) 39 : 49, c ) 39 : 42, d ) 39 : 47, e ) 17 : 20
Output: "explanation : 1 : 2 a = 2 / 3 * 210 = 140 340 : 400 17 : 20 answer : e"
So the final answer is e

Input: what least number must be subtracted from 427398 so that remaining no. is divisible by 10 a ) 3, b ) 5, c ) 6, d ) 7, e ) 8
Output: "explanation : on dividing 427398 by 10 we get the remainder 8, so 8 should be subtracted answer : option e"
So the final answer is e

Input: a store reduced the price of all items in the store by 8 % on the first day and by another 10 % on the second day. the price of items on the second day was what percent of the price before the first reduction took place? a ) 80.0, b ) 80.9, c ) 81.0, d ) 81.1, e ) 82.8
Output: consider price of the all items as $ 100 after a initial reduction of 8 % price becomes = 0.92 * 100 = $ 92 after the final reduction of 10 % price becomes = 0.9 * 92 = $ 82.8 price of all items on second day is 82.8 % of price on first day correct answer option e
So the final answer is e

Input: a car traveling at a certain constant speed takes 5 seconds longer to travel 1 km than it would take to travel 1 km at 75 km / hour. at what speed, in km / hr, is the car traveling? a ) 70, b ) 72, c ) 74, d ) 75, e ) 78
Output: "time to cover 1 kilometer at 80 kilometers per hour is 1 / 75 hours = 3,600 / 75 seconds = 48 seconds ; time to cover 1 kilometer at regular speed is 48 + 5 = 53 seconds = 53 / 3,600 hours = 1 / 70 hours ; so, we get that to cover 1 kilometer 1 / 70 hours is needed - - > regular speed 70 kilometers per hour ( rate is a reciprocal of time or rate = distance / time ). answer : a"
So the final answer is a

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o17-tmathqa-s20-rTrue-m4096
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s20-rFalse --seed 20 --max-prompt-length 4096 --num-out-domain 9 1 3 5 7 9 True False 4096 5
Evaluating commonsenseqa :   0%|          | 0/200 [00:07<?, ?it/s]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 97, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 94, in main
    inference_main(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 90, in inference_main
    query_ids, response_ids, answers, indices = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 65, in run_model
    gen_out = model.generate(
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 1454, in generate
    return self.sample(
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 2568, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
[2023-09-22 09:29:23,699] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2989795) of binary: /home/ylu130/.conda/envs/ood/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-22_09:29:24
  host      : ia1.wse.jhu.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2989795)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o19-tmathqa-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 19 11 13 15 17 19 True False 4096 5
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 1 1 3 5 7 9 True False 4096 5
[2023-09-22 09:29:29,104] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-22 09:29:31,807] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o19-tmathqa-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 19
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o19-tmathqa-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 190359.71it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 1
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o1-tmathqa-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 843624.10it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.12s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.84s/it]
 > number of parameters: 6738415616
[2023-09-22 09:29:47,141] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-09-22 09:29:47,141] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-22 09:29:47,600] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-22 09:29:47,602] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-09-22 09:29:47,603] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-22 09:29:47,603] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-22 09:29:47,603] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-09-22 09:29:47,603] [INFO] [config.py:971:print]   amp_params ................... False
[2023-09-22 09:29:47,603] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-22 09:29:47,603] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7faddc52eda0>
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   dump_state ................... False
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-09-22 09:29:47,604] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 2048
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   pld_params ................... False
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   world_size ................... 1
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  True
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   zero_enabled ................. False
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-22 09:29:47,605] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0
[2023-09-22 09:29:47,606] [INFO] [config.py:957:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/200 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: a man cycling along the road noticed that every 18 minutes a bus overtakes him and every 6 minutes he meets an oncoming bus. if all buses and the cyclist move at a constant speed, what is the time interval between consecutive buses? a ) 5 minutes, b ) 6 minutes, c ) 8 minutes, d ) 9 minutes, e ) 10 minutes
Output: let's say the distance between the buses is d. we want to determine interval = \ frac { d } { b }, where b is the speed of bus. let the speed of cyclist be c. every 18 minutes a bus overtakes cyclist : \ frac { d } { b - c } = 18, d = 18 b - 18 c ; every 6 minutes cyclist meets an oncoming bus : \ frac { d } { b + c } = 6, d = 6 b + 6 c ; d = 18 b - 18 c = 6 b + 6 c, - - > b = 2 c, - - > d = 18 b - 9 b = 9 b. interval = \ frac { d } { b } = \ frac { 9 b } { b } = 9 answer : d ( 9 minutes ).
So the final answer is d

Input: an athlete runs 200 metres race in 24 seconds. what is his speed? a ) 20 km / hr, b ) 30 km / hr, c ) 25 km / hr, d ) 35 km / hr, e ) 40 km / hr
Output: speed = dist / time = 200 / 24 200 / 24 * 18 / 5 = 40 * 3 / 4 = 30 km / hr answer b
So the final answer is b

Input: two numbers are in the ratio 3 : 4. if the sum of numbers is 63, find the numbers. a ) 27 and 36., b ) 25 and 30., c ) 23 and 44., d ) 63 and 12., e ) 12 and 36.
Output: "sum of the terms of the ratio = 3 + 4 = 7 sum of numbers = 63 therefore, first number = 3 / 7 × 63 = 27 second number = 4 / 7 × 63 = 36 therefore, the two numbers are 27 and 36. answer is a"
So the final answer is a

Input: a company wants to spend equal amounts of money for the purchase of two types of computer printers costing $ 375 and $ 150 per unit, respectively. what is the fewest number of computer printers that the company can purchase? a ) 3, b ) 4, c ) 5, d ) 6, e ) 7
Output: the smallest amount that the company can spend is the lcm of 375 and 150, which is 750 for each, which is total 1500. the number of 1 st type of computers which costing $ 375 = 750 / 375 = 2. the number of 2 nd type of computers which costing $ 150 = 750 / 150 = 5. total = 2 + 5 = 7 answer is e.
So the final answer is e

Input: a certain music store stocks 800 cellos and 600 violas. of these instruments, there are 80 cello - viola pairs, such that a cello and a viola were both made with wood from the same tree ( each tree can make at most one viola and one cello, so there are no pairs other than these 90 ). if one viola and one cello are chosen at random, what is the probability that the two instruments are made with wood from the same tree? a ) 3 / 16,000, b ) 1 / 8,100, c ) 1 / 600, d ) 1 / 90, e ) 2 / 45
Output: "solution provided by stanford 2012 is correct : 80 / 800 choosing one of the cellos which has a pair viola, 1 / 600 choosing the viola which is the pair of chosen cello - - > p = 80 / 800 * 1 / 600 = 1 / 6,000. answer : c."
So the final answer is c

Input: what is the difference between local value & face value of 7 in the numeral 65793? a ) 693, b ) 656, c ) 691, d ) 9890, e ) 10000
Output: ( local value of 7 ) - ( face value of 7 ) = ( 700 - 7 ) = 693 a
So the final answer is a

Input: in an office in singapore there are 60 % female employees. 50 % of all the male employees are computer literate. if there are total 62 % employees computer literate out of total 1100 employees, then the no. of female employees who are computer literate? a ) 462, b ) 674, c ) 672, d ) 960, e ) none
Output: "solution : total employees, = 1100 female employees, 60 % of 1100. = ( 60 * 1100 ) / 100 = 660. then male employees, = 440 50 % of male are computer literate, = 220 male computer literate. 62 % of total employees are computer literate, = ( 62 * 1100 ) / 100 = 682 computer literate. thus, female computer literate = 682 - 220 = 462. answer : option a"
So the final answer is a

Input: the cost price of 16 articles is the same as the selling price of 12 articles. find the loss / profit percentages. a ) 30 %, b ) 32.50 %, c ) 33 1 / 3 %, d ) 40 %, e ) 50 %
Output: "explanation : the gain is 4 out of 12 articles. therefore, gain percentage = 4 x 100 / 12 = 100 / 3 = 33 1 / 3 % answer : option c"
So the final answer is c

Input: mike drives his new corvette from san francisco to las vegas, a journey of 640 miles. he drives the first half of the trip at an average rate of 80 miles per hour, but has to slow down for the second half of his journey. if the second half of the trip takes him 200 percent longer than the first half, what is his average rate q in miles per hour for the entire trip? a ) q = 26.7, b ) q = 30.0, c ) q = 40.0, d ) q = 53.3, e ) q = 60.0
Output: "veritas prepofficial solution correct answer : c using the formula : time = distance / rate, we find that mike takes 4 hours to cover the first 320 miles of his trip. since the 2 nd 320 miles take 200 % longer than the first, it takes mike 8 hours longer, or 12 hours. ( note : 200 % longer than the first half is not 200 % of the first half. ) the overall time is 4 hours + 12 hours or 16 hours. since the definition of average rate = total distance traveled / total time of travel, mike's average rate = 640 / 16 or 40 miles per hour. answer choice c is correct."
So the final answer is c

Input: lisa and robert have taken the same number of photos on their school trip. lisa has taken 3 times as many photos as claire and robert has taken 20 more photos than claire. how many photos has claire taken? a ) 6, b ) 8, c ) 10, d ) 12, e ) 14
Output: "l = r l = 3 c r = c + 20 3 c = c + 20 c = 10 the answer is c."
So the final answer is c

Input: meena wrote all the numbers from 1 to 69,999 inclusive. how many digits did she write in total? a ) 278,889, b ) 308,889, c ) 338,889, d ) 368,889, e ) 398,889
Output: "1 - 9 = > 1 * 9 digits 10 - 99 = > 2 * 90 = 180 ( numbers between 10 - 99 is 90 where each has 2 digits ) 100 - 999 = > 3 * 900 = 2700 1000 - 9999 = > 4 * 9000 = 36,000 10000 - 69999 = > 5 * 60,000 = 300,000 the answer is 338,889 the answer is c."
So the final answer is c

Input: a pipe takes a hours to fill the tank. but because of a leakage it took 2 times of its original time. find the time taken by the leakage to empty the tank a ) 50 min, b ) 60 min, c ) 90 min, d ) 80 min, e ) 120 min
Output: "pipe a can do a work 60 min. lets leakage time is x ; then 1 / 60 - 1 / x = 1 / 120 x = 120 min answer : e"
So the final answer is e

Input: if x is an integer such that 0 < x < 7, 0 < x < 15, 5 > x > – 1, 3 > x > 0, and x + 2 < 4, then x is a ) 1, b ) 2, c ) 3, d ) 4, e ) 5
Output: "0 < x < 7, 0 < x < 15, 5 > x > – 1 3 > x > 0 x < 2 from above : 0 < x < 2 - - > x = 1. answer : a."
So the final answer is a

Input: a and b put in rs. 200 and rs. 400 respectively into a business. a reinvests into the business his share of the first year's profit of rs. 210 where as b does not. in what ratio should they divide the second year's profit? a ) 39 : 40, b ) 39 : 49, c ) 39 : 42, d ) 39 : 47, e ) 17 : 20
Output: "explanation : 1 : 2 a = 2 / 3 * 210 = 140 340 : 400 17 : 20 answer : e"
So the final answer is e

Input: what least number must be subtracted from 427398 so that remaining no. is divisible by 10 a ) 3, b ) 5, c ) 6, d ) 7, e ) 8
Output: "explanation : on dividing 427398 by 10 we get the remainder 8, so 8 should be subtracted answer : option e"
So the final answer is e

Input: a store reduced the price of all items in the store by 8 % on the first day and by another 10 % on the second day. the price of items on the second day was what percent of the price before the first reduction took place? a ) 80.0, b ) 80.9, c ) 81.0, d ) 81.1, e ) 82.8
Output: consider price of the all items as $ 100 after a initial reduction of 8 % price becomes = 0.92 * 100 = $ 92 after the final reduction of 10 % price becomes = 0.9 * 92 = $ 82.8 price of all items on second day is 82.8 % of price on first day correct answer option e
So the final answer is e

Input: a car traveling at a certain constant speed takes 5 seconds longer to travel 1 km than it would take to travel 1 km at 75 km / hour. at what speed, in km / hr, is the car traveling? a ) 70, b ) 72, c ) 74, d ) 75, e ) 78
Output: "time to cover 1 kilometer at 80 kilometers per hour is 1 / 75 hours = 3,600 / 75 seconds = 48 seconds ; time to cover 1 kilometer at regular speed is 48 + 5 = 53 seconds = 53 / 3,600 hours = 1 / 70 hours ; so, we get that to cover 1 kilometer 1 / 70 hours is needed - - > regular speed 70 kilometers per hour ( rate is a reciprocal of time or rate = distance / time ). answer : a"
So the final answer is a

Input: find the compound ratio of ( 1 : 2 ), ( 2 : 3 ) and ( 3 : 4 ) is a ) 1 : 2, b ) 3 : 5, c ) 5 : 7, d ) 1 : 4, e ) 2 : 1
Output: "required ratio = 1 / 2 * 2 / 3 * 3 / 4 = 1 / 4 = 1 : 4 answer is d"
So the final answer is d

Input: in a manufacturing plant, it takes 48 machines 4 hours of continuous work to fill 8 standard orders. at this rate, how many hours of continuous work by 72 machines are required to fill 12 standard orders? a ) 3, b ) 6, c ) 8, d ) 9, e ) 12
Output: "the choices give away the answer.. 48 machines take 4 hours to fill 8 standard orders.. in next eq we are doubling the machines from 48 to 72, but the work is not doubling ( only 1 1 / 2 times ), = 4 * 48 / 72 * 12 / 8 = 4 ans a"
So the final answer is a

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o19-tmathqa-s20-rTrue-m4096
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.62s/it]
 > number of parameters: 6738415616
[2023-09-22 09:29:48,746] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-09-22 09:29:48,746] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-22 09:29:49,187] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-22 09:29:49,189] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   amp_params ................... False
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7efc3cf2eef0>
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   dump_state ................... False
[2023-09-22 09:29:49,190] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 2048
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-22 09:29:49,191] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   pld_params ................... False
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   world_size ................... 1
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  True
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   zero_enabled ................. False
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-22 09:29:49,192] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0
[2023-09-22 09:29:49,192] [INFO] [config.py:957:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/200 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: the mean of 30 values was 140. it was detected on rechecking that one value 145 was wrongly copied as 135 for the computation of the mean. find the correct mean. a ) 151, b ) 140.33, c ) 152, d ) 148, e ) none of the above
Output: "corrected mean = 140 × 30 − 135 + 145 / 30 = 4200 − 135 + 145 / 30 = 4210 / 30 = 140.33 answer b"
So the final answer is b

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o1-tmathqa-s30-rTrue-m4096
Evaluating commonsenseqa :   0%|          | 0/200 [00:07<?, ?it/s]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 97, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 94, in main
    inference_main(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 90, in inference_main
    query_ids, response_ids, answers, indices = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 65, in run_model
    gen_out = model.generate(
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 1454, in generate
    return self.sample(
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 2568, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2990270) of binary: /home/ylu130/.conda/envs/ood/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-22_09:29:56
  host      : ia1.wse.jhu.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2990270)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s20-rFalse --seed 20 --max-prompt-length 4096 --num-out-domain 11 11 13 15 17 19 True False 4096 5
[2023-09-22 09:30:01,467] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s20-rFalse --seed 20 --max-prompt-length 4096 --num-out-domain 13 11 13 15 17 19 True False 4096 5
[2023-09-22 09:30:13,737] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s20-rFalse --seed 20 --max-prompt-length 4096 --num-out-domain 15 11 13 15 17 19 True False 4096 5
[2023-09-22 09:30:26,168] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o17-tmathqa-s20-rFalse --seed 20 --max-prompt-length 4096 --num-out-domain 17 11 13 15 17 19 True False 4096 5
[2023-09-22 09:30:38,527] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o19-tmathqa-s20-rFalse --seed 20 --max-prompt-length 4096 --num-out-domain 19 11 13 15 17 19 True False 4096 5
[2023-09-22 09:30:50,870] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 11 11 13 15 17 19 True False 4096 5
[2023-09-22 09:31:03,206] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 13 11 13 15 17 19 True False 4096 5
[2023-09-22 09:31:15,499] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 15 11 13 15 17 19 True False 4096 5
[2023-09-22 09:31:27,948] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o17-tmathqa-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 17 11 13 15 17 19 True False 4096 5
[2023-09-22 09:31:40,287] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o19-tmathqa-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 19 11 13 15 17 19 True False 4096 5
[2023-09-22 09:31:52,777] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o19-tmathqa-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 19
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o19-tmathqa-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 188433.49it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.58s/it]
 > number of parameters: 6738415616
[2023-09-22 09:32:10,148] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-09-22 09:32:10,148] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-22 09:32:10,591] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-22 09:32:10,594] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-09-22 09:32:10,594] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-22 09:32:10,594] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-22 09:32:10,594] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-09-22 09:32:10,594] [INFO] [config.py:971:print]   amp_params ................... False
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb93622eda0>
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   dump_state ................... False
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-09-22 09:32:10,595] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 2048
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   pld_params ................... False
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-09-22 09:32:10,596] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-09-22 09:32:10,597] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-09-22 09:32:10,597] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-09-22 09:32:10,597] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-09-22 09:32:10,597] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-09-22 09:32:10,597] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-09-22 09:32:10,597] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-09-22 09:32:10,597] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-09-22 09:32:10,597] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-09-22 09:32:10,597] [INFO] [config.py:971:print]   world_size ................... 1
[2023-09-22 09:32:10,597] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  True
[2023-09-22 09:32:10,597] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-22 09:32:10,597] [INFO] [config.py:971:print]   zero_enabled ................. False
[2023-09-22 09:32:10,597] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-22 09:32:10,597] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0
[2023-09-22 09:32:10,597] [INFO] [config.py:957:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/200 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: the mean of 30 values was 140. it was detected on rechecking that one value 145 was wrongly copied as 135 for the computation of the mean. find the correct mean. a ) 151, b ) 140.33, c ) 152, d ) 148, e ) none of the above
Output: "corrected mean = 140 × 30 − 135 + 145 / 30 = 4200 − 135 + 145 / 30 = 4210 / 30 = 140.33 answer b"
So the final answer is b

Input: the measurement of a rectangular box with lid is 25 cmx 24 cmx 18 cm. find the volume of the largest sphere that can be inscribed in the box ( in terms of π cm 3 ). ( hint : the lowest measure of rectangular box represents the diameter of the largest sphere ) a ) 288, b ) 2302, c ) 2304, d ) 8640, e ) 964
Output: "d = 24, r = 12 ; volume of the largest sphere = 4 / 3 π r 3 = 4 / 3 * π * 12 * 12 * 12 = 2304 π cm 3 answer : c"
So the final answer is c

Input: a certain fraction has the same ratio to 1 / 33, as 3 / 4 does to 7 / 11. what is this certain fraction? a ) 1 / 14, b ) 1 / 18, c ) 1 / 21, d ) 1 / 25, e ) 1 / 28
Output: "x / ( 1 / 33 ) = ( 3 / 4 ) / ( 7 / 11 ) x = 3 * 11 * 1 / 33 * 4 * 7 = 1 / 28 the answer is e."
So the final answer is e

Input: what is the sum of all remainders obtained when the first 110 natural numbers are divided by 9? a ) 397, b ) 401, c ) 403, d ) 405, e ) 399
Output: "a positive integer can give only the following 9 remainders when divided by 9 : 1, 2, 3, 4, 5, 6, 7, 8, and 0. 1 divided by 9 gives the remainder of 1 ; 2 divided by 9 gives the remainder of 2 ;... 8 divided by 9 gives the remainder of 8 ; 9 divided by 9 gives the remainder of 0. we'll have 11 such blocks, since 99 / 9 = 11. the last will be : 91 divided by 9 gives the remainder of 1 ; 92 divided by 9 gives the remainder of 2 ;... 98 divided by 9 gives the remainder of 8 ; 99 divided by 9 gives the remainder of 0. the last number, 100, gives the remainder of 1 when divided by 9, thus the sum of all remainders will be : 11 ( 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 0 ) + 1 = 403. answer : c."
So the final answer is c

Input: a ’ s speed is 20 / 15 times that of b. if a and b run a race, what part of the length of the race should a give b as a head start, so that the race ends in a dead heat? a ) 1 / 17, b ) 3 / 17, c ) 1 / 10, d ) 5 / 20, e ) 3 / 10
Output: "we have the ratio of a ’ s speed and b ’ s speed. this means, we know how much distance a covers compared with b in the same time. this is what the beginning of the race will look like : ( start ) a _________ b ______________________________ if a covers 20 meters, b covers 15 meters in that time. so if the race is 20 meters long, when a reaches the finish line, b would be 5 meters behind him. if we want the race to end in a dead heat, we want b to be at the finish line too at the same time. this means b should get a head start of 5 meters so that he doesn ’ t need to cover that. in that case, the time required by a ( to cover 20 meters ) would be the same as the time required by b ( to cover 15 meters ) to reach the finish line. so b should get a head start of 5 / 20 th of the race. answer ( d )"
So the final answer is d

Input: find the average of all the numbers between 6 and 34 which are divisible by 7. a ) 18, b ) 20, c ) 17.5, d ) 30, e ) 32
Output: "solution average = ( 7 + 14 + 21 + 28 ) / 4 ) = 70 / 4 = 17.5. answer c"
So the final answer is c

Input: instead of multiplying a number by 5, the number is divided by 10. what is the percentage of error obtained? a ) 96 %, b ) 98 %, c ) 95 %, d ) 97 %, e ) 96 %
Output: "let the number be x the right number is 5 x the wrong number is x / 10 error is ( 5 x - x / 10 ) = 49 x / 10 percentage of error is ( ( 49 x / 10 ) / 5 x ) * 100 = 98 % answer : b"
So the final answer is b

Input: by selling an article at rs. 150, a profit of 25 % is made. find its cost price? a ) s. 486, b ) s. 455, c ) s. 487, d ) s. 120, e ) s. 489
Output: "sp = 150 cp = ( sp ) * [ 100 / ( 100 + p ) ] = 150 * [ 100 / ( 100 + 25 ) ] = 150 * [ 100 / 125 ] = rs. 120 answer : d"
So the final answer is d

Input: if 1 = 6, 2 = 12, 3 = 18, 4 = 24, 5 = 30, then 6 =? a ) 5, b ) 3, c ) 1, d ) 7, e ) 9
Output: solution : 1 as stated 1 = 6 = > 6 = 1 answer c
So the final answer is c

Input: 62 small identical cubes are used to form a large cube. how many more cubes are needed to add one top layer of small cube all over the surface of the large cube? a ) 64, b ) 128, c ) 152, d ) 154, e ) 256
Output: "62 small cube will make a large cube with 4 cubes in each line i. e. adding one layer will require one cube at each end and hence new cube will have 6 cubes in each line. total number of small cubes in new cube = 6 ^ 3 = 216 extra cube required = 216 - 62 = 154 hence, d is the answer."
So the final answer is d

Input: a boat running upstream takes 640 min to cover a certain distance, while it takes 280 min to cover the same distance running down stream. what is the ratio between the speed of the boat and speed of water current respectively? a ) 23 : 7, b ) 22 : 9, c ) 23 : 8, d ) 23 : 9, e ) 23 : 11
Output: let the speed of the boat = x and speed of the current = y relative speed in upstream = x - y and in downstream = x + y let the distance'd'is to be covered in downstream and upstream, then time taken in upstream = d / ( x - y ) = 640 or d = 640 ( x - y ) - - - - ( i ) and time taken in downstream = d / ( x + y ) = 280 or d = 280 ( x + y ) - - - - ( ii ) comparing ( i ) & ( ii ), 640 ( x - y ) = 280 ( x + y ) = > 16 ( x - y ) = 7 ( x + y ) = > 16 x - 16 y = 7 x - 7 y = > 9 x = 23 y = > x / y = 23 / 9 answer : d
So the final answer is d

Input: there are 2 positive integers x and y. what is the probability that x + y is odd? a ) 1 / 4, b ) 1 / 2, c ) 1 / 3, d ) 1 / 5, e ) 1 / 9
Output: explanation : the addition of two integer results in an odd number only if any one of the integers is an odd number. the probability of an odd among the two integer is 1 / 2. hence, the required probability is 1 / 2. answer : b
So the final answer is b

Input: two numbers have a h. c. f of 9 and a product of two numbers is 1800. find the l. c. m of the two numbers? a ) 200, b ) 150, c ) 160, d ) 170, e ) 180
Output: "l. c. m of two numbers is given by ( product of the two numbers ) / ( h. c. f of the two numbers ) = 1800 / 9 = 200. answer : a"
So the final answer is a

Input: a metallic sheet is of rectangular shape with dimensions 48 m x 36 m. from each of its corners, a square is cut off so as to make an open box. if the length of the square is 8 m, the volume of the box ( in m 3 ) is a ) 3220, b ) 4830, c ) 5120, d ) 6420, e ) none
Output: "solution clearly, l = ( 48 - 16 ) m = 32 m ‹ = › b = ( 36 - 16 ) m = 20 m, ‹ = › h = 8 m.. volume of the box = ( 32 x 20 x 8 ) m 3 = 5120 m 3. answer c"
So the final answer is c

Input: amar takes as much time in running 24 meters as a car takes in covering 60 meters. what will be the distance covered by amar during the time the car covers 2.2 km? a ) 700 m, b ) 500 m, c ) 870 m, d ) 880 m, e ) 840 m
Output: "distance covered by amar = 24 / 60 ( 2.2 km ) = 2 / 5 ( 2200 ) = 880 m answer : d"
So the final answer is d

Input: the area of a triangle is with base 3 m and height 10 m? a ) 88 m 2, b ) 10 m 2, c ) 66 m 2, d ) 15 m 2, e ) 31 m 2
Output: "1 / 2 * 3 * 5 = 15 m 2 answer : c"
So the final answer is c

Input: there is a total of 84 marbles in a box, each of which is red, green, blue, or white. if one marble is drawn from the box at random, the probability that it will be white is 1 / 4 and the probability that it will be green is 1 / 7. what is the probability that the marble will be either red or blue? a ) 4 / 7, b ) 9 / 14, c ) 11 / 14, d ) 17 / 28, e ) 23 / 28
Output: "p ( red or blue ) = 1 - p ( white ) - p ( green ) = 28 / 28 - 7 / 28 - 4 / 28 = 17 / 28 the answer is d."
So the final answer is d

Input: if 60 % of ( x - y ) = 20 % of ( x + y ), then what percent of x is y? a ) 50 %, b ) 44.4 %, c ) 22.2 %, d ) 33.3 %, e ) 25 %
Output: "60 % of ( x - y ) = 20 % of ( x + y ) 60 / 100 ( x - y ) = 20 / 100 ( x + y ) 4 x = 8 y required percentage = y / x * 100 = 4 y / 8 y * 100 = 50 % answer is a"
So the final answer is a

Input: the sum of ages of 4 children born 2 years different each is 36 years. what is the age of the elder child? a ) 8, b ) 9, c ) 10, d ) 12, e ) 17
Output: "let the ages of children be x, ( x + 2 ), ( x + 4 ), ( x + 6 ) years. then, x + ( x + 2 ) + ( x + 4 ) + ( x + 6 ) = 36 4 x = 24 x = 6. x + 6 = 6 + 6 = 12 answer : d"
So the final answer is d

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o19-tmathqa-s30-rTrue-m4096
Evaluating commonsenseqa :   0%|          | 1/200 [02:23<7:56:36, 143.70s/it]Evaluating commonsenseqa :   0%|          | 0/200 [00:07<?, ?it/s]
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 97, in <module>
    main()
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 94, in main
    inference_main(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 90, in inference_main
    query_ids, response_ids, answers, indices = run_model(args, tokenizer, model, dataset, device)
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 65, in run_model
    gen_out = model.generate(
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 1454, in generate
    return self.sample(
  File "/home/ylu130/workspace/in-context-generalization/transformers/src/transformers/generation/utils.py", line 2568, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2992556) of binary: /home/ylu130/.conda/envs/ood/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-22_09:32:20
  host      : ia1.wse.jhu.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2992556)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o11-tmathqa-s30-rFalse --seed 30 --max-prompt-length 4096 --num-out-domain 11 11 13 15 17 19 True False 4096 5
[2023-09-22 09:32:25,159] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s30-rFalse --seed 30 --max-prompt-length 4096 --num-out-domain 13 11 13 15 17 19 True False 4096 5
[2023-09-22 09:32:37,323] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o15-tmathqa-s30-rFalse --seed 30 --max-prompt-length 4096 --num-out-domain 15 11 13 15 17 19 True False 4096 5
[2023-09-22 09:32:49,694] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o17-tmathqa-s30-rFalse --seed 30 --max-prompt-length 4096 --num-out-domain 17 11 13 15 17 19 True False 4096 5
[2023-09-22 09:33:01,966] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10181 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o19-tmathqa-s30-rFalse --seed 30 --max-prompt-length 4096 --num-out-domain 19 11 13 15 17 19 True False 4096 5
[2023-09-22 09:33:14,275] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
Evaluating commonsenseqa :   1%|          | 2/200 [04:43<7:47:07, 141.55s/it]Evaluating commonsenseqa :   2%|▏         | 3/200 [07:00<7:37:00, 139.19s/it]Evaluating commonsenseqa :   2%|▏         | 4/200 [09:16<7:30:58, 138.05s/it]Evaluating commonsenseqa :   2%|▎         | 5/200 [11:30<7:24:26, 136.75s/it]Evaluating commonsenseqa :   3%|▎         | 6/200 [13:46<7:21:05, 136.42s/it]Evaluating commonsenseqa :   4%|▎         | 7/200 [16:00<7:15:46, 135.47s/it]Evaluating commonsenseqa :   4%|▍         | 8/200 [18:16<7:14:27, 135.77s/it]Evaluating commonsenseqa :   4%|▍         | 9/200 [20:34<7:14:06, 136.37s/it]Evaluating commonsenseqa :   5%|▌         | 10/200 [22:50<7:12:07, 136.46s/it]Evaluating commonsenseqa :   6%|▌         | 11/200 [25:08<7:11:15, 136.91s/it]Evaluating commonsenseqa :   6%|▌         | 12/200 [27:23<7:07:13, 136.35s/it]Evaluating commonsenseqa :   6%|▋         | 13/200 [29:41<7:06:29, 136.84s/it]Evaluating commonsenseqa :   7%|▋         | 14/200 [31:56<7:01:43, 136.04s/it]Evaluating commonsenseqa :   8%|▊         | 15/200 [34:10<6:57:33, 135.42s/it]Evaluating commonsenseqa :   8%|▊         | 16/200 [36:27<6:56:55, 135.95s/it]Evaluating commonsenseqa :   8%|▊         | 17/200 [38:45<6:56:18, 136.49s/it]Evaluating commonsenseqa :   9%|▉         | 18/200 [40:11<6:08:36, 121.52s/it]Evaluating commonsenseqa :  10%|▉         | 19/200 [41:28<5:26:15, 108.15s/it]Evaluating commonsenseqa :  10%|█         | 20/200 [43:44<5:49:42, 116.57s/it]Evaluating commonsenseqa :  10%|█         | 21/200 [45:58<6:03:22, 121.80s/it]Evaluating commonsenseqa :  11%|█         | 22/200 [47:37<5:40:48, 114.88s/it]Evaluating commonsenseqa :  12%|█▏        | 23/200 [49:55<5:59:03, 121.72s/it]Evaluating commonsenseqa :  12%|█▏        | 24/200 [52:09<6:07:52, 125.41s/it]Evaluating commonsenseqa :  12%|█▎        | 25/200 [53:50<5:44:55, 118.26s/it]Evaluating commonsenseqa :  13%|█▎        | 26/200 [56:09<6:00:21, 124.26s/it]Evaluating commonsenseqa :  14%|█▎        | 27/200 [57:22<5:14:15, 108.99s/it]Evaluating commonsenseqa :  14%|█▍        | 28/200 [59:37<5:34:45, 116.78s/it]Evaluating commonsenseqa :  14%|█▍        | 29/200 [1:01:54<5:50:09, 122.86s/it]Evaluating commonsenseqa :  15%|█▌        | 30/200 [1:02:38<4:40:59, 99.17s/it] Evaluating commonsenseqa :  16%|█▌        | 31/200 [1:04:53<5:09:20, 109.83s/it]Evaluating commonsenseqa :  16%|█▌        | 32/200 [1:07:07<5:28:09, 117.20s/it]Evaluating commonsenseqa :  16%|█▋        | 33/200 [1:09:21<5:40:03, 122.18s/it]Evaluating commonsenseqa :  17%|█▋        | 34/200 [1:11:37<5:49:45, 126.42s/it]Evaluating commonsenseqa :  18%|█▊        | 35/200 [1:13:50<5:52:38, 128.24s/it]Evaluating commonsenseqa :  18%|█▊        | 36/200 [1:16:07<5:57:56, 130.95s/it]Evaluating commonsenseqa :  18%|█▊        | 37/200 [1:18:22<5:59:05, 132.18s/it]Evaluating commonsenseqa :  19%|█▉        | 38/200 [1:20:35<5:57:26, 132.38s/it]Evaluating commonsenseqa :  20%|█▉        | 39/200 [1:22:53<5:59:36, 134.02s/it]Evaluating commonsenseqa :  20%|██        | 40/200 [1:24:07<5:09:37, 116.11s/it]Evaluating commonsenseqa :  20%|██        | 41/200 [1:26:19<5:20:45, 121.04s/it]Evaluating commonsenseqa :  21%|██        | 42/200 [1:28:35<5:30:08, 125.37s/it]Evaluating commonsenseqa :  22%|██▏       | 43/200 [1:30:54<5:38:47, 129.47s/it]Evaluating commonsenseqa :  22%|██▏       | 44/200 [1:33:11<5:42:35, 131.77s/it]Evaluating commonsenseqa :  22%|██▎       | 45/200 [1:35:26<5:43:01, 132.78s/it]Evaluating commonsenseqa :  23%|██▎       | 46/200 [1:36:51<5:04:04, 118.47s/it]Evaluating commonsenseqa :  24%|██▎       | 47/200 [1:39:09<5:16:40, 124.19s/it]Evaluating commonsenseqa :  24%|██▍       | 48/200 [1:41:21<5:20:59, 126.71s/it]Evaluating commonsenseqa :  24%|██▍       | 49/200 [1:43:35<5:24:22, 128.89s/it]Evaluating commonsenseqa :  25%|██▌       | 50/200 [1:45:53<5:28:55, 131.57s/it]Evaluating commonsenseqa :  26%|██▌       | 51/200 [1:48:12<5:31:51, 133.63s/it]Evaluating commonsenseqa :  26%|██▌       | 52/200 [1:50:28<5:31:33, 134.41s/it]Evaluating commonsenseqa :  26%|██▋       | 53/200 [1:52:45<5:31:32, 135.33s/it]Evaluating commonsenseqa :  27%|██▋       | 54/200 [1:54:59<5:27:46, 134.70s/it]Evaluating commonsenseqa :  28%|██▊       | 55/200 [1:57:16<5:27:15, 135.42s/it]Evaluating commonsenseqa :  28%|██▊       | 56/200 [1:59:33<5:26:25, 136.01s/it]Evaluating commonsenseqa :  28%|██▊       | 57/200 [2:01:49<5:23:52, 135.89s/it]Evaluating commonsenseqa :  29%|██▉       | 58/200 [2:04:07<5:23:19, 136.61s/it]Evaluating commonsenseqa :  30%|██▉       | 59/200 [2:05:14<4:32:02, 115.76s/it]Evaluating commonsenseqa :  30%|███       | 60/200 [2:07:30<4:44:12, 121.81s/it]Evaluating commonsenseqa :  30%|███       | 61/200 [2:09:47<4:52:21, 126.20s/it]Evaluating commonsenseqa :  31%|███       | 62/200 [2:11:37<4:39:45, 121.63s/it]Evaluating commonsenseqa :  32%|███▏      | 63/200 [2:13:53<4:47:22, 125.85s/it]Evaluating commonsenseqa :  32%|███▏      | 64/200 [2:16:06<4:50:13, 128.04s/it]Evaluating commonsenseqa :  32%|███▎      | 65/200 [2:18:20<4:52:07, 129.83s/it]Evaluating commonsenseqa :  33%|███▎      | 66/200 [2:20:34<4:52:34, 131.00s/it]Evaluating commonsenseqa :  34%|███▎      | 67/200 [2:22:48<4:52:01, 131.74s/it]Evaluating commonsenseqa :  34%|███▍      | 68/200 [2:25:06<4:54:15, 133.75s/it]Evaluating commonsenseqa :  34%|███▍      | 69/200 [2:26:53<4:34:45, 125.84s/it]Evaluating commonsenseqa :  35%|███▌      | 70/200 [2:29:07<4:38:02, 128.33s/it]Evaluating commonsenseqa :  36%|███▌      | 71/200 [2:31:22<4:39:48, 130.14s/it]Evaluating commonsenseqa :  36%|███▌      | 72/200 [2:33:36<4:40:29, 131.48s/it]Evaluating commonsenseqa :  36%|███▋      | 73/200 [2:34:53<4:03:21, 114.97s/it]Evaluating commonsenseqa :  37%|███▋      | 74/200 [2:37:11<4:16:12, 122.00s/it]Evaluating commonsenseqa :  38%|███▊      | 75/200 [2:39:29<4:23:59, 126.72s/it]Evaluating commonsenseqa :  38%|███▊      | 76/200 [2:41:11<4:06:19, 119.19s/it]Evaluating commonsenseqa :  38%|███▊      | 77/200 [2:42:50<3:52:12, 113.27s/it]Evaluating commonsenseqa :  39%|███▉      | 78/200 [2:45:08<4:05:02, 120.52s/it]Evaluating commonsenseqa :  40%|███▉      | 79/200 [2:47:26<4:13:44, 125.82s/it]Evaluating commonsenseqa :  40%|████      | 80/200 [2:48:39<3:39:53, 109.94s/it]Evaluating commonsenseqa :  40%|████      | 81/200 [2:50:55<3:53:38, 117.80s/it]Evaluating commonsenseqa :  41%|████      | 82/200 [2:53:09<4:01:18, 122.70s/it]Evaluating commonsenseqa :  42%|████▏     | 83/200 [2:55:23<4:06:11, 126.26s/it]Evaluating commonsenseqa :  42%|████▏     | 84/200 [2:57:39<4:09:43, 129.17s/it]Evaluating commonsenseqa :  42%|████▎     | 85/200 [2:59:55<4:11:01, 130.97s/it]Evaluating commonsenseqa :  43%|████▎     | 86/200 [3:02:09<4:10:35, 131.89s/it]Evaluating commonsenseqa :  44%|████▎     | 87/200 [3:04:21<4:08:39, 132.03s/it]Evaluating commonsenseqa :  44%|████▍     | 88/200 [3:06:39<4:09:34, 133.70s/it]Evaluating commonsenseqa :  44%|████▍     | 89/200 [3:08:55<4:08:47, 134.49s/it]Evaluating commonsenseqa :  45%|████▌     | 90/200 [3:11:10<4:06:57, 134.70s/it]Evaluating commonsenseqa :  46%|████▌     | 91/200 [3:13:27<4:05:41, 135.24s/it]Evaluating commonsenseqa :  46%|████▌     | 92/200 [3:15:39<4:02:07, 134.51s/it]Evaluating commonsenseqa :  46%|████▋     | 93/200 [3:17:52<3:58:35, 133.79s/it]Evaluating commonsenseqa :  47%|████▋     | 94/200 [3:20:04<3:55:30, 133.31s/it]Evaluating commonsenseqa :  48%|████▊     | 95/200 [3:22:16<3:52:35, 132.91s/it]Evaluating commonsenseqa :  48%|████▊     | 96/200 [3:24:31<3:51:24, 133.51s/it]Evaluating commonsenseqa :  48%|████▊     | 97/200 [3:26:47<3:50:53, 134.50s/it]Evaluating commonsenseqa :  49%|████▉     | 98/200 [3:29:01<3:48:07, 134.19s/it]Evaluating commonsenseqa :  50%|████▉     | 99/200 [3:31:14<3:45:19, 133.85s/it]Evaluating commonsenseqa :  50%|█████     | 100/200 [3:33:32<3:45:22, 135.23s/it]Evaluating commonsenseqa :  50%|█████     | 101/200 [3:35:28<3:33:14, 129.23s/it]Evaluating commonsenseqa :  51%|█████     | 102/200 [3:37:39<3:32:08, 129.89s/it]Evaluating commonsenseqa :  52%|█████▏    | 103/200 [3:39:55<3:33:01, 131.77s/it]Evaluating commonsenseqa :  52%|█████▏    | 104/200 [3:42:09<3:31:38, 132.28s/it]Evaluating commonsenseqa :  52%|█████▎    | 105/200 [3:44:26<3:31:59, 133.89s/it]Evaluating commonsenseqa :  53%|█████▎    | 106/200 [3:46:42<3:30:32, 134.39s/it]Evaluating commonsenseqa :  54%|█████▎    | 107/200 [3:48:54<3:27:26, 133.83s/it]Evaluating commonsenseqa :  54%|█████▍    | 108/200 [3:51:06<3:24:03, 133.08s/it]Evaluating commonsenseqa :  55%|█████▍    | 109/200 [3:53:23<3:23:35, 134.24s/it]Evaluating commonsenseqa :  55%|█████▌    | 110/200 [3:55:34<3:19:57, 133.31s/it]Evaluating commonsenseqa :  56%|█████▌    | 111/200 [3:57:05<2:59:10, 120.79s/it]Evaluating commonsenseqa :  56%|█████▌    | 112/200 [3:59:20<3:03:13, 124.93s/it]Evaluating commonsenseqa :  56%|█████▋    | 113/200 [4:01:38<3:06:57, 128.94s/it]Evaluating commonsenseqa :  57%|█████▋    | 114/200 [4:03:52<3:07:05, 130.52s/it]Evaluating commonsenseqa :  57%|█████▊    | 115/200 [4:06:06<3:06:15, 131.47s/it]Evaluating commonsenseqa :  58%|█████▊    | 116/200 [4:08:22<3:06:02, 132.89s/it]Evaluating commonsenseqa :  58%|█████▊    | 117/200 [4:10:39<3:05:19, 133.97s/it]Evaluating commonsenseqa :  59%|█████▉    | 118/200 [4:12:57<3:04:41, 135.15s/it]Evaluating commonsenseqa :  60%|█████▉    | 119/200 [4:14:31<2:45:49, 122.83s/it]Evaluating commonsenseqa :  60%|██████    | 120/200 [4:16:45<2:48:21, 126.27s/it]Evaluating commonsenseqa :  60%|██████    | 121/200 [4:18:44<2:43:16, 124.01s/it]Evaluating commonsenseqa :  61%|██████    | 122/200 [4:21:00<2:46:03, 127.74s/it]Evaluating commonsenseqa :  62%|██████▏   | 123/200 [4:23:15<2:46:38, 129.86s/it]Evaluating commonsenseqa :  62%|██████▏   | 124/200 [4:25:31<2:46:40, 131.59s/it]Evaluating commonsenseqa :  62%|██████▎   | 125/200 [4:27:47<2:46:14, 132.99s/it]Evaluating commonsenseqa :  63%|██████▎   | 126/200 [4:30:01<2:44:20, 133.25s/it]Evaluating commonsenseqa :  64%|██████▎   | 127/200 [4:32:17<2:43:14, 134.17s/it]Evaluating commonsenseqa :  64%|██████▍   | 128/200 [4:34:33<2:41:30, 134.59s/it]Evaluating commonsenseqa :  64%|██████▍   | 129/200 [4:36:00<2:22:35, 120.50s/it]Evaluating commonsenseqa :  65%|██████▌   | 130/200 [4:37:48<2:15:54, 116.49s/it]Evaluating commonsenseqa :  66%|██████▌   | 131/200 [4:38:04<1:39:21, 86.40s/it] Evaluating commonsenseqa :  66%|██████▌   | 132/200 [4:40:19<1:54:34, 101.10s/it]Evaluating commonsenseqa :  66%|██████▋   | 133/200 [4:41:31<1:43:16, 92.48s/it] Evaluating commonsenseqa :  67%|██████▋   | 134/200 [4:43:46<1:55:42, 105.19s/it]Evaluating commonsenseqa :  68%|██████▊   | 135/200 [4:46:02<2:03:49, 114.30s/it]Evaluating commonsenseqa :  68%|██████▊   | 136/200 [4:48:17<2:08:27, 120.43s/it]Evaluating commonsenseqa :  68%|██████▊   | 137/200 [4:50:30<2:10:39, 124.44s/it]Evaluating commonsenseqa :  69%|██████▉   | 138/200 [4:52:46<2:12:08, 127.88s/it]Evaluating commonsenseqa :  70%|██████▉   | 139/200 [4:55:01<2:11:57, 129.79s/it]Evaluating commonsenseqa :  70%|███████   | 140/200 [4:57:16<2:11:37, 131.62s/it]Evaluating commonsenseqa :  70%|███████   | 141/200 [4:59:31<2:10:20, 132.55s/it]Evaluating commonsenseqa :  71%|███████   | 142/200 [5:01:49<2:09:33, 134.02s/it]Evaluating commonsenseqa :  72%|███████▏  | 143/200 [5:04:02<2:07:01, 133.71s/it]Evaluating commonsenseqa :  72%|███████▏  | 144/200 [5:06:17<2:05:18, 134.26s/it]Evaluating commonsenseqa :  72%|███████▎  | 145/200 [5:08:31<2:02:58, 134.16s/it]Evaluating commonsenseqa :  73%|███████▎  | 146/200 [5:10:46<2:01:01, 134.47s/it]Evaluating commonsenseqa :  74%|███████▎  | 147/200 [5:12:24<1:49:09, 123.58s/it]Evaluating commonsenseqa :  74%|███████▍  | 148/200 [5:14:41<1:50:27, 127.46s/it]Evaluating commonsenseqa :  74%|███████▍  | 149/200 [5:16:56<1:50:17, 129.75s/it]Evaluating commonsenseqa :  75%|███████▌  | 150/200 [5:17:33<1:24:56, 101.94s/it]Evaluating commonsenseqa :  76%|███████▌  | 151/200 [5:19:47<1:30:57, 111.39s/it]Evaluating commonsenseqa :  76%|███████▌  | 152/200 [5:20:47<1:16:47, 95.98s/it] Evaluating commonsenseqa :  76%|███████▋  | 153/200 [5:23:00<1:23:57, 107.19s/it]Evaluating commonsenseqa :  77%|███████▋  | 154/200 [5:25:13<1:28:06, 114.92s/it]Evaluating commonsenseqa :  78%|███████▊  | 155/200 [5:26:55<1:23:16, 111.03s/it]Evaluating commonsenseqa :  78%|███████▊  | 156/200 [5:29:08<1:26:17, 117.66s/it]Evaluating commonsenseqa :  78%|███████▊  | 157/200 [5:31:24<1:28:17, 123.21s/it]Evaluating commonsenseqa :  79%|███████▉  | 158/200 [5:33:40<1:28:52, 126.96s/it]Evaluating commonsenseqa :  80%|███████▉  | 159/200 [5:35:10<1:19:11, 115.89s/it]Evaluating commonsenseqa :  80%|████████  | 160/200 [5:37:27<1:21:25, 122.13s/it]Evaluating commonsenseqa :  80%|████████  | 161/200 [5:39:39<1:21:24, 125.24s/it]Evaluating commonsenseqa :  81%|████████  | 162/200 [5:41:17<1:14:06, 117.01s/it]Evaluating commonsenseqa :  82%|████████▏ | 163/200 [5:43:31<1:15:23, 122.27s/it]Evaluating commonsenseqa :  82%|████████▏ | 164/200 [5:45:44<1:15:15, 125.43s/it]Evaluating commonsenseqa :  82%|████████▎ | 165/200 [5:46:58<1:04:07, 109.94s/it]Evaluating commonsenseqa :  83%|████████▎ | 166/200 [5:49:15<1:06:52, 118.03s/it]Evaluating commonsenseqa :  84%|████████▎ | 167/200 [5:51:28<1:07:24, 122.57s/it]Evaluating commonsenseqa :  84%|████████▍ | 168/200 [5:53:42<1:07:15, 126.12s/it]Evaluating commonsenseqa :  84%|████████▍ | 169/200 [5:55:58<1:06:35, 128.88s/it]Evaluating commonsenseqa :  85%|████████▌ | 170/200 [5:58:12<1:05:09, 130.33s/it]Evaluating commonsenseqa :  86%|████████▌ | 171/200 [6:00:27<1:03:45, 131.90s/it]Evaluating commonsenseqa :  86%|████████▌ | 172/200 [6:02:39<1:01:31, 131.85s/it]Evaluating commonsenseqa :  86%|████████▋ | 173/200 [6:03:45<50:27, 112.12s/it]  Evaluating commonsenseqa :  87%|████████▋ | 174/200 [6:06:01<51:44, 119.42s/it]Evaluating commonsenseqa :  88%|████████▊ | 175/200 [6:08:19<52:01, 124.86s/it]Evaluating commonsenseqa :  88%|████████▊ | 176/200 [6:10:33<51:04, 127.67s/it]Evaluating commonsenseqa :  88%|████████▊ | 177/200 [6:12:47<49:36, 129.43s/it]Evaluating commonsenseqa :  89%|████████▉ | 178/200 [6:14:59<47:47, 130.34s/it]Evaluating commonsenseqa :  90%|████████▉ | 179/200 [6:16:56<44:11, 126.25s/it]Evaluating commonsenseqa :  90%|█████████ | 180/200 [6:18:09<36:45, 110.29s/it]Evaluating commonsenseqa :  90%|█████████ | 181/200 [6:20:24<37:16, 117.69s/it]Evaluating commonsenseqa :  91%|█████████ | 182/200 [6:22:42<37:08, 123.81s/it]Evaluating commonsenseqa :  92%|█████████▏| 183/200 [6:24:58<36:09, 127.62s/it]Evaluating commonsenseqa :  92%|█████████▏| 184/200 [6:27:10<34:18, 128.67s/it]Evaluating commonsenseqa :  92%|█████████▎| 185/200 [6:29:04<31:06, 124.41s/it]Evaluating commonsenseqa :  93%|█████████▎| 186/200 [6:31:19<29:46, 127.61s/it]Evaluating commonsenseqa :  94%|█████████▎| 187/200 [6:33:35<28:11, 130.12s/it]Evaluating commonsenseqa :  94%|█████████▍| 188/200 [6:35:20<24:31, 122.60s/it]Evaluating commonsenseqa :  94%|█████████▍| 189/200 [6:37:35<23:09, 126.33s/it]Evaluating commonsenseqa :  95%|█████████▌| 190/200 [6:39:47<21:19, 127.98s/it]Evaluating commonsenseqa :  96%|█████████▌| 191/200 [6:42:03<19:33, 130.43s/it]Evaluating commonsenseqa :  96%|█████████▌| 192/200 [6:44:17<17:30, 131.36s/it]Evaluating commonsenseqa :  96%|█████████▋| 193/200 [6:46:35<15:34, 133.45s/it]Evaluating commonsenseqa :  97%|█████████▋| 194/200 [6:48:50<13:24, 134.03s/it]Evaluating commonsenseqa :  98%|█████████▊| 195/200 [6:50:10<09:48, 117.79s/it]Evaluating commonsenseqa :  98%|█████████▊| 196/200 [6:52:27<08:14, 123.52s/it]Evaluating commonsenseqa :  98%|█████████▊| 197/200 [6:54:45<06:23, 127.86s/it]Evaluating commonsenseqa :  99%|█████████▉| 198/200 [6:55:51<03:38, 109.24s/it]Evaluating commonsenseqa : 100%|█████████▉| 199/200 [6:58:05<01:56, 116.61s/it]Evaluating commonsenseqa : 100%|██████████| 200/200 [6:58:57<00:00, 97.28s/it] Evaluating commonsenseqa : 100%|██████████| 200/200 [6:58:57<00:00, 125.69s/it]
name: commonsenseqa | avg. gen lenth: 263.98 | time: 25138.472324371338s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 3 1 3 5 7 9 True False 4096 5
[2023-09-22 16:28:56,296] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s30-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 3
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o3-tmathqa-s30-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 1090908.77it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.25s/it]
 > number of parameters: 6738415616
[2023-09-22 16:29:09,574] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-09-22 16:29:09,575] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-22 16:29:09,922] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-22 16:29:09,924] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-09-22 16:29:09,924] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-22 16:29:09,924] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-22 16:29:09,924] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-09-22 16:29:09,924] [INFO] [config.py:971:print]   amp_params ................... False
[2023-09-22 16:29:09,924] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-22 16:29:09,924] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-09-22 16:29:09,924] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-09-22 16:29:09,924] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-09-22 16:29:09,924] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-09-22 16:29:09,924] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa839222ef0>
[2023-09-22 16:29:09,924] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-09-22 16:29:09,924] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-22 16:29:09,924] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-09-22 16:29:09,924] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   dump_state ................... False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 2048
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   pld_params ................... False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   world_size ................... 1
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  True
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-22 16:29:09,925] [INFO] [config.py:971:print]   zero_enabled ................. False
[2023-09-22 16:29:09,926] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-22 16:29:09,926] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0
[2023-09-22 16:29:09,926] [INFO] [config.py:957:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/200 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: the mean of 30 values was 140. it was detected on rechecking that one value 145 was wrongly copied as 135 for the computation of the mean. find the correct mean. a ) 151, b ) 140.33, c ) 152, d ) 148, e ) none of the above
Output: "corrected mean = 140 × 30 − 135 + 145 / 30 = 4200 − 135 + 145 / 30 = 4210 / 30 = 140.33 answer b"
So the final answer is b

Input: the measurement of a rectangular box with lid is 25 cmx 24 cmx 18 cm. find the volume of the largest sphere that can be inscribed in the box ( in terms of π cm 3 ). ( hint : the lowest measure of rectangular box represents the diameter of the largest sphere ) a ) 288, b ) 2302, c ) 2304, d ) 8640, e ) 964
Output: "d = 24, r = 12 ; volume of the largest sphere = 4 / 3 π r 3 = 4 / 3 * π * 12 * 12 * 12 = 2304 π cm 3 answer : c"
So the final answer is c

Input: a certain fraction has the same ratio to 1 / 33, as 3 / 4 does to 7 / 11. what is this certain fraction? a ) 1 / 14, b ) 1 / 18, c ) 1 / 21, d ) 1 / 25, e ) 1 / 28
Output: "x / ( 1 / 33 ) = ( 3 / 4 ) / ( 7 / 11 ) x = 3 * 11 * 1 / 33 * 4 * 7 = 1 / 28 the answer is e."
So the final answer is e

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o3-tmathqa-s30-rTrue-m4096
Evaluating commonsenseqa :   0%|          | 1/200 [02:04<6:52:28, 124.36s/it]Evaluating commonsenseqa :   1%|          | 2/200 [04:10<6:53:21, 125.26s/it]Evaluating commonsenseqa :   2%|▏         | 3/200 [06:14<6:50:16, 124.96s/it]Evaluating commonsenseqa :   2%|▏         | 4/200 [08:01<6:25:11, 117.91s/it]Evaluating commonsenseqa :   2%|▎         | 5/200 [10:04<6:28:26, 119.52s/it]Evaluating commonsenseqa :   3%|▎         | 6/200 [12:07<6:30:32, 120.79s/it]Evaluating commonsenseqa :   4%|▎         | 7/200 [14:08<6:28:50, 120.88s/it]Evaluating commonsenseqa :   4%|▍         | 8/200 [16:10<6:27:50, 121.20s/it]Evaluating commonsenseqa :   4%|▍         | 9/200 [18:14<6:28:27, 122.03s/it]Evaluating commonsenseqa :   5%|▌         | 10/200 [20:18<6:28:43, 122.76s/it]Evaluating commonsenseqa :   6%|▌         | 11/200 [22:20<6:25:19, 122.32s/it]Evaluating commonsenseqa :   6%|▌         | 12/200 [24:22<6:23:13, 122.31s/it]Evaluating commonsenseqa :   6%|▋         | 13/200 [26:23<6:19:49, 121.87s/it]Evaluating commonsenseqa :   7%|▋         | 14/200 [28:28<6:20:41, 122.80s/it]Evaluating commonsenseqa :   8%|▊         | 15/200 [30:29<6:17:37, 122.47s/it]Evaluating commonsenseqa :   8%|▊         | 16/200 [32:32<6:15:55, 122.58s/it]Evaluating commonsenseqa :   8%|▊         | 17/200 [34:35<6:13:52, 122.58s/it]Evaluating commonsenseqa :   9%|▉         | 18/200 [36:38<6:12:37, 122.84s/it]Evaluating commonsenseqa :  10%|▉         | 19/200 [38:39<6:08:36, 122.19s/it]Evaluating commonsenseqa :  10%|█         | 20/200 [40:42<6:07:05, 122.36s/it]Evaluating commonsenseqa :  10%|█         | 21/200 [42:44<6:05:23, 122.48s/it]Evaluating commonsenseqa :  11%|█         | 22/200 [44:46<6:02:07, 122.06s/it]Evaluating commonsenseqa :  12%|█▏        | 23/200 [46:49<6:01:04, 122.40s/it]Evaluating commonsenseqa :  12%|█▏        | 24/200 [48:13<5:25:30, 110.97s/it]Evaluating commonsenseqa :  12%|█▎        | 25/200 [50:17<5:35:07, 114.90s/it]Evaluating commonsenseqa :  13%|█▎        | 26/200 [52:19<5:39:20, 117.01s/it]Evaluating commonsenseqa :  14%|█▎        | 27/200 [54:21<5:41:21, 118.39s/it]Evaluating commonsenseqa :  14%|█▍        | 28/200 [56:24<5:43:46, 119.92s/it]Evaluating commonsenseqa :  14%|█▍        | 29/200 [58:25<5:42:48, 120.29s/it]Evaluating commonsenseqa :  15%|█▌        | 30/200 [1:00:25<5:40:38, 120.23s/it]Evaluating commonsenseqa :  16%|█▌        | 31/200 [1:02:28<5:40:19, 120.83s/it]Evaluating commonsenseqa :  16%|█▌        | 32/200 [1:04:32<5:41:22, 121.92s/it]Evaluating commonsenseqa :  16%|█▋        | 33/200 [1:06:33<5:38:26, 121.59s/it]Evaluating commonsenseqa :  17%|█▋        | 34/200 [1:08:37<5:38:05, 122.20s/it]Evaluating commonsenseqa :  18%|█▊        | 35/200 [1:10:38<5:35:34, 122.03s/it]Evaluating commonsenseqa :  18%|█▊        | 36/200 [1:12:41<5:33:59, 122.19s/it]Evaluating commonsenseqa :  18%|█▊        | 37/200 [1:14:40<5:29:12, 121.18s/it]Evaluating commonsenseqa :  19%|█▉        | 38/200 [1:16:43<5:29:01, 121.86s/it]Evaluating commonsenseqa :  20%|█▉        | 39/200 [1:18:44<5:26:13, 121.58s/it]Evaluating commonsenseqa :  20%|██        | 40/200 [1:19:52<4:41:43, 105.65s/it]Evaluating commonsenseqa :  20%|██        | 41/200 [1:21:53<4:52:09, 110.25s/it]Evaluating commonsenseqa :  21%|██        | 42/200 [1:23:53<4:57:36, 113.02s/it]Evaluating commonsenseqa :  22%|██▏       | 43/200 [1:25:53<5:01:24, 115.19s/it]Evaluating commonsenseqa :  22%|██▏       | 44/200 [1:27:56<5:05:16, 117.41s/it]Evaluating commonsenseqa :  22%|██▎       | 45/200 [1:29:58<5:07:13, 118.93s/it]Evaluating commonsenseqa :  23%|██▎       | 46/200 [1:32:01<5:08:15, 120.10s/it]Evaluating commonsenseqa :  24%|██▎       | 47/200 [1:34:03<5:07:54, 120.75s/it]Evaluating commonsenseqa :  24%|██▍       | 48/200 [1:36:08<5:09:02, 121.99s/it]Evaluating commonsenseqa :  24%|██▍       | 49/200 [1:38:13<5:08:51, 122.72s/it]Evaluating commonsenseqa :  25%|██▌       | 50/200 [1:38:41<3:55:59, 94.40s/it] Evaluating commonsenseqa :  26%|██▌       | 51/200 [1:40:43<4:14:58, 102.67s/it]Evaluating commonsenseqa :  26%|██▌       | 52/200 [1:42:47<4:28:59, 109.05s/it]Evaluating commonsenseqa :  26%|██▋       | 53/200 [1:44:48<4:36:19, 112.79s/it]Evaluating commonsenseqa :  27%|██▋       | 54/200 [1:46:50<4:41:10, 115.55s/it]Evaluating commonsenseqa :  28%|██▊       | 55/200 [1:48:14<4:15:47, 105.84s/it]Evaluating commonsenseqa :  28%|██▊       | 56/200 [1:50:17<4:27:01, 111.26s/it]Evaluating commonsenseqa :  28%|██▊       | 57/200 [1:52:17<4:31:03, 113.73s/it]Evaluating commonsenseqa :  29%|██▉       | 58/200 [1:54:20<4:35:53, 116.57s/it]Evaluating commonsenseqa :  30%|██▉       | 59/200 [1:56:26<4:40:17, 119.27s/it]Evaluating commonsenseqa :  30%|███       | 60/200 [1:57:25<3:56:12, 101.23s/it]Evaluating commonsenseqa :  30%|███       | 61/200 [1:58:56<3:47:48, 98.34s/it] Evaluating commonsenseqa :  31%|███       | 62/200 [2:00:59<4:03:10, 105.73s/it]Evaluating commonsenseqa :  32%|███▏      | 63/200 [2:03:01<4:12:18, 110.50s/it]Evaluating commonsenseqa :  32%|███▏      | 64/200 [2:05:04<4:18:45, 114.16s/it]Evaluating commonsenseqa :  32%|███▎      | 65/200 [2:07:06<4:22:35, 116.71s/it]Evaluating commonsenseqa :  33%|███▎      | 66/200 [2:09:09<4:24:18, 118.35s/it]Evaluating commonsenseqa :  34%|███▎      | 67/200 [2:10:57<4:15:55, 115.46s/it]Evaluating commonsenseqa :  34%|███▍      | 68/200 [2:12:58<4:17:38, 117.11s/it]Evaluating commonsenseqa :  34%|███▍      | 69/200 [2:14:59<4:18:16, 118.30s/it]Evaluating commonsenseqa :  35%|███▌      | 70/200 [2:16:26<3:55:31, 108.70s/it]Evaluating commonsenseqa :  36%|███▌      | 71/200 [2:17:22<3:20:08, 93.09s/it] Evaluating commonsenseqa :  36%|███▌      | 72/200 [2:19:23<3:36:31, 101.50s/it]Evaluating commonsenseqa :  36%|███▋      | 73/200 [2:21:26<3:48:04, 107.76s/it]Evaluating commonsenseqa :  37%|███▋      | 74/200 [2:23:30<3:56:24, 112.57s/it]Evaluating commonsenseqa :  38%|███▊      | 75/200 [2:25:31<4:00:21, 115.37s/it]Evaluating commonsenseqa :  38%|███▊      | 76/200 [2:27:30<4:00:42, 116.47s/it]Evaluating commonsenseqa :  38%|███▊      | 77/200 [2:29:26<3:58:19, 116.26s/it]Evaluating commonsenseqa :  39%|███▉      | 78/200 [2:31:27<3:59:09, 117.62s/it]Evaluating commonsenseqa :  40%|███▉      | 79/200 [2:32:52<3:37:28, 107.84s/it]Evaluating commonsenseqa :  40%|████      | 80/200 [2:34:55<3:44:55, 112.47s/it]Evaluating commonsenseqa :  40%|████      | 81/200 [2:36:58<3:48:53, 115.40s/it]Evaluating commonsenseqa :  41%|████      | 82/200 [2:39:00<3:51:07, 117.52s/it]Evaluating commonsenseqa :  42%|████▏     | 83/200 [2:41:03<3:52:04, 119.01s/it]Evaluating commonsenseqa :  42%|████▏     | 84/200 [2:43:06<3:52:58, 120.50s/it]Evaluating commonsenseqa :  42%|████▎     | 85/200 [2:45:07<3:51:04, 120.56s/it]Evaluating commonsenseqa :  43%|████▎     | 86/200 [2:47:09<3:49:52, 120.98s/it]Evaluating commonsenseqa :  44%|████▎     | 87/200 [2:48:13<3:15:46, 103.95s/it]Evaluating commonsenseqa :  44%|████▍     | 88/200 [2:50:13<3:23:03, 108.78s/it]Evaluating commonsenseqa :  44%|████▍     | 89/200 [2:52:17<3:29:21, 113.17s/it]Evaluating commonsenseqa :  45%|████▌     | 90/200 [2:54:18<3:31:49, 115.54s/it]Evaluating commonsenseqa :  46%|████▌     | 91/200 [2:56:18<3:32:09, 116.79s/it]Evaluating commonsenseqa :  46%|████▌     | 92/200 [2:58:20<3:33:07, 118.40s/it]Evaluating commonsenseqa :  46%|████▋     | 93/200 [3:00:21<3:32:48, 119.33s/it]Evaluating commonsenseqa :  47%|████▋     | 94/200 [3:02:22<3:31:28, 119.70s/it]Evaluating commonsenseqa :  48%|████▊     | 95/200 [3:04:24<3:30:38, 120.37s/it]Evaluating commonsenseqa :  48%|████▊     | 96/200 [3:06:28<3:30:31, 121.46s/it]Evaluating commonsenseqa :  48%|████▊     | 97/200 [3:08:32<3:29:54, 122.28s/it]Evaluating commonsenseqa :  49%|████▉     | 98/200 [3:10:34<3:27:55, 122.31s/it]Evaluating commonsenseqa :  50%|████▉     | 99/200 [3:12:36<3:25:38, 122.16s/it]Evaluating commonsenseqa :  50%|█████     | 100/200 [3:14:40<3:24:19, 122.59s/it]Evaluating commonsenseqa :  50%|█████     | 101/200 [3:16:40<3:21:01, 121.84s/it]Evaluating commonsenseqa :  51%|█████     | 102/200 [3:18:43<3:19:30, 122.15s/it]Evaluating commonsenseqa :  52%|█████▏    | 103/200 [3:20:47<3:18:40, 122.90s/it]Evaluating commonsenseqa :  52%|█████▏    | 104/200 [3:22:49<3:15:56, 122.47s/it]Evaluating commonsenseqa :  52%|█████▎    | 105/200 [3:24:50<3:13:14, 122.04s/it]Evaluating commonsenseqa :  53%|█████▎    | 106/200 [3:25:22<2:28:47, 94.98s/it] Evaluating commonsenseqa :  54%|█████▎    | 107/200 [3:27:24<2:39:50, 103.12s/it]Evaluating commonsenseqa :  54%|█████▍    | 108/200 [3:28:59<2:34:31, 100.78s/it]Evaluating commonsenseqa :  55%|█████▍    | 109/200 [3:30:59<2:41:27, 106.45s/it]Evaluating commonsenseqa :  55%|█████▌    | 110/200 [3:33:00<2:46:25, 110.95s/it]Evaluating commonsenseqa :  56%|█████▌    | 111/200 [3:35:02<2:49:20, 114.17s/it]Evaluating commonsenseqa :  56%|█████▌    | 112/200 [3:37:05<2:51:27, 116.90s/it]Evaluating commonsenseqa :  56%|█████▋    | 113/200 [3:39:09<2:52:33, 119.01s/it]Evaluating commonsenseqa :  57%|█████▋    | 114/200 [3:41:12<2:52:15, 120.17s/it]Evaluating commonsenseqa :  57%|█████▊    | 115/200 [3:43:15<2:51:20, 120.94s/it]Evaluating commonsenseqa :  58%|█████▊    | 116/200 [3:45:18<2:50:18, 121.65s/it]Evaluating commonsenseqa :  58%|█████▊    | 117/200 [3:47:23<2:49:32, 122.56s/it]Evaluating commonsenseqa :  59%|█████▉    | 118/200 [3:49:25<2:47:33, 122.61s/it]Evaluating commonsenseqa :  60%|█████▉    | 119/200 [3:51:27<2:45:03, 122.27s/it]Evaluating commonsenseqa :  60%|██████    | 120/200 [3:53:27<2:42:07, 121.59s/it]Evaluating commonsenseqa :  60%|██████    | 121/200 [3:55:33<2:41:44, 122.84s/it]Evaluating commonsenseqa :  61%|██████    | 122/200 [3:57:37<2:40:19, 123.33s/it]Evaluating commonsenseqa :  62%|██████▏   | 123/200 [3:59:39<2:37:36, 122.81s/it]Evaluating commonsenseqa :  62%|██████▏   | 124/200 [4:01:40<2:35:00, 122.37s/it]Evaluating commonsenseqa :  62%|██████▎   | 125/200 [4:03:42<2:32:46, 122.21s/it]Evaluating commonsenseqa :  63%|██████▎   | 126/200 [4:05:46<2:31:20, 122.70s/it]Evaluating commonsenseqa :  64%|██████▎   | 127/200 [4:07:50<2:29:54, 123.21s/it]Evaluating commonsenseqa :  64%|██████▍   | 128/200 [4:09:53<2:27:33, 122.97s/it]Evaluating commonsenseqa :  64%|██████▍   | 129/200 [4:11:53<2:24:35, 122.19s/it]Evaluating commonsenseqa :  65%|██████▌   | 130/200 [4:13:53<2:21:55, 121.64s/it]Evaluating commonsenseqa :  66%|██████▌   | 131/200 [4:15:55<2:19:45, 121.52s/it]Evaluating commonsenseqa :  66%|██████▌   | 132/200 [4:17:59<2:18:38, 122.34s/it]Evaluating commonsenseqa :  66%|██████▋   | 133/200 [4:20:02<2:16:58, 122.67s/it]Evaluating commonsenseqa :  67%|██████▋   | 134/200 [4:22:06<2:15:22, 123.07s/it]Evaluating commonsenseqa :  68%|██████▊   | 135/200 [4:24:10<2:13:32, 123.27s/it]Evaluating commonsenseqa :  68%|██████▊   | 136/200 [4:26:15<2:11:52, 123.64s/it]Evaluating commonsenseqa :  68%|██████▊   | 137/200 [4:28:17<2:09:22, 123.22s/it]Evaluating commonsenseqa :  69%|██████▉   | 138/200 [4:30:05<2:02:32, 118.59s/it]Evaluating commonsenseqa :  70%|██████▉   | 139/200 [4:32:07<2:01:46, 119.78s/it]Evaluating commonsenseqa :  70%|███████   | 140/200 [4:34:10<2:00:39, 120.66s/it]Evaluating commonsenseqa :  70%|███████   | 141/200 [4:36:13<1:59:24, 121.43s/it]Evaluating commonsenseqa :  71%|███████   | 142/200 [4:38:15<1:57:26, 121.49s/it]Evaluating commonsenseqa :  72%|███████▏  | 143/200 [4:40:20<1:56:28, 122.61s/it]Evaluating commonsenseqa :  72%|███████▏  | 144/200 [4:42:23<1:54:27, 122.64s/it]Evaluating commonsenseqa :  72%|███████▎  | 145/200 [4:44:28<1:53:15, 123.55s/it]Evaluating commonsenseqa :  73%|███████▎  | 146/200 [4:46:32<1:51:13, 123.58s/it]Evaluating commonsenseqa :  74%|███████▎  | 147/200 [4:48:36<1:49:14, 123.66s/it]Evaluating commonsenseqa :  74%|███████▍  | 148/200 [4:50:37<1:46:31, 122.91s/it]Evaluating commonsenseqa :  74%|███████▍  | 149/200 [4:52:38<1:44:06, 122.49s/it]Evaluating commonsenseqa :  75%|███████▌  | 150/200 [4:54:43<1:42:40, 123.21s/it]Evaluating commonsenseqa :  76%|███████▌  | 151/200 [4:56:47<1:40:44, 123.35s/it]Evaluating commonsenseqa :  76%|███████▌  | 152/200 [4:58:47<1:37:58, 122.46s/it]Evaluating commonsenseqa :  76%|███████▋  | 153/200 [5:00:50<1:36:03, 122.62s/it]Evaluating commonsenseqa :  77%|███████▋  | 154/200 [5:02:51<1:33:38, 122.14s/it]Evaluating commonsenseqa :  78%|███████▊  | 155/200 [5:04:56<1:32:09, 122.87s/it]Evaluating commonsenseqa :  78%|███████▊  | 156/200 [5:06:57<1:29:35, 122.18s/it]Evaluating commonsenseqa :  78%|███████▊  | 157/200 [5:08:59<1:27:35, 122.22s/it]Evaluating commonsenseqa :  79%|███████▉  | 158/200 [5:11:01<1:25:32, 122.21s/it]Evaluating commonsenseqa :  80%|███████▉  | 159/200 [5:12:40<1:18:38, 115.09s/it]Evaluating commonsenseqa :  80%|████████  | 160/200 [5:14:39<1:17:41, 116.54s/it]Evaluating commonsenseqa :  80%|████████  | 161/200 [5:16:44<1:17:23, 119.07s/it]Evaluating commonsenseqa :  81%|████████  | 162/200 [5:18:48<1:16:20, 120.55s/it]Evaluating commonsenseqa :  82%|████████▏ | 163/200 [5:20:51<1:14:48, 121.30s/it]Evaluating commonsenseqa :  82%|████████▏ | 164/200 [5:22:52<1:12:34, 120.97s/it]Evaluating commonsenseqa :  82%|████████▎ | 165/200 [5:24:53<1:10:40, 121.16s/it]Evaluating commonsenseqa :  83%|████████▎ | 166/200 [5:26:53<1:08:28, 120.83s/it]Evaluating commonsenseqa :  84%|████████▎ | 167/200 [5:28:34<1:03:06, 114.73s/it]Evaluating commonsenseqa :  84%|████████▍ | 168/200 [5:30:39<1:02:54, 117.95s/it]Evaluating commonsenseqa :  84%|████████▍ | 169/200 [5:32:43<1:01:52, 119.76s/it]Evaluating commonsenseqa :  85%|████████▌ | 170/200 [5:34:47<1:00:32, 121.08s/it]Evaluating commonsenseqa :  86%|████████▌ | 171/200 [5:36:51<58:50, 121.74s/it]  Evaluating commonsenseqa :  86%|████████▌ | 172/200 [5:38:52<56:46, 121.65s/it]Evaluating commonsenseqa :  86%|████████▋ | 173/200 [5:40:52<54:28, 121.05s/it]Evaluating commonsenseqa :  87%|████████▋ | 174/200 [5:42:52<52:23, 120.89s/it]Evaluating commonsenseqa :  88%|████████▊ | 175/200 [5:44:57<50:51, 122.07s/it]Evaluating commonsenseqa :  88%|████████▊ | 176/200 [5:47:01<49:06, 122.75s/it]Evaluating commonsenseqa :  88%|████████▊ | 177/200 [5:49:06<47:14, 123.25s/it]Evaluating commonsenseqa :  89%|████████▉ | 178/200 [5:51:10<45:13, 123.36s/it]Evaluating commonsenseqa :  90%|████████▉ | 179/200 [5:53:10<42:54, 122.58s/it]Evaluating commonsenseqa :  90%|█████████ | 180/200 [5:55:12<40:46, 122.31s/it]Evaluating commonsenseqa :  90%|█████████ | 181/200 [5:57:13<38:35, 121.89s/it]Evaluating commonsenseqa :  91%|█████████ | 182/200 [5:59:15<36:35, 121.99s/it]Evaluating commonsenseqa :  92%|█████████▏| 183/200 [6:01:15<34:20, 121.22s/it]Evaluating commonsenseqa :  92%|█████████▏| 184/200 [6:03:17<32:23, 121.47s/it]Evaluating commonsenseqa :  92%|█████████▎| 185/200 [6:05:20<30:28, 121.92s/it]Evaluating commonsenseqa :  93%|█████████▎| 186/200 [6:07:23<28:32, 122.30s/it]Evaluating commonsenseqa :  94%|█████████▎| 187/200 [6:09:21<26:15, 121.20s/it]Evaluating commonsenseqa :  94%|█████████▍| 188/200 [6:11:23<24:14, 121.20s/it]Evaluating commonsenseqa :  94%|█████████▍| 189/200 [6:13:27<22:25, 122.29s/it]Evaluating commonsenseqa :  95%|█████████▌| 190/200 [6:15:34<20:35, 123.58s/it]Evaluating commonsenseqa :  96%|█████████▌| 191/200 [6:17:37<18:31, 123.47s/it]Evaluating commonsenseqa :  96%|█████████▌| 192/200 [6:19:37<16:18, 122.37s/it]Evaluating commonsenseqa :  96%|█████████▋| 193/200 [6:21:42<14:21, 123.09s/it]Evaluating commonsenseqa :  97%|█████████▋| 194/200 [6:23:45<12:19, 123.17s/it]Evaluating commonsenseqa :  98%|█████████▊| 195/200 [6:25:47<10:13, 122.64s/it]Evaluating commonsenseqa :  98%|█████████▊| 196/200 [6:27:49<08:10, 122.59s/it]Evaluating commonsenseqa :  98%|█████████▊| 197/200 [6:29:51<06:07, 122.37s/it]Evaluating commonsenseqa :  99%|█████████▉| 198/200 [6:31:54<04:05, 122.54s/it]Evaluating commonsenseqa : 100%|█████████▉| 199/200 [6:33:57<02:02, 122.62s/it]Evaluating commonsenseqa : 100%|██████████| 200/200 [6:36:02<00:00, 123.44s/it]Evaluating commonsenseqa : 100%|██████████| 200/200 [6:36:02<00:00, 118.81s/it]
name: commonsenseqa | avg. gen lenth: 290.404 | time: 23763.607476472855s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 5 1 3 5 7 9 True False 4096 5
[2023-09-22 23:05:19,811] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 7 1 3 5 7 9 True False 4096 5
[2023-09-22 23:05:26,250] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s30-rTrue --seed 30 --max-prompt-length 4096 --rationales --num-out-domain 9 1 3 5 7 9 True False 4096 5
[2023-09-22 23:05:32,697] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s30-rFalse --seed 30 --max-prompt-length 4096 --num-out-domain 1 1 3 5 7 9 True False 4096 5
[2023-09-22 23:05:39,146] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tmathqa-s30-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 1
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o1-tmathqa-s30-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 1810783.82it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.35s/it]
 > number of parameters: 6738415616
[2023-09-22 23:05:52,574] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-09-22 23:05:52,575] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-22 23:05:52,874] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-22 23:05:52,875] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-09-22 23:05:52,875] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   amp_params ................... False
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5520706da0>
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   dump_state ................... False
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 2048
[2023-09-22 23:05:52,876] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   pld_params ................... False
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   world_size ................... 1
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  True
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   zero_enabled ................. False
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-22 23:05:52,877] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0
[2023-09-22 23:05:52,877] [INFO] [config.py:957:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/200 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: the mean of 30 values was 140. it was detected on rechecking that one value 145 was wrongly copied as 135 for the computation of the mean. find the correct mean. a ) 151, b ) 140.33, c ) 152, d ) 148, e ) none of the above
Output: b

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o1-tmathqa-s30-rFalse-m4096
Evaluating commonsenseqa :   0%|          | 1/200 [02:24<7:58:05, 144.15s/it]Evaluating commonsenseqa :   1%|          | 2/200 [04:15<6:51:31, 124.70s/it]Evaluating commonsenseqa :   2%|▏         | 3/200 [06:34<7:11:32, 131.43s/it]Evaluating commonsenseqa :   2%|▏         | 4/200 [07:58<6:08:06, 112.69s/it]Evaluating commonsenseqa :   2%|▎         | 5/200 [10:14<6:33:02, 120.94s/it]Evaluating commonsenseqa :   3%|▎         | 6/200 [12:32<6:50:37, 127.00s/it]Evaluating commonsenseqa :   4%|▎         | 7/200 [14:51<7:00:43, 130.80s/it]Evaluating commonsenseqa :   4%|▍         | 8/200 [15:35<5:30:26, 103.26s/it]Evaluating commonsenseqa :   4%|▍         | 9/200 [17:50<6:00:05, 113.12s/it]Evaluating commonsenseqa :   5%|▌         | 10/200 [20:07<6:21:47, 120.57s/it]Evaluating commonsenseqa :   6%|▌         | 11/200 [22:26<6:36:42, 125.94s/it]Evaluating commonsenseqa :   6%|▌         | 12/200 [24:40<6:43:11, 128.68s/it]Evaluating commonsenseqa :   6%|▋         | 13/200 [26:58<6:49:46, 131.48s/it]Evaluating commonsenseqa :   7%|▋         | 14/200 [29:17<6:54:18, 133.65s/it]Evaluating commonsenseqa :   8%|▊         | 15/200 [31:36<6:57:05, 135.28s/it]Evaluating commonsenseqa :   8%|▊         | 16/200 [33:54<6:57:41, 136.20s/it]Evaluating commonsenseqa :   8%|▊         | 17/200 [36:12<6:56:16, 136.49s/it]Evaluating commonsenseqa :   9%|▉         | 18/200 [37:45<6:15:04, 123.65s/it]Evaluating commonsenseqa :  10%|▉         | 19/200 [40:06<6:28:19, 128.73s/it]Evaluating commonsenseqa :  10%|█         | 20/200 [41:05<5:23:00, 107.67s/it]Evaluating commonsenseqa :  10%|█         | 21/200 [43:02<5:30:03, 110.64s/it]Evaluating commonsenseqa :  11%|█         | 22/200 [45:21<5:53:24, 119.13s/it]Evaluating commonsenseqa :  12%|█▏        | 23/200 [47:41<6:09:46, 125.35s/it]Evaluating commonsenseqa :  12%|█▏        | 24/200 [49:59<6:18:44, 129.12s/it]Evaluating commonsenseqa :  12%|█▎        | 25/200 [52:19<6:26:38, 132.57s/it]Evaluating commonsenseqa :  13%|█▎        | 26/200 [54:37<6:29:10, 134.20s/it]Evaluating commonsenseqa :  14%|█▎        | 27/200 [56:59<6:33:04, 136.33s/it]Evaluating commonsenseqa :  14%|█▍        | 28/200 [59:18<6:33:01, 137.10s/it]Evaluating commonsenseqa :  14%|█▍        | 29/200 [1:01:36<6:31:56, 137.52s/it]Evaluating commonsenseqa :  15%|█▌        | 30/200 [1:02:23<5:12:38, 110.35s/it]Evaluating commonsenseqa :  16%|█▌        | 31/200 [1:04:40<5:33:43, 118.48s/it]Evaluating commonsenseqa :  16%|█▌        | 32/200 [1:05:59<4:58:18, 106.54s/it]Evaluating commonsenseqa :  16%|█▋        | 33/200 [1:07:35<4:47:34, 103.32s/it]Evaluating commonsenseqa :  17%|█▋        | 34/200 [1:09:53<5:14:25, 113.65s/it]Evaluating commonsenseqa :  18%|█▊        | 35/200 [1:12:10<5:31:44, 120.63s/it]Evaluating commonsenseqa :  18%|█▊        | 36/200 [1:14:27<5:43:38, 125.72s/it]Evaluating commonsenseqa :  18%|█▊        | 37/200 [1:16:45<5:51:10, 129.27s/it]Evaluating commonsenseqa :  19%|█▉        | 38/200 [1:17:56<5:02:18, 111.97s/it]Evaluating commonsenseqa :  20%|█▉        | 39/200 [1:19:32<4:46:59, 106.96s/it]Evaluating commonsenseqa :  20%|██        | 40/200 [1:21:50<5:10:20, 116.38s/it]Evaluating commonsenseqa :  20%|██        | 41/200 [1:24:08<5:25:23, 122.79s/it]Evaluating commonsenseqa :  21%|██        | 42/200 [1:26:28<5:37:16, 128.08s/it]Evaluating commonsenseqa :  22%|██▏       | 43/200 [1:27:31<4:43:50, 108.48s/it]Evaluating commonsenseqa :  22%|██▏       | 44/200 [1:29:08<4:33:13, 105.09s/it]Evaluating commonsenseqa :  22%|██▎       | 45/200 [1:31:27<4:57:46, 115.27s/it]Evaluating commonsenseqa :  23%|██▎       | 46/200 [1:33:38<5:07:31, 119.82s/it]Evaluating commonsenseqa :  24%|██▎       | 47/200 [1:35:57<5:20:27, 125.67s/it]Evaluating commonsenseqa :  24%|██▍       | 48/200 [1:37:21<4:46:24, 113.06s/it]Evaluating commonsenseqa :  24%|██▍       | 49/200 [1:39:40<5:04:10, 120.86s/it]Evaluating commonsenseqa :  25%|██▌       | 50/200 [1:41:57<5:14:40, 125.87s/it]Evaluating commonsenseqa :  26%|██▌       | 51/200 [1:44:17<5:23:04, 130.10s/it]Evaluating commonsenseqa :  26%|██▌       | 52/200 [1:46:37<5:28:11, 133.05s/it]Evaluating commonsenseqa :  26%|██▋       | 53/200 [1:48:54<5:28:44, 134.18s/it]Evaluating commonsenseqa :  27%|██▋       | 54/200 [1:51:12<5:29:17, 135.32s/it]Evaluating commonsenseqa :  28%|██▊       | 55/200 [1:53:31<5:30:02, 136.57s/it]Evaluating commonsenseqa :  28%|██▊       | 56/200 [1:55:49<5:28:53, 137.04s/it]Evaluating commonsenseqa :  28%|██▊       | 57/200 [1:57:40<5:07:19, 128.95s/it]Evaluating commonsenseqa :  29%|██▉       | 58/200 [2:00:01<5:13:58, 132.66s/it]Evaluating commonsenseqa :  30%|██▉       | 59/200 [2:02:20<5:16:01, 134.48s/it]Evaluating commonsenseqa :  30%|███       | 60/200 [2:04:07<4:54:41, 126.30s/it]Evaluating commonsenseqa :  30%|███       | 61/200 [2:06:25<5:00:56, 129.90s/it]Evaluating commonsenseqa :  31%|███       | 62/200 [2:07:17<4:04:47, 106.43s/it]Evaluating commonsenseqa :  32%|███▏      | 63/200 [2:09:37<4:26:02, 116.51s/it]Evaluating commonsenseqa :  32%|███▏      | 64/200 [2:10:59<4:00:41, 106.18s/it]Evaluating commonsenseqa :  32%|███▎      | 65/200 [2:13:17<4:20:39, 115.85s/it]Evaluating commonsenseqa :  33%|███▎      | 66/200 [2:15:22<4:24:56, 118.63s/it]Evaluating commonsenseqa :  34%|███▎      | 67/200 [2:17:35<4:32:22, 122.88s/it]Evaluating commonsenseqa :  34%|███▍      | 68/200 [2:19:26<4:22:38, 119.38s/it]Evaluating commonsenseqa :  34%|███▍      | 69/200 [2:21:44<4:32:38, 124.87s/it]Evaluating commonsenseqa :  35%|███▌      | 70/200 [2:24:04<4:40:22, 129.40s/it]Evaluating commonsenseqa :  36%|███▌      | 71/200 [2:26:22<4:43:45, 131.98s/it]Evaluating commonsenseqa :  36%|███▌      | 72/200 [2:28:45<4:48:27, 135.21s/it]Evaluating commonsenseqa :  36%|███▋      | 73/200 [2:30:42<4:35:00, 129.92s/it]Evaluating commonsenseqa :  37%|███▋      | 74/200 [2:33:03<4:39:23, 133.05s/it]Evaluating commonsenseqa :  38%|███▊      | 75/200 [2:35:20<4:39:53, 134.35s/it]Evaluating commonsenseqa :  38%|███▊      | 76/200 [2:37:08<4:21:15, 126.41s/it]Evaluating commonsenseqa :  38%|███▊      | 77/200 [2:39:30<4:28:43, 131.09s/it]Evaluating commonsenseqa :  39%|███▉      | 78/200 [2:41:50<4:31:57, 133.75s/it]Evaluating commonsenseqa :  40%|███▉      | 79/200 [2:44:13<4:35:31, 136.63s/it]Evaluating commonsenseqa :  40%|████      | 80/200 [2:46:34<4:35:28, 137.73s/it]Evaluating commonsenseqa :  40%|████      | 81/200 [2:48:52<4:33:36, 137.95s/it]Evaluating commonsenseqa :  41%|████      | 82/200 [2:51:11<4:31:56, 138.27s/it]Evaluating commonsenseqa :  42%|████▏     | 83/200 [2:52:25<3:52:00, 118.98s/it]Evaluating commonsenseqa :  42%|████▏     | 84/200 [2:54:44<4:01:31, 124.93s/it]Evaluating commonsenseqa :  42%|████▎     | 85/200 [2:57:03<4:07:36, 129.18s/it]Evaluating commonsenseqa :  43%|████▎     | 86/200 [2:59:24<4:12:25, 132.85s/it]Evaluating commonsenseqa :  44%|████▎     | 87/200 [3:01:44<4:14:06, 134.92s/it]Evaluating commonsenseqa :  44%|████▍     | 88/200 [3:03:59<4:11:49, 134.91s/it]Evaluating commonsenseqa :  44%|████▍     | 89/200 [3:06:18<4:11:45, 136.08s/it]Evaluating commonsenseqa :  45%|████▌     | 90/200 [3:08:35<4:10:15, 136.51s/it]Evaluating commonsenseqa :  46%|████▌     | 91/200 [3:09:19<3:17:20, 108.63s/it]Evaluating commonsenseqa :  46%|████▌     | 92/200 [3:10:26<2:52:50, 96.02s/it] Evaluating commonsenseqa :  46%|████▋     | 93/200 [3:12:43<3:13:14, 108.36s/it]Evaluating commonsenseqa :  47%|████▋     | 94/200 [3:15:00<3:26:43, 117.01s/it]Evaluating commonsenseqa :  48%|████▊     | 95/200 [3:17:02<3:27:11, 118.39s/it]Evaluating commonsenseqa :  48%|████▊     | 96/200 [3:19:20<3:35:41, 124.44s/it]Evaluating commonsenseqa :  48%|████▊     | 97/200 [3:21:35<3:38:53, 127.51s/it]Evaluating commonsenseqa :  49%|████▉     | 98/200 [3:23:27<3:28:57, 122.92s/it]Evaluating commonsenseqa :  50%|████▉     | 99/200 [3:25:50<3:36:56, 128.87s/it]Evaluating commonsenseqa :  50%|█████     | 100/200 [3:28:07<3:38:52, 131.32s/it]Evaluating commonsenseqa :  50%|█████     | 101/200 [3:30:26<3:40:33, 133.68s/it]Evaluating commonsenseqa :  51%|█████     | 102/200 [3:32:46<3:41:25, 135.57s/it]Evaluating commonsenseqa :  52%|█████▏    | 103/200 [3:34:13<3:15:25, 120.89s/it]Evaluating commonsenseqa :  52%|█████▏    | 104/200 [3:36:30<3:21:27, 125.91s/it]Evaluating commonsenseqa :  52%|█████▎    | 105/200 [3:37:48<2:56:34, 111.52s/it]Evaluating commonsenseqa :  53%|█████▎    | 106/200 [3:40:09<3:08:23, 120.25s/it]Evaluating commonsenseqa :  54%|█████▎    | 107/200 [3:42:30<3:16:10, 126.56s/it]Evaluating commonsenseqa :  54%|█████▍    | 108/200 [3:44:49<3:19:59, 130.43s/it]Evaluating commonsenseqa :  55%|█████▍    | 109/200 [3:47:10<3:22:20, 133.41s/it]Evaluating commonsenseqa :  55%|█████▌    | 110/200 [3:49:25<3:20:48, 133.87s/it]Evaluating commonsenseqa :  56%|█████▌    | 111/200 [3:51:07<3:04:38, 124.48s/it]Evaluating commonsenseqa :  56%|█████▌    | 112/200 [3:53:29<3:10:19, 129.77s/it]Evaluating commonsenseqa :  56%|█████▋    | 113/200 [3:55:46<3:10:59, 131.72s/it]Evaluating commonsenseqa :  57%|█████▋    | 114/200 [3:57:44<3:02:54, 127.61s/it]Evaluating commonsenseqa :  57%|█████▊    | 115/200 [4:00:03<3:05:47, 131.15s/it]Evaluating commonsenseqa :  58%|█████▊    | 116/200 [4:02:25<3:07:59, 134.28s/it]Evaluating commonsenseqa :  58%|█████▊    | 117/200 [4:04:42<3:06:59, 135.17s/it]Evaluating commonsenseqa :  59%|█████▉    | 118/200 [4:06:58<3:05:17, 135.57s/it]Evaluating commonsenseqa :  60%|█████▉    | 119/200 [4:09:18<3:04:42, 136.83s/it]Evaluating commonsenseqa :  60%|██████    | 120/200 [4:10:55<2:46:30, 124.89s/it]Evaluating commonsenseqa :  60%|██████    | 121/200 [4:13:16<2:50:53, 129.79s/it]Evaluating commonsenseqa :  61%|██████    | 122/200 [4:15:33<2:51:21, 131.82s/it]Evaluating commonsenseqa :  62%|██████▏   | 123/200 [4:17:53<2:52:06, 134.12s/it]Evaluating commonsenseqa :  62%|██████▏   | 124/200 [4:20:13<2:52:11, 135.94s/it]Evaluating commonsenseqa :  62%|██████▎   | 125/200 [4:22:31<2:50:49, 136.66s/it]Evaluating commonsenseqa :  63%|██████▎   | 126/200 [4:24:28<2:41:17, 130.77s/it]Evaluating commonsenseqa :  64%|██████▎   | 127/200 [4:25:18<2:09:31, 106.46s/it]Evaluating commonsenseqa :  64%|██████▍   | 128/200 [4:27:13<2:10:55, 109.11s/it]Evaluating commonsenseqa :  64%|██████▍   | 129/200 [4:29:33<2:20:11, 118.47s/it]Evaluating commonsenseqa :  65%|██████▌   | 130/200 [4:31:52<2:25:13, 124.48s/it]Evaluating commonsenseqa :  66%|██████▌   | 131/200 [4:34:09<2:27:37, 128.37s/it]Evaluating commonsenseqa :  66%|██████▌   | 132/200 [4:36:33<2:30:39, 132.94s/it]Evaluating commonsenseqa :  66%|██████▋   | 133/200 [4:38:36<2:25:09, 130.00s/it]Evaluating commonsenseqa :  67%|██████▋   | 134/200 [4:40:40<2:21:06, 128.28s/it]Evaluating commonsenseqa :  68%|██████▊   | 135/200 [4:41:37<1:55:49, 106.92s/it]Evaluating commonsenseqa :  68%|██████▊   | 136/200 [4:43:54<2:03:39, 115.92s/it]Evaluating commonsenseqa :  68%|██████▊   | 137/200 [4:46:15<2:09:25, 123.27s/it]Evaluating commonsenseqa :  69%|██████▉   | 138/200 [4:48:36<2:12:53, 128.60s/it]Evaluating commonsenseqa :  70%|██████▉   | 139/200 [4:50:57<2:14:26, 132.24s/it]Evaluating commonsenseqa :  70%|███████   | 140/200 [4:53:15<2:14:10, 134.18s/it]Evaluating commonsenseqa :  70%|███████   | 141/200 [4:55:33<2:12:52, 135.13s/it]Evaluating commonsenseqa :  71%|███████   | 142/200 [4:57:49<2:11:06, 135.63s/it]Evaluating commonsenseqa :  72%|███████▏  | 143/200 [5:00:06<2:08:59, 135.78s/it]Evaluating commonsenseqa :  72%|███████▏  | 144/200 [5:02:24<2:07:31, 136.64s/it]Evaluating commonsenseqa :  72%|███████▎  | 145/200 [5:02:49<1:34:25, 103.01s/it]Evaluating commonsenseqa :  73%|███████▎  | 146/200 [5:05:08<1:42:21, 113.74s/it]Evaluating commonsenseqa :  74%|███████▎  | 147/200 [5:07:25<1:46:47, 120.90s/it]Evaluating commonsenseqa :  74%|███████▍  | 148/200 [5:08:49<1:35:02, 109.66s/it]Evaluating commonsenseqa :  74%|███████▍  | 149/200 [5:10:50<1:36:07, 113.08s/it]Evaluating commonsenseqa :  75%|███████▌  | 150/200 [5:13:09<1:40:52, 121.05s/it]Evaluating commonsenseqa :  76%|███████▌  | 151/200 [5:15:31<1:43:53, 127.21s/it]Evaluating commonsenseqa :  76%|███████▌  | 152/200 [5:17:49<1:44:24, 130.52s/it]Evaluating commonsenseqa :  76%|███████▋  | 153/200 [5:20:09<1:44:23, 133.26s/it]Evaluating commonsenseqa :  77%|███████▋  | 154/200 [5:22:27<1:43:23, 134.86s/it]Evaluating commonsenseqa :  78%|███████▊  | 155/200 [5:24:47<1:42:13, 136.30s/it]Evaluating commonsenseqa :  78%|███████▊  | 156/200 [5:27:03<1:39:46, 136.07s/it]Evaluating commonsenseqa :  78%|███████▊  | 157/200 [5:29:21<1:37:56, 136.67s/it]Evaluating commonsenseqa :  79%|███████▉  | 158/200 [5:31:40<1:36:12, 137.43s/it]Evaluating commonsenseqa :  80%|███████▉  | 159/200 [5:33:59<1:34:15, 137.95s/it]Evaluating commonsenseqa :  80%|████████  | 160/200 [5:36:14<1:31:28, 137.20s/it]Evaluating commonsenseqa :  80%|████████  | 161/200 [5:38:32<1:29:15, 137.31s/it]Evaluating commonsenseqa :  81%|████████  | 162/200 [5:40:50<1:27:11, 137.66s/it]Evaluating commonsenseqa :  82%|████████▏ | 163/200 [5:43:10<1:25:18, 138.34s/it]Evaluating commonsenseqa :  82%|████████▏ | 164/200 [5:45:01<1:18:04, 130.12s/it]Evaluating commonsenseqa :  82%|████████▎ | 165/200 [5:46:17<1:06:23, 113.81s/it]Evaluating commonsenseqa :  83%|████████▎ | 166/200 [5:48:36<1:08:41, 121.21s/it]Evaluating commonsenseqa :  84%|████████▎ | 167/200 [5:49:58<1:00:16, 109.59s/it]Evaluating commonsenseqa :  84%|████████▍ | 168/200 [5:52:16<1:02:56, 118.02s/it]Evaluating commonsenseqa :  84%|████████▍ | 169/200 [5:54:32<1:03:44, 123.37s/it]Evaluating commonsenseqa :  85%|████████▌ | 170/200 [5:56:53<1:04:26, 128.90s/it]Evaluating commonsenseqa :  86%|████████▌ | 171/200 [5:59:13<1:03:51, 132.13s/it]Evaluating commonsenseqa :  86%|████████▌ | 172/200 [6:01:35<1:02:58, 134.93s/it]Evaluating commonsenseqa :  86%|████████▋ | 173/200 [6:03:54<1:01:20, 136.33s/it]Evaluating commonsenseqa :  87%|████████▋ | 174/200 [6:06:14<59:34, 137.48s/it]  Evaluating commonsenseqa :  88%|████████▊ | 175/200 [6:08:33<57:27, 137.89s/it]Evaluating commonsenseqa :  88%|████████▊ | 176/200 [6:10:54<55:29, 138.74s/it]Evaluating commonsenseqa :  88%|████████▊ | 177/200 [6:13:13<53:12, 138.81s/it]Evaluating commonsenseqa :  89%|████████▉ | 178/200 [6:15:33<51:04, 139.29s/it]Evaluating commonsenseqa :  90%|████████▉ | 179/200 [6:17:43<47:47, 136.54s/it]Evaluating commonsenseqa :  90%|█████████ | 180/200 [6:20:04<45:55, 137.75s/it]Evaluating commonsenseqa :  90%|█████████ | 181/200 [6:21:11<36:53, 116.52s/it]Evaluating commonsenseqa :  91%|█████████ | 182/200 [6:23:29<36:54, 123.02s/it]Evaluating commonsenseqa :  92%|█████████▏| 183/200 [6:25:47<36:08, 127.57s/it]Evaluating commonsenseqa :  92%|█████████▏| 184/200 [6:26:23<26:38, 99.91s/it] Evaluating commonsenseqa :  92%|█████████▎| 185/200 [6:28:45<28:11, 112.74s/it]Evaluating commonsenseqa :  93%|█████████▎| 186/200 [6:31:03<28:02, 120.17s/it]Evaluating commonsenseqa :  94%|█████████▎| 187/200 [6:33:23<27:21, 126.26s/it]Evaluating commonsenseqa :  94%|█████████▍| 188/200 [6:35:41<25:56, 129.71s/it]Evaluating commonsenseqa :  94%|█████████▍| 189/200 [6:38:02<24:22, 132.96s/it]Evaluating commonsenseqa :  95%|█████████▌| 190/200 [6:40:19<22:22, 134.26s/it]Evaluating commonsenseqa :  96%|█████████▌| 191/200 [6:42:39<20:24, 136.03s/it]Evaluating commonsenseqa :  96%|█████████▌| 192/200 [6:44:59<18:17, 137.14s/it]Evaluating commonsenseqa :  96%|█████████▋| 193/200 [6:47:18<16:04, 137.84s/it]Evaluating commonsenseqa :  97%|█████████▋| 194/200 [6:49:03<12:47, 127.89s/it]Evaluating commonsenseqa :  98%|█████████▊| 195/200 [6:51:21<10:55, 131.08s/it]Evaluating commonsenseqa :  98%|█████████▊| 196/200 [6:53:42<08:56, 134.02s/it]Evaluating commonsenseqa :  98%|█████████▊| 197/200 [6:56:04<06:49, 136.39s/it]Evaluating commonsenseqa :  99%|█████████▉| 198/200 [6:57:31<04:02, 121.39s/it]Evaluating commonsenseqa : 100%|█████████▉| 199/200 [6:58:13<01:37, 97.81s/it] Evaluating commonsenseqa : 100%|██████████| 200/200 [6:59:52<00:00, 97.97s/it]Evaluating commonsenseqa : 100%|██████████| 200/200 [6:59:52<00:00, 125.96s/it]
name: commonsenseqa | avg. gen lenth: 242.265 | time: 25193.227343320847s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s30-rFalse --seed 30 --max-prompt-length 4096 --num-out-domain 3 1 3 5 7 9 True False 4096 5
[2023-09-23 06:05:52,984] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o3-tmathqa-s30-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 3
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o3-tmathqa-s30-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 1273747.20it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.38s/it]
 > number of parameters: 6738415616
[2023-09-23 06:06:06,551] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-09-23 06:06:06,551] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-23 06:06:06,865] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-23 06:06:06,866] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-09-23 06:06:06,866] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-23 06:06:06,866] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-23 06:06:06,866] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-09-23 06:06:06,866] [INFO] [config.py:971:print]   amp_params ................... False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f40289d6da0>
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   dump_state ................... False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 2048
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-09-23 06:06:06,867] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   pld_params ................... False
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   world_size ................... 1
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  True
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   zero_enabled ................. False
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-23 06:06:06,868] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0
[2023-09-23 06:06:06,868] [INFO] [config.py:957:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/200 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: the mean of 30 values was 140. it was detected on rechecking that one value 145 was wrongly copied as 135 for the computation of the mean. find the correct mean. a ) 151, b ) 140.33, c ) 152, d ) 148, e ) none of the above
Output: b

Input: the measurement of a rectangular box with lid is 25 cmx 24 cmx 18 cm. find the volume of the largest sphere that can be inscribed in the box ( in terms of π cm 3 ). ( hint : the lowest measure of rectangular box represents the diameter of the largest sphere ) a ) 288, b ) 2302, c ) 2304, d ) 8640, e ) 964
Output: c

Input: a certain fraction has the same ratio to 1 / 33, as 3 / 4 does to 7 / 11. what is this certain fraction? a ) 1 / 14, b ) 1 / 18, c ) 1 / 21, d ) 1 / 25, e ) 1 / 28
Output: e

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o3-tmathqa-s30-rFalse-m4096
Evaluating commonsenseqa :   0%|          | 1/200 [00:53<2:59:02, 53.98s/it]Evaluating commonsenseqa :   1%|          | 2/200 [03:06<5:31:16, 100.38s/it]Evaluating commonsenseqa :   2%|▏         | 3/200 [05:16<6:13:55, 113.89s/it]Evaluating commonsenseqa :   2%|▏         | 4/200 [07:28<6:35:13, 120.99s/it]Evaluating commonsenseqa :   2%|▎         | 5/200 [09:40<6:45:53, 124.89s/it]Evaluating commonsenseqa :   3%|▎         | 6/200 [10:31<5:22:33, 99.76s/it] Evaluating commonsenseqa :   4%|▎         | 7/200 [11:00<4:06:38, 76.68s/it]Evaluating commonsenseqa :   4%|▍         | 8/200 [12:54<4:42:59, 88.44s/it]Evaluating commonsenseqa :   4%|▍         | 9/200 [15:04<5:23:32, 101.63s/it]Evaluating commonsenseqa :   5%|▌         | 10/200 [17:19<5:53:51, 111.74s/it]Evaluating commonsenseqa :   6%|▌         | 11/200 [19:30<6:10:49, 117.72s/it]Evaluating commonsenseqa :   6%|▌         | 12/200 [21:43<6:23:25, 122.37s/it]Evaluating commonsenseqa :   6%|▋         | 13/200 [23:04<5:42:11, 109.79s/it]Evaluating commonsenseqa :   7%|▋         | 14/200 [24:40<5:27:50, 105.76s/it]Evaluating commonsenseqa :   8%|▊         | 15/200 [25:47<4:49:59, 94.05s/it] Evaluating commonsenseqa :   8%|▊         | 16/200 [27:03<4:31:50, 88.64s/it]Evaluating commonsenseqa :   8%|▊         | 17/200 [28:31<4:29:48, 88.46s/it]Evaluating commonsenseqa :   9%|▉         | 18/200 [29:27<3:58:17, 78.56s/it]Evaluating commonsenseqa :  10%|▉         | 19/200 [31:39<4:45:33, 94.66s/it]Evaluating commonsenseqa :  10%|█         | 20/200 [33:51<5:17:57, 105.99s/it]Evaluating commonsenseqa :  10%|█         | 21/200 [36:04<5:39:54, 113.94s/it]Evaluating commonsenseqa :  11%|█         | 22/200 [38:15<5:53:13, 119.06s/it]Evaluating commonsenseqa :  12%|█▏        | 23/200 [40:26<6:01:35, 122.57s/it]Evaluating commonsenseqa :  12%|█▏        | 24/200 [42:05<5:39:26, 115.72s/it]Evaluating commonsenseqa :  12%|█▎        | 25/200 [42:49<4:34:35, 94.15s/it] Evaluating commonsenseqa :  13%|█▎        | 26/200 [45:02<5:06:38, 105.74s/it]Evaluating commonsenseqa :  14%|█▎        | 27/200 [47:15<5:28:37, 113.97s/it]Evaluating commonsenseqa :  14%|█▍        | 28/200 [49:27<5:42:12, 119.37s/it]Evaluating commonsenseqa :  14%|█▍        | 29/200 [51:37<5:49:34, 122.66s/it]Evaluating commonsenseqa :  15%|█▌        | 30/200 [53:33<5:41:36, 120.57s/it]Evaluating commonsenseqa :  16%|█▌        | 31/200 [55:41<5:45:43, 122.74s/it]Evaluating commonsenseqa :  16%|█▌        | 32/200 [57:53<5:51:13, 125.44s/it]Evaluating commonsenseqa :  16%|█▋        | 33/200 [59:06<5:05:28, 109.75s/it]Evaluating commonsenseqa :  17%|█▋        | 34/200 [1:01:06<5:12:27, 112.94s/it]Evaluating commonsenseqa :  18%|█▊        | 35/200 [1:02:42<4:56:46, 107.92s/it]Evaluating commonsenseqa :  18%|█▊        | 36/200 [1:03:14<3:52:35, 85.09s/it] Evaluating commonsenseqa :  18%|█▊        | 37/200 [1:05:24<4:27:37, 98.51s/it]Evaluating commonsenseqa :  19%|█▉        | 38/200 [1:07:33<4:50:18, 107.52s/it]Evaluating commonsenseqa :  20%|█▉        | 39/200 [1:09:33<4:58:44, 111.34s/it]Evaluating commonsenseqa :  20%|██        | 40/200 [1:11:44<5:12:28, 117.18s/it]Evaluating commonsenseqa :  20%|██        | 41/200 [1:13:55<5:21:44, 121.41s/it]Evaluating commonsenseqa :  21%|██        | 42/200 [1:16:05<5:26:50, 124.12s/it]Evaluating commonsenseqa :  22%|██▏       | 43/200 [1:18:15<5:29:13, 125.82s/it]Evaluating commonsenseqa :  22%|██▏       | 44/200 [1:18:39<4:07:55, 95.35s/it] Evaluating commonsenseqa :  22%|██▎       | 45/200 [1:20:06<3:59:06, 92.56s/it]Evaluating commonsenseqa :  23%|██▎       | 46/200 [1:22:14<4:25:05, 103.29s/it]Evaluating commonsenseqa :  24%|██▎       | 47/200 [1:23:32<4:03:50, 95.62s/it] Evaluating commonsenseqa :  24%|██▍       | 48/200 [1:24:34<3:37:11, 85.73s/it]Evaluating commonsenseqa :  24%|██▍       | 49/200 [1:26:45<4:09:38, 99.20s/it]Evaluating commonsenseqa :  25%|██▌       | 50/200 [1:28:57<4:32:28, 108.99s/it]Evaluating commonsenseqa :  26%|██▌       | 51/200 [1:31:06<4:45:51, 115.11s/it]Evaluating commonsenseqa :  26%|██▌       | 52/200 [1:32:09<4:05:30, 99.53s/it] Evaluating commonsenseqa :  26%|██▋       | 53/200 [1:33:41<3:58:18, 97.27s/it]Evaluating commonsenseqa :  27%|██▋       | 54/200 [1:35:03<3:45:11, 92.54s/it]Evaluating commonsenseqa :  28%|██▊       | 55/200 [1:36:08<3:24:08, 84.47s/it]Evaluating commonsenseqa :  28%|██▊       | 56/200 [1:38:18<3:55:05, 97.96s/it]Evaluating commonsenseqa :  28%|██▊       | 57/200 [1:40:11<4:04:14, 102.48s/it]Evaluating commonsenseqa :  29%|██▉       | 58/200 [1:42:22<4:23:04, 111.16s/it]Evaluating commonsenseqa :  30%|██▉       | 59/200 [1:44:32<4:33:58, 116.59s/it]Evaluating commonsenseqa :  30%|███       | 60/200 [1:45:30<3:51:33, 99.24s/it] Evaluating commonsenseqa :  30%|███       | 61/200 [1:46:26<3:19:35, 86.15s/it]Evaluating commonsenseqa :  31%|███       | 62/200 [1:48:25<3:40:34, 95.91s/it]Evaluating commonsenseqa :  32%|███▏      | 63/200 [1:50:12<3:47:12, 99.51s/it]Evaluating commonsenseqa :  32%|███▏      | 64/200 [1:52:22<4:05:45, 108.43s/it]Evaluating commonsenseqa :  32%|███▎      | 65/200 [1:54:28<4:16:18, 113.92s/it]Evaluating commonsenseqa :  33%|███▎      | 66/200 [1:56:38<4:24:51, 118.59s/it]Evaluating commonsenseqa :  34%|███▎      | 67/200 [1:58:45<4:28:34, 121.16s/it]Evaluating commonsenseqa :  34%|███▍      | 68/200 [2:00:55<4:32:27, 123.84s/it]Evaluating commonsenseqa :  34%|███▍      | 69/200 [2:02:06<3:55:41, 107.95s/it]Evaluating commonsenseqa :  35%|███▌      | 70/200 [2:04:14<4:07:02, 114.02s/it]Evaluating commonsenseqa :  36%|███▌      | 71/200 [2:06:23<4:14:21, 118.31s/it]Evaluating commonsenseqa :  36%|███▌      | 72/200 [2:08:30<4:18:24, 121.13s/it]Evaluating commonsenseqa :  36%|███▋      | 73/200 [2:09:27<3:35:36, 101.86s/it]Evaluating commonsenseqa :  37%|███▋      | 74/200 [2:11:38<3:51:57, 110.45s/it]Evaluating commonsenseqa :  38%|███▊      | 75/200 [2:13:48<4:02:43, 116.50s/it]Evaluating commonsenseqa :  38%|███▊      | 76/200 [2:15:40<3:57:46, 115.05s/it]Evaluating commonsenseqa :  38%|███▊      | 77/200 [2:17:47<4:03:03, 118.56s/it]Evaluating commonsenseqa :  39%|███▉      | 78/200 [2:19:44<4:00:03, 118.07s/it]Evaluating commonsenseqa :  40%|███▉      | 79/200 [2:21:42<3:58:22, 118.20s/it]Evaluating commonsenseqa :  40%|████      | 80/200 [2:23:54<4:04:20, 122.17s/it]Evaluating commonsenseqa :  40%|████      | 81/200 [2:25:57<4:03:04, 122.56s/it]Evaluating commonsenseqa :  41%|████      | 82/200 [2:28:09<4:06:21, 125.26s/it]Evaluating commonsenseqa :  42%|████▏     | 83/200 [2:30:21<4:08:09, 127.26s/it]Evaluating commonsenseqa :  42%|████▏     | 84/200 [2:32:31<4:07:48, 128.18s/it]Evaluating commonsenseqa :  42%|████▎     | 85/200 [2:34:34<4:02:41, 126.62s/it]Evaluating commonsenseqa :  43%|████▎     | 86/200 [2:35:43<3:27:47, 109.36s/it]Evaluating commonsenseqa :  44%|████▎     | 87/200 [2:37:39<3:29:59, 111.50s/it]Evaluating commonsenseqa :  44%|████▍     | 88/200 [2:39:48<3:37:50, 116.70s/it]Evaluating commonsenseqa :  44%|████▍     | 89/200 [2:41:58<3:43:07, 120.61s/it]Evaluating commonsenseqa :  45%|████▌     | 90/200 [2:42:56<3:06:55, 101.96s/it]Evaluating commonsenseqa :  46%|████▌     | 91/200 [2:45:05<3:19:35, 109.87s/it]Evaluating commonsenseqa :  46%|████▌     | 92/200 [2:45:54<2:45:13, 91.79s/it] Evaluating commonsenseqa :  46%|████▋     | 93/200 [2:48:04<3:03:59, 103.18s/it]Evaluating commonsenseqa :  47%|████▋     | 94/200 [2:49:47<3:02:18, 103.20s/it]Evaluating commonsenseqa :  48%|████▊     | 95/200 [2:51:57<3:14:40, 111.24s/it]Evaluating commonsenseqa :  48%|████▊     | 96/200 [2:54:07<3:22:23, 116.76s/it]Evaluating commonsenseqa :  48%|████▊     | 97/200 [2:56:18<3:27:57, 121.14s/it]Evaluating commonsenseqa :  49%|████▉     | 98/200 [2:58:28<3:30:11, 123.64s/it]Evaluating commonsenseqa :  50%|████▉     | 99/200 [3:00:37<3:31:07, 125.42s/it]Evaluating commonsenseqa :  50%|█████     | 100/200 [3:02:37<3:25:56, 123.56s/it]Evaluating commonsenseqa :  50%|█████     | 101/200 [3:04:19<3:13:12, 117.10s/it]Evaluating commonsenseqa :  51%|█████     | 102/200 [3:06:29<3:17:35, 120.97s/it]Evaluating commonsenseqa :  52%|█████▏    | 103/200 [3:08:11<3:06:26, 115.32s/it]Evaluating commonsenseqa :  52%|█████▏    | 104/200 [3:10:21<3:11:27, 119.67s/it]Evaluating commonsenseqa :  52%|█████▎    | 105/200 [3:11:22<2:41:51, 102.22s/it]Evaluating commonsenseqa :  53%|█████▎    | 106/200 [3:12:55<2:35:46, 99.43s/it] Evaluating commonsenseqa :  54%|█████▎    | 107/200 [3:14:36<2:34:56, 99.97s/it]Evaluating commonsenseqa :  54%|█████▍    | 108/200 [3:15:33<2:13:29, 87.06s/it]Evaluating commonsenseqa :  55%|█████▍    | 109/200 [3:17:45<2:32:23, 100.48s/it]Evaluating commonsenseqa :  55%|█████▌    | 110/200 [3:19:55<2:44:11, 109.46s/it]Evaluating commonsenseqa :  56%|█████▌    | 111/200 [3:20:50<2:17:49, 92.92s/it] Evaluating commonsenseqa :  56%|█████▌    | 112/200 [3:23:02<2:33:38, 104.76s/it]Evaluating commonsenseqa :  56%|█████▋    | 113/200 [3:24:41<2:29:08, 102.86s/it]Evaluating commonsenseqa :  57%|█████▋    | 114/200 [3:26:51<2:39:08, 111.03s/it]Evaluating commonsenseqa :  57%|█████▊    | 115/200 [3:28:57<2:43:59, 115.76s/it]Evaluating commonsenseqa :  58%|█████▊    | 116/200 [3:31:06<2:47:31, 119.66s/it]Evaluating commonsenseqa :  58%|█████▊    | 117/200 [3:32:53<2:40:18, 115.89s/it]Evaluating commonsenseqa :  59%|█████▉    | 118/200 [3:33:53<2:15:33, 99.18s/it] Evaluating commonsenseqa :  60%|█████▉    | 119/200 [3:35:01<2:00:52, 89.54s/it]Evaluating commonsenseqa :  60%|██████    | 120/200 [3:37:12<2:16:14, 102.19s/it]Evaluating commonsenseqa :  60%|██████    | 121/200 [3:39:15<2:22:48, 108.46s/it]Evaluating commonsenseqa :  61%|██████    | 122/200 [3:41:28<2:30:17, 115.61s/it]Evaluating commonsenseqa :  62%|██████▏   | 123/200 [3:43:36<2:33:06, 119.31s/it]Evaluating commonsenseqa :  62%|██████▏   | 124/200 [3:45:42<2:33:46, 121.41s/it]Evaluating commonsenseqa :  62%|██████▎   | 125/200 [3:47:51<2:34:41, 123.75s/it]Evaluating commonsenseqa :  63%|██████▎   | 126/200 [3:49:49<2:30:34, 122.09s/it]Evaluating commonsenseqa :  64%|██████▎   | 127/200 [3:51:15<2:15:12, 111.13s/it]Evaluating commonsenseqa :  64%|██████▍   | 128/200 [3:52:13<1:54:22, 95.31s/it] Evaluating commonsenseqa :  64%|██████▍   | 129/200 [3:54:24<2:05:18, 105.89s/it]Evaluating commonsenseqa :  65%|██████▌   | 130/200 [3:56:31<2:11:01, 112.31s/it]Evaluating commonsenseqa :  66%|██████▌   | 131/200 [3:58:41<2:15:18, 117.66s/it]Evaluating commonsenseqa :  66%|██████▌   | 132/200 [3:59:58<1:59:23, 105.34s/it]Evaluating commonsenseqa :  66%|██████▋   | 133/200 [4:02:11<2:06:53, 113.63s/it]Evaluating commonsenseqa :  67%|██████▋   | 134/200 [4:03:19<1:50:02, 100.03s/it]Evaluating commonsenseqa :  68%|██████▊   | 135/200 [4:05:27<1:57:33, 108.52s/it]Evaluating commonsenseqa :  68%|██████▊   | 136/200 [4:07:37<2:02:28, 114.82s/it]Evaluating commonsenseqa :  68%|██████▊   | 137/200 [4:09:47<2:05:24, 119.44s/it]Evaluating commonsenseqa :  69%|██████▉   | 138/200 [4:10:21<1:36:51, 93.74s/it] Evaluating commonsenseqa :  70%|██████▉   | 139/200 [4:12:28<1:45:33, 103.82s/it]Evaluating commonsenseqa :  70%|███████   | 140/200 [4:14:06<1:41:59, 101.98s/it]Evaluating commonsenseqa :  70%|███████   | 141/200 [4:15:34<1:36:06, 97.74s/it] Evaluating commonsenseqa :  71%|███████   | 142/200 [4:16:54<1:29:22, 92.46s/it]Evaluating commonsenseqa :  72%|███████▏  | 143/200 [4:18:47<1:33:37, 98.55s/it]Evaluating commonsenseqa :  72%|███████▏  | 144/200 [4:20:41<1:36:20, 103.22s/it]Evaluating commonsenseqa :  72%|███████▎  | 145/200 [4:22:53<1:42:26, 111.76s/it]Evaluating commonsenseqa :  73%|███████▎  | 146/200 [4:25:04<1:45:57, 117.73s/it]Evaluating commonsenseqa :  74%|███████▎  | 147/200 [4:25:40<1:22:23, 93.26s/it] Evaluating commonsenseqa :  74%|███████▍  | 148/200 [4:26:48<1:14:11, 85.61s/it]Evaluating commonsenseqa :  74%|███████▍  | 149/200 [4:28:58<1:23:57, 98.77s/it]Evaluating commonsenseqa :  75%|███████▌  | 150/200 [4:31:04<1:29:20, 107.20s/it]Evaluating commonsenseqa :  76%|███████▌  | 151/200 [4:33:16<1:33:26, 114.41s/it]Evaluating commonsenseqa :  76%|███████▌  | 152/200 [4:35:24<1:34:53, 118.61s/it]Evaluating commonsenseqa :  76%|███████▋  | 153/200 [4:37:35<1:35:50, 122.35s/it]Evaluating commonsenseqa :  77%|███████▋  | 154/200 [4:39:24<1:30:44, 118.37s/it]Evaluating commonsenseqa :  78%|███████▊  | 155/200 [4:41:35<1:31:33, 122.08s/it]Evaluating commonsenseqa :  78%|███████▊  | 156/200 [4:43:47<1:31:37, 124.95s/it]Evaluating commonsenseqa :  78%|███████▊  | 157/200 [4:45:54<1:29:58, 125.54s/it]Evaluating commonsenseqa :  79%|███████▉  | 158/200 [4:48:03<1:28:43, 126.75s/it]Evaluating commonsenseqa :  80%|███████▉  | 159/200 [4:50:13<1:27:17, 127.75s/it]Evaluating commonsenseqa :  80%|████████  | 160/200 [4:52:21<1:25:09, 127.75s/it]Evaluating commonsenseqa :  80%|████████  | 161/200 [4:53:16<1:08:54, 106.01s/it]Evaluating commonsenseqa :  81%|████████  | 162/200 [4:55:27<1:11:46, 113.32s/it]Evaluating commonsenseqa :  82%|████████▏ | 163/200 [4:57:03<1:06:43, 108.19s/it]Evaluating commonsenseqa :  82%|████████▏ | 164/200 [4:59:15<1:09:10, 115.29s/it]Evaluating commonsenseqa :  82%|████████▎ | 165/200 [5:00:34<1:00:51, 104.33s/it]Evaluating commonsenseqa :  83%|████████▎ | 166/200 [5:02:31<1:01:20, 108.26s/it]Evaluating commonsenseqa :  84%|████████▎ | 167/200 [5:04:43<1:03:24, 115.29s/it]Evaluating commonsenseqa :  84%|████████▍ | 168/200 [5:06:54<1:03:59, 119.98s/it]Evaluating commonsenseqa :  84%|████████▍ | 169/200 [5:07:40<50:39, 98.06s/it]   Evaluating commonsenseqa :  85%|████████▌ | 170/200 [5:09:36<51:41, 103.38s/it]Evaluating commonsenseqa :  86%|████████▌ | 171/200 [5:11:47<53:53, 111.50s/it]Evaluating commonsenseqa :  86%|████████▌ | 172/200 [5:13:58<54:45, 117.33s/it]Evaluating commonsenseqa :  86%|████████▋ | 173/200 [5:16:08<54:35, 121.32s/it]Evaluating commonsenseqa :  87%|████████▋ | 174/200 [5:18:21<54:01, 124.69s/it]Evaluating commonsenseqa :  88%|████████▊ | 175/200 [5:20:31<52:42, 126.49s/it]Evaluating commonsenseqa :  88%|████████▊ | 176/200 [5:21:36<43:13, 108.05s/it]Evaluating commonsenseqa :  88%|████████▊ | 177/200 [5:23:45<43:44, 114.10s/it]Evaluating commonsenseqa :  89%|████████▉ | 178/200 [5:25:55<43:39, 119.08s/it]Evaluating commonsenseqa :  90%|████████▉ | 179/200 [5:28:05<42:44, 122.14s/it]Evaluating commonsenseqa :  90%|█████████ | 180/200 [5:29:33<37:19, 111.97s/it]Evaluating commonsenseqa :  90%|█████████ | 181/200 [5:31:44<37:14, 117.61s/it]Evaluating commonsenseqa :  91%|█████████ | 182/200 [5:33:37<34:52, 116.23s/it]Evaluating commonsenseqa :  92%|█████████▏| 183/200 [5:35:47<34:05, 120.33s/it]Evaluating commonsenseqa :  92%|█████████▏| 184/200 [5:37:56<32:50, 123.15s/it]Evaluating commonsenseqa :  92%|█████████▎| 185/200 [5:40:06<31:18, 125.21s/it]Evaluating commonsenseqa :  93%|█████████▎| 186/200 [5:41:32<26:27, 113.40s/it]Evaluating commonsenseqa :  94%|█████████▎| 187/200 [5:43:42<25:37, 118.26s/it]Evaluating commonsenseqa :  94%|█████████▍| 188/200 [5:45:51<24:18, 121.51s/it]Evaluating commonsenseqa :  94%|█████████▍| 189/200 [5:48:00<22:42, 123.84s/it]Evaluating commonsenseqa :  95%|█████████▌| 190/200 [5:50:08<20:49, 124.99s/it]Evaluating commonsenseqa :  96%|█████████▌| 191/200 [5:52:19<19:01, 126.86s/it]Evaluating commonsenseqa :  96%|█████████▌| 192/200 [5:54:27<16:58, 127.28s/it]Evaluating commonsenseqa :  96%|█████████▋| 193/200 [5:56:37<14:56, 128.13s/it]Evaluating commonsenseqa :  97%|█████████▋| 194/200 [5:57:55<11:18, 113.09s/it]Evaluating commonsenseqa :  98%|█████████▊| 195/200 [5:58:45<07:49, 93.98s/it] Evaluating commonsenseqa :  98%|█████████▊| 196/200 [6:00:56<07:00, 105.21s/it]Evaluating commonsenseqa :  98%|█████████▊| 197/200 [6:03:04<05:35, 111.94s/it]Evaluating commonsenseqa :  99%|█████████▉| 198/200 [6:05:14<03:55, 117.51s/it]Evaluating commonsenseqa : 100%|█████████▉| 199/200 [6:07:25<02:01, 121.34s/it]Evaluating commonsenseqa : 100%|██████████| 200/200 [6:09:33<00:00, 123.48s/it]Evaluating commonsenseqa : 100%|██████████| 200/200 [6:09:33<00:00, 110.87s/it]
name: commonsenseqa | avg. gen lenth: 221.682 | time: 22174.587270259857s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s30-rFalse --seed 30 --max-prompt-length 4096 --num-out-domain 5 1 3 5 7 9 True False 4096 5
[2023-09-23 12:15:46,022] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s30-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 5
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o5-tmathqa-s30-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 1111384.45it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.42s/it]
 > number of parameters: 6738415616
[2023-09-23 12:15:59,852] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-09-23 12:15:59,853] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-23 12:16:00,203] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-23 12:16:00,205] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   amp_params ................... False
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f18584f2da0>
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   dump_state ................... False
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-09-23 12:16:00,205] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 2048
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   pld_params ................... False
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   world_size ................... 1
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  True
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   zero_enabled ................. False
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-23 12:16:00,206] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0
[2023-09-23 12:16:00,206] [INFO] [config.py:957:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/200 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: the mean of 30 values was 140. it was detected on rechecking that one value 145 was wrongly copied as 135 for the computation of the mean. find the correct mean. a ) 151, b ) 140.33, c ) 152, d ) 148, e ) none of the above
Output: b

Input: the measurement of a rectangular box with lid is 25 cmx 24 cmx 18 cm. find the volume of the largest sphere that can be inscribed in the box ( in terms of π cm 3 ). ( hint : the lowest measure of rectangular box represents the diameter of the largest sphere ) a ) 288, b ) 2302, c ) 2304, d ) 8640, e ) 964
Output: c

Input: a certain fraction has the same ratio to 1 / 33, as 3 / 4 does to 7 / 11. what is this certain fraction? a ) 1 / 14, b ) 1 / 18, c ) 1 / 21, d ) 1 / 25, e ) 1 / 28
Output: e

Input: what is the sum of all remainders obtained when the first 110 natural numbers are divided by 9? a ) 397, b ) 401, c ) 403, d ) 405, e ) 399
Output: c

Input: a ’ s speed is 20 / 15 times that of b. if a and b run a race, what part of the length of the race should a give b as a head start, so that the race ends in a dead heat? a ) 1 / 17, b ) 3 / 17, c ) 1 / 10, d ) 5 / 20, e ) 3 / 10
Output: d

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o5-tmathqa-s30-rFalse-m4096
Evaluating commonsenseqa :   0%|          | 1/200 [00:23<1:18:21, 23.62s/it]Evaluating commonsenseqa :   1%|          | 2/200 [02:29<4:37:13, 84.01s/it]Evaluating commonsenseqa :   2%|▏         | 3/200 [04:23<5:19:43, 97.38s/it]Evaluating commonsenseqa :   2%|▏         | 4/200 [06:25<5:49:48, 107.09s/it]Evaluating commonsenseqa :   2%|▎         | 5/200 [08:12<5:48:08, 107.12s/it]Evaluating commonsenseqa :   3%|▎         | 6/200 [10:18<6:06:57, 113.49s/it]Evaluating commonsenseqa :   4%|▎         | 7/200 [11:20<5:10:47, 96.62s/it] Evaluating commonsenseqa :   4%|▍         | 8/200 [13:27<5:40:11, 106.31s/it]Evaluating commonsenseqa :   4%|▍         | 9/200 [14:26<4:51:24, 91.54s/it] Evaluating commonsenseqa :   5%|▌         | 10/200 [16:31<5:23:10, 102.06s/it]Evaluating commonsenseqa :   6%|▌         | 11/200 [17:24<4:34:19, 87.09s/it] Evaluating commonsenseqa :   6%|▌         | 12/200 [18:40<4:22:19, 83.72s/it]Evaluating commonsenseqa :   6%|▋         | 13/200 [20:42<4:56:57, 95.28s/it]Evaluating commonsenseqa :   7%|▋         | 14/200 [21:37<4:17:25, 83.04s/it]Evaluating commonsenseqa :   8%|▊         | 15/200 [23:42<4:54:50, 95.62s/it]Evaluating commonsenseqa :   8%|▊         | 16/200 [24:33<4:12:10, 82.23s/it]Evaluating commonsenseqa :   8%|▊         | 17/200 [26:40<4:51:23, 95.54s/it]Evaluating commonsenseqa :   9%|▉         | 18/200 [28:30<5:03:09, 99.94s/it]Evaluating commonsenseqa :  10%|▉         | 19/200 [30:35<5:24:14, 107.48s/it]Evaluating commonsenseqa :  10%|█         | 20/200 [31:45<4:48:47, 96.27s/it] Evaluating commonsenseqa :  10%|█         | 21/200 [32:44<4:13:43, 85.05s/it]Evaluating commonsenseqa :  11%|█         | 22/200 [34:47<4:46:22, 96.53s/it]Evaluating commonsenseqa :  12%|█▏        | 23/200 [36:13<4:35:15, 93.31s/it]Evaluating commonsenseqa :  12%|█▏        | 24/200 [38:20<5:03:26, 103.45s/it]Evaluating commonsenseqa :  12%|█▎        | 25/200 [40:25<5:20:45, 109.98s/it]Evaluating commonsenseqa :  13%|█▎        | 26/200 [42:31<5:32:45, 114.74s/it]Evaluating commonsenseqa :  14%|█▎        | 27/200 [44:36<5:39:24, 117.71s/it]Evaluating commonsenseqa :  14%|█▍        | 28/200 [46:38<5:41:35, 119.16s/it]Evaluating commonsenseqa :  14%|█▍        | 29/200 [48:41<5:42:33, 120.20s/it]Evaluating commonsenseqa :  15%|█▌        | 30/200 [50:44<5:43:12, 121.13s/it]Evaluating commonsenseqa :  16%|█▌        | 31/200 [52:36<5:33:36, 118.44s/it]Evaluating commonsenseqa :  16%|█▌        | 32/200 [53:06<4:16:57, 91.77s/it] Evaluating commonsenseqa :  16%|█▋        | 33/200 [55:11<4:43:28, 101.85s/it]Evaluating commonsenseqa :  17%|█▋        | 34/200 [57:18<5:02:47, 109.45s/it]Evaluating commonsenseqa :  18%|█▊        | 35/200 [59:21<5:11:34, 113.30s/it]Evaluating commonsenseqa :  18%|█▊        | 36/200 [1:01:26<5:19:37, 116.94s/it]Evaluating commonsenseqa :  18%|█▊        | 37/200 [1:03:30<5:23:43, 119.16s/it]Evaluating commonsenseqa :  19%|█▉        | 38/200 [1:04:47<4:47:09, 106.36s/it]Evaluating commonsenseqa :  20%|█▉        | 39/200 [1:06:51<4:59:44, 111.70s/it]Evaluating commonsenseqa :  20%|██        | 40/200 [1:08:56<5:08:18, 115.61s/it]Evaluating commonsenseqa :  20%|██        | 41/200 [1:11:00<5:13:16, 118.22s/it]Evaluating commonsenseqa :  21%|██        | 42/200 [1:13:05<5:16:27, 120.18s/it]Evaluating commonsenseqa :  22%|██▏       | 43/200 [1:15:08<5:16:31, 120.97s/it]Evaluating commonsenseqa :  22%|██▏       | 44/200 [1:16:39<4:51:15, 112.02s/it]Evaluating commonsenseqa :  22%|██▎       | 45/200 [1:18:43<4:58:29, 115.55s/it]Evaluating commonsenseqa :  23%|██▎       | 46/200 [1:20:46<5:02:33, 117.88s/it]Evaluating commonsenseqa :  24%|██▎       | 47/200 [1:22:25<4:45:50, 112.09s/it]Evaluating commonsenseqa :  24%|██▍       | 48/200 [1:23:25<4:04:40, 96.58s/it] Evaluating commonsenseqa :  24%|██▍       | 49/200 [1:25:29<4:23:53, 104.86s/it]Evaluating commonsenseqa :  25%|██▌       | 50/200 [1:27:34<4:37:25, 110.97s/it]Evaluating commonsenseqa :  26%|██▌       | 51/200 [1:29:42<4:47:41, 115.85s/it]Evaluating commonsenseqa :  26%|██▌       | 52/200 [1:31:47<4:53:06, 118.83s/it]Evaluating commonsenseqa :  26%|██▋       | 53/200 [1:33:06<4:21:55, 106.91s/it]Evaluating commonsenseqa :  27%|██▋       | 54/200 [1:35:10<4:32:17, 111.90s/it]Evaluating commonsenseqa :  28%|██▊       | 55/200 [1:37:15<4:39:55, 115.83s/it]Evaluating commonsenseqa :  28%|██▊       | 56/200 [1:39:20<4:44:37, 118.59s/it]Evaluating commonsenseqa :  28%|██▊       | 57/200 [1:41:26<4:47:43, 120.72s/it]Evaluating commonsenseqa :  29%|██▉       | 58/200 [1:43:30<4:47:54, 121.65s/it]Evaluating commonsenseqa :  30%|██▉       | 59/200 [1:45:36<4:49:02, 123.00s/it]Evaluating commonsenseqa :  30%|███       | 60/200 [1:46:52<4:14:03, 108.89s/it]Evaluating commonsenseqa :  30%|███       | 61/200 [1:48:05<3:47:42, 98.29s/it] Evaluating commonsenseqa :  31%|███       | 62/200 [1:50:12<4:05:40, 106.82s/it]Evaluating commonsenseqa :  32%|███▏      | 63/200 [1:52:17<4:16:06, 112.16s/it]Evaluating commonsenseqa :  32%|███▏      | 64/200 [1:54:21<4:22:48, 115.94s/it]Evaluating commonsenseqa :  32%|███▎      | 65/200 [1:56:26<4:27:01, 118.68s/it]Evaluating commonsenseqa :  33%|███▎      | 66/200 [1:58:30<4:28:08, 120.06s/it]Evaluating commonsenseqa :  34%|███▎      | 67/200 [2:00:34<4:28:47, 121.26s/it]Evaluating commonsenseqa :  34%|███▍      | 68/200 [2:02:38<4:28:40, 122.12s/it]Evaluating commonsenseqa :  34%|███▍      | 69/200 [2:03:21<3:34:57, 98.46s/it] Evaluating commonsenseqa :  35%|███▌      | 70/200 [2:04:47<3:25:25, 94.81s/it]Evaluating commonsenseqa :  36%|███▌      | 71/200 [2:06:52<3:42:59, 103.71s/it]Evaluating commonsenseqa :  36%|███▌      | 72/200 [2:08:21<3:31:43, 99.24s/it] Evaluating commonsenseqa :  36%|███▋      | 73/200 [2:10:26<3:46:30, 107.01s/it]Evaluating commonsenseqa :  37%|███▋      | 74/200 [2:11:32<3:19:09, 94.83s/it] Evaluating commonsenseqa :  38%|███▊      | 75/200 [2:13:37<3:36:07, 103.74s/it]Evaluating commonsenseqa :  38%|███▊      | 76/200 [2:15:41<3:46:59, 109.84s/it]Evaluating commonsenseqa :  38%|███▊      | 77/200 [2:17:48<3:56:02, 115.14s/it]Evaluating commonsenseqa :  39%|███▉      | 78/200 [2:19:03<3:29:40, 103.12s/it]Evaluating commonsenseqa :  40%|███▉      | 79/200 [2:20:46<3:27:35, 102.94s/it]Evaluating commonsenseqa :  40%|████      | 80/200 [2:22:50<3:38:47, 109.39s/it]Evaluating commonsenseqa :  40%|████      | 81/200 [2:24:54<3:45:27, 113.68s/it]Evaluating commonsenseqa :  41%|████      | 82/200 [2:25:39<3:03:18, 93.20s/it] Evaluating commonsenseqa :  42%|████▏     | 83/200 [2:26:12<2:26:25, 75.09s/it]Evaluating commonsenseqa :  42%|████▏     | 84/200 [2:27:07<2:13:34, 69.09s/it]Evaluating commonsenseqa :  42%|████▎     | 85/200 [2:29:12<2:44:20, 85.74s/it]Evaluating commonsenseqa :  43%|████▎     | 86/200 [2:30:34<2:40:33, 84.50s/it]Evaluating commonsenseqa :  44%|████▎     | 87/200 [2:31:05<2:09:20, 68.67s/it]Evaluating commonsenseqa :  44%|████▍     | 88/200 [2:33:09<2:39:06, 85.24s/it]Evaluating commonsenseqa :  44%|████▍     | 89/200 [2:35:13<2:59:03, 96.79s/it]Evaluating commonsenseqa :  45%|████▌     | 90/200 [2:37:18<3:12:51, 105.20s/it]Evaluating commonsenseqa :  46%|████▌     | 91/200 [2:39:11<3:15:17, 107.50s/it]Evaluating commonsenseqa :  46%|████▌     | 92/200 [2:40:43<3:05:06, 102.84s/it]Evaluating commonsenseqa :  46%|████▋     | 93/200 [2:41:53<2:45:56, 93.05s/it] Evaluating commonsenseqa :  47%|████▋     | 94/200 [2:43:58<3:01:29, 102.73s/it]Evaluating commonsenseqa :  48%|████▊     | 95/200 [2:44:32<2:23:47, 82.17s/it] Evaluating commonsenseqa :  48%|████▊     | 96/200 [2:45:57<2:23:36, 82.85s/it]Evaluating commonsenseqa :  48%|████▊     | 97/200 [2:48:00<2:43:15, 95.10s/it]Evaluating commonsenseqa :  49%|████▉     | 98/200 [2:50:08<2:58:20, 104.91s/it]Evaluating commonsenseqa :  50%|████▉     | 99/200 [2:52:14<3:07:10, 111.19s/it]Evaluating commonsenseqa :  50%|█████     | 100/200 [2:54:20<3:12:33, 115.53s/it]Evaluating commonsenseqa :  50%|█████     | 101/200 [2:55:39<2:52:30, 104.55s/it]Evaluating commonsenseqa :  51%|█████     | 102/200 [2:57:42<3:00:08, 110.29s/it]Evaluating commonsenseqa :  52%|█████▏    | 103/200 [2:58:49<2:37:05, 97.17s/it] Evaluating commonsenseqa :  52%|█████▏    | 104/200 [3:00:53<2:48:36, 105.38s/it]Evaluating commonsenseqa :  52%|█████▎    | 105/200 [3:02:58<2:55:53, 111.09s/it]Evaluating commonsenseqa :  53%|█████▎    | 106/200 [3:05:06<3:01:49, 116.06s/it]Evaluating commonsenseqa :  54%|█████▎    | 107/200 [3:05:57<2:29:40, 96.57s/it] Evaluating commonsenseqa :  54%|█████▍    | 108/200 [3:08:02<2:41:12, 105.13s/it]Evaluating commonsenseqa :  55%|█████▍    | 109/200 [3:10:05<2:47:53, 110.70s/it]Evaluating commonsenseqa :  55%|█████▌    | 110/200 [3:12:05<2:49:58, 113.32s/it]Evaluating commonsenseqa :  56%|█████▌    | 111/200 [3:13:54<2:46:15, 112.08s/it]Evaluating commonsenseqa :  56%|█████▌    | 112/200 [3:15:57<2:49:07, 115.32s/it]Evaluating commonsenseqa :  56%|█████▋    | 113/200 [3:17:45<2:44:14, 113.28s/it]Evaluating commonsenseqa :  57%|█████▋    | 114/200 [3:19:47<2:46:04, 115.87s/it]Evaluating commonsenseqa :  57%|█████▊    | 115/200 [3:21:50<2:47:05, 117.95s/it]Evaluating commonsenseqa :  58%|█████▊    | 116/200 [3:22:39<2:15:59, 97.13s/it] Evaluating commonsenseqa :  58%|█████▊    | 117/200 [3:24:42<2:25:01, 104.84s/it]Evaluating commonsenseqa :  59%|█████▉    | 118/200 [3:26:48<2:32:15, 111.40s/it]Evaluating commonsenseqa :  60%|█████▉    | 119/200 [3:28:52<2:35:29, 115.17s/it]Evaluating commonsenseqa :  60%|██████    | 120/200 [3:30:58<2:37:41, 118.27s/it]Evaluating commonsenseqa :  60%|██████    | 121/200 [3:33:03<2:38:26, 120.34s/it]Evaluating commonsenseqa :  61%|██████    | 122/200 [3:35:08<2:38:24, 121.85s/it]Evaluating commonsenseqa :  62%|██████▏   | 123/200 [3:37:13<2:37:18, 122.58s/it]Evaluating commonsenseqa :  62%|██████▏   | 124/200 [3:38:52<2:26:22, 115.56s/it]Evaluating commonsenseqa :  62%|██████▎   | 125/200 [3:40:57<2:28:01, 118.41s/it]Evaluating commonsenseqa :  63%|██████▎   | 126/200 [3:43:01<2:28:10, 120.14s/it]Evaluating commonsenseqa :  64%|██████▎   | 127/200 [3:44:26<2:13:29, 109.72s/it]Evaluating commonsenseqa :  64%|██████▍   | 128/200 [3:46:02<2:06:44, 105.62s/it]Evaluating commonsenseqa :  64%|██████▍   | 129/200 [3:48:05<2:10:50, 110.58s/it]Evaluating commonsenseqa :  65%|██████▌   | 130/200 [3:50:11<2:14:30, 115.29s/it]Evaluating commonsenseqa :  66%|██████▌   | 131/200 [3:52:17<2:16:23, 118.60s/it]Evaluating commonsenseqa :  66%|██████▌   | 132/200 [3:54:25<2:17:41, 121.49s/it]Evaluating commonsenseqa :  66%|██████▋   | 133/200 [3:56:29<2:16:20, 122.09s/it]Evaluating commonsenseqa :  67%|██████▋   | 134/200 [3:58:30<2:13:52, 121.71s/it]Evaluating commonsenseqa :  68%|██████▊   | 135/200 [4:00:37<2:13:34, 123.30s/it]Evaluating commonsenseqa :  68%|██████▊   | 136/200 [4:02:27<2:07:26, 119.48s/it]Evaluating commonsenseqa :  68%|██████▊   | 137/200 [4:04:23<2:04:07, 118.22s/it]Evaluating commonsenseqa :  69%|██████▉   | 138/200 [4:06:25<2:03:36, 119.61s/it]Evaluating commonsenseqa :  70%|██████▉   | 139/200 [4:08:31<2:03:28, 121.45s/it]Evaluating commonsenseqa :  70%|███████   | 140/200 [4:10:35<2:02:09, 122.16s/it]Evaluating commonsenseqa :  70%|███████   | 141/200 [4:12:32<1:58:34, 120.58s/it]Evaluating commonsenseqa :  71%|███████   | 142/200 [4:14:37<1:57:51, 121.92s/it]Evaluating commonsenseqa :  72%|███████▏  | 143/200 [4:16:40<1:56:02, 122.16s/it]Evaluating commonsenseqa :  72%|███████▏  | 144/200 [4:17:30<1:33:59, 100.70s/it]Evaluating commonsenseqa :  72%|███████▎  | 145/200 [4:19:37<1:39:22, 108.40s/it]Evaluating commonsenseqa :  73%|███████▎  | 146/200 [4:21:42<1:42:01, 113.35s/it]Evaluating commonsenseqa :  74%|███████▎  | 147/200 [4:23:44<1:42:38, 116.20s/it]Evaluating commonsenseqa :  74%|███████▍  | 148/200 [4:25:48<1:42:32, 118.32s/it]Evaluating commonsenseqa :  74%|███████▍  | 149/200 [4:27:55<1:42:56, 121.11s/it]Evaluating commonsenseqa :  75%|███████▌  | 150/200 [4:30:01<1:41:57, 122.34s/it]Evaluating commonsenseqa :  76%|███████▌  | 151/200 [4:32:04<1:40:13, 122.72s/it]Evaluating commonsenseqa :  76%|███████▌  | 152/200 [4:34:09<1:38:47, 123.49s/it]Evaluating commonsenseqa :  76%|███████▋  | 153/200 [4:36:13<1:36:40, 123.41s/it]Evaluating commonsenseqa :  77%|███████▋  | 154/200 [4:38:17<1:34:53, 123.77s/it]Evaluating commonsenseqa :  78%|███████▊  | 155/200 [4:40:20<1:32:39, 123.55s/it]Evaluating commonsenseqa :  78%|███████▊  | 156/200 [4:42:27<1:31:12, 124.38s/it]Evaluating commonsenseqa :  78%|███████▊  | 157/200 [4:44:31<1:29:04, 124.28s/it]Evaluating commonsenseqa :  79%|███████▉  | 158/200 [4:46:36<1:27:08, 124.50s/it]Evaluating commonsenseqa :  80%|███████▉  | 159/200 [4:48:40<1:25:03, 124.49s/it]Evaluating commonsenseqa :  80%|████████  | 160/200 [4:50:46<1:23:12, 124.81s/it]Evaluating commonsenseqa :  80%|████████  | 161/200 [4:52:52<1:21:21, 125.16s/it]Evaluating commonsenseqa :  81%|████████  | 162/200 [4:54:55<1:18:57, 124.67s/it]Evaluating commonsenseqa :  82%|████████▏ | 163/200 [4:55:59<1:05:32, 106.29s/it]Evaluating commonsenseqa :  82%|████████▏ | 164/200 [4:58:03<1:06:57, 111.59s/it]Evaluating commonsenseqa :  82%|████████▎ | 165/200 [4:59:38<1:02:13, 106.67s/it]Evaluating commonsenseqa :  83%|████████▎ | 166/200 [5:01:42<1:03:31, 112.09s/it]Evaluating commonsenseqa :  84%|████████▎ | 167/200 [5:03:18<58:56, 107.18s/it]  Evaluating commonsenseqa :  84%|████████▍ | 168/200 [5:04:58<55:55, 104.87s/it]Evaluating commonsenseqa :  84%|████████▍ | 169/200 [5:06:18<50:18, 97.37s/it] Evaluating commonsenseqa :  85%|████████▌ | 170/200 [5:08:23<52:54, 105.82s/it]Evaluating commonsenseqa :  86%|████████▌ | 171/200 [5:10:14<51:53, 107.36s/it]Evaluating commonsenseqa :  86%|████████▌ | 172/200 [5:12:20<52:38, 112.82s/it]Evaluating commonsenseqa :  86%|████████▋ | 173/200 [5:13:58<48:46, 108.39s/it]Evaluating commonsenseqa :  87%|████████▋ | 174/200 [5:16:03<49:13, 113.58s/it]Evaluating commonsenseqa :  88%|████████▊ | 175/200 [5:18:06<48:28, 116.36s/it]Evaluating commonsenseqa :  88%|████████▊ | 176/200 [5:19:16<40:56, 102.36s/it]Evaluating commonsenseqa :  88%|████████▊ | 177/200 [5:20:10<33:42, 87.93s/it] Evaluating commonsenseqa :  89%|████████▉ | 178/200 [5:22:16<36:24, 99.32s/it]Evaluating commonsenseqa :  90%|████████▉ | 179/200 [5:24:15<36:52, 105.35s/it]Evaluating commonsenseqa :  90%|█████████ | 180/200 [5:26:19<36:54, 110.73s/it]Evaluating commonsenseqa :  90%|█████████ | 181/200 [5:28:00<34:09, 107.89s/it]Evaluating commonsenseqa :  91%|█████████ | 182/200 [5:29:44<32:01, 106.76s/it]Evaluating commonsenseqa :  92%|█████████▏| 183/200 [5:30:02<22:42, 80.13s/it] Evaluating commonsenseqa :  92%|█████████▏| 184/200 [5:32:08<25:00, 93.77s/it]Evaluating commonsenseqa :  92%|█████████▎| 185/200 [5:34:15<25:56, 103.74s/it]Evaluating commonsenseqa :  93%|█████████▎| 186/200 [5:36:19<25:36, 109.77s/it]Evaluating commonsenseqa :  94%|█████████▎| 187/200 [5:38:23<24:42, 114.02s/it]Evaluating commonsenseqa :  94%|█████████▍| 188/200 [5:40:27<23:26, 117.18s/it]Evaluating commonsenseqa :  94%|█████████▍| 189/200 [5:42:32<21:56, 119.65s/it]Evaluating commonsenseqa :  95%|█████████▌| 190/200 [5:43:47<17:40, 106.02s/it]Evaluating commonsenseqa :  96%|█████████▌| 191/200 [5:45:52<16:46, 111.82s/it]Evaluating commonsenseqa :  96%|█████████▌| 192/200 [5:46:56<13:00, 97.59s/it] Evaluating commonsenseqa :  96%|█████████▋| 193/200 [5:49:01<12:20, 105.79s/it]Evaluating commonsenseqa :  97%|█████████▋| 194/200 [5:51:09<11:13, 112.33s/it]Evaluating commonsenseqa :  98%|█████████▊| 195/200 [5:52:12<08:07, 97.54s/it] Evaluating commonsenseqa :  98%|█████████▊| 196/200 [5:54:17<07:03, 105.91s/it]Evaluating commonsenseqa :  98%|█████████▊| 197/200 [5:55:28<04:45, 95.33s/it] Evaluating commonsenseqa :  99%|█████████▉| 198/200 [5:57:32<03:28, 104.05s/it]Evaluating commonsenseqa : 100%|█████████▉| 199/200 [5:59:08<01:41, 101.65s/it]Evaluating commonsenseqa : 100%|██████████| 200/200 [6:01:15<00:00, 109.15s/it]Evaluating commonsenseqa : 100%|██████████| 200/200 [6:01:15<00:00, 108.38s/it]
name: commonsenseqa | avg. gen lenth: 219.068 | time: 21676.654386997223s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s30-rFalse --seed 30 --max-prompt-length 4096 --num-out-domain 7 1 3 5 7 9 True False 4096 5
[2023-09-23 18:17:22,795] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o7-tmathqa-s30-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 7
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o7-tmathqa-s30-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 30
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 973358.31it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.14s/it]
 > number of parameters: 6738415616
[2023-09-23 18:17:35,867] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-09-23 18:17:35,867] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-23 18:17:36,238] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-23 18:17:36,239] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-09-23 18:17:36,239] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-23 18:17:36,239] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-23 18:17:36,239] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-09-23 18:17:36,239] [INFO] [config.py:971:print]   amp_params ................... False
[2023-09-23 18:17:36,239] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb66d30ed10>
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   dump_state ................... False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 2048
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   pld_params ................... False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-09-23 18:17:36,240] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-09-23 18:17:36,241] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-09-23 18:17:36,241] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-09-23 18:17:36,241] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-09-23 18:17:36,241] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-09-23 18:17:36,241] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-09-23 18:17:36,241] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-09-23 18:17:36,241] [INFO] [config.py:971:print]   world_size ................... 1
[2023-09-23 18:17:36,241] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  True
[2023-09-23 18:17:36,241] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-23 18:17:36,241] [INFO] [config.py:971:print]   zero_enabled ................. False
[2023-09-23 18:17:36,241] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-23 18:17:36,241] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0
[2023-09-23 18:17:36,241] [INFO] [config.py:957:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/200 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: the mean of 30 values was 140. it was detected on rechecking that one value 145 was wrongly copied as 135 for the computation of the mean. find the correct mean. a ) 151, b ) 140.33, c ) 152, d ) 148, e ) none of the above
Output: b

Input: the measurement of a rectangular box with lid is 25 cmx 24 cmx 18 cm. find the volume of the largest sphere that can be inscribed in the box ( in terms of π cm 3 ). ( hint : the lowest measure of rectangular box represents the diameter of the largest sphere ) a ) 288, b ) 2302, c ) 2304, d ) 8640, e ) 964
Output: c

Input: a certain fraction has the same ratio to 1 / 33, as 3 / 4 does to 7 / 11. what is this certain fraction? a ) 1 / 14, b ) 1 / 18, c ) 1 / 21, d ) 1 / 25, e ) 1 / 28
Output: e

Input: what is the sum of all remainders obtained when the first 110 natural numbers are divided by 9? a ) 397, b ) 401, c ) 403, d ) 405, e ) 399
Output: c

Input: a ’ s speed is 20 / 15 times that of b. if a and b run a race, what part of the length of the race should a give b as a head start, so that the race ends in a dead heat? a ) 1 / 17, b ) 3 / 17, c ) 1 / 10, d ) 5 / 20, e ) 3 / 10
Output: d

Input: find the average of all the numbers between 6 and 34 which are divisible by 7. a ) 18, b ) 20, c ) 17.5, d ) 30, e ) 32
Output: c

Input: instead of multiplying a number by 5, the number is divided by 10. what is the percentage of error obtained? a ) 96 %, b ) 98 %, c ) 95 %, d ) 97 %, e ) 96 %
Output: b

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/mathqa/o7-tmathqa-s30-rFalse-m4096
Evaluating commonsenseqa :   0%|          | 1/200 [02:05<6:54:45, 125.05s/it]Evaluating commonsenseqa :   1%|          | 2/200 [04:06<6:44:56, 122.71s/it]Evaluating commonsenseqa :   2%|▏         | 3/200 [06:07<6:41:00, 122.13s/it]Evaluating commonsenseqa :   2%|▏         | 4/200 [08:06<6:34:12, 120.68s/it]Evaluating commonsenseqa :   2%|▎         | 5/200 [10:06<6:32:28, 120.76s/it]Evaluating commonsenseqa :   3%|▎         | 6/200 [11:55<6:17:10, 116.65s/it]Evaluating commonsenseqa :   4%|▎         | 7/200 [13:16<5:37:53, 105.04s/it]Evaluating commonsenseqa :   4%|▍         | 8/200 [14:47<5:21:29, 100.46s/it]Evaluating commonsenseqa :   4%|▍         | 9/200 [16:49<5:41:12, 107.19s/it]Evaluating commonsenseqa :   5%|▌         | 10/200 [17:23<4:28:13, 84.70s/it]Evaluating commonsenseqa :   6%|▌         | 11/200 [19:24<5:01:27, 95.70s/it]Evaluating commonsenseqa :   6%|▌         | 12/200 [20:39<4:40:22, 89.48s/it]Evaluating commonsenseqa :   6%|▋         | 13/200 [22:38<5:06:11, 98.24s/it]Evaluating commonsenseqa :   7%|▋         | 14/200 [23:42<4:33:14, 88.14s/it]Evaluating commonsenseqa :   8%|▊         | 15/200 [25:43<5:02:06, 97.98s/it]Evaluating commonsenseqa :   8%|▊         | 16/200 [27:44<5:21:19, 104.78s/it]Evaluating commonsenseqa :   8%|▊         | 17/200 [29:42<5:31:37, 108.73s/it]Evaluating commonsenseqa :   9%|▉         | 18/200 [30:48<4:50:57, 95.92s/it] Evaluating commonsenseqa :  10%|▉         | 19/200 [32:08<4:34:54, 91.13s/it]Evaluating commonsenseqa :  10%|█         | 20/200 [34:08<4:59:38, 99.88s/it]Evaluating commonsenseqa :  10%|█         | 21/200 [35:41<4:51:40, 97.77s/it]Evaluating commonsenseqa :  11%|█         | 22/200 [37:23<4:53:48, 99.03s/it]Evaluating commonsenseqa :  12%|█▏        | 23/200 [39:23<5:11:01, 105.43s/it]Evaluating commonsenseqa :  12%|█▏        | 24/200 [41:00<5:01:46, 102.88s/it]Evaluating commonsenseqa :  12%|█▎        | 25/200 [42:18<4:38:22, 95.44s/it] Evaluating commonsenseqa :  13%|█▎        | 26/200 [43:29<4:15:07, 87.98s/it]Evaluating commonsenseqa :  14%|█▎        | 27/200 [45:30<4:42:45, 98.07s/it]Evaluating commonsenseqa :  14%|█▍        | 28/200 [47:31<5:00:42, 104.90s/it]Evaluating commonsenseqa :  14%|█▍        | 29/200 [49:27<5:08:28, 108.24s/it]Evaluating commonsenseqa :  15%|█▌        | 30/200 [51:27<5:16:12, 111.60s/it]Evaluating commonsenseqa :  16%|█▌        | 31/200 [53:28<5:22:57, 114.66s/it]Evaluating commonsenseqa :  16%|█▌        | 32/200 [55:07<5:07:56, 109.98s/it]Evaluating commonsenseqa :  16%|█▋        | 33/200 [57:08<5:15:17, 113.28s/it]Evaluating commonsenseqa :  17%|█▋        | 34/200 [59:07<5:18:05, 114.98s/it]Evaluating commonsenseqa :  18%|█▊        | 35/200 [1:01:06<5:19:30, 116.18s/it]Evaluating commonsenseqa :  18%|█▊        | 36/200 [1:03:05<5:19:13, 116.79s/it]Evaluating commonsenseqa :  18%|█▊        | 37/200 [1:05:05<5:19:58, 117.78s/it]Evaluating commonsenseqa :  19%|█▉        | 38/200 [1:07:04<5:19:06, 118.19s/it]Evaluating commonsenseqa :  20%|█▉        | 39/200 [1:09:06<5:20:34, 119.47s/it]Evaluating commonsenseqa :  20%|██        | 40/200 [1:09:48<4:16:26, 96.17s/it] Evaluating commonsenseqa :  20%|██        | 41/200 [1:11:49<4:34:46, 103.69s/it]Evaluating commonsenseqa :  21%|██        | 42/200 [1:13:45<4:42:43, 107.37s/it]Evaluating commonsenseqa :  22%|██▏       | 43/200 [1:14:43<4:02:06, 92.53s/it] Evaluating commonsenseqa :  22%|██▏       | 44/200 [1:16:14<3:59:34, 92.14s/it]Evaluating commonsenseqa :  22%|██▎       | 45/200 [1:17:11<3:30:23, 81.44s/it]Evaluating commonsenseqa :  23%|██▎       | 46/200 [1:18:54<3:45:45, 87.96s/it]Evaluating commonsenseqa :  24%|██▎       | 47/200 [1:20:53<4:08:01, 97.26s/it]Evaluating commonsenseqa :  24%|██▍       | 48/200 [1:22:54<4:24:22, 104.36s/it]Evaluating commonsenseqa :  24%|██▍       | 49/200 [1:24:51<4:32:04, 108.11s/it]Evaluating commonsenseqa :  25%|██▌       | 50/200 [1:25:38<3:44:51, 89.94s/it] Evaluating commonsenseqa :  26%|██▌       | 51/200 [1:27:37<4:04:27, 98.44s/it]Evaluating commonsenseqa :  26%|██▌       | 52/200 [1:29:35<4:17:21, 104.33s/it]Evaluating commonsenseqa :  26%|██▋       | 53/200 [1:30:59<4:00:32, 98.18s/it] Evaluating commonsenseqa :  27%|██▋       | 54/200 [1:32:59<4:14:51, 104.74s/it]Evaluating commonsenseqa :  28%|██▊       | 55/200 [1:35:00<4:24:56, 109.63s/it]Evaluating commonsenseqa :  28%|██▊       | 56/200 [1:36:17<3:59:47, 99.91s/it] Evaluating commonsenseqa :  28%|██▊       | 57/200 [1:37:06<3:21:28, 84.54s/it]Evaluating commonsenseqa :  29%|██▉       | 58/200 [1:39:05<3:44:44, 94.96s/it]Evaluating commonsenseqa :  30%|██▉       | 59/200 [1:41:04<4:00:04, 102.16s/it]Evaluating commonsenseqa :  30%|███       | 60/200 [1:43:02<4:09:25, 106.90s/it]Evaluating commonsenseqa :  30%|███       | 61/200 [1:45:01<4:16:30, 110.72s/it]Evaluating commonsenseqa :  31%|███       | 62/200 [1:47:04<4:22:56, 114.32s/it]Evaluating commonsenseqa :  32%|███▏      | 63/200 [1:48:14<3:50:16, 100.85s/it]Evaluating commonsenseqa :  32%|███▏      | 64/200 [1:50:16<4:03:32, 107.45s/it]Evaluating commonsenseqa :  32%|███▎      | 65/200 [1:52:19<4:11:51, 111.94s/it]Evaluating commonsenseqa :  33%|███▎      | 66/200 [1:54:20<4:16:09, 114.70s/it]Evaluating commonsenseqa :  34%|███▎      | 67/200 [1:56:20<4:18:09, 116.46s/it]Evaluating commonsenseqa :  34%|███▍      | 68/200 [1:58:21<4:18:42, 117.59s/it]Evaluating commonsenseqa :  34%|███▍      | 69/200 [2:00:23<4:19:42, 118.95s/it]Evaluating commonsenseqa :  35%|███▌      | 70/200 [2:02:24<4:19:16, 119.66s/it]Evaluating commonsenseqa :  36%|███▌      | 71/200 [2:04:24<4:17:28, 119.75s/it]Evaluating commonsenseqa :  36%|███▌      | 72/200 [2:06:23<4:14:59, 119.53s/it]Evaluating commonsenseqa :  36%|███▋      | 73/200 [2:08:24<4:13:44, 119.88s/it]Evaluating commonsenseqa :  37%|███▋      | 74/200 [2:10:19<4:08:31, 118.34s/it]Evaluating commonsenseqa :  38%|███▊      | 75/200 [2:12:18<4:07:20, 118.73s/it]Evaluating commonsenseqa :  38%|███▊      | 76/200 [2:14:19<4:06:50, 119.44s/it]Evaluating commonsenseqa :  38%|███▊      | 77/200 [2:16:19<4:05:01, 119.53s/it]Evaluating commonsenseqa :  39%|███▉      | 78/200 [2:18:22<4:05:03, 120.52s/it]Evaluating commonsenseqa :  40%|███▉      | 79/200 [2:20:24<4:03:50, 120.92s/it]Evaluating commonsenseqa :  40%|████      | 80/200 [2:22:24<4:01:32, 120.77s/it]Evaluating commonsenseqa :  40%|████      | 81/200 [2:23:37<3:31:04, 106.43s/it]Evaluating commonsenseqa :  41%|████      | 82/200 [2:25:36<3:36:38, 110.16s/it]Evaluating commonsenseqa :  42%|████▏     | 83/200 [2:27:36<3:40:26, 113.05s/it]Evaluating commonsenseqa :  42%|████▏     | 84/200 [2:29:38<3:43:50, 115.78s/it]Evaluating commonsenseqa :  42%|████▎     | 85/200 [2:31:38<3:44:18, 117.03s/it]Evaluating commonsenseqa :  43%|████▎     | 86/200 [2:32:54<3:18:46, 104.62s/it]Evaluating commonsenseqa :  44%|████▎     | 87/200 [2:33:45<2:47:08, 88.75s/it] Evaluating commonsenseqa :  44%|████▍     | 88/200 [2:35:46<3:03:20, 98.22s/it]Evaluating commonsenseqa :  44%|████▍     | 89/200 [2:37:46<3:13:46, 104.74s/it]Evaluating commonsenseqa :  45%|████▌     | 90/200 [2:39:02<2:56:22, 96.20s/it] Evaluating commonsenseqa :  46%|████▌     | 91/200 [2:41:02<3:07:54, 103.43s/it]Evaluating commonsenseqa :  46%|████▌     | 92/200 [2:43:00<3:14:12, 107.90s/it]Evaluating commonsenseqa :  46%|████▋     | 93/200 [2:45:00<3:18:47, 111.48s/it]Evaluating commonsenseqa :  47%|████▋     | 94/200 [2:47:01<3:21:54, 114.29s/it]Evaluating commonsenseqa :  48%|████▊     | 95/200 [2:47:32<2:36:13, 89.27s/it] Evaluating commonsenseqa :  48%|████▊     | 96/200 [2:49:10<2:39:06, 91.80s/it]Evaluating commonsenseqa :  48%|████▊     | 97/200 [2:50:23<2:28:08, 86.30s/it]Evaluating commonsenseqa :  49%|████▉     | 98/200 [2:52:22<2:43:21, 96.09s/it]Evaluating commonsenseqa :  50%|████▉     | 99/200 [2:54:21<2:53:21, 102.98s/it]Evaluating commonsenseqa :  50%|█████     | 100/200 [2:56:21<3:00:13, 108.13s/it]Evaluating commonsenseqa :  50%|█████     | 101/200 [2:57:15<2:31:17, 91.69s/it] Evaluating commonsenseqa :  51%|█████     | 102/200 [2:58:06<2:10:10, 79.70s/it]Evaluating commonsenseqa :  52%|█████▏    | 103/200 [2:58:35<1:44:04, 64.37s/it]Evaluating commonsenseqa :  52%|█████▏    | 104/200 [3:00:35<2:09:52, 81.17s/it]Evaluating commonsenseqa :  52%|█████▎    | 105/200 [3:02:36<2:27:10, 92.95s/it]Evaluating commonsenseqa :  53%|█████▎    | 106/200 [3:04:38<2:39:15, 101.65s/it]Evaluating commonsenseqa :  54%|█████▎    | 107/200 [3:05:19<2:09:17, 83.42s/it] Evaluating commonsenseqa :  54%|█████▍    | 108/200 [3:07:04<2:18:01, 90.02s/it]Evaluating commonsenseqa :  55%|█████▍    | 109/200 [3:09:03<2:29:54, 98.84s/it]Evaluating commonsenseqa :  55%|█████▌    | 110/200 [3:10:30<2:22:50, 95.23s/it]Evaluating commonsenseqa :  56%|█████▌    | 111/200 [3:12:28<2:31:06, 101.88s/it]Evaluating commonsenseqa :  56%|█████▌    | 112/200 [3:14:27<2:37:19, 107.27s/it]Evaluating commonsenseqa :  56%|█████▋    | 113/200 [3:16:30<2:41:57, 111.70s/it]Evaluating commonsenseqa :  57%|█████▋    | 114/200 [3:18:31<2:44:06, 114.49s/it]Evaluating commonsenseqa :  57%|█████▊    | 115/200 [3:20:02<2:32:29, 107.64s/it]Evaluating commonsenseqa :  58%|█████▊    | 116/200 [3:22:02<2:35:51, 111.33s/it]Evaluating commonsenseqa :  58%|█████▊    | 117/200 [3:24:02<2:37:39, 113.97s/it]Evaluating commonsenseqa :  59%|█████▉    | 118/200 [3:26:03<2:38:21, 115.87s/it]Evaluating commonsenseqa :  60%|█████▉    | 119/200 [3:28:03<2:38:12, 117.20s/it]Evaluating commonsenseqa :  60%|██████    | 120/200 [3:30:04<2:37:49, 118.37s/it]Evaluating commonsenseqa :  60%|██████    | 121/200 [3:32:02<2:35:41, 118.25s/it]Evaluating commonsenseqa :  61%|██████    | 122/200 [3:34:01<2:34:06, 118.55s/it]Evaluating commonsenseqa :  62%|██████▏   | 123/200 [3:35:00<2:09:06, 100.60s/it]Evaluating commonsenseqa :  62%|██████▏   | 124/200 [3:36:39<2:06:49, 100.13s/it]Evaluating commonsenseqa :  62%|██████▎   | 125/200 [3:38:40<2:13:05, 106.48s/it]Evaluating commonsenseqa :  63%|██████▎   | 126/200 [3:40:42<2:17:03, 111.13s/it]Evaluating commonsenseqa :  64%|██████▎   | 127/200 [3:42:23<2:11:26, 108.04s/it]Evaluating commonsenseqa :  64%|██████▍   | 128/200 [3:44:22<2:13:26, 111.21s/it]Evaluating commonsenseqa :  64%|██████▍   | 129/200 [3:45:07<1:48:14, 91.47s/it] Evaluating commonsenseqa :  65%|██████▌   | 130/200 [3:46:13<1:37:49, 83.85s/it]Evaluating commonsenseqa :  66%|██████▌   | 131/200 [3:48:14<1:49:15, 95.00s/it]Evaluating commonsenseqa :  66%|██████▌   | 132/200 [3:50:14<1:56:06, 102.45s/it]Evaluating commonsenseqa :  66%|██████▋   | 133/200 [3:51:45<1:50:32, 98.99s/it] Evaluating commonsenseqa :  67%|██████▋   | 134/200 [3:52:57<1:39:58, 90.89s/it]Evaluating commonsenseqa :  68%|██████▊   | 135/200 [3:54:07<1:31:52, 84.81s/it]Evaluating commonsenseqa :  68%|██████▊   | 136/200 [3:56:07<1:41:41, 95.34s/it]Evaluating commonsenseqa :  68%|██████▊   | 137/200 [3:56:37<1:19:25, 75.64s/it]Evaluating commonsenseqa :  69%|██████▉   | 138/200 [3:58:38<1:32:18, 89.33s/it]Evaluating commonsenseqa :  70%|██████▉   | 139/200 [4:00:39<1:40:28, 98.83s/it]Evaluating commonsenseqa :  70%|███████   | 140/200 [4:01:59<1:33:02, 93.04s/it]Evaluating commonsenseqa :  70%|███████   | 141/200 [4:04:00<1:39:46, 101.46s/it]Evaluating commonsenseqa :  71%|███████   | 142/200 [4:04:49<1:22:57, 85.82s/it] Evaluating commonsenseqa :  72%|███████▏  | 143/200 [4:06:36<1:27:21, 91.95s/it]Evaluating commonsenseqa :  72%|███████▏  | 144/200 [4:07:53<1:21:49, 87.66s/it]Evaluating commonsenseqa :  72%|███████▎  | 145/200 [4:09:55<1:29:44, 97.90s/it]Evaluating commonsenseqa :  73%|███████▎  | 146/200 [4:11:53<1:33:33, 103.95s/it]Evaluating commonsenseqa :  74%|███████▎  | 147/200 [4:13:55<1:36:29, 109.24s/it]Evaluating commonsenseqa :  74%|███████▍  | 148/200 [4:15:53<1:36:56, 111.85s/it]Evaluating commonsenseqa :  74%|███████▍  | 149/200 [4:17:54<1:37:31, 114.73s/it]Evaluating commonsenseqa :  75%|███████▌  | 150/200 [4:19:39<1:33:07, 111.76s/it]Evaluating commonsenseqa :  76%|███████▌  | 151/200 [4:21:38<1:33:04, 113.96s/it]Evaluating commonsenseqa :  76%|███████▌  | 152/200 [4:23:38<1:32:34, 115.72s/it]Evaluating commonsenseqa :  76%|███████▋  | 153/200 [4:25:39<1:31:53, 117.31s/it]Evaluating commonsenseqa :  77%|███████▋  | 154/200 [4:27:39<1:30:43, 118.33s/it]Evaluating commonsenseqa :  78%|███████▊  | 155/200 [4:29:39<1:28:54, 118.54s/it]Evaluating commonsenseqa :  78%|███████▊  | 156/200 [4:31:40<1:27:33, 119.40s/it]Evaluating commonsenseqa :  78%|███████▊  | 157/200 [4:32:53<1:15:37, 105.52s/it]Evaluating commonsenseqa :  79%|███████▉  | 158/200 [4:34:54<1:17:00, 110.02s/it]Evaluating commonsenseqa :  80%|███████▉  | 159/200 [4:36:25<1:11:21, 104.43s/it]Evaluating commonsenseqa :  80%|████████  | 160/200 [4:38:27<1:13:10, 109.77s/it]Evaluating commonsenseqa :  80%|████████  | 161/200 [4:39:35<1:03:09, 97.17s/it] Evaluating commonsenseqa :  81%|████████  | 162/200 [4:41:39<1:06:36, 105.17s/it]Evaluating commonsenseqa :  82%|████████▏ | 163/200 [4:42:47<57:56, 93.96s/it]   Evaluating commonsenseqa :  82%|████████▏ | 164/200 [4:44:48<1:01:23, 102.31s/it]Evaluating commonsenseqa :  82%|████████▎ | 165/200 [4:46:47<1:02:33, 107.25s/it]Evaluating commonsenseqa :  83%|████████▎ | 166/200 [4:48:34<1:00:39, 107.05s/it]Evaluating commonsenseqa :  84%|████████▎ | 167/200 [4:50:31<1:00:32, 110.06s/it]Evaluating commonsenseqa :  84%|████████▍ | 168/200 [4:52:33<1:00:38, 113.71s/it]Evaluating commonsenseqa :  84%|████████▍ | 169/200 [4:54:35<1:00:01, 116.17s/it]Evaluating commonsenseqa :  85%|████████▌ | 170/200 [4:56:34<58:30, 117.00s/it]  Evaluating commonsenseqa :  86%|████████▌ | 171/200 [4:58:19<54:47, 113.37s/it]Evaluating commonsenseqa :  86%|████████▌ | 172/200 [5:00:18<53:41, 115.07s/it]Evaluating commonsenseqa :  86%|████████▋ | 173/200 [5:01:21<44:46, 99.49s/it] Evaluating commonsenseqa :  87%|████████▋ | 174/200 [5:03:20<45:42, 105.49s/it]Evaluating commonsenseqa :  88%|████████▊ | 175/200 [5:05:20<45:44, 109.78s/it]Evaluating commonsenseqa :  88%|████████▊ | 176/200 [5:06:38<40:07, 100.30s/it]Evaluating commonsenseqa :  88%|████████▊ | 177/200 [5:08:37<40:31, 105.73s/it]Evaluating commonsenseqa :  89%|████████▉ | 178/200 [5:10:35<40:05, 109.35s/it]Evaluating commonsenseqa :  90%|████████▉ | 179/200 [5:12:32<39:05, 111.71s/it]Evaluating commonsenseqa :  90%|█████████ | 180/200 [5:13:42<33:06, 99.32s/it] Evaluating commonsenseqa :  90%|█████████ | 181/200 [5:15:42<33:20, 105.31s/it]Evaluating commonsenseqa :  91%|█████████ | 182/200 [5:17:40<32:45, 109.22s/it]Evaluating commonsenseqa :  92%|█████████▏| 183/200 [5:19:41<31:57, 112.81s/it]Evaluating commonsenseqa :  92%|█████████▏| 184/200 [5:21:42<30:42, 115.17s/it]Evaluating commonsenseqa :  92%|█████████▎| 185/200 [5:23:43<29:17, 117.13s/it]Evaluating commonsenseqa :  93%|█████████▎| 186/200 [5:25:42<27:27, 117.65s/it]Evaluating commonsenseqa :  94%|█████████▎| 187/200 [5:27:43<25:40, 118.48s/it]Evaluating commonsenseqa :  94%|█████████▍| 188/200 [5:29:09<21:45, 108.76s/it]Evaluating commonsenseqa :  94%|█████████▍| 189/200 [5:31:08<20:29, 111.81s/it]Evaluating commonsenseqa :  95%|█████████▌| 190/200 [5:33:08<19:02, 114.24s/it]Evaluating commonsenseqa :  96%|█████████▌| 191/200 [5:35:05<17:17, 115.27s/it]Evaluating commonsenseqa :  96%|█████████▌| 192/200 [5:37:04<15:31, 116.38s/it]Evaluating commonsenseqa :  96%|█████████▋| 193/200 [5:39:04<13:42, 117.48s/it]Evaluating commonsenseqa :  97%|█████████▋| 194/200 [5:41:04<11:48, 118.12s/it]Evaluating commonsenseqa :  98%|█████████▊| 195/200 [5:43:04<09:54, 118.83s/it]Evaluating commonsenseqa :  98%|█████████▊| 196/200 [5:44:51<07:41, 115.28s/it]Evaluating commonsenseqa :  98%|█████████▊| 197/200 [5:46:27<05:28, 109.50s/it]Evaluating commonsenseqa :  99%|█████████▉| 198/200 [5:48:30<03:46, 113.45s/it]Evaluating commonsenseqa : 100%|█████████▉| 199/200 [5:50:11<01:49, 109.57s/it]Evaluating commonsenseqa : 100%|██████████| 200/200 [5:52:08<00:00, 112.01s/it]Evaluating commonsenseqa : 100%|██████████| 200/200 [5:52:08<00:00, 105.64s/it]
name: commonsenseqa | avg. gen lenth: 223.241 | time: 21129.9234251976s
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10182 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s30-rFalse --seed 30 --max-prompt-length 4096 --num-out-domain 9 1 3 5 7 9 True False 4096 5
[2023-09-24 00:09:53,965] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
