WORLD_SIZE=1
MASTER_ADDR=icgpu05
WORLD_SIZE=1
WORLD_SIZE=1
MASTER_ADDR=icgpu05
MASTER_ADDR=icgpu05
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13644 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 5 5 7 True False 4096 10
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13646 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 13 13 15 True False 4096 10
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu05 --master_port 13645 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name mathqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 9 9 11 True False 4096 10
[2023-09-09 17:19:11,842] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-09 17:19:11,842] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-09 17:19:11,842] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
using world size: 1
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o5-tmathqa-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 5
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o5-tmathqa-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample ......speed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 1340355.46it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s].......... 1
  world_size ................... 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o13-tmathqa-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 13
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tmathqa-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o9-tmathqa-s1-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 9
  out_domain_data_name ......... mathqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o9-tmathqa-s1-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 679676.52it/s]
Num instances: 1000
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 507133.65it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 400371.55it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.86s/it]
 > number of parameters: 6738415616
[2023-09-09 17:19:23,948] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-09 17:19:24,150] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-09 17:19:24,152] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554b5f82430>
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-09 17:19:24,152] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-09 17:19:24,153] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-09 17:19:24,154] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-09 17:19:24,154] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: "we assume that 80 is 100 % assume'x'is value we looking for here, 80 = 100 % and x = 12 % therefore, 80 / x = 100 % / 12 % 80 / x = 8.33 x = 9.6 b"
So the final answer is b

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o1-tmathqa-s1-rTrue-m4096
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s][2023-09-09 17:19:25,622] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-09 17:19:25,624] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-09 17:19:25,624] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-09 17:19:25,624] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-09 17:19:25,624] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-09 17:19:25,624] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-09 17:19:25,624] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-09 17:19:25,624] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554b5f111f0>
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-09 17:19:25,625] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-09 17:19:25,626] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-09 17:19:25,626] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s][2023-09-09 17:19:25,641] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-09 17:19:25,643] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba15f280>
[2023-09-09 17:19:25,643] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-09 17:19:25,644] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-09 17:19:25,645] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-09 17:19:25,645] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: "we assume that 80 is 100 % assume'x'is value we looking for here, 80 = 100 % and x = 12 % therefore, 80 / x = 100 % / 12 % 80 / x = 8.33 x = 9.6 b"
So the final answer is b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: a : b = 92000 : 20000 = 92 : 20 = 46 : 10 = 23 : 5 answer : c
So the final answer is c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: "actual cost = $ 60 sp = actual cost + mark up = actual cost + 20 % sp = 60 * 100 / 80 on sale sp = 80 / 100 ( 60 * 100 / 80 ) = 60 gross profit = $ 0 answer is a"
So the final answer is a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: "apparently it's q = x ^ 2 - x ^ 3 instead of q = x ^ 2 - q ^ 3. in this case : the x - intercept is the value ( s ) of x for q = 0. 0 = x ^ 2 - x ^ 3 ; 0 = x ^ 2 ( 1 - x ) ; x = 0 or x = 1. answer : c."
So the final answer is c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: "official solution : the problem asks for the amount of weekly sales it takes for angelo and isabella to earn the same amount of money. you can write an equation that sets angelo ’ s and isabella ’ s weekly earnings equal to each other, with x representing weekly sales. weekly earnings for each salesperson equal base salary plus commission. so angelo ’ s earnings are 550 + ( 0.08 ) ( x – 3,000 ), and isabella ’ s are 0.10 x. set up the equation and solve : 550 + ( 0.08 ) ( x – 3,000 ) = 0.10 x distribute the 0.08 : 550 + 0.08 x – 240 = 0.10 x combine terms and subtract 0.08 x from both sides : 310 = 0.02 x divide both sides by 0.02 : 15,500 = x your answer is b."
So the final answer is b

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o5-tmathqa-s1-rTrue-m4096
############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: "we assume that 80 is 100 % assume'x'is value we looking for here, 80 = 100 % and x = 12 % therefore, 80 / x = 100 % / 12 % 80 / x = 8.33 x = 9.6 b"
So the final answer is b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: a : b = 92000 : 20000 = 92 : 20 = 46 : 10 = 23 : 5 answer : c
So the final answer is c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: "actual cost = $ 60 sp = actual cost + mark up = actual cost + 20 % sp = 60 * 100 / 80 on sale sp = 80 / 100 ( 60 * 100 / 80 ) = 60 gross profit = $ 0 answer is a"
So the final answer is a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: "apparently it's q = x ^ 2 - x ^ 3 instead of q = x ^ 2 - q ^ 3. in this case : the x - intercept is the value ( s ) of x for q = 0. 0 = x ^ 2 - x ^ 3 ; 0 = x ^ 2 ( 1 - x ) ; x = 0 or x = 1. answer : c."
So the final answer is c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: "official solution : the problem asks for the amount of weekly sales it takes for angelo and isabella to earn the same amount of money. you can write an equation that sets angelo ’ s and isabella ’ s weekly earnings equal to each other, with x representing weekly sales. weekly earnings for each salesperson equal base salary plus commission. so angelo ’ s earnings are 550 + ( 0.08 ) ( x – 3,000 ), and isabella ’ s are 0.10 x. set up the equation and solve : 550 + ( 0.08 ) ( x – 3,000 ) = 0.10 x distribute the 0.08 : 550 + 0.08 x – 240 = 0.10 x combine terms and subtract 0.08 x from both sides : 310 = 0.02 x divide both sides by 0.02 : 15,500 = x your answer is b."
So the final answer is b

Input: find the total number of prime factors in the expression ( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 a ) 26, b ) 22, c ) 25, d ) 30, e ) 29
Output: "( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = ( 2 x 2 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = 2 ^ 11 x 2 ^ 11 x 7 ^ 5 x 11 ^ 3 = 2 ^ 22 x 7 ^ 5 x 11 ^ 3 total number of prime factors = ( 22 + 5 + 3 ) = 30. answer is d."
So the final answer is d

Input: the length of the bridge, which a train 140 m long and traveling at 45 km / hr can cross in 30 sec is? a ) 235, b ) 240, c ) 245, d ) 250, e ) 255
Output: "speed = 45 * 5 / 18 = 25 / 2 m / sec. time = 30 sec let the length of bridge be x meters. then, ( 140 + x ) / 30 = 25 / 2 x = 235 m. answer : option a"
So the final answer is a

Input: 360 metres long yard, 31 trees are palnted at equal distances, one tree being at each end of the yard. what is the distance between 2 consecutive trees a ) 10, b ) 12, c ) 14, d ) 16, e ) 17
Output: "31 trees have 30 gaps between them, required distance ( 360 / 30 ) = 12 b"
So the final answer is b

Input: a vessel of capacity 45 litres is fully filled with pure milk. nine litres of milk is removed from the vessel and replaced with water. nine litres of the solution thus formed is removed and replaced with water. find the quantity of pure milk in the final milk solution? a ) 28.8, b ) 72.9, c ) 38.3, d ) 78.3, e ) 79.3
Output: "explanation : let the initial quantity of milk in vessel be t litres. let us say y litres of the mixture is taken out and replaced by water for n times, alternatively. quantity of milk finally in the vessel is then given by [ ( t - y ) / t ] ^ n * t for the given problem, t = 45, y = 9 and n = 2. hence, quantity of milk finally in the vessel = [ ( 45 - 9 ) / 45 ] ^ 2 ( 45 ) = 28.8 litres. answer : option a"
So the final answer is a

Input: a certain telescope increases the visual range at a particular location from 90 kilometers to 150 kilometers. by what percent is the visual range increased by using the telescope? a ) 30 %, b ) 33 1 / 2 %, c ) 40 %, d ) 60 %, e ) 66 2 / 3 %
Output: "original visual range = 90 km new visual range = 150 km percent increase in the visual range by using the telescope = ( 150 - 90 ) / 90 * 100 % = 2 / 3 * 100 % = 66.67 % answer e"
So the final answer is e

Input: a bag contains 6 black and 3 white balls. one ball is drawn at random. what is the probability that the ball drawn is white? a ) 3 / 4, b ) 1 / 3, c ) 1 / 7, d ) 1 / 8, e ) 4 / 3
Output: "let number of balls = ( 6 + 3 ) = 9. number of white balls = 3. p ( drawing a white ball ) = 3 / 9 = 1 / 3. option b."
So the final answer is b

Input: rajat, vikas and abhishek are submitting questions in the ratio 7 : 3 : 2. if total number of questions submitted by them is 24. find the number of questions submitted by vikas. a ) 3, b ) 4, c ) 5, d ) 6, e ) 7
Output: explanation : number of questions submitted by vikas = ( 24 * 3 ) / ( 7 + 3 + 2 ) = 6 answer : d
So the final answer is d

Input: at a certain restaurant, the ratio of the number of cooks to the number of waiters is 3 to 8. when 12 more waiters are hired, the ratio of the number of cooks to the number of waiters changes to 1 to 4. how many cooks does the restaurant have? a ) 4, b ) 6, c ) 9, d ) 12, e ) 15
Output: originally there were 3 k cooks and 8 k waiters. the new ratio is 1 : 4 which equals 3 : 12. 12 k = 8 k + 12 k = 3 there are 9 cooks. the answer is c.
So the final answer is c

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tmathqa-s1-rTrue-m4096
############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: what is 12 percent of 80? a ) 11.21, b ) 9.6, c ) 8.66, d ) 12.23, e ) 13.1
Output: "we assume that 80 is 100 % assume'x'is value we looking for here, 80 = 100 % and x = 12 % therefore, 80 / x = 100 % / 12 % 80 / x = 8.33 x = 9.6 b"
So the final answer is b

Input: a and b started a business investing rs. 92,000 and rs 20,000 respectively. in what ratio the profit earned after 2 years be divided between a and b respectively? a ) 9 : 2, b ) 3 : 2, c ) 23 : 5, d ) 18 : 4, e ) 17 : 4
Output: a : b = 92000 : 20000 = 92 : 20 = 46 : 10 = 23 : 5 answer : c
So the final answer is c

Input: a merchant purchased a jacket for $ 60 and then determined a selling price that equalled the purchase price of the jacket plus a markup that was 20 percent of the selling price. during a sale, the merchant discounted the selling price by 20 percent and sold the jacket. what was the merchant ’ s gross profit on this sale? a ) $ 0, b ) $ 3, c ) $ 4, d ) $ 12, e ) $ 15
Output: "actual cost = $ 60 sp = actual cost + mark up = actual cost + 20 % sp = 60 * 100 / 80 on sale sp = 80 / 100 ( 60 * 100 / 80 ) = 60 gross profit = $ 0 answer is a"
So the final answer is a

Input: on a two - dimensional coordinate plane, the line q = x ^ 2 - x ^ 3 touches the x - axis in how many places? a ) 0, b ) 1, c ) 2, d ) 3, e ) 4
Output: "apparently it's q = x ^ 2 - x ^ 3 instead of q = x ^ 2 - q ^ 3. in this case : the x - intercept is the value ( s ) of x for q = 0. 0 = x ^ 2 - x ^ 3 ; 0 = x ^ 2 ( 1 - x ) ; x = 0 or x = 1. answer : c."
So the final answer is c

Input: angelo and isabella are both salespersons. in any given week, angelo makes $ 550 in base salary plus 8 percent of the portion of his sales above $ 3,000 for that week. isabella makes 10 percent of her total sales for any given week. for what amount of weekly sales would angelo and isabella earn the same amount of money? a ) 23,500, b ) 15,500, c ) 25,500, d ) 26,500, e ) 27,500
Output: "official solution : the problem asks for the amount of weekly sales it takes for angelo and isabella to earn the same amount of money. you can write an equation that sets angelo ’ s and isabella ’ s weekly earnings equal to each other, with x representing weekly sales. weekly earnings for each salesperson equal base salary plus commission. so angelo ’ s earnings are 550 + ( 0.08 ) ( x – 3,000 ), and isabella ’ s are 0.10 x. set up the equation and solve : 550 + ( 0.08 ) ( x – 3,000 ) = 0.10 x distribute the 0.08 : 550 + 0.08 x – 240 = 0.10 x combine terms and subtract 0.08 x from both sides : 310 = 0.02 x divide both sides by 0.02 : 15,500 = x your answer is b."
So the final answer is b

Input: find the total number of prime factors in the expression ( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 a ) 26, b ) 22, c ) 25, d ) 30, e ) 29
Output: "( 4 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = ( 2 x 2 ) ^ 11 x ( 7 ) ^ 5 x ( 11 ) ^ 3 = 2 ^ 11 x 2 ^ 11 x 7 ^ 5 x 11 ^ 3 = 2 ^ 22 x 7 ^ 5 x 11 ^ 3 total number of prime factors = ( 22 + 5 + 3 ) = 30. answer is d."
So the final answer is d

Input: the length of the bridge, which a train 140 m long and traveling at 45 km / hr can cross in 30 sec is? a ) 235, b ) 240, c ) 245, d ) 250, e ) 255
Output: "speed = 45 * 5 / 18 = 25 / 2 m / sec. time = 30 sec let the length of bridge be x meters. then, ( 140 + x ) / 30 = 25 / 2 x = 235 m. answer : option a"
So the final answer is a

Input: 360 metres long yard, 31 trees are palnted at equal distances, one tree being at each end of the yard. what is the distance between 2 consecutive trees a ) 10, b ) 12, c ) 14, d ) 16, e ) 17
Output: "31 trees have 30 gaps between them, required distance ( 360 / 30 ) = 12 b"
So the final answer is b

Input: a vessel of capacity 45 litres is fully filled with pure milk. nine litres of milk is removed from the vessel and replaced with water. nine litres of the solution thus formed is removed and replaced with water. find the quantity of pure milk in the final milk solution? a ) 28.8, b ) 72.9, c ) 38.3, d ) 78.3, e ) 79.3
Output: "explanation : let the initial quantity of milk in vessel be t litres. let us say y litres of the mixture is taken out and replaced by water for n times, alternatively. quantity of milk finally in the vessel is then given by [ ( t - y ) / t ] ^ n * t for the given problem, t = 45, y = 9 and n = 2. hence, quantity of milk finally in the vessel = [ ( 45 - 9 ) / 45 ] ^ 2 ( 45 ) = 28.8 litres. answer : option a"
So the final answer is a

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o9-tmathqa-s1-rTrue-m4096
Evaluating commonsenseqa :   1%|          | 1/100 [02:03<3:23:56, 123.60s/it]Evaluating commonsenseqa :   1%|          | 1/100 [02:38<4:21:59, 158.78s/it]Evaluating commonsenseqa :   1%|          | 1/100 [03:30<5:47:36, 210.67s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [04:05<3:20:38, 122.84s/it]                                                                              Evaluating commonsenseqa :   2%|▏         | 2/100 [05:14<4:16:39, 157.14s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [06:09<3:18:53, 123.02s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [06:57<5:40:42, 208.60s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [07:47<4:11:02, 155.28s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [08:10<3:15:34, 122.23s/it]                                                                                Evaluating commonsenseqa :   5%|▌         | 5/100 [10:11<3:12:50, 121.80s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [10:23<4:08:48, 155.50s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [10:25<5:36:25, 208.10s/it]