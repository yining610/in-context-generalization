WORLD_SIZE=1
MASTER_ADDR=ia1

CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10080 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 4 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 1 1 2 3 4 5 True 4096 4
[2023-09-18 22:16:21,375] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Traceback (most recent call last):
  File "/home/ylu130/workspace/in-context-generalization/inference.py", line 21, in <module>
    from inference_main import inference_main, prepare_dataset_main
  File "/home/ylu130/workspace/in-context-generalization/inference_main.py", line 2, in <module>
    from transformers import GenerationConfig
ImportError: cannot import name 'GenerationConfig' from 'transformers' (/opt/anaconda3/lib/python3.10/site-packages/transformers/__init__.py)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 1518266) of binary: /opt/anaconda3/bin/python
Traceback (most recent call last):
  File "/home/ylu130/.local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ylu130/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ylu130/workspace/in-context-generalization/inference.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-18_22:16:24
  host      : ia1.wse.jhu.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1518266)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torchrun --nproc_per_node 1 --nnodes 1 --master_addr ia1 --master_port 10080 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 4 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o2-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 2 1 2 3 4 5 True 4096 4
slurmstepd: error: *** JOB 80 ON ia1 CANCELLED AT 2023-09-18T22:16:25 ***
