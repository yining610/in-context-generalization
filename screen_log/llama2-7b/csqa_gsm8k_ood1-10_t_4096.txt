WORLD_SIZE=1
MASTER_ADDR=icgpu01
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu01 --master_port 17107 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o1-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 1 1 2 3 4 5 True 4096 10
[2023-09-09 14:50:55,385] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu01 --master_port 17107 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o2-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 2 1 2 3 4 5 True 4096 10
[2023-09-09 14:51:02,109] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu01 --master_port 17107 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o3-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 3 1 2 3 4 5 True 4096 10
[2023-09-09 14:51:09,060] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu01 --master_port 17107 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o4-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 4 1 2 3 4 5 True 4096 10
[2023-09-09 14:51:15,935] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu01 --master_port 17107 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o5-tgsm8k-s1-rTrue --seed 1 --max-prompt-length 4096 --rationales --num-out-domain 5 1 2 3 4 5 True 4096 10
[2023-09-09 14:51:22,934] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu01 --master_port 17107 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o1-tgsm8k-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 1 1 2 3 4 5 True 4096 10
[2023-09-09 14:51:29,847] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu01 --master_port 17107 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o2-tgsm8k-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 2 1 2 3 4 5 True 4096 10
[2023-09-09 14:51:36,678] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu01 --master_port 17107 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o3-tgsm8k-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 3 1 2 3 4 5 True 4096 10
[2023-09-09 14:51:43,535] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu01 --master_port 17107 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o4-tgsm8k-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 4 1 2 3 4 5 True 4096 10
[2023-09-09 14:51:50,481] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu01 --master_port 17107 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o5-tgsm8k-s10-rTrue --seed 10 --max-prompt-length 4096 --rationales --num-out-domain 5 1 2 3 4 5 True 4096 10
[2023-09-09 14:51:57,384] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu01 --master_port 17107 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o1-tgsm8k-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 1 1 2 3 4 5 True 4096 10
[2023-09-09 14:52:04,250] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu01 --master_port 17107 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o2-tgsm8k-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 2 1 2 3 4 5 True 4096 10
[2023-09-09 14:52:11,149] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu01 --master_port 17107 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o3-tgsm8k-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 3 1 2 3 4 5 True 4096 10
[2023-09-09 14:52:18,041] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
Answers already exist, exiting...
torchrun --nproc_per_node 1 --nnodes 1 --master_addr icgpu01 --master_port 17107 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch4/danielk/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --is-slurm --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 10 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o4-tgsm8k-s20-rTrue --seed 20 --max-prompt-length 4096 --rationales --num-out-domain 4 1 2 3 4 5 True 4096 10
[2023-09-09 14:52:24,930] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch4/danielk/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch4/danielk/ylu130/processed_data/commonsenseqa/out-domain/o4-tgsm8k-s20-rTrue
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 4
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o4-tgsm8k-s20-rTrue-m4096
  rationales ................... True
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 10
  seed ......................... 20
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  gpus_per_node ................ 1
  world_size ................... 1
Loading data:   0%|          | 0/9741 [00:00<?, ?it/s]Loading data: 100%|██████████| 9741/9741 [00:00<00:00, 739568.37it/s]
Num instances: 1000
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.25s/it]
 > number of parameters: 6738415616
[2023-09-09 14:52:37,062] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-09 14:52:37,372] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-09 14:52:37,374] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-09 14:52:37,374] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-09 14:52:37,374] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-09 14:52:37,374] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-09 14:52:37,374] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-09 14:52:37,374] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-09 14:52:37,374] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-09 14:52:37,374] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-09 14:52:37,374] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-09 14:52:37,374] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-09 14:52:37,374] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1554ba3c21f0>
[2023-09-09 14:52:37,374] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-09 14:52:37,374] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-09 14:52:37,375] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-09 14:52:37,376] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-09 14:52:37,376] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-09 14:52:37,376] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-09 14:52:37,376] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-09-09 14:52:37,376] [INFO] [config.py:964:print]   train_batch_size ............. 1
[2023-09-09 14:52:37,376] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-09 14:52:37,376] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-09 14:52:37,376] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-09 14:52:37,376] [INFO] [config.py:964:print]   world_size ................... 1
[2023-09-09 14:52:37,376] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-09 14:52:37,376] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-09 14:52:37,376] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-09 14:52:37,376] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-09 14:52:37,376] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-09 14:52:37,376] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|          | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Tapanga and Corey have 66 candies together. However, Tapanga has 8 more candies than Corey. How many candies does Corey have?
Output: Let x = the total number of candies Corey has.
x + 8 = the total number of candies Tapanga has.
The equation for the total number of candies is x + (x + 8) = 66
Combining like terms, we get 2x + 8 = 66
Subtracting 8 from both sides, we get 2x = 58
Dividing both sides by 2, we get x = <<29=29>>29, so Corey has 29 candies.
So the final answer is 29

Input: Freddy is calling his family on New Year's Eve. He calls his dad, who lives in the same city as him, and they talk for 45 minutes. Then he calls his brother, who lives on the other side of the world, and they talk for 31 minutes. Local calls cost 5 cents a minute, while international calls cost 25 cents a minute. How many dollars did Freddy spend calling his family on New Year's Eve?
Output: At 5 cents a minute, calling his father cost Freddy 5* 45 = <<5*45=225>>225 cents.
At 25 cents a minute, calling his brother cost Freddy 25 * 31 = <<25*31=775>>775 cents.
Adding the cost of calling his father and brother, we find that Freddy paid a total of 225 + 775 = <<225+775=1000>>1000 cents.
Since each dollar has 100 cents, Freddy paid 1000 / 100 = <<1000/100=10>>10 dollars
So the final answer is 10

Input: Lawrence worked 8 hours each day on Monday, Tuesday and Friday. He worked 5.5 hours on both Wednesday and Thursday. How many hours would Lawrence work each day if he worked the same number of hours each day?
Output: 8 hours * 3 = <<8*3=24>>24 hours
5.5 * 2 = <<5.5*2=11>>11 hours
24 + 11 = <<24+11=35>>35 hours
35/7 = <<35/7=5>>5 hours
Lawrence would work 5 hours each of the 7 days in a week.
So the final answer is 5

Input: Ali had a stock of 800 books in his Room. He sold 60 on Monday, 10 on Tuesday, 20 on Wednesday, 44 on Thursday and 66 on Friday. How many books were not sold?
Output: We look first for the total number of books that were sold: 60 + 10 + 20 + 44 + 66 = <<60+10+20+44+66=200>>200 books.
So the total number of books that were not sold is: 800 – 200 = <<800-200=600>>600 books.
So the final answer is 600

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o4-tgsm8k-s20-rTrue-m4096
Evaluating commonsenseqa :   1%|          | 1/100 [03:52<6:24:22, 232.96s/it]Evaluating commonsenseqa :   2%|▏         | 2/100 [07:38<6:13:44, 228.82s/it]Evaluating commonsenseqa :   3%|▎         | 3/100 [11:24<6:07:54, 227.57s/it]Evaluating commonsenseqa :   4%|▍         | 4/100 [15:12<6:04:10, 227.61s/it]Evaluating commonsenseqa :   5%|▌         | 5/100 [18:58<5:59:17, 226.92s/it]Evaluating commonsenseqa :   6%|▌         | 6/100 [22:44<5:55:10, 226.71s/it]Evaluating commonsenseqa :   7%|▋         | 7/100 [26:30<5:50:54, 226.39s/it]Evaluating commonsenseqa :   8%|▊         | 8/100 [30:17<5:47:22, 226.54s/it]Evaluating commonsenseqa :   9%|▉         | 9/100 [34:08<5:45:41, 227.93s/it]Evaluating commonsenseqa :  10%|█         | 10/100 [37:55<5:41:30, 227.68s/it]Evaluating commonsenseqa :  11%|█         | 11/100 [41:45<5:38:45, 228.38s/it]Evaluating commonsenseqa :  12%|█▏        | 12/100 [45:36<5:36:08, 229.19s/it]Evaluating commonsenseqa :  13%|█▎        | 13/100 [49:28<5:33:25, 229.95s/it]Evaluating commonsenseqa :  14%|█▍        | 14/100 [53:15<5:28:26, 229.15s/it]Evaluating commonsenseqa :  15%|█▌        | 15/100 [57:02<5:23:54, 228.64s/it]Evaluating commonsenseqa :  16%|█▌        | 16/100 [1:00:57<5:22:28, 230.34s/it]Evaluating commonsenseqa :  17%|█▋        | 17/100 [1:04:49<5:19:18, 230.82s/it]Evaluating commonsenseqa :  18%|█▊        | 18/100 [1:08:32<5:12:37, 228.75s/it]Evaluating commonsenseqa :  19%|█▉        | 19/100 [1:12:20<5:08:23, 228.44s/it]Evaluating commonsenseqa :  20%|██        | 20/100 [1:16:11<5:05:40, 229.25s/it]Evaluating commonsenseqa :  21%|██        | 21/100 [1:19:55<4:59:47, 227.69s/it]Evaluating commonsenseqa :  22%|██▏       | 22/100 [1:23:45<4:56:47, 228.31s/it]Evaluating commonsenseqa :  23%|██▎       | 23/100 [1:27:35<4:53:27, 228.66s/it]Evaluating commonsenseqa :  24%|██▍       | 24/100 [1:31:18<4:47:38, 227.08s/it]Evaluating commonsenseqa :  25%|██▌       | 25/100 [1:35:04<4:43:26, 226.75s/it]Evaluating commonsenseqa :  26%|██▌       | 26/100 [1:38:48<4:38:40, 225.96s/it]Evaluating commonsenseqa :  27%|██▋       | 27/100 [1:42:38<4:36:18, 227.10s/it]Evaluating commonsenseqa :  28%|██▊       | 28/100 [1:46:28<4:33:28, 227.90s/it]Evaluating commonsenseqa :  29%|██▉       | 29/100 [1:50:17<4:30:22, 228.49s/it]Evaluating commonsenseqa :  30%|███       | 30/100 [1:54:08<4:27:12, 229.03s/it]Evaluating commonsenseqa :  31%|███       | 31/100 [1:57:57<4:23:29, 229.12s/it]Evaluating commonsenseqa :  32%|███▏      | 32/100 [2:01:43<4:18:32, 228.13s/it]Evaluating commonsenseqa :  33%|███▎      | 33/100 [2:05:28<4:13:50, 227.32s/it]Evaluating commonsenseqa :  34%|███▍      | 34/100 [2:09:19<4:11:16, 228.43s/it]Evaluating commonsenseqa :  35%|███▌      | 35/100 [2:13:05<4:06:34, 227.61s/it]Evaluating commonsenseqa :  36%|███▌      | 36/100 [2:16:53<4:02:57, 227.78s/it]Evaluating commonsenseqa :  37%|███▋      | 37/100 [2:20:42<3:59:22, 227.97s/it]Evaluating commonsenseqa :  38%|███▊      | 38/100 [2:24:30<3:55:42, 228.11s/it]Evaluating commonsenseqa :  39%|███▉      | 39/100 [2:28:19<3:52:03, 228.26s/it]Evaluating commonsenseqa :  40%|████      | 40/100 [2:32:10<3:49:09, 229.16s/it]Evaluating commonsenseqa :  41%|████      | 41/100 [2:35:58<3:45:03, 228.87s/it]Evaluating commonsenseqa :  42%|████▏     | 42/100 [2:39:46<3:40:58, 228.59s/it]Evaluating commonsenseqa :  43%|████▎     | 43/100 [2:43:33<3:36:49, 228.23s/it]Evaluating commonsenseqa :  44%|████▍     | 44/100 [2:47:21<3:32:49, 228.03s/it]Evaluating commonsenseqa :  45%|████▌     | 45/100 [2:51:10<3:29:10, 228.20s/it]Evaluating commonsenseqa :  46%|████▌     | 46/100 [2:55:02<3:26:37, 229.59s/it]Evaluating commonsenseqa :  47%|████▋     | 47/100 [2:58:49<3:22:01, 228.71s/it]Evaluating commonsenseqa :  48%|████▊     | 48/100 [3:02:37<3:17:59, 228.46s/it]Evaluating commonsenseqa :  49%|████▉     | 49/100 [3:06:30<3:15:13, 229.68s/it]Evaluating commonsenseqa :  50%|█████     | 50/100 [3:10:19<3:11:16, 229.54s/it]Evaluating commonsenseqa :  51%|█████     | 51/100 [3:14:05<3:06:32, 228.42s/it]Evaluating commonsenseqa :  52%|█████▏    | 52/100 [3:17:56<3:03:27, 229.33s/it]Evaluating commonsenseqa :  53%|█████▎    | 53/100 [3:21:41<2:58:36, 228.01s/it]Evaluating commonsenseqa :  54%|█████▍    | 54/100 [3:25:30<2:54:59, 228.25s/it]Evaluating commonsenseqa :  55%|█████▌    | 55/100 [3:29:16<2:50:45, 227.68s/it]Evaluating commonsenseqa :  56%|█████▌    | 56/100 [3:33:04<2:47:00, 227.73s/it]Evaluating commonsenseqa :  57%|█████▋    | 57/100 [3:36:52<2:43:19, 227.89s/it]Evaluating commonsenseqa :  58%|█████▊    | 58/100 [3:40:38<2:39:09, 227.37s/it]Evaluating commonsenseqa :  59%|█████▉    | 59/100 [3:44:27<2:35:40, 227.83s/it]Evaluating commonsenseqa :  60%|██████    | 60/100 [3:48:13<2:31:27, 227.18s/it]Evaluating commonsenseqa :  61%|██████    | 61/100 [3:52:01<2:27:55, 227.59s/it]Evaluating commonsenseqa :  62%|██████▏   | 62/100 [3:55:51<2:24:25, 228.05s/it]Evaluating commonsenseqa :  63%|██████▎   | 63/100 [3:59:35<2:19:55, 226.91s/it]Evaluating commonsenseqa :  64%|██████▍   | 64/100 [4:03:20<2:15:55, 226.53s/it]Evaluating commonsenseqa :  65%|██████▌   | 65/100 [4:07:08<2:12:19, 226.85s/it]Evaluating commonsenseqa :  66%|██████▌   | 66/100 [4:10:55<2:08:35, 226.93s/it]Evaluating commonsenseqa :  67%|██████▋   | 67/100 [4:14:43<2:04:53, 227.08s/it]Evaluating commonsenseqa :  68%|██████▊   | 68/100 [4:18:29<2:00:57, 226.78s/it]