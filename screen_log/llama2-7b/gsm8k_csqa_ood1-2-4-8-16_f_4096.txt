torchrun --nproc_per_node 2 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 12355 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name gsm8k --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name commonsenseqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/gsm8k/out-domain/o1-tcommonsenseqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 00:31:18,100] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 00:31:18,100] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 2
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... gsm8k
  data_dir ..................... /scratch/ylu130/processed_data/gsm8k/out-domain/o1-tcommonsenseqa-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 1
  out_domain_data_name ......... commonsenseqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/gsm8k/out-domain/o1-tcommonsenseqa-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 2
Loading data:   0%|                                                           | 0/7473 [00:00<?, ?it/s]Loading data: 100%|███████████████████████████████████████████| 7473/7473 [00:00<00:00, 1147502.61it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                 | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                 | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|████████████████████▌                    | 1/2 [00:06<00:06,  6.60s/it]Loading checkpoint shards:  50%|████████████████████▌                    | 1/2 [00:07<00:07,  7.08s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████| 2/2 [00:08<00:00,  4.04s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████| 2/2 [00:08<00:00,  4.42s/it]
[2023-08-30 00:31:28,083] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████████| 2/2 [00:09<00:00,  4.27s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████| 2/2 [00:09<00:00,  4.70s/it]
 > number of parameters: 6738415616
[2023-08-30 00:31:28,692] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 00:31:29,113] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 00:31:29,115] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 00:31:29,115] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 00:31:29,115] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 00:31:29,115] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 00:31:29,115] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1ee07c9300>
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 00:31:29,116] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   train_batch_size ............. 2
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   world_size ................... 2
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 00:31:29,117] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 00:31:29,117] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating gsm8k :   0%|                                                       | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following grade math question.

Input: Where could you find some plumbing that would not be of use to you if you are thirsty? Choices:  A: oil refineries B: wall C: show D: own home E: water fountain
Output: A: oil refineries

Input:Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/gsm8k/out-domain/o1-tcommonsenseqa-s1-rFalse-m4096
Evaluating gsm8k :   1%|▍                                           | 1/100 [02:22<3:55:47, 142.90s/it]Evaluating gsm8k :   2%|▉                                           | 2/100 [04:41<3:49:40, 140.62s/it]Evaluating gsm8k :   3%|█▎                                          | 3/100 [06:55<3:42:29, 137.62s/it]Evaluating gsm8k :   4%|█▊                                          | 4/100 [09:16<3:42:10, 138.86s/it]Evaluating gsm8k :   5%|██▏                                         | 5/100 [11:35<3:39:41, 138.75s/it]Evaluating gsm8k :   6%|██▋                                         | 6/100 [13:57<3:39:20, 140.01s/it]Evaluating gsm8k :   7%|███                                         | 7/100 [16:19<3:37:46, 140.50s/it]Evaluating gsm8k :   8%|███▌                                        | 8/100 [18:39<3:35:10, 140.33s/it]Evaluating gsm8k :   9%|███▉                                        | 9/100 [20:57<3:31:55, 139.73s/it]Evaluating gsm8k :  10%|████▎                                      | 10/100 [23:19<3:30:31, 140.35s/it]Evaluating gsm8k :  11%|████▋                                      | 11/100 [24:42<3:02:00, 122.70s/it]Evaluating gsm8k :  12%|█████▏                                     | 12/100 [26:58<3:06:07, 126.90s/it]Evaluating gsm8k :  13%|█████▌                                     | 13/100 [28:17<2:42:59, 112.40s/it]Evaluating gsm8k :  14%|██████                                     | 14/100 [30:35<2:52:16, 120.19s/it]Evaluating gsm8k :  15%|██████▍                                    | 15/100 [32:34<2:49:29, 119.64s/it]Evaluating gsm8k :  16%|██████▉                                    | 16/100 [34:52<2:55:33, 125.40s/it]Evaluating gsm8k :  17%|███████▎                                   | 17/100 [37:11<2:58:47, 129.25s/it]Evaluating gsm8k :  18%|███████▋                                   | 18/100 [39:33<3:01:49, 133.05s/it]Evaluating gsm8k :  19%|████████▏                                  | 19/100 [41:41<2:57:55, 131.80s/it]Evaluating gsm8k :  20%|████████▌                                  | 20/100 [44:02<2:59:20, 134.51s/it]Evaluating gsm8k :  21%|█████████                                  | 21/100 [46:25<3:00:19, 136.96s/it]Evaluating gsm8k :  22%|█████████▍                                 | 22/100 [48:42<2:58:02, 136.96s/it]Evaluating gsm8k :  23%|█████████▉                                 | 23/100 [51:05<2:58:02, 138.74s/it]Evaluating gsm8k :  24%|██████████▎                                | 24/100 [52:00<2:23:59, 113.68s/it]Evaluating gsm8k :  25%|██████████▊                                | 25/100 [53:37<2:15:47, 108.64s/it]Evaluating gsm8k :  26%|███████████▏                               | 26/100 [55:59<2:26:12, 118.54s/it]Evaluating gsm8k :  27%|███████████▌                               | 27/100 [58:20<2:32:28, 125.32s/it]Evaluating gsm8k :  28%|████████████                               | 28/100 [59:01<2:00:19, 100.28s/it]Evaluating gsm8k :  29%|███████████▉                             | 29/100 [1:01:21<2:12:35, 112.05s/it]Evaluating gsm8k :  30%|████████████▎                            | 30/100 [1:03:11<2:10:01, 111.44s/it]Evaluating gsm8k :  31%|████████████▋                            | 31/100 [1:05:34<2:18:51, 120.75s/it]Evaluating gsm8k :  32%|█████████████                            | 32/100 [1:07:54<2:23:25, 126.56s/it]Evaluating gsm8k :  33%|█████████████▌                           | 33/100 [1:10:16<2:26:31, 131.21s/it]Evaluating gsm8k :  34%|█████████████▉                           | 34/100 [1:12:35<2:26:59, 133.63s/it]Evaluating gsm8k :  35%|██████████████▎                          | 35/100 [1:14:57<2:27:33, 136.21s/it]Evaluating gsm8k :  36%|██████████████▊                          | 36/100 [1:17:08<2:23:26, 134.48s/it]Evaluating gsm8k :  37%|███████████████▏                         | 37/100 [1:19:30<2:23:48, 136.96s/it]Evaluating gsm8k :  38%|███████████████▌                         | 38/100 [1:21:50<2:22:28, 137.88s/it]Evaluating gsm8k :  39%|███████████████▉                         | 39/100 [1:24:13<2:21:28, 139.16s/it]Evaluating gsm8k :  40%|████████████████▍                        | 40/100 [1:26:31<2:18:56, 138.94s/it]Evaluating gsm8k :  41%|████████████████▊                        | 41/100 [1:27:10<1:47:04, 108.89s/it]Evaluating gsm8k :  42%|█████████████████▏                       | 42/100 [1:29:31<1:54:47, 118.74s/it]Evaluating gsm8k :  43%|█████████████████▋                       | 43/100 [1:31:52<1:59:06, 125.37s/it]Evaluating gsm8k :  44%|██████████████████                       | 44/100 [1:34:11<2:00:52, 129.51s/it]Evaluating gsm8k :  45%|██████████████████▍                      | 45/100 [1:35:40<1:47:33, 117.33s/it]Evaluating gsm8k :  46%|██████████████████▊                      | 46/100 [1:37:58<1:51:09, 123.51s/it]Evaluating gsm8k :  47%|███████████████████▎                     | 47/100 [1:40:21<1:54:07, 129.20s/it]Evaluating gsm8k :  48%|███████████████████▋                     | 48/100 [1:42:30<1:51:57, 129.19s/it]Evaluating gsm8k :  49%|████████████████████                     | 49/100 [1:44:52<1:53:11, 133.17s/it]Evaluating gsm8k :  50%|████████████████████▌                    | 50/100 [1:47:13<1:52:42, 135.26s/it]Evaluating gsm8k :  51%|████████████████████▉                    | 51/100 [1:49:18<1:47:57, 132.19s/it]Evaluating gsm8k :  52%|█████████████████████▎                   | 52/100 [1:50:09<1:26:21, 107.95s/it]Evaluating gsm8k :  53%|█████████████████████▋                   | 53/100 [1:52:27<1:31:37, 116.96s/it]Evaluating gsm8k :  54%|██████████████████████▏                  | 54/100 [1:54:45<1:34:33, 123.35s/it]Evaluating gsm8k :  55%|██████████████████████▌                  | 55/100 [1:57:09<1:37:01, 129.36s/it]Evaluating gsm8k :  56%|██████████████████████▉                  | 56/100 [1:58:48<1:28:14, 120.32s/it]Evaluating gsm8k :  57%|███████████████████████▎                 | 57/100 [2:01:08<1:30:28, 126.25s/it]Evaluating gsm8k :  58%|███████████████████████▊                 | 58/100 [2:03:29<1:31:35, 130.84s/it]Evaluating gsm8k :  59%|████████████████████████▏                | 59/100 [2:05:49<1:31:09, 133.39s/it]Evaluating gsm8k :  60%|████████████████████████▌                | 60/100 [2:08:09<1:30:19, 135.48s/it]Evaluating gsm8k :  61%|█████████████████████████                | 61/100 [2:10:29<1:29:00, 136.92s/it]Evaluating gsm8k :  62%|█████████████████████████▍               | 62/100 [2:11:56<1:17:09, 121.82s/it]Evaluating gsm8k :  63%|█████████████████████████▊               | 63/100 [2:13:54<1:14:21, 120.59s/it]Evaluating gsm8k :  64%|██████████████████████████▏              | 64/100 [2:16:16<1:16:14, 127.08s/it]Evaluating gsm8k :  65%|██████████████████████████▋              | 65/100 [2:17:34<1:05:29, 112.28s/it]Evaluating gsm8k :  66%|███████████████████████████              | 66/100 [2:19:58<1:09:04, 121.90s/it]Evaluating gsm8k :  67%|███████████████████████████▍             | 67/100 [2:22:22<1:10:38, 128.43s/it]Evaluating gsm8k :  68%|███████████████████████████▉             | 68/100 [2:24:21<1:07:03, 125.74s/it]Evaluating gsm8k :  69%|████████████████████████████▎            | 69/100 [2:26:48<1:08:17, 132.18s/it]Evaluating gsm8k :  70%|████████████████████████████▋            | 70/100 [2:28:51<1:04:39, 129.33s/it]Evaluating gsm8k :  71%|█████████████████████████████            | 71/100 [2:31:13<1:04:22, 133.18s/it]Evaluating gsm8k :  72%|█████████████████████████████▌           | 72/100 [2:33:34<1:03:09, 135.34s/it]Evaluating gsm8k :  73%|███████████████████████████████▍           | 73/100 [2:35:20<56:55, 126.50s/it]Evaluating gsm8k :  74%|███████████████████████████████▊           | 74/100 [2:37:38<56:24, 130.15s/it]Evaluating gsm8k :  75%|████████████████████████████████▎          | 75/100 [2:39:58<55:28, 133.14s/it]Evaluating gsm8k :  76%|████████████████████████████████▋          | 76/100 [2:41:12<46:09, 115.38s/it]Evaluating gsm8k :  77%|█████████████████████████████████          | 77/100 [2:43:33<47:06, 122.89s/it]Evaluating gsm8k :  78%|█████████████████████████████████▌         | 78/100 [2:45:51<46:43, 127.41s/it]Evaluating gsm8k :  79%|█████████████████████████████████▉         | 79/100 [2:48:15<46:20, 132.39s/it]Evaluating gsm8k :  80%|██████████████████████████████████▍        | 80/100 [2:49:16<37:04, 111.21s/it]Evaluating gsm8k :  81%|██████████████████████████████████▊        | 81/100 [2:51:40<38:15, 120.80s/it]Evaluating gsm8k :  82%|███████████████████████████████████▎       | 82/100 [2:54:01<38:06, 127.05s/it]Evaluating gsm8k :  83%|███████████████████████████████████▋       | 83/100 [2:56:24<37:19, 131.71s/it]Evaluating gsm8k :  84%|████████████████████████████████████       | 84/100 [2:58:46<35:59, 134.97s/it]Evaluating gsm8k :  85%|████████████████████████████████████▌      | 85/100 [3:01:04<33:54, 135.65s/it]Evaluating gsm8k :  86%|████████████████████████████████████▉      | 86/100 [3:03:26<32:07, 137.70s/it]Evaluating gsm8k :  87%|█████████████████████████████████████▍     | 87/100 [3:05:42<29:44, 137.24s/it]Evaluating gsm8k :  88%|█████████████████████████████████████▊     | 88/100 [3:08:04<27:42, 138.58s/it]Evaluating gsm8k :  89%|██████████████████████████████████████▎    | 89/100 [3:09:59<24:06, 131.46s/it]Evaluating gsm8k :  90%|██████████████████████████████████████▋    | 90/100 [3:12:19<22:20, 134.05s/it]Evaluating gsm8k :  91%|███████████████████████████████████████▏   | 91/100 [3:14:39<20:23, 135.99s/it]Evaluating gsm8k :  92%|███████████████████████████████████████▌   | 92/100 [3:16:55<18:07, 135.99s/it]Evaluating gsm8k :  93%|███████████████████████████████████████▉   | 93/100 [3:18:44<14:54, 127.84s/it]Evaluating gsm8k :  94%|████████████████████████████████████████▍  | 94/100 [3:21:06<13:11, 131.95s/it]Evaluating gsm8k :  95%|████████████████████████████████████████▊  | 95/100 [3:22:35<09:55, 119.15s/it]Evaluating gsm8k :  96%|█████████████████████████████████████████▎ | 96/100 [3:24:58<08:25, 126.25s/it]Evaluating gsm8k :  97%|█████████████████████████████████████████▋ | 97/100 [3:26:45<06:01, 120.42s/it]Evaluating gsm8k :  98%|██████████████████████████████████████████▏| 98/100 [3:29:08<04:14, 127.23s/it]Evaluating gsm8k :  99%|██████████████████████████████████████████▌| 99/100 [3:31:29<02:11, 131.35s/it]Evaluating gsm8k : 100%|██████████████████████████████████████████| 100/100 [3:33:48<00:00, 133.73s/it]Evaluating gsm8k : 100%|██████████████████████████████████████████| 100/100 [3:33:48<00:00, 128.29s/it]
name: gsm8k | avg. gen lenth: 231.682 | time: 12829.034383296967s
torchrun --nproc_per_node 2 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 12355 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name gsm8k --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name commonsenseqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/gsm8k/out-domain/o2-tcommonsenseqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 04:05:23,539] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 04:05:23,552] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 2
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... gsm8k
  data_dir ..................... /scratch/ylu130/processed_data/gsm8k/out-domain/o2-tcommonsenseqa-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 2
  out_domain_data_name ......... commonsenseqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/gsm8k/out-domain/o2-tcommonsenseqa-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 2
Loading data:   0%|                                                           | 0/7473 [00:00<?, ?it/s]Loading data: 100%|███████████████████████████████████████████| 7473/7473 [00:00<00:00, 1064313.54it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                 | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                 | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|████████████████████▌                    | 1/2 [00:06<00:06,  6.26s/it]Loading checkpoint shards:  50%|████████████████████▌                    | 1/2 [00:06<00:06,  6.79s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████| 2/2 [00:08<00:00,  3.73s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████| 2/2 [00:08<00:00,  4.11s/it]
 > number of parameters: 6738415616
[2023-08-30 04:05:32,964] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████████| 2/2 [00:09<00:00,  4.35s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████| 2/2 [00:09<00:00,  4.72s/it]
[2023-08-30 04:05:34,141] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 04:05:34,632] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 04:05:34,634] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f68bd5bd2d0>
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 04:05:34,634] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   train_batch_size ............. 2
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   world_size ................... 2
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 04:05:34,635] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 04:05:34,636] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating gsm8k :   0%|                                                       | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following grade math question.

Input: Where could you find some plumbing that would not be of use to you if you are thirsty? Choices:  A: oil refineries B: wall C: show D: own home E: water fountain
Output: A: oil refineries

Input: When a person is beginning work, what aren't they doing yet? Choices:  A: working B: resting C: tiredness D: accomplishing E: momentum
Output: D: accomplishing

Input:Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/gsm8k/out-domain/o2-tcommonsenseqa-s1-rFalse-m4096
Evaluating gsm8k :   1%|▍                                           | 1/100 [02:04<3:24:37, 124.02s/it]Evaluating gsm8k :   2%|▉                                           | 2/100 [03:36<2:51:48, 105.19s/it]Evaluating gsm8k :   3%|█▎                                          | 3/100 [05:52<3:12:48, 119.27s/it]Evaluating gsm8k :   4%|█▊                                          | 4/100 [08:07<3:21:08, 125.71s/it]Evaluating gsm8k :   5%|██▏                                         | 5/100 [10:27<3:27:03, 130.77s/it]Evaluating gsm8k :   6%|██▋                                         | 6/100 [12:23<3:17:08, 125.84s/it]Evaluating gsm8k :   7%|███                                         | 7/100 [14:41<3:21:20, 129.90s/it]Evaluating gsm8k :   8%|███▌                                        | 8/100 [16:37<3:12:18, 125.42s/it]Evaluating gsm8k :   9%|███▉                                        | 9/100 [18:56<3:16:27, 129.53s/it]Evaluating gsm8k :  10%|████▎                                      | 10/100 [21:18<3:20:00, 133.34s/it]Evaluating gsm8k :  11%|████▋                                      | 11/100 [23:34<3:19:19, 134.37s/it]Evaluating gsm8k :  12%|█████▏                                     | 12/100 [25:50<3:17:49, 134.88s/it]Evaluating gsm8k :  13%|█████▌                                     | 13/100 [27:06<2:49:36, 116.97s/it]Evaluating gsm8k :  14%|██████                                     | 14/100 [29:20<2:54:57, 122.07s/it]Evaluating gsm8k :  15%|██████▍                                    | 15/100 [31:38<2:59:39, 126.82s/it]Evaluating gsm8k :  16%|██████▉                                    | 16/100 [33:55<3:01:52, 129.90s/it]Evaluating gsm8k :  17%|███████▎                                   | 17/100 [36:13<3:03:04, 132.34s/it]Evaluating gsm8k :  18%|███████▋                                   | 18/100 [38:31<3:03:01, 133.92s/it]Evaluating gsm8k :  19%|████████▏                                  | 19/100 [40:46<3:01:29, 134.43s/it]Evaluating gsm8k :  20%|████████▌                                  | 20/100 [42:09<2:38:40, 119.01s/it]Evaluating gsm8k :  21%|█████████                                  | 21/100 [44:27<2:44:01, 124.57s/it]Evaluating gsm8k :  22%|█████████▍                                 | 22/100 [46:43<2:46:24, 128.00s/it]Evaluating gsm8k :  23%|█████████▉                                 | 23/100 [49:02<2:48:28, 131.28s/it]Evaluating gsm8k :  24%|██████████▎                                | 24/100 [51:17<2:47:44, 132.43s/it]Evaluating gsm8k :  25%|██████████▊                                | 25/100 [53:34<2:47:15, 133.81s/it]Evaluating gsm8k :  26%|███████████▏                               | 26/100 [55:50<2:46:03, 134.64s/it]Evaluating gsm8k :  27%|███████████▌                               | 27/100 [58:08<2:44:45, 135.42s/it]Evaluating gsm8k :  28%|███████████▍                             | 28/100 [1:00:27<2:43:53, 136.58s/it]Evaluating gsm8k :  29%|███████████▉                             | 29/100 [1:02:46<2:42:30, 137.33s/it]Evaluating gsm8k :  30%|████████████▎                            | 30/100 [1:04:25<2:26:53, 125.91s/it]Evaluating gsm8k :  31%|████████████▋                            | 31/100 [1:06:42<2:28:21, 129.01s/it]Evaluating gsm8k :  32%|█████████████                            | 32/100 [1:08:09<2:12:09, 116.61s/it]Evaluating gsm8k :  33%|█████████████▌                           | 33/100 [1:10:27<2:17:27, 123.10s/it]Evaluating gsm8k :  34%|█████████████▉                           | 34/100 [1:12:44<2:19:45, 127.05s/it]Evaluating gsm8k :  35%|██████████████▎                          | 35/100 [1:14:59<2:20:16, 129.49s/it]Evaluating gsm8k :  36%|██████████████▊                          | 36/100 [1:17:16<2:20:29, 131.71s/it]Evaluating gsm8k :  37%|███████████████▏                         | 37/100 [1:19:32<2:19:48, 133.16s/it]Evaluating gsm8k :  38%|███████████████▌                         | 38/100 [1:21:49<2:18:42, 134.23s/it]Evaluating gsm8k :  39%|███████████████▉                         | 39/100 [1:24:09<2:18:17, 136.03s/it]Evaluating gsm8k :  40%|████████████████▍                        | 40/100 [1:26:27<2:16:25, 136.42s/it]Evaluating gsm8k :  41%|████████████████▊                        | 41/100 [1:28:42<2:13:47, 136.05s/it]Evaluating gsm8k :  42%|█████████████████▏                       | 42/100 [1:31:00<2:12:07, 136.68s/it]Evaluating gsm8k :  43%|█████████████████▋                       | 43/100 [1:33:18<2:10:14, 137.10s/it]Evaluating gsm8k :  44%|██████████████████                       | 44/100 [1:34:56<1:57:08, 125.50s/it]Evaluating gsm8k :  45%|██████████████████▍                      | 45/100 [1:37:16<1:59:01, 129.85s/it]Evaluating gsm8k :  46%|██████████████████▊                      | 46/100 [1:39:06<1:51:30, 123.89s/it]Evaluating gsm8k :  47%|███████████████████▎                     | 47/100 [1:41:22<1:52:32, 127.41s/it]Evaluating gsm8k :  48%|███████████████████▋                     | 48/100 [1:43:04<1:43:41, 119.64s/it]Evaluating gsm8k :  49%|████████████████████                     | 49/100 [1:45:21<1:46:07, 124.85s/it]Evaluating gsm8k :  50%|████████████████████▌                    | 50/100 [1:47:19<1:42:32, 123.05s/it]Evaluating gsm8k :  51%|████████████████████▉                    | 51/100 [1:49:38<1:44:24, 127.84s/it]Evaluating gsm8k :  52%|█████████████████████▎                   | 52/100 [1:51:57<1:44:54, 131.13s/it]Evaluating gsm8k :  53%|█████████████████████▋                   | 53/100 [1:53:07<1:28:24, 112.86s/it]Evaluating gsm8k :  54%|██████████████████████▏                  | 54/100 [1:55:23<1:31:44, 119.67s/it]Evaluating gsm8k :  55%|██████████████████████▌                  | 55/100 [1:57:38<1:33:09, 124.21s/it]Evaluating gsm8k :  56%|██████████████████████▉                  | 56/100 [1:59:36<1:29:39, 122.27s/it]Evaluating gsm8k :  57%|███████████████████████▎                 | 57/100 [2:01:51<1:30:32, 126.34s/it]Evaluating gsm8k :  58%|███████████████████████▊                 | 58/100 [2:04:09<1:30:53, 129.84s/it]Evaluating gsm8k :  59%|████████████████████████▏                | 59/100 [2:06:28<1:30:29, 132.44s/it]Evaluating gsm8k :  60%|████████████████████████▌                | 60/100 [2:08:28<1:25:52, 128.81s/it]Evaluating gsm8k :  61%|█████████████████████████                | 61/100 [2:10:32<1:22:42, 127.25s/it]Evaluating gsm8k :  62%|█████████████████████████▍               | 62/100 [2:11:50<1:11:15, 112.51s/it]Evaluating gsm8k :  63%|█████████████████████████▊               | 63/100 [2:14:05<1:13:33, 119.27s/it]Evaluating gsm8k :  64%|██████████████████████████▏              | 64/100 [2:16:23<1:14:54, 124.86s/it]Evaluating gsm8k :  65%|██████████████████████████▋              | 65/100 [2:18:16<1:10:48, 121.39s/it]Evaluating gsm8k :  66%|███████████████████████████              | 66/100 [2:20:35<1:11:39, 126.46s/it]Evaluating gsm8k :  67%|███████████████████████████▍             | 67/100 [2:22:16<1:05:22, 118.85s/it]Evaluating gsm8k :  68%|███████████████████████████▉             | 68/100 [2:24:29<1:05:38, 123.07s/it]Evaluating gsm8k :  69%|████████████████████████████▎            | 69/100 [2:26:25<1:02:34, 121.11s/it]Evaluating gsm8k :  70%|████████████████████████████▋            | 70/100 [2:28:46<1:03:33, 127.13s/it]Evaluating gsm8k :  71%|█████████████████████████████            | 71/100 [2:31:05<1:03:08, 130.64s/it]Evaluating gsm8k :  72%|██████████████████████████████▉            | 72/100 [2:32:40<56:00, 120.01s/it]Evaluating gsm8k :  73%|███████████████████████████████▍           | 73/100 [2:34:56<56:10, 124.83s/it]Evaluating gsm8k :  74%|███████████████████████████████▊           | 74/100 [2:36:05<46:48, 108.02s/it]Evaluating gsm8k :  75%|████████████████████████████████▎          | 75/100 [2:38:21<48:29, 116.40s/it]Evaluating gsm8k :  76%|████████████████████████████████▋          | 76/100 [2:40:38<49:02, 122.60s/it]Evaluating gsm8k :  77%|█████████████████████████████████          | 77/100 [2:42:55<48:38, 126.90s/it]Evaluating gsm8k :  78%|█████████████████████████████████▌         | 78/100 [2:45:10<47:25, 129.33s/it]Evaluating gsm8k :  79%|█████████████████████████████████▉         | 79/100 [2:46:34<40:27, 115.61s/it]Evaluating gsm8k :  80%|██████████████████████████████████▍        | 80/100 [2:48:47<40:19, 120.99s/it]Evaluating gsm8k :  81%|██████████████████████████████████▊        | 81/100 [2:50:59<39:18, 124.11s/it]Evaluating gsm8k :  82%|███████████████████████████████████▎       | 82/100 [2:53:16<38:25, 128.08s/it]Evaluating gsm8k :  83%|███████████████████████████████████▋       | 83/100 [2:55:34<37:05, 130.92s/it]Evaluating gsm8k :  84%|████████████████████████████████████       | 84/100 [2:57:51<35:25, 132.83s/it]Evaluating gsm8k :  85%|████████████████████████████████████▌      | 85/100 [3:00:11<33:43, 134.92s/it]Evaluating gsm8k :  86%|████████████████████████████████████▉      | 86/100 [3:02:28<31:38, 135.59s/it]Evaluating gsm8k :  87%|█████████████████████████████████████▍     | 87/100 [3:04:45<29:28, 136.07s/it]Evaluating gsm8k :  88%|█████████████████████████████████████▊     | 88/100 [3:06:55<26:50, 134.19s/it]Evaluating gsm8k :  89%|██████████████████████████████████████▎    | 89/100 [3:09:07<24:28, 133.52s/it]Evaluating gsm8k :  90%|██████████████████████████████████████▋    | 90/100 [3:11:08<21:38, 129.87s/it]Evaluating gsm8k :  91%|███████████████████████████████████████▏   | 91/100 [3:13:24<19:45, 131.68s/it]Evaluating gsm8k :  92%|███████████████████████████████████████▌   | 92/100 [3:15:41<17:46, 133.31s/it]Evaluating gsm8k :  93%|███████████████████████████████████████▉   | 93/100 [3:17:57<15:37, 133.95s/it]Evaluating gsm8k :  94%|████████████████████████████████████████▍  | 94/100 [3:20:15<13:32, 135.38s/it]Evaluating gsm8k :  95%|████████████████████████████████████████▊  | 95/100 [3:22:31<11:17, 135.52s/it]Evaluating gsm8k :  96%|█████████████████████████████████████████▎ | 96/100 [3:24:48<09:04, 136.05s/it]Evaluating gsm8k :  97%|█████████████████████████████████████████▋ | 97/100 [3:26:47<06:32, 130.69s/it]Evaluating gsm8k :  98%|██████████████████████████████████████████▏| 98/100 [3:29:04<04:25, 132.76s/it]Evaluating gsm8k :  99%|██████████████████████████████████████████▌| 99/100 [3:30:46<02:03, 123.62s/it]Evaluating gsm8k : 100%|██████████████████████████████████████████| 100/100 [3:33:02<00:00, 127.27s/it]Evaluating gsm8k : 100%|██████████████████████████████████████████| 100/100 [3:33:02<00:00, 127.83s/it]
name: gsm8k | avg. gen lenth: 257.428 | time: 12783.228007555008s
torchrun --nproc_per_node 2 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 12355 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --is-opensource --data-name gsm8k --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name commonsenseqa --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 5 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/gsm8k/out-domain/o4-tcommonsenseqa-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-30 07:38:44,040] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 07:38:44,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 2
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  is_opensource ................ True
  is_slurm ..................... False
  data_name .................... gsm8k
  data_dir ..................... /scratch/ylu130/processed_data/gsm8k/out-domain/o4-tcommonsenseqa-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 4
  out_domain_data_name ......... commonsenseqa
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/gsm8k/out-domain/o4-tcommonsenseqa-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 5
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 2
Loading data:   0%|                                                           | 0/7473 [00:00<?, ?it/s]Loading data: 100%|████████████████████████████████████████████| 7473/7473 [00:00<00:00, 981310.35it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                                 | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                 | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|████████████████████▌                    | 1/2 [00:06<00:06,  6.76s/it]Loading checkpoint shards:  50%|████████████████████▌                    | 1/2 [00:06<00:06,  6.86s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████| 2/2 [00:08<00:00,  4.42s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████████| 2/2 [00:09<00:00,  4.09s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████| 2/2 [00:09<00:00,  4.51s/it]
 > number of parameters: 6738415616
[2023-08-30 07:38:54,132] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 07:38:54,244] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-30 07:38:54,685] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-30 07:38:54,687] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f47f44d12d0>
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 07:38:54,687] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   train_batch_size ............. 2
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   world_size ................... 2
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 07:38:54,688] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 07:38:54,688] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating gsm8k :   0%|                                                       | 0/100 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following grade math question.

Input: Where could you find some plumbing that would not be of use to you if you are thirsty? Choices:  A: oil refineries B: wall C: show D: own home E: water fountain
Output: A: oil refineries

Input: When a person is beginning work, what aren't they doing yet? Choices:  A: working B: resting C: tiredness D: accomplishing E: momentum
Output: D: accomplishing

Input: Where might I find pens with a company logo? Choices:  A: office B: on a pencil C: write sentences on paper D: school E: backpack
Output: A: office

Input: Billy called out to John, and listened for what? Choices:  A: silence B: response C: communication D: hanging up E: whisper
Output: B: response

Input:Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/gsm8k/out-domain/o4-tcommonsenseqa-s1-rFalse-m4096
Evaluating gsm8k :   1%|▍                                           | 1/100 [02:17<3:46:22, 137.20s/it]Evaluating gsm8k :   2%|▉                                           | 2/100 [04:31<3:41:44, 135.76s/it]Evaluating gsm8k :   3%|█▎                                          | 3/100 [06:46<3:38:49, 135.35s/it]Evaluating gsm8k :   4%|█▊                                          | 4/100 [09:03<3:37:12, 135.76s/it]Evaluating gsm8k :   5%|██▏                                         | 5/100 [11:19<3:35:03, 135.83s/it]Evaluating gsm8k :   6%|██▋                                         | 6/100 [13:19<3:24:17, 130.40s/it]Evaluating gsm8k :   7%|███                                         | 7/100 [15:34<3:24:52, 132.17s/it]Evaluating gsm8k :   8%|███▌                                        | 8/100 [17:47<3:22:49, 132.28s/it]Evaluating gsm8k :   9%|███▉                                        | 9/100 [18:30<2:38:27, 104.48s/it]Evaluating gsm8k :  10%|████▎                                      | 10/100 [20:26<2:42:08, 108.10s/it]Evaluating gsm8k :  11%|████▋                                      | 11/100 [22:44<2:53:54, 117.24s/it]Evaluating gsm8k :  12%|█████▏                                     | 12/100 [25:00<3:00:08, 122.82s/it]Evaluating gsm8k :  13%|█████▌                                     | 13/100 [27:16<3:03:53, 126.83s/it]Evaluating gsm8k :  14%|██████                                     | 14/100 [28:23<2:35:51, 108.74s/it]Evaluating gsm8k :  15%|██████▍                                    | 15/100 [30:35<2:43:52, 115.68s/it]Evaluating gsm8k :  16%|██████▉                                    | 16/100 [32:50<2:50:01, 121.44s/it]Evaluating gsm8k :  17%|███████▎                                   | 17/100 [35:04<2:53:19, 125.29s/it]Evaluating gsm8k :  18%|███████▋                                   | 18/100 [37:19<2:55:19, 128.29s/it]Evaluating gsm8k :  19%|████████▏                                  | 19/100 [38:44<2:35:29, 115.17s/it]Evaluating gsm8k :  20%|████████▌                                  | 20/100 [40:59<2:41:30, 121.13s/it]Evaluating gsm8k :  21%|█████████                                  | 21/100 [43:14<2:45:14, 125.50s/it]Evaluating gsm8k :  22%|█████████▍                                 | 22/100 [45:26<2:45:25, 127.25s/it]Evaluating gsm8k :  23%|█████████▉                                 | 23/100 [47:40<2:46:05, 129.42s/it]Evaluating gsm8k :  24%|██████████▎                                | 24/100 [49:53<2:45:24, 130.58s/it]Evaluating gsm8k :  25%|██████████▊                                | 25/100 [52:08<2:44:41, 131.75s/it]Evaluating gsm8k :  26%|███████████▏                               | 26/100 [53:14<2:18:18, 112.14s/it]Evaluating gsm8k :  27%|███████████▌                               | 27/100 [55:30<2:24:52, 119.07s/it]Evaluating gsm8k :  28%|████████████                               | 28/100 [57:45<2:28:55, 124.10s/it]Evaluating gsm8k :  29%|███████████▉                             | 29/100 [1:00:02<2:31:21, 127.91s/it]Evaluating gsm8k :  30%|████████████▎                            | 30/100 [1:02:09<2:28:54, 127.64s/it]Evaluating gsm8k :  31%|████████████▋                            | 31/100 [1:04:26<2:29:45, 130.23s/it]Evaluating gsm8k :  32%|█████████████                            | 32/100 [1:06:41<2:29:30, 131.92s/it]Evaluating gsm8k :  33%|█████████████▌                           | 33/100 [1:07:55<2:07:41, 114.35s/it]Evaluating gsm8k :  34%|█████████████▉                           | 34/100 [1:10:10<2:12:38, 120.58s/it]Evaluating gsm8k :  35%|██████████████▎                          | 35/100 [1:12:26<2:15:50, 125.39s/it]Evaluating gsm8k :  36%|██████████████▊                          | 36/100 [1:14:40<2:16:24, 127.89s/it]Evaluating gsm8k :  37%|███████████████▏                         | 37/100 [1:16:42<2:12:23, 126.09s/it]Evaluating gsm8k :  38%|███████████████▌                         | 38/100 [1:18:18<2:01:01, 117.13s/it]Evaluating gsm8k :  39%|███████████████▉                         | 39/100 [1:20:32<2:04:10, 122.15s/it]Evaluating gsm8k :  40%|████████████████▍                        | 40/100 [1:22:50<2:06:44, 126.74s/it]Evaluating gsm8k :  41%|████████████████▊                        | 41/100 [1:25:01<2:06:08, 128.28s/it]Evaluating gsm8k :  42%|█████████████████▏                       | 42/100 [1:26:09<1:46:18, 109.97s/it]Evaluating gsm8k :  43%|█████████████████▋                       | 43/100 [1:28:25<1:51:54, 117.79s/it]Evaluating gsm8k :  44%|██████████████████                       | 44/100 [1:30:11<1:46:41, 114.31s/it]Evaluating gsm8k :  45%|██████████████████▍                      | 45/100 [1:32:27<1:50:47, 120.86s/it]Evaluating gsm8k :  46%|██████████████████▊                      | 46/100 [1:34:43<1:52:45, 125.28s/it]Evaluating gsm8k :  47%|███████████████████▎                     | 47/100 [1:36:58<1:53:25, 128.41s/it]Evaluating gsm8k :  48%|███████████████████▋                     | 48/100 [1:38:29<1:41:20, 116.93s/it]Evaluating gsm8k :  49%|████████████████████                     | 49/100 [1:40:45<1:44:23, 122.81s/it]Evaluating gsm8k :  50%|████████████████████▌                    | 50/100 [1:43:00<1:45:19, 126.39s/it]Evaluating gsm8k :  51%|████████████████████▉                    | 51/100 [1:45:15<1:45:17, 128.93s/it]Evaluating gsm8k :  52%|█████████████████████▎                   | 52/100 [1:47:26<1:43:48, 129.76s/it]Evaluating gsm8k :  53%|█████████████████████▋                   | 53/100 [1:49:42<1:43:04, 131.58s/it]Evaluating gsm8k :  54%|██████████████████████▏                  | 54/100 [1:51:54<1:40:58, 131.70s/it]Evaluating gsm8k :  55%|██████████████████████▌                  | 55/100 [1:53:39<1:32:45, 123.69s/it]Evaluating gsm8k :  56%|██████████████████████▉                  | 56/100 [1:54:56<1:20:19, 109.54s/it]Evaluating gsm8k :  57%|███████████████████████▎                 | 57/100 [1:57:10<1:23:48, 116.94s/it]Evaluating gsm8k :  58%|███████████████████████▊                 | 58/100 [1:59:27<1:26:05, 122.98s/it]Evaluating gsm8k :  59%|████████████████████████▏                | 59/100 [2:01:20<1:21:58, 119.96s/it]Evaluating gsm8k :  60%|████████████████████████▌                | 60/100 [2:03:04<1:16:45, 115.14s/it]Evaluating gsm8k :  61%|█████████████████████████                | 61/100 [2:05:17<1:18:26, 120.68s/it]Evaluating gsm8k :  62%|█████████████████████████▍               | 62/100 [2:07:02<1:13:27, 115.98s/it]Evaluating gsm8k :  63%|███████████████████████████▋                | 63/100 [2:07:28<54:44, 88.77s/it]Evaluating gsm8k :  64%|██████████████████████████▏              | 64/100 [2:09:44<1:01:45, 102.93s/it]Evaluating gsm8k :  65%|██████████████████████████▋              | 65/100 [2:11:58<1:05:30, 112.30s/it]Evaluating gsm8k :  66%|███████████████████████████              | 66/100 [2:14:12<1:07:23, 118.94s/it]Evaluating gsm8k :  67%|███████████████████████████▍             | 67/100 [2:16:20<1:06:47, 121.44s/it]Evaluating gsm8k :  68%|███████████████████████████▉             | 68/100 [2:18:34<1:06:54, 125.45s/it]Evaluating gsm8k :  69%|████████████████████████████▎            | 69/100 [2:20:47<1:05:57, 127.67s/it]Evaluating gsm8k :  70%|██████████████████████████████             | 70/100 [2:22:28<59:44, 119.50s/it]Evaluating gsm8k :  71%|██████████████████████████████▌            | 71/100 [2:24:41<59:45, 123.65s/it]Evaluating gsm8k :  72%|██████████████████████████████▉            | 72/100 [2:26:59<59:43, 127.97s/it]Evaluating gsm8k :  73%|███████████████████████████████▍           | 73/100 [2:28:23<51:41, 114.86s/it]Evaluating gsm8k :  74%|███████████████████████████████▊           | 74/100 [2:29:30<43:33, 100.53s/it]Evaluating gsm8k :  75%|████████████████████████████████▎          | 75/100 [2:31:27<43:53, 105.33s/it]Evaluating gsm8k :  76%|████████████████████████████████▋          | 76/100 [2:33:41<45:33, 113.90s/it]Evaluating gsm8k :  77%|█████████████████████████████████          | 77/100 [2:35:56<46:08, 120.35s/it]Evaluating gsm8k :  78%|█████████████████████████████████▌         | 78/100 [2:38:14<46:02, 125.56s/it]Evaluating gsm8k :  79%|█████████████████████████████████▉         | 79/100 [2:40:28<44:50, 128.12s/it]Evaluating gsm8k :  80%|██████████████████████████████████▍        | 80/100 [2:42:43<43:24, 130.22s/it]Evaluating gsm8k :  81%|██████████████████████████████████▊        | 81/100 [2:43:48<35:03, 110.70s/it]Evaluating gsm8k :  82%|███████████████████████████████████▎       | 82/100 [2:46:03<35:22, 117.94s/it]Evaluating gsm8k :  83%|███████████████████████████████████▋       | 83/100 [2:48:04<33:38, 118.72s/it]Evaluating gsm8k :  84%|████████████████████████████████████       | 84/100 [2:50:20<33:03, 123.95s/it]Evaluating gsm8k :  85%|████████████████████████████████████▌      | 85/100 [2:52:31<31:31, 126.07s/it]Evaluating gsm8k :  86%|████████████████████████████████████▉      | 86/100 [2:53:31<24:50, 106.45s/it]Evaluating gsm8k :  87%|█████████████████████████████████████▍     | 87/100 [2:55:47<24:55, 115.01s/it]Evaluating gsm8k :  88%|█████████████████████████████████████▊     | 88/100 [2:57:30<22:20, 111.69s/it]Evaluating gsm8k :  89%|███████████████████████████████████████▏    | 89/100 [2:58:32<17:44, 96.80s/it]Evaluating gsm8k :  90%|██████████████████████████████████████▋    | 90/100 [3:00:44<17:51, 107.10s/it]Evaluating gsm8k :  91%|███████████████████████████████████████▏   | 91/100 [3:02:33<16:10, 107.84s/it]Evaluating gsm8k :  92%|███████████████████████████████████████▌   | 92/100 [3:04:52<15:36, 117.04s/it]Evaluating gsm8k :  93%|███████████████████████████████████████▉   | 93/100 [3:07:09<14:21, 123.04s/it]Evaluating gsm8k :  94%|████████████████████████████████████████▍  | 94/100 [3:09:26<12:43, 127.30s/it]Evaluating gsm8k :  95%|████████████████████████████████████████▊  | 95/100 [3:11:35<10:39, 127.83s/it]Evaluating gsm8k :  96%|█████████████████████████████████████████▎ | 96/100 [3:13:49<08:38, 129.55s/it]Evaluating gsm8k :  97%|█████████████████████████████████████████▋ | 97/100 [3:15:02<05:37, 112.58s/it]Evaluating gsm8k :  98%|██████████████████████████████████████████▏| 98/100 [3:17:15<03:57, 118.76s/it]Evaluating gsm8k :  99%|██████████████████████████████████████████▌| 99/100 [3:19:28<02:03, 123.22s/it]Evaluating gsm8k : 100%|██████████████████████████████████████████| 100/100 [3:20:28<00:00, 104.08s/it]Evaluating gsm8k : 100%|██████████████████████████████████████████| 100/100 [3:20:28<00:00, 120.28s/it]
name: gsm8k | avg. gen lenth: 228.254 | time: 12028.83826494217s
