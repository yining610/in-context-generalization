torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 29501 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 4 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tgsm8k-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 1
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-27 13:09:31,901] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 13:09:31,903] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 13:09:31,966] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 13:09:31,973] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o1-tgsm8k-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 1
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o1-tgsm8k-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                       | 0/9741 [00:00<?, ?it/s]Loading data: 100%|███████████████████████████████████████| 9741/9741 [00:00<00:00, 1156791.39it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:08<00:08,  8.00s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:08<00:08,  8.03s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:08<00:08,  8.17s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:08<00:08,  8.25s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:10<00:00,  4.70s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:10<00:00,  5.20s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:10<00:00,  4.72s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:10<00:00,  5.22s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:10<00:00,  4.77s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:10<00:00,  5.28s/it]
[2023-08-27 13:09:43,677] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-27 13:09:43,702] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:10<00:00,  4.72s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:10<00:00,  5.25s/it]
[2023-08-27 13:09:43,753] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-27 13:09:43,915] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-27 13:09:44,484] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-27 13:09:44,486] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-27 13:09:44,486] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-27 13:09:44,486] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-27 13:09:44,486] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-27 13:09:44,486] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-27 13:09:44,486] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-27 13:09:44,486] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-27 13:09:44,486] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7942c1f640>
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-27 13:09:44,487] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-27 13:09:44,488] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-27 13:09:44,488] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-27 13:09:44,488] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-27 13:09:44,488] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-27 13:09:44,488] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-27 13:09:44,488] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-27 13:09:44,488] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-27 13:09:44,488] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-27 13:09:44,488] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-27 13:09:44,488] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-27 13:09:44,488] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-27 13:09:44,488] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-27 13:09:44,488] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-27 13:09:44,488] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                            | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: 44

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o1-tgsm8k-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▌                                | 1/63 [01:56<2:00:07, 116.25s/it]Evaluating commonsenseqa :   3%|█                                | 2/63 [03:48<1:55:44, 113.85s/it]Evaluating commonsenseqa :   5%|█▌                               | 3/63 [05:42<1:53:55, 113.92s/it]Evaluating commonsenseqa :   6%|██                               | 4/63 [07:39<1:53:17, 115.21s/it]Evaluating commonsenseqa :   8%|██▋                               | 5/63 [08:28<1:28:20, 91.38s/it]Evaluating commonsenseqa :  10%|███▏                              | 6/63 [10:22<1:34:02, 98.98s/it]Evaluating commonsenseqa :  11%|███▊                              | 7/63 [11:58<1:31:33, 98.09s/it]Evaluating commonsenseqa :  13%|████▏                            | 8/63 [13:53<1:34:55, 103.55s/it]Evaluating commonsenseqa :  14%|████▋                            | 9/63 [15:50<1:36:50, 107.60s/it]Evaluating commonsenseqa :  16%|█████▏                           | 10/63 [16:07<1:10:17, 79.57s/it]Evaluating commonsenseqa :  17%|█████▊                           | 11/63 [17:54<1:16:24, 88.17s/it]Evaluating commonsenseqa :  19%|██████▎                          | 12/63 [19:11<1:11:58, 84.68s/it]Evaluating commonsenseqa :  21%|██████▊                          | 13/63 [21:08<1:18:47, 94.55s/it]Evaluating commonsenseqa :  22%|███████                         | 14/63 [23:04<1:22:20, 100.83s/it]Evaluating commonsenseqa :  24%|███████▊                         | 15/63 [24:35<1:18:26, 98.05s/it]Evaluating commonsenseqa :  25%|████████▍                        | 16/63 [25:54<1:12:13, 92.20s/it]Evaluating commonsenseqa :  27%|████████▉                        | 17/63 [27:49<1:15:56, 99.05s/it]Evaluating commonsenseqa :  29%|█████████▍                       | 18/63 [29:07<1:09:36, 92.81s/it]Evaluating commonsenseqa :  30%|█████████▉                       | 19/63 [31:00<1:12:30, 98.88s/it]Evaluating commonsenseqa :  32%|██████████▏                     | 20/63 [32:54<1:14:02, 103.31s/it]Evaluating commonsenseqa :  33%|██████████▋                     | 21/63 [34:50<1:14:58, 107.10s/it]Evaluating commonsenseqa :  35%|███████████▏                    | 22/63 [36:43<1:14:22, 108.85s/it]Evaluating commonsenseqa :  37%|███████████▋                    | 23/63 [38:38<1:13:45, 110.65s/it]Evaluating commonsenseqa :  38%|████████████▏                   | 24/63 [40:34<1:13:00, 112.33s/it]Evaluating commonsenseqa :  40%|████████████▋                   | 25/63 [42:30<1:11:46, 113.32s/it]Evaluating commonsenseqa :  41%|█████████████▏                  | 26/63 [44:23<1:09:58, 113.48s/it]Evaluating commonsenseqa :  43%|█████████████▋                  | 27/63 [45:32<1:00:00, 100.03s/it]Evaluating commonsenseqa :  44%|██████████████▏                 | 28/63 [47:26<1:00:46, 104.19s/it]Evaluating commonsenseqa :  46%|██████████████▋                 | 29/63 [49:20<1:00:45, 107.22s/it]Evaluating commonsenseqa :  48%|███████████████▏                | 30/63 [51:16<1:00:20, 109.72s/it]Evaluating commonsenseqa :  49%|█████████████████▏                 | 31/63 [52:18<50:52, 95.40s/it]Evaluating commonsenseqa :  51%|█████████████████▎                | 32/63 [54:13<52:19, 101.28s/it]Evaluating commonsenseqa :  52%|█████████████████▊                | 33/63 [55:55<50:50, 101.68s/it]Evaluating commonsenseqa :  54%|██████████████████▎               | 34/63 [57:51<51:09, 105.83s/it]Evaluating commonsenseqa :  56%|██████████████████▉               | 35/63 [59:45<50:33, 108.36s/it]Evaluating commonsenseqa :  57%|██████████████████▎             | 36/63 [1:01:40<49:36, 110.25s/it]Evaluating commonsenseqa :  59%|██████████████████▊             | 37/63 [1:03:37<48:42, 112.39s/it]Evaluating commonsenseqa :  60%|███████████████████▎            | 38/63 [1:05:31<47:03, 112.92s/it]Evaluating commonsenseqa :  62%|███████████████████▊            | 39/63 [1:07:26<45:25, 113.56s/it]Evaluating commonsenseqa :  63%|████████████████████▎           | 40/63 [1:09:24<44:01, 114.85s/it]Evaluating commonsenseqa :  65%|████████████████████▊           | 41/63 [1:11:20<42:10, 115.01s/it]Evaluating commonsenseqa :  67%|█████████████████████▎          | 42/63 [1:13:14<40:13, 114.91s/it]Evaluating commonsenseqa :  68%|█████████████████████▊          | 43/63 [1:14:22<33:35, 100.77s/it]Evaluating commonsenseqa :  70%|██████████████████████▎         | 44/63 [1:16:10<32:33, 102.83s/it]Evaluating commonsenseqa :  71%|██████████████████████▊         | 45/63 [1:18:05<32:00, 106.68s/it]Evaluating commonsenseqa :  73%|███████████████████████▎        | 46/63 [1:19:45<29:35, 104.42s/it]Evaluating commonsenseqa :  75%|███████████████████████▊        | 47/63 [1:21:39<28:40, 107.52s/it]Evaluating commonsenseqa :  76%|████████████████████████▍       | 48/63 [1:23:35<27:31, 110.09s/it]Evaluating commonsenseqa :  78%|█████████████████████████▋       | 49/63 [1:24:49<23:10, 99.31s/it]Evaluating commonsenseqa :  79%|██████████████████████████▏      | 50/63 [1:25:08<16:15, 75.06s/it]Evaluating commonsenseqa :  81%|██████████████████████████▋      | 51/63 [1:27:01<17:17, 86.47s/it]Evaluating commonsenseqa :  83%|███████████████████████████▏     | 52/63 [1:28:32<16:07, 87.95s/it]Evaluating commonsenseqa :  84%|███████████████████████████▊     | 53/63 [1:30:27<15:59, 95.99s/it]Evaluating commonsenseqa :  86%|███████████████████████████▍    | 54/63 [1:32:25<15:22, 102.46s/it]Evaluating commonsenseqa :  87%|███████████████████████████▉    | 55/63 [1:34:20<14:09, 106.25s/it]Evaluating commonsenseqa :  89%|████████████████████████████▍   | 56/63 [1:36:14<12:39, 108.55s/it]Evaluating commonsenseqa :  90%|█████████████████████████████▊   | 57/63 [1:37:08<09:14, 92.35s/it]Evaluating commonsenseqa :  92%|██████████████████████████████▍  | 58/63 [1:39:01<08:12, 98.48s/it]Evaluating commonsenseqa :  94%|██████████████████████████████▉  | 59/63 [1:40:25<06:15, 93.95s/it]Evaluating commonsenseqa :  95%|██████████████████████████████▍ | 60/63 [1:42:20<05:00, 100.28s/it]Evaluating commonsenseqa :  97%|███████████████████████████████▉ | 61/63 [1:43:50<03:14, 97.39s/it]Evaluating commonsenseqa :  98%|████████████████████████████████▍| 62/63 [1:44:37<01:22, 82.35s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:45:42<00:00, 77.08s/it]Evaluating commonsenseqa : 100%|████████████████████████████████| 63/63 [1:45:42<00:00, 100.68s/it]
name: commonsenseqa | avg. gen lenth: 271.712 | time: 6342.977553129196s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 29501 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 4 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o2-tgsm8k-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 2
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-27 14:58:05,376] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 14:58:05,902] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 14:58:05,904] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 14:58:05,923] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o2-tgsm8k-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 2
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o2-tgsm8k-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                       | 0/9741 [00:00<?, ?it/s]Loading data: 100%|███████████████████████████████████████| 9741/9741 [00:00<00:00, 1049114.50it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.08s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.11s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.23s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:09<00:09,  9.06s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.20s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.63s/it]
[2023-08-27 14:58:16,419] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.29s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.73s/it]
[2023-08-27 14:58:16,577] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.50s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.89s/it]
[2023-08-27 14:58:16,972] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:11<00:00,  4.93s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:11<00:00,  5.55s/it]
 > number of parameters: 6738415616
[2023-08-27 14:58:18,252] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-27 14:58:18,801] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-27 14:58:18,802] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7faec2267520>
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-27 14:58:18,803] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-27 14:58:18,804] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-27 14:58:18,804] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                            | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: 6

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o2-tgsm8k-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▌                                | 1/63 [01:49<1:53:03, 109.42s/it]Evaluating commonsenseqa :   3%|█                                | 2/63 [03:41<1:52:36, 110.76s/it]Evaluating commonsenseqa :   5%|█▌                                | 3/63 [04:42<1:27:59, 87.99s/it]Evaluating commonsenseqa :   6%|██▏                               | 4/63 [06:31<1:35:01, 96.64s/it]Evaluating commonsenseqa :   8%|██▌                              | 5/63 [08:20<1:37:26, 100.80s/it]Evaluating commonsenseqa :  10%|███▏                             | 6/63 [10:08<1:38:17, 103.47s/it]Evaluating commonsenseqa :  11%|███▋                             | 7/63 [11:55<1:37:37, 104.61s/it]Evaluating commonsenseqa :  13%|████▎                             | 8/63 [12:54<1:22:25, 89.91s/it]Evaluating commonsenseqa :  14%|████▊                             | 9/63 [14:44<1:26:32, 96.16s/it]Evaluating commonsenseqa :  16%|█████                           | 10/63 [16:32<1:28:24, 100.08s/it]Evaluating commonsenseqa :  17%|█████▌                          | 11/63 [18:21<1:28:55, 102.60s/it]Evaluating commonsenseqa :  19%|██████                          | 12/63 [20:10<1:28:55, 104.63s/it]Evaluating commonsenseqa :  21%|██████▌                         | 13/63 [22:00<1:28:38, 106.38s/it]Evaluating commonsenseqa :  22%|███████▎                         | 14/63 [23:04<1:16:21, 93.49s/it]Evaluating commonsenseqa :  24%|████████▎                          | 15/63 [23:36<59:57, 74.95s/it]Evaluating commonsenseqa :  25%|████████▍                        | 16/63 [25:24<1:06:24, 84.78s/it]Evaluating commonsenseqa :  27%|████████▉                        | 17/63 [27:12<1:10:27, 91.89s/it]Evaluating commonsenseqa :  29%|█████████▍                       | 18/63 [29:02<1:13:01, 97.37s/it]Evaluating commonsenseqa :  30%|█████████▋                      | 19/63 [30:55<1:14:44, 101.91s/it]Evaluating commonsenseqa :  32%|██████████▏                     | 20/63 [32:44<1:14:32, 104.00s/it]Evaluating commonsenseqa :  33%|██████████▋                     | 21/63 [34:22<1:11:40, 102.40s/it]Evaluating commonsenseqa :  35%|███████████▌                     | 22/63 [35:34<1:03:46, 93.32s/it]Evaluating commonsenseqa :  37%|████████████                     | 23/63 [37:24<1:05:32, 98.30s/it]Evaluating commonsenseqa :  38%|████████████▏                   | 24/63 [39:16<1:06:28, 102.27s/it]Evaluating commonsenseqa :  40%|█████████████▉                     | 25/63 [40:20<57:33, 90.89s/it]Evaluating commonsenseqa :  41%|██████████████▍                    | 26/63 [42:09<59:22, 96.27s/it]Evaluating commonsenseqa :  43%|█████████████▋                  | 27/63 [44:01<1:00:31, 100.89s/it]Evaluating commonsenseqa :  44%|███████████████▌                   | 28/63 [45:10<53:24, 91.56s/it]Evaluating commonsenseqa :  46%|████████████████                   | 29/63 [46:58<54:34, 96.32s/it]Evaluating commonsenseqa :  48%|████████████████▏                 | 30/63 [48:49<55:26, 100.81s/it]Evaluating commonsenseqa :  49%|████████████████▋                 | 31/63 [50:44<56:01, 105.05s/it]Evaluating commonsenseqa :  51%|█████████████████▎                | 32/63 [52:35<55:07, 106.69s/it]Evaluating commonsenseqa :  52%|█████████████████▊                | 33/63 [54:26<54:02, 108.09s/it]Evaluating commonsenseqa :  54%|██████████████████▎               | 34/63 [56:16<52:26, 108.51s/it]Evaluating commonsenseqa :  56%|██████████████████▉               | 35/63 [57:43<47:43, 102.28s/it]Evaluating commonsenseqa :  57%|███████████████████▍              | 36/63 [59:34<47:09, 104.80s/it]Evaluating commonsenseqa :  59%|███████████████████▍             | 37/63 [1:00:43<40:47, 94.14s/it]Evaluating commonsenseqa :  60%|███████████████████▉             | 38/63 [1:02:30<40:47, 97.89s/it]Evaluating commonsenseqa :  62%|███████████████████▊            | 39/63 [1:04:17<40:19, 100.81s/it]Evaluating commonsenseqa :  63%|████████████████████▉            | 40/63 [1:05:35<36:00, 93.93s/it]Evaluating commonsenseqa :  65%|█████████████████████▍           | 41/63 [1:06:24<29:28, 80.37s/it]Evaluating commonsenseqa :  67%|██████████████████████           | 42/63 [1:08:13<31:09, 89.04s/it]Evaluating commonsenseqa :  68%|██████████████████████▌          | 43/63 [1:09:33<28:41, 86.08s/it]Evaluating commonsenseqa :  70%|███████████████████████          | 44/63 [1:11:21<29:25, 92.92s/it]Evaluating commonsenseqa :  71%|███████████████████████▌         | 45/63 [1:12:11<23:58, 79.92s/it]Evaluating commonsenseqa :  73%|████████████████████████         | 46/63 [1:14:04<25:28, 89.89s/it]Evaluating commonsenseqa :  75%|████████████████████████▌        | 47/63 [1:15:55<25:38, 96.14s/it]Evaluating commonsenseqa :  76%|████████████████████████▍       | 48/63 [1:17:44<25:01, 100.10s/it]Evaluating commonsenseqa :  78%|████████████████████████▉       | 49/63 [1:19:34<24:03, 103.08s/it]Evaluating commonsenseqa :  79%|██████████████████████████▏      | 50/63 [1:20:37<19:43, 91.03s/it]Evaluating commonsenseqa :  81%|██████████████████████████▋      | 51/63 [1:22:25<19:14, 96.19s/it]Evaluating commonsenseqa :  83%|██████████████████████████▍     | 52/63 [1:24:17<18:29, 100.86s/it]Evaluating commonsenseqa :  84%|███████████████████████████▊     | 53/63 [1:24:46<13:11, 79.18s/it]Evaluating commonsenseqa :  86%|████████████████████████████▎    | 54/63 [1:26:35<13:13, 88.21s/it]Evaluating commonsenseqa :  87%|████████████████████████████▊    | 55/63 [1:28:26<12:41, 95.15s/it]Evaluating commonsenseqa :  89%|█████████████████████████████▎   | 56/63 [1:30:17<11:38, 99.77s/it]Evaluating commonsenseqa :  90%|████████████████████████████▉   | 57/63 [1:32:00<10:03, 100.63s/it]Evaluating commonsenseqa :  92%|██████████████████████████████▍  | 58/63 [1:33:11<07:39, 91.81s/it]Evaluating commonsenseqa :  94%|██████████████████████████████▉  | 59/63 [1:34:53<06:20, 95.08s/it]Evaluating commonsenseqa :  95%|███████████████████████████████▍ | 60/63 [1:36:44<04:59, 99.77s/it]Evaluating commonsenseqa :  97%|███████████████████████████████▉ | 61/63 [1:37:35<02:50, 85.10s/it]Evaluating commonsenseqa :  98%|████████████████████████████████▍| 62/63 [1:39:24<01:32, 92.35s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:40:26<00:00, 83.08s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:40:26<00:00, 95.66s/it]
name: commonsenseqa | avg. gen lenth: 259.568 | time: 6026.538503170013s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 29501 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 4 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o3-tgsm8k-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 3
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-27 16:38:54,523] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 16:38:54,524] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 16:38:54,524] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 16:38:54,524] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o3-tgsm8k-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 3
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o3-tgsm8k-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                       | 0/9741 [00:00<?, ?it/s]Loading data: 100%|███████████████████████████████████████| 9741/9741 [00:00<00:00, 1436239.86it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.04s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.13s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.39s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.44s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.19s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.63s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.27s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.69s/it]
[2023-08-27 16:39:05,153] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-27 16:39:05,173] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.31s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.77s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.31s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.78s/it]
[2023-08-27 16:39:05,490] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-27 16:39:05,513] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-27 16:39:06,076] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-27 16:39:06,077] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7f7737f640>
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-27 16:39:06,078] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-27 16:39:06,079] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-27 16:39:06,080] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-27 16:39:06,080] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-27 16:39:06,080] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-27 16:39:06,080] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-27 16:39:06,080] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                            | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: 322

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o3-tgsm8k-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▌                                | 1/63 [01:48<1:52:32, 108.91s/it]Evaluating commonsenseqa :   3%|█                                | 2/63 [03:38<1:51:18, 109.49s/it]Evaluating commonsenseqa :   5%|█▌                               | 3/63 [05:26<1:48:31, 108.53s/it]Evaluating commonsenseqa :   6%|██                               | 4/63 [07:13<1:46:04, 107.86s/it]Evaluating commonsenseqa :   8%|██▌                              | 5/63 [09:02<1:44:44, 108.35s/it]Evaluating commonsenseqa :  10%|███▏                             | 6/63 [10:51<1:43:14, 108.67s/it]Evaluating commonsenseqa :  11%|███▊                              | 7/63 [11:56<1:28:12, 94.50s/it]Evaluating commonsenseqa :  13%|████▎                             | 8/63 [13:35<1:27:57, 95.95s/it]Evaluating commonsenseqa :  14%|████▊                             | 9/63 [15:24<1:29:52, 99.87s/it]Evaluating commonsenseqa :  16%|█████                           | 10/63 [17:12<1:30:24, 102.34s/it]Evaluating commonsenseqa :  17%|█████▌                          | 11/63 [19:03<1:31:02, 105.04s/it]Evaluating commonsenseqa :  19%|██████                          | 12/63 [20:52<1:30:25, 106.37s/it]Evaluating commonsenseqa :  21%|██████▌                         | 13/63 [22:42<1:29:33, 107.47s/it]Evaluating commonsenseqa :  22%|███████                         | 14/63 [24:31<1:28:05, 107.88s/it]Evaluating commonsenseqa :  24%|███████▌                        | 15/63 [26:21<1:26:47, 108.49s/it]Evaluating commonsenseqa :  25%|████████▏                       | 16/63 [28:08<1:24:35, 108.00s/it]Evaluating commonsenseqa :  27%|████████▋                       | 17/63 [29:54<1:22:26, 107.53s/it]Evaluating commonsenseqa :  29%|█████████▏                      | 18/63 [31:23<1:16:24, 101.88s/it]Evaluating commonsenseqa :  30%|█████████▋                      | 19/63 [33:13<1:16:32, 104.36s/it]Evaluating commonsenseqa :  32%|██████████▏                     | 20/63 [35:01<1:15:34, 105.46s/it]Evaluating commonsenseqa :  33%|██████████▋                     | 21/63 [36:48<1:14:07, 105.89s/it]Evaluating commonsenseqa :  35%|███████████▏                    | 22/63 [38:35<1:12:31, 106.14s/it]Evaluating commonsenseqa :  37%|███████████▋                    | 23/63 [40:22<1:11:02, 106.57s/it]Evaluating commonsenseqa :  38%|████████████▏                   | 24/63 [42:09<1:09:11, 106.44s/it]Evaluating commonsenseqa :  40%|████████████▋                   | 25/63 [43:56<1:07:37, 106.77s/it]Evaluating commonsenseqa :  41%|█████████████▏                  | 26/63 [45:44<1:05:59, 107.02s/it]Evaluating commonsenseqa :  43%|█████████████▋                  | 27/63 [47:34<1:04:42, 107.85s/it]Evaluating commonsenseqa :  44%|██████████████▏                 | 28/63 [49:22<1:03:05, 108.15s/it]Evaluating commonsenseqa :  46%|██████████████▋                 | 29/63 [51:11<1:01:19, 108.23s/it]Evaluating commonsenseqa :  48%|████████████████▏                 | 30/63 [52:58<59:22, 107.95s/it]Evaluating commonsenseqa :  49%|████████████████▋                 | 31/63 [54:48<57:51, 108.48s/it]Evaluating commonsenseqa :  51%|█████████████████▎                | 32/63 [56:38<56:18, 108.98s/it]Evaluating commonsenseqa :  52%|█████████████████▊                | 33/63 [58:28<54:37, 109.26s/it]Evaluating commonsenseqa :  54%|█████████████████▎              | 34/63 [1:00:11<51:57, 107.50s/it]Evaluating commonsenseqa :  56%|█████████████████▊              | 35/63 [1:01:58<50:02, 107.22s/it]Evaluating commonsenseqa :  57%|██████████████████▎             | 36/63 [1:03:48<48:38, 108.08s/it]Evaluating commonsenseqa :  59%|██████████████████▊             | 37/63 [1:05:39<47:13, 108.99s/it]Evaluating commonsenseqa :  60%|███████████████████▎            | 38/63 [1:07:27<45:17, 108.71s/it]Evaluating commonsenseqa :  62%|████████████████████▍            | 39/63 [1:08:38<38:53, 97.23s/it]Evaluating commonsenseqa :  63%|████████████████████▎           | 40/63 [1:10:26<38:33, 100.60s/it]Evaluating commonsenseqa :  65%|████████████████████▊           | 41/63 [1:12:15<37:48, 103.10s/it]Evaluating commonsenseqa :  67%|█████████████████████▎          | 42/63 [1:14:03<36:37, 104.66s/it]Evaluating commonsenseqa :  68%|██████████████████████▌          | 43/63 [1:15:13<31:22, 94.14s/it]Evaluating commonsenseqa :  70%|███████████████████████          | 44/63 [1:17:02<31:16, 98.74s/it]Evaluating commonsenseqa :  71%|███████████████████████▌         | 45/63 [1:17:44<24:27, 81.51s/it]Evaluating commonsenseqa :  73%|████████████████████████         | 46/63 [1:19:32<25:23, 89.64s/it]Evaluating commonsenseqa :  75%|████████████████████████▌        | 47/63 [1:21:20<25:23, 95.22s/it]Evaluating commonsenseqa :  76%|█████████████████████████▏       | 48/63 [1:23:08<24:44, 98.97s/it]Evaluating commonsenseqa :  78%|█████████████████████████▋       | 49/63 [1:24:33<22:08, 94.87s/it]Evaluating commonsenseqa :  79%|██████████████████████████▏      | 50/63 [1:26:22<21:28, 99.10s/it]Evaluating commonsenseqa :  81%|██████████████████████████▋      | 51/63 [1:27:07<16:31, 82.63s/it]Evaluating commonsenseqa :  83%|███████████████████████████▏     | 52/63 [1:28:56<16:36, 90.57s/it]Evaluating commonsenseqa :  84%|███████████████████████████▊     | 53/63 [1:29:20<11:45, 70.53s/it]Evaluating commonsenseqa :  86%|████████████████████████████▎    | 54/63 [1:31:07<12:15, 81.75s/it]Evaluating commonsenseqa :  87%|████████████████████████████▊    | 55/63 [1:32:54<11:54, 89.28s/it]Evaluating commonsenseqa :  89%|█████████████████████████████▎   | 56/63 [1:34:40<10:59, 94.28s/it]Evaluating commonsenseqa :  90%|█████████████████████████████▊   | 57/63 [1:36:28<09:50, 98.39s/it]Evaluating commonsenseqa :  92%|█████████████████████████████▍  | 58/63 [1:38:19<08:30, 102.02s/it]Evaluating commonsenseqa :  94%|██████████████████████████████▉  | 59/63 [1:39:35<06:16, 94.15s/it]Evaluating commonsenseqa :  95%|███████████████████████████████▍ | 60/63 [1:41:21<04:53, 97.77s/it]Evaluating commonsenseqa :  97%|██████████████████████████████▉ | 61/63 [1:43:07<03:20, 100.19s/it]Evaluating commonsenseqa :  98%|████████████████████████████████▍| 62/63 [1:44:14<01:30, 90.34s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:44:27<00:00, 67.16s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:44:27<00:00, 99.48s/it]
name: commonsenseqa | avg. gen lenth: 271.164 | time: 6267.746102333069s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 29501 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 4 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o4-tgsm8k-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-27 18:23:43,083] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 18:23:43,116] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 18:23:43,134] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 18:23:43,135] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o4-tgsm8k-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 4
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o4-tgsm8k-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                       | 0/9741 [00:00<?, ?it/s]Loading data: 100%|███████████████████████████████████████| 9741/9741 [00:00<00:00, 1289669.04it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:06<00:06,  6.85s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:06<00:06,  6.89s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:06<00:06,  6.94s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.31s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.19s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.59s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.22s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.62s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.18s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.60s/it]
 > number of parameters: 6738415616
[2023-08-27 18:23:53,532] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-27 18:23:53,542] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-27 18:23:53,591] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.23s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.69s/it]
[2023-08-27 18:23:53,823] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-27 18:23:54,464] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-27 18:23:54,465] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-27 18:23:54,465] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-27 18:23:54,465] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f52eaa6f640>
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-27 18:23:54,466] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-27 18:23:54,467] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-27 18:23:54,467] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                            | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: 3

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o4-tgsm8k-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▌                                | 1/63 [01:46<1:50:12, 106.66s/it]Evaluating commonsenseqa :   3%|█                                 | 2/63 [02:44<1:19:29, 78.18s/it]Evaluating commonsenseqa :   5%|█▌                                | 3/63 [04:30<1:30:48, 90.81s/it]Evaluating commonsenseqa :   6%|██▏                               | 4/63 [06:16<1:34:56, 96.55s/it]Evaluating commonsenseqa :   8%|██▋                               | 5/63 [08:01<1:36:29, 99.82s/it]Evaluating commonsenseqa :  10%|███▏                              | 6/63 [09:22<1:28:49, 93.50s/it]Evaluating commonsenseqa :  11%|███▊                              | 7/63 [11:07<1:30:40, 97.15s/it]Evaluating commonsenseqa :  13%|████▏                            | 8/63 [12:54<1:32:00, 100.38s/it]Evaluating commonsenseqa :  14%|████▊                             | 9/63 [13:47<1:16:57, 85.52s/it]Evaluating commonsenseqa :  16%|█████▏                           | 10/63 [15:05<1:13:24, 83.10s/it]Evaluating commonsenseqa :  17%|█████▊                           | 11/63 [15:56<1:03:30, 73.27s/it]Evaluating commonsenseqa :  19%|██████▎                          | 12/63 [17:42<1:10:42, 83.19s/it]Evaluating commonsenseqa :  21%|██████▊                          | 13/63 [19:28<1:15:09, 90.19s/it]Evaluating commonsenseqa :  22%|███████▎                         | 14/63 [21:15<1:17:53, 95.37s/it]Evaluating commonsenseqa :  24%|███████▊                         | 15/63 [23:00<1:18:28, 98.10s/it]Evaluating commonsenseqa :  25%|████████▏                       | 16/63 [24:46<1:18:42, 100.48s/it]Evaluating commonsenseqa :  27%|████████▋                       | 17/63 [26:32<1:18:18, 102.14s/it]Evaluating commonsenseqa :  29%|█████████▍                       | 18/63 [27:15<1:03:13, 84.31s/it]Evaluating commonsenseqa :  30%|█████████▉                       | 19/63 [29:00<1:06:22, 90.51s/it]Evaluating commonsenseqa :  32%|██████████▍                      | 20/63 [30:48<1:08:43, 95.89s/it]Evaluating commonsenseqa :  33%|███████████▋                       | 21/63 [31:39<57:43, 82.45s/it]Evaluating commonsenseqa :  35%|███████████▌                     | 22/63 [33:25<1:01:02, 89.33s/it]Evaluating commonsenseqa :  37%|████████████                     | 23/63 [35:10<1:02:42, 94.07s/it]Evaluating commonsenseqa :  38%|████████████▌                    | 24/63 [36:57<1:03:47, 98.14s/it]Evaluating commonsenseqa :  40%|████████████▋                   | 25/63 [38:44<1:03:51, 100.82s/it]Evaluating commonsenseqa :  41%|██████████████▍                    | 26/63 [39:49<55:24, 89.86s/it]Evaluating commonsenseqa :  43%|███████████████                    | 27/63 [41:35<56:49, 94.70s/it]Evaluating commonsenseqa :  44%|███████████████▌                   | 28/63 [43:19<56:59, 97.70s/it]Evaluating commonsenseqa :  46%|███████████████▋                  | 29/63 [45:06<56:52, 100.36s/it]Evaluating commonsenseqa :  48%|████████████████▋                  | 30/63 [45:46<45:13, 82.22s/it]Evaluating commonsenseqa :  49%|█████████████████▏                 | 31/63 [47:08<43:52, 82.25s/it]Evaluating commonsenseqa :  51%|█████████████████▊                 | 32/63 [48:54<46:06, 89.23s/it]Evaluating commonsenseqa :  52%|██████████████████▎                | 33/63 [50:40<47:13, 94.45s/it]Evaluating commonsenseqa :  54%|██████████████████▉                | 34/63 [52:25<47:07, 97.49s/it]Evaluating commonsenseqa :  56%|███████████████████▍               | 35/63 [53:32<41:12, 88.29s/it]Evaluating commonsenseqa :  57%|████████████████████               | 36/63 [55:17<41:59, 93.31s/it]Evaluating commonsenseqa :  59%|████████████████████▌              | 37/63 [56:22<36:48, 84.94s/it]Evaluating commonsenseqa :  60%|█████████████████████              | 38/63 [58:08<38:01, 91.28s/it]Evaluating commonsenseqa :  62%|█████████████████████▋             | 39/63 [59:07<32:36, 81.53s/it]Evaluating commonsenseqa :  63%|████████████████████▉            | 40/63 [1:00:52<33:59, 88.66s/it]Evaluating commonsenseqa :  65%|█████████████████████▍           | 41/63 [1:02:38<34:24, 93.83s/it]Evaluating commonsenseqa :  67%|██████████████████████           | 42/63 [1:03:31<28:33, 81.62s/it]Evaluating commonsenseqa :  68%|██████████████████████▌          | 43/63 [1:05:18<29:42, 89.15s/it]Evaluating commonsenseqa :  70%|███████████████████████          | 44/63 [1:07:02<29:37, 93.54s/it]Evaluating commonsenseqa :  71%|███████████████████████▌         | 45/63 [1:07:34<22:32, 75.14s/it]Evaluating commonsenseqa :  73%|████████████████████████         | 46/63 [1:08:12<18:05, 63.85s/it]Evaluating commonsenseqa :  75%|████████████████████████▌        | 47/63 [1:09:57<20:22, 76.41s/it]Evaluating commonsenseqa :  76%|█████████████████████████▏       | 48/63 [1:11:39<20:59, 83.98s/it]Evaluating commonsenseqa :  78%|█████████████████████████▋       | 49/63 [1:13:23<20:58, 89.89s/it]Evaluating commonsenseqa :  79%|██████████████████████████▏      | 50/63 [1:14:01<16:06, 74.38s/it]Evaluating commonsenseqa :  81%|██████████████████████████▋      | 51/63 [1:15:43<16:34, 82.88s/it]Evaluating commonsenseqa :  83%|███████████████████████████▏     | 52/63 [1:17:28<16:23, 89.41s/it]Evaluating commonsenseqa :  84%|███████████████████████████▊     | 53/63 [1:19:09<15:27, 92.75s/it]Evaluating commonsenseqa :  86%|████████████████████████████▎    | 54/63 [1:20:54<14:27, 96.40s/it]Evaluating commonsenseqa :  87%|████████████████████████████▊    | 55/63 [1:22:17<12:19, 92.47s/it]Evaluating commonsenseqa :  89%|█████████████████████████████▎   | 56/63 [1:22:55<08:53, 76.18s/it]Evaluating commonsenseqa :  90%|█████████████████████████████▊   | 57/63 [1:24:39<08:26, 84.49s/it]Evaluating commonsenseqa :  92%|██████████████████████████████▍  | 58/63 [1:26:27<07:37, 91.54s/it]Evaluating commonsenseqa :  94%|██████████████████████████████▉  | 59/63 [1:28:09<06:19, 94.83s/it]Evaluating commonsenseqa :  95%|███████████████████████████████▍ | 60/63 [1:29:59<04:57, 99.18s/it]Evaluating commonsenseqa :  97%|██████████████████████████████▉ | 61/63 [1:31:42<03:20, 100.42s/it]Evaluating commonsenseqa :  98%|███████████████████████████████▍| 62/63 [1:33:30<01:42, 102.65s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:34:08<00:00, 83.20s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:34:08<00:00, 89.65s/it]
name: commonsenseqa | avg. gen lenth: 261.936 | time: 5648.49773812294s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 29501 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 4 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o5-tgsm8k-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 5
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-27 20:06:06,745] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 20:06:06,746] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 20:06:06,776] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 20:06:06,842] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o5-tgsm8k-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 5
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o5-tgsm8k-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                       | 0/9741 [00:00<?, ?it/s]Loading data: 100%|████████████████████████████████████████| 9741/9741 [00:00<00:00, 793072.49it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:06<00:06,  6.97s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.21s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.21s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.30s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.15s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.58s/it]
[2023-08-27 20:06:17,235] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.24s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.69s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.31s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.74s/it]
 > number of parameters: 6738415616
[2023-08-27 20:06:17,473] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.30s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.75s/it]
[2023-08-27 20:06:17,570] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-27 20:06:17,647] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-27 20:06:18,231] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-27 20:06:18,232] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-27 20:06:18,232] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-27 20:06:18,232] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-27 20:06:18,232] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8bf6657610>
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-27 20:06:18,233] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-27 20:06:18,234] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-27 20:06:18,234] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                            | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: 27

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o5-tgsm8k-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▌                                | 1/63 [01:40<1:44:18, 100.94s/it]Evaluating commonsenseqa :   3%|█                                 | 2/63 [02:39<1:17:18, 76.05s/it]Evaluating commonsenseqa :   5%|█▌                                | 3/63 [04:21<1:27:58, 87.98s/it]Evaluating commonsenseqa :   6%|██▏                               | 4/63 [05:08<1:10:38, 71.84s/it]Evaluating commonsenseqa :   8%|██▋                               | 5/63 [06:49<1:19:31, 82.27s/it]Evaluating commonsenseqa :  10%|███▏                              | 6/63 [08:32<1:24:47, 89.25s/it]Evaluating commonsenseqa :  11%|███▊                              | 7/63 [10:05<1:24:31, 90.56s/it]Evaluating commonsenseqa :  13%|████▎                             | 8/63 [11:47<1:26:20, 94.20s/it]Evaluating commonsenseqa :  14%|████▊                             | 9/63 [12:48<1:15:23, 83.76s/it]Evaluating commonsenseqa :  16%|█████▏                           | 10/63 [14:23<1:16:55, 87.09s/it]Evaluating commonsenseqa :  17%|█████▊                           | 11/63 [16:06<1:19:48, 92.09s/it]Evaluating commonsenseqa :  19%|██████▎                          | 12/63 [17:45<1:20:04, 94.20s/it]Evaluating commonsenseqa :  21%|██████▊                          | 13/63 [19:27<1:20:30, 96.61s/it]Evaluating commonsenseqa :  22%|███████▎                         | 14/63 [20:59<1:17:50, 95.31s/it]Evaluating commonsenseqa :  24%|███████▊                         | 15/63 [21:29<1:00:28, 75.59s/it]Evaluating commonsenseqa :  25%|████████▍                        | 16/63 [23:12<1:05:34, 83.71s/it]Evaluating commonsenseqa :  27%|█████████▍                         | 17/63 [24:17<59:51, 78.08s/it]Evaluating commonsenseqa :  29%|█████████▍                       | 18/63 [26:00<1:04:14, 85.66s/it]Evaluating commonsenseqa :  30%|█████████▉                       | 19/63 [27:42<1:06:17, 90.41s/it]Evaluating commonsenseqa :  32%|██████████▍                      | 20/63 [29:23<1:07:09, 93.70s/it]Evaluating commonsenseqa :  33%|███████████                      | 21/63 [30:52<1:04:38, 92.35s/it]Evaluating commonsenseqa :  35%|███████████▌                     | 22/63 [32:34<1:05:07, 95.30s/it]Evaluating commonsenseqa :  37%|████████████                     | 23/63 [34:16<1:04:46, 97.15s/it]Evaluating commonsenseqa :  38%|████████████▌                    | 24/63 [35:42<1:01:02, 93.92s/it]Evaluating commonsenseqa :  40%|█████████████                    | 25/63 [37:23<1:00:42, 95.86s/it]Evaluating commonsenseqa :  41%|██████████████▍                    | 26/63 [39:03<59:57, 97.23s/it]Evaluating commonsenseqa :  43%|███████████████                    | 27/63 [40:39<58:07, 96.89s/it]Evaluating commonsenseqa :  44%|███████████████▌                   | 28/63 [42:21<57:18, 98.24s/it]Evaluating commonsenseqa :  46%|████████████████                   | 29/63 [43:53<54:44, 96.61s/it]Evaluating commonsenseqa :  48%|████████████████▋                  | 30/63 [45:02<48:35, 88.35s/it]Evaluating commonsenseqa :  49%|█████████████████▏                 | 31/63 [45:46<39:56, 74.90s/it]Evaluating commonsenseqa :  51%|█████████████████▊                 | 32/63 [46:39<35:16, 68.29s/it]Evaluating commonsenseqa :  52%|██████████████████▎                | 33/63 [48:21<39:10, 78.36s/it]Evaluating commonsenseqa :  54%|██████████████████▉                | 34/63 [50:02<41:14, 85.34s/it]Evaluating commonsenseqa :  56%|███████████████████▍               | 35/63 [51:41<41:40, 89.29s/it]Evaluating commonsenseqa :  57%|████████████████████               | 36/63 [53:21<41:36, 92.46s/it]Evaluating commonsenseqa :  59%|████████████████████▌              | 37/63 [55:00<40:59, 94.58s/it]Evaluating commonsenseqa :  60%|█████████████████████              | 38/63 [56:41<40:14, 96.58s/it]Evaluating commonsenseqa :  62%|█████████████████████▋             | 39/63 [58:22<39:06, 97.78s/it]Evaluating commonsenseqa :  63%|████████████████████▉            | 40/63 [1:00:03<37:50, 98.73s/it]Evaluating commonsenseqa :  65%|████████████████████▊           | 41/63 [1:01:46<36:43, 100.14s/it]Evaluating commonsenseqa :  67%|█████████████████████▎          | 42/63 [1:03:28<35:12, 100.60s/it]Evaluating commonsenseqa :  68%|█████████████████████▊          | 43/63 [1:05:11<33:48, 101.41s/it]Evaluating commonsenseqa :  70%|██████████████████████▎         | 44/63 [1:06:55<32:19, 102.06s/it]Evaluating commonsenseqa :  71%|██████████████████████▊         | 45/63 [1:08:38<30:42, 102.38s/it]Evaluating commonsenseqa :  73%|███████████████████████▎        | 46/63 [1:10:22<29:05, 102.70s/it]Evaluating commonsenseqa :  75%|███████████████████████▊        | 47/63 [1:12:02<27:13, 102.12s/it]Evaluating commonsenseqa :  76%|████████████████████████▍       | 48/63 [1:13:45<25:32, 102.16s/it]Evaluating commonsenseqa :  78%|█████████████████████████▋       | 49/63 [1:14:44<20:49, 89.27s/it]Evaluating commonsenseqa :  79%|██████████████████████████▏      | 50/63 [1:16:26<20:10, 93.12s/it]Evaluating commonsenseqa :  81%|██████████████████████████▋      | 51/63 [1:17:49<18:02, 90.18s/it]Evaluating commonsenseqa :  83%|███████████████████████████▏     | 52/63 [1:19:31<17:11, 93.74s/it]Evaluating commonsenseqa :  84%|███████████████████████████▊     | 53/63 [1:21:13<16:00, 96.07s/it]Evaluating commonsenseqa :  86%|████████████████████████████▎    | 54/63 [1:22:53<14:35, 97.33s/it]Evaluating commonsenseqa :  87%|████████████████████████████▊    | 55/63 [1:24:34<13:07, 98.38s/it]Evaluating commonsenseqa :  89%|█████████████████████████████▎   | 56/63 [1:25:48<10:38, 91.24s/it]Evaluating commonsenseqa :  90%|█████████████████████████████▊   | 57/63 [1:27:29<09:24, 94.04s/it]Evaluating commonsenseqa :  92%|██████████████████████████████▍  | 58/63 [1:29:12<08:03, 96.76s/it]Evaluating commonsenseqa :  94%|██████████████████████████████▉  | 59/63 [1:30:55<06:34, 98.53s/it]Evaluating commonsenseqa :  95%|███████████████████████████████▍ | 60/63 [1:32:37<04:58, 99.62s/it]Evaluating commonsenseqa :  97%|███████████████████████████████▉ | 61/63 [1:34:17<03:19, 99.81s/it]Evaluating commonsenseqa :  98%|███████████████████████████████▍| 62/63 [1:35:58<01:40, 100.25s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:36:33<00:00, 80.70s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:36:33<00:00, 91.97s/it]
name: commonsenseqa | avg. gen lenth: 270.976 | time: 5794.271151304245s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 29501 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 4 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o6-tgsm8k-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 6
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-27 21:43:50,229] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 21:43:50,236] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 21:43:50,240] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 21:43:50,291] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o6-tgsm8k-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 6
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o6-tgsm8k-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                       | 0/9741 [00:00<?, ?it/s]Loading data: 100%|████████████████████████████████████████| 9741/9741 [00:00<00:00, 738886.25it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.06s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.39s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.47s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.31s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.13s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.57s/it]
[2023-08-27 21:44:00,764] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.27s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.74s/it]
[2023-08-27 21:44:01,151] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.39s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.85s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.37s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.81s/it]
 > number of parameters: 6738415616
[2023-08-27 21:44:01,319] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-27 21:44:01,414] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-27 21:44:01,971] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-27 21:44:01,972] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-27 21:44:01,972] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-27 21:44:01,972] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-27 21:44:01,972] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-27 21:44:01,972] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2250a63640>
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-27 21:44:01,973] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-27 21:44:01,974] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-27 21:44:01,974] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                            | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: 180

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o6-tgsm8k-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▌                                 | 1/63 [01:39<1:43:13, 99.89s/it]Evaluating commonsenseqa :   3%|█                                 | 2/63 [03:18<1:40:45, 99.10s/it]Evaluating commonsenseqa :   5%|█▌                                | 3/63 [04:56<1:38:44, 98.74s/it]Evaluating commonsenseqa :   6%|██▏                               | 4/63 [06:36<1:37:35, 99.25s/it]Evaluating commonsenseqa :   8%|██▋                               | 5/63 [07:22<1:17:20, 80.02s/it]Evaluating commonsenseqa :  10%|███▏                              | 6/63 [09:01<1:22:11, 86.52s/it]Evaluating commonsenseqa :  11%|███▊                              | 7/63 [10:41<1:24:45, 90.81s/it]Evaluating commonsenseqa :  13%|████▎                             | 8/63 [11:43<1:14:50, 81.64s/it]Evaluating commonsenseqa :  14%|████▊                             | 9/63 [13:03<1:13:02, 81.15s/it]Evaluating commonsenseqa :  16%|█████▏                           | 10/63 [13:52<1:02:52, 71.19s/it]Evaluating commonsenseqa :  17%|██████                             | 11/63 [14:21<50:27, 58.21s/it]Evaluating commonsenseqa :  19%|██████▋                            | 12/63 [15:13<47:55, 56.38s/it]Evaluating commonsenseqa :  21%|███████▏                           | 13/63 [16:52<57:41, 69.23s/it]Evaluating commonsenseqa :  22%|███████▎                         | 14/63 [18:31<1:03:59, 78.35s/it]Evaluating commonsenseqa :  24%|███████▊                         | 15/63 [20:10<1:07:35, 84.49s/it]Evaluating commonsenseqa :  25%|████████▍                        | 16/63 [21:51<1:10:01, 89.40s/it]Evaluating commonsenseqa :  27%|████████▉                        | 17/63 [23:29<1:10:33, 92.03s/it]Evaluating commonsenseqa :  29%|█████████▍                       | 18/63 [25:09<1:10:44, 94.33s/it]Evaluating commonsenseqa :  30%|█████████▉                       | 19/63 [26:48<1:10:13, 95.76s/it]Evaluating commonsenseqa :  32%|██████████▍                      | 20/63 [28:28<1:09:41, 97.25s/it]Evaluating commonsenseqa :  33%|███████████                      | 21/63 [30:06<1:08:03, 97.24s/it]Evaluating commonsenseqa :  35%|███████████▌                     | 22/63 [31:46<1:07:06, 98.21s/it]Evaluating commonsenseqa :  37%|████████████                     | 23/63 [33:26<1:05:48, 98.72s/it]Evaluating commonsenseqa :  38%|█████████████▎                     | 24/63 [34:23<55:56, 86.08s/it]Evaluating commonsenseqa :  40%|█████████████▉                     | 25/63 [35:15<48:02, 75.86s/it]Evaluating commonsenseqa :  41%|██████████████▍                    | 26/63 [36:54<51:11, 83.00s/it]Evaluating commonsenseqa :  43%|███████████████                    | 27/63 [38:31<52:21, 87.28s/it]Evaluating commonsenseqa :  44%|███████████████▌                   | 28/63 [40:11<53:01, 90.91s/it]Evaluating commonsenseqa :  46%|████████████████                   | 29/63 [41:42<51:36, 91.07s/it]Evaluating commonsenseqa :  48%|████████████████▋                  | 30/63 [43:23<51:40, 93.96s/it]Evaluating commonsenseqa :  49%|█████████████████▏                 | 31/63 [45:02<50:51, 95.36s/it]Evaluating commonsenseqa :  51%|█████████████████▊                 | 32/63 [46:37<49:18, 95.43s/it]Evaluating commonsenseqa :  52%|██████████████████▎                | 33/63 [48:18<48:31, 97.05s/it]Evaluating commonsenseqa :  54%|██████████████████▉                | 34/63 [49:57<47:11, 97.63s/it]Evaluating commonsenseqa :  56%|███████████████████▍               | 35/63 [51:22<43:44, 93.75s/it]Evaluating commonsenseqa :  57%|████████████████████               | 36/63 [51:49<33:09, 73.67s/it]Evaluating commonsenseqa :  59%|████████████████████▌              | 37/63 [53:28<35:18, 81.47s/it]Evaluating commonsenseqa :  60%|█████████████████████              | 38/63 [55:06<35:58, 86.34s/it]Evaluating commonsenseqa :  62%|█████████████████████▋             | 39/63 [56:43<35:51, 89.66s/it]Evaluating commonsenseqa :  63%|██████████████████████▏            | 40/63 [58:23<35:28, 92.55s/it]Evaluating commonsenseqa :  65%|█████████████████████▍           | 41/63 [1:00:01<34:35, 94.32s/it]Evaluating commonsenseqa :  67%|██████████████████████           | 42/63 [1:01:41<33:39, 96.16s/it]Evaluating commonsenseqa :  68%|██████████████████████▌          | 43/63 [1:03:22<32:26, 97.31s/it]Evaluating commonsenseqa :  70%|███████████████████████          | 44/63 [1:05:02<31:07, 98.28s/it]Evaluating commonsenseqa :  71%|███████████████████████▌         | 45/63 [1:06:45<29:54, 99.71s/it]Evaluating commonsenseqa :  73%|███████████████████████▎        | 46/63 [1:08:28<28:31, 100.68s/it]Evaluating commonsenseqa :  75%|████████████████████████▌        | 47/63 [1:10:06<26:39, 99.98s/it]Evaluating commonsenseqa :  76%|█████████████████████████▏       | 48/63 [1:11:44<24:48, 99.25s/it]Evaluating commonsenseqa :  78%|█████████████████████████▋       | 49/63 [1:13:24<23:13, 99.57s/it]Evaluating commonsenseqa :  79%|██████████████████████████▏      | 50/63 [1:15:03<21:30, 99.30s/it]Evaluating commonsenseqa :  81%|██████████████████████████▋      | 51/63 [1:16:41<19:48, 99.07s/it]Evaluating commonsenseqa :  83%|███████████████████████████▏     | 52/63 [1:17:11<14:21, 78.30s/it]Evaluating commonsenseqa :  84%|███████████████████████████▊     | 53/63 [1:18:51<14:07, 84.74s/it]Evaluating commonsenseqa :  86%|████████████████████████████▎    | 54/63 [1:20:32<13:25, 89.51s/it]Evaluating commonsenseqa :  87%|████████████████████████████▊    | 55/63 [1:20:48<08:59, 67.42s/it]Evaluating commonsenseqa :  89%|█████████████████████████████▎   | 56/63 [1:22:28<09:00, 77.19s/it]Evaluating commonsenseqa :  90%|█████████████████████████████▊   | 57/63 [1:24:10<08:27, 84.64s/it]Evaluating commonsenseqa :  92%|██████████████████████████████▍  | 58/63 [1:25:48<07:23, 88.76s/it]Evaluating commonsenseqa :  94%|██████████████████████████████▉  | 59/63 [1:26:46<05:17, 79.41s/it]Evaluating commonsenseqa :  95%|███████████████████████████████▍ | 60/63 [1:27:20<03:18, 66.06s/it]Evaluating commonsenseqa :  97%|███████████████████████████████▉ | 61/63 [1:29:00<02:32, 76.16s/it]Evaluating commonsenseqa :  98%|████████████████████████████████▍| 62/63 [1:29:51<01:08, 68.43s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:30:30<00:00, 59.61s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:30:30<00:00, 86.19s/it]
name: commonsenseqa | avg. gen lenth: 265.996 | time: 5430.389454841614s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 29501 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 4 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o7-tgsm8k-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 7
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-27 23:21:33,523] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 23:21:33,602] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 23:21:33,613] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-27 23:21:33,630] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o7-tgsm8k-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 7
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o7-tgsm8k-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                       | 0/9741 [00:00<?, ?it/s]Loading data: 100%|████████████████████████████████████████| 9741/9741 [00:00<00:00, 723114.91it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.07s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.40s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.44s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.64s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.12s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.56s/it]
[2023-08-27 23:21:44,093] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.34s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.80s/it]
[2023-08-27 23:21:44,629] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.44s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.89s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.49s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.96s/it]
 > number of parameters: 6738415616
[2023-08-27 23:21:44,820] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-27 23:21:44,920] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-27 23:21:45,470] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-27 23:21:45,471] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-27 23:21:45,471] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-27 23:21:45,471] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8d50447520>
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-27 23:21:45,472] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-27 23:21:45,473] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-27 23:21:45,473] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                            | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: 240

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o7-tgsm8k-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▌                                 | 1/63 [01:38<1:41:29, 98.22s/it]Evaluating commonsenseqa :   3%|█                                 | 2/63 [03:16<1:39:41, 98.06s/it]Evaluating commonsenseqa :   5%|█▌                                | 3/63 [04:55<1:38:33, 98.55s/it]Evaluating commonsenseqa :   6%|██▏                               | 4/63 [06:32<1:36:24, 98.05s/it]Evaluating commonsenseqa :   8%|██▋                               | 5/63 [08:11<1:35:14, 98.53s/it]Evaluating commonsenseqa :  10%|███▏                              | 6/63 [09:49<1:33:10, 98.08s/it]Evaluating commonsenseqa :  11%|███▊                              | 7/63 [11:26<1:31:22, 97.90s/it]Evaluating commonsenseqa :  13%|████▎                             | 8/63 [12:27<1:18:53, 86.05s/it]Evaluating commonsenseqa :  14%|████▊                             | 9/63 [14:05<1:20:43, 89.69s/it]Evaluating commonsenseqa :  16%|█████▏                           | 10/63 [15:45<1:22:15, 93.13s/it]Evaluating commonsenseqa :  17%|█████▊                           | 11/63 [16:32<1:08:18, 78.82s/it]Evaluating commonsenseqa :  19%|██████▎                          | 12/63 [18:02<1:09:52, 82.20s/it]Evaluating commonsenseqa :  21%|██████▊                          | 13/63 [19:42<1:12:56, 87.53s/it]Evaluating commonsenseqa :  22%|███████▎                         | 14/63 [21:18<1:13:39, 90.20s/it]Evaluating commonsenseqa :  24%|███████▊                         | 15/63 [22:56<1:13:58, 92.46s/it]Evaluating commonsenseqa :  25%|████████▍                        | 16/63 [24:35<1:14:02, 94.52s/it]Evaluating commonsenseqa :  27%|█████████▍                         | 17/63 [25:13<59:26, 77.53s/it]Evaluating commonsenseqa :  29%|█████████▍                       | 18/63 [26:51<1:02:46, 83.70s/it]Evaluating commonsenseqa :  30%|█████████▉                       | 19/63 [28:29<1:04:29, 87.94s/it]Evaluating commonsenseqa :  32%|██████████▍                      | 20/63 [30:08<1:05:26, 91.32s/it]Evaluating commonsenseqa :  33%|███████████                      | 21/63 [31:46<1:05:13, 93.19s/it]Evaluating commonsenseqa :  35%|███████████▌                     | 22/63 [33:24<1:04:41, 94.66s/it]Evaluating commonsenseqa :  37%|████████████                     | 23/63 [35:02<1:03:49, 95.74s/it]Evaluating commonsenseqa :  38%|████████████▌                    | 24/63 [36:41<1:02:56, 96.82s/it]Evaluating commonsenseqa :  40%|█████████████                    | 25/63 [38:18<1:01:17, 96.78s/it]Evaluating commonsenseqa :  41%|██████████████▍                    | 26/63 [38:54<48:32, 78.71s/it]Evaluating commonsenseqa :  43%|███████████████                    | 27/63 [39:46<42:15, 70.44s/it]Evaluating commonsenseqa :  44%|███████████████▌                   | 28/63 [41:25<46:04, 78.97s/it]Evaluating commonsenseqa :  46%|████████████████                   | 29/63 [42:40<44:14, 78.06s/it]Evaluating commonsenseqa :  48%|████████████████▋                  | 30/63 [44:21<46:42, 84.94s/it]Evaluating commonsenseqa :  49%|█████████████████▏                 | 31/63 [45:59<47:14, 88.58s/it]Evaluating commonsenseqa :  51%|█████████████████▊                 | 32/63 [47:39<47:34, 92.09s/it]Evaluating commonsenseqa :  52%|██████████████████▎                | 33/63 [49:17<46:59, 93.99s/it]Evaluating commonsenseqa :  54%|██████████████████▉                | 34/63 [50:57<46:18, 95.82s/it]Evaluating commonsenseqa :  56%|███████████████████▍               | 35/63 [52:35<44:59, 96.40s/it]Evaluating commonsenseqa :  57%|████████████████████               | 36/63 [54:12<43:29, 96.66s/it]Evaluating commonsenseqa :  59%|████████████████████▌              | 37/63 [55:51<42:05, 97.14s/it]Evaluating commonsenseqa :  60%|█████████████████████              | 38/63 [56:45<35:06, 84.25s/it]Evaluating commonsenseqa :  62%|█████████████████████▋             | 39/63 [58:22<35:17, 88.23s/it]Evaluating commonsenseqa :  63%|████████████████████▉            | 40/63 [1:00:02<35:09, 91.74s/it]Evaluating commonsenseqa :  65%|█████████████████████▍           | 41/63 [1:01:43<34:39, 94.52s/it]Evaluating commonsenseqa :  67%|██████████████████████           | 42/63 [1:03:20<33:19, 95.23s/it]Evaluating commonsenseqa :  68%|██████████████████████▌          | 43/63 [1:05:00<32:13, 96.70s/it]Evaluating commonsenseqa :  70%|███████████████████████          | 44/63 [1:06:38<30:41, 96.91s/it]Evaluating commonsenseqa :  71%|███████████████████████▌         | 45/63 [1:08:17<29:16, 97.59s/it]Evaluating commonsenseqa :  73%|████████████████████████         | 46/63 [1:09:56<27:48, 98.15s/it]Evaluating commonsenseqa :  75%|████████████████████████▌        | 47/63 [1:11:36<26:16, 98.51s/it]Evaluating commonsenseqa :  76%|█████████████████████████▏       | 48/63 [1:13:14<24:38, 98.54s/it]Evaluating commonsenseqa :  78%|█████████████████████████▋       | 49/63 [1:14:54<23:04, 98.88s/it]Evaluating commonsenseqa :  79%|██████████████████████████▏      | 50/63 [1:16:34<21:30, 99.23s/it]Evaluating commonsenseqa :  81%|██████████████████████████▋      | 51/63 [1:18:11<19:44, 98.67s/it]Evaluating commonsenseqa :  83%|███████████████████████████▏     | 52/63 [1:19:48<18:00, 98.19s/it]Evaluating commonsenseqa :  84%|███████████████████████████▊     | 53/63 [1:21:27<16:23, 98.35s/it]Evaluating commonsenseqa :  86%|████████████████████████████▎    | 54/63 [1:23:05<14:43, 98.18s/it]Evaluating commonsenseqa :  87%|████████████████████████████▊    | 55/63 [1:24:20<12:10, 91.31s/it]Evaluating commonsenseqa :  89%|█████████████████████████████▎   | 56/63 [1:25:57<10:51, 93.05s/it]Evaluating commonsenseqa :  90%|█████████████████████████████▊   | 57/63 [1:27:36<09:28, 94.81s/it]Evaluating commonsenseqa :  92%|██████████████████████████████▍  | 58/63 [1:29:13<07:57, 95.51s/it]Evaluating commonsenseqa :  94%|██████████████████████████████▉  | 59/63 [1:30:52<06:26, 96.52s/it]Evaluating commonsenseqa :  95%|███████████████████████████████▍ | 60/63 [1:32:31<04:51, 97.12s/it]Evaluating commonsenseqa :  97%|███████████████████████████████▉ | 61/63 [1:34:08<03:14, 97.29s/it]Evaluating commonsenseqa :  98%|████████████████████████████████▍| 62/63 [1:35:46<01:37, 97.49s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:36:42<00:00, 84.86s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:36:42<00:00, 92.10s/it]
name: commonsenseqa | avg. gen lenth: 272.5 | time: 5802.544800758362s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 29501 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 4 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o8-tgsm8k-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 8
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 00:58:36,040] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 00:58:36,572] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 00:58:36,578] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 00:58:36,601] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o8-tgsm8k-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 8
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o8-tgsm8k-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                       | 0/9741 [00:00<?, ?it/s]Loading data: 100%|████████████████████████████████████████| 9741/9741 [00:00<00:00, 678987.51it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.04s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.06s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.50s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.56s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.08s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.52s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.08s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.52s/it]
[2023-08-28 00:58:46,933] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 00:58:46,962] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.38s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.85s/it]
[2023-08-28 00:58:47,585] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.40s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.87s/it]
 > number of parameters: 6738415616
[2023-08-28 00:58:47,718] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 00:58:48,358] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-28 00:58:48,360] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f00d3e97310>
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-28 00:58:48,360] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-28 00:58:48,361] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-28 00:58:48,362] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                            | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: 240

Input: Sarah, Mary, and Tuan decided to go to the restaurant for a meal. They decided to split the cost of the meal evenly. If the total price of the meal comes to $67 and they have a coupon for $4, how much does each person need to contribute to the bill?
Output: 21

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o8-tgsm8k-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▌                                 | 1/63 [01:36<1:39:55, 96.69s/it]Evaluating commonsenseqa :   3%|█                                 | 2/63 [03:14<1:39:06, 97.49s/it]Evaluating commonsenseqa :   5%|█▌                                | 3/63 [03:59<1:13:20, 73.33s/it]Evaluating commonsenseqa :   6%|██▏                               | 4/63 [05:21<1:15:40, 76.96s/it]Evaluating commonsenseqa :   8%|██▋                               | 5/63 [06:58<1:21:25, 84.24s/it]Evaluating commonsenseqa :  10%|███▏                              | 6/63 [08:36<1:24:19, 88.76s/it]Evaluating commonsenseqa :  11%|███▊                              | 7/63 [10:04<1:22:34, 88.47s/it]Evaluating commonsenseqa :  13%|████▎                             | 8/63 [10:47<1:07:57, 74.13s/it]Evaluating commonsenseqa :  14%|████▊                             | 9/63 [12:23<1:12:46, 80.86s/it]Evaluating commonsenseqa :  16%|█████▏                           | 10/63 [13:22<1:05:30, 74.16s/it]Evaluating commonsenseqa :  17%|█████▊                           | 11/63 [15:00<1:10:25, 81.27s/it]Evaluating commonsenseqa :  19%|██████▎                          | 12/63 [16:35<1:12:45, 85.60s/it]Evaluating commonsenseqa :  21%|██████▊                          | 13/63 [18:10<1:13:40, 88.42s/it]Evaluating commonsenseqa :  22%|███████▎                         | 14/63 [19:40<1:12:39, 88.96s/it]Evaluating commonsenseqa :  24%|███████▊                         | 15/63 [21:14<1:12:16, 90.34s/it]Evaluating commonsenseqa :  25%|████████▍                        | 16/63 [22:51<1:12:22, 92.39s/it]Evaluating commonsenseqa :  27%|████████▉                        | 17/63 [24:26<1:11:32, 93.31s/it]Evaluating commonsenseqa :  29%|█████████▍                       | 18/63 [25:37<1:04:59, 86.66s/it]Evaluating commonsenseqa :  30%|█████████▉                       | 19/63 [27:13<1:05:30, 89.32s/it]Evaluating commonsenseqa :  32%|██████████▍                      | 20/63 [28:48<1:05:08, 90.88s/it]Evaluating commonsenseqa :  33%|███████████                      | 21/63 [30:25<1:04:55, 92.76s/it]Evaluating commonsenseqa :  35%|███████████▌                     | 22/63 [32:00<1:03:58, 93.61s/it]Evaluating commonsenseqa :  37%|████████████                     | 23/63 [33:36<1:02:52, 94.30s/it]Evaluating commonsenseqa :  38%|████████████▌                    | 24/63 [35:12<1:01:37, 94.81s/it]Evaluating commonsenseqa :  40%|█████████████                    | 25/63 [36:50<1:00:34, 95.63s/it]Evaluating commonsenseqa :  41%|██████████████▍                    | 26/63 [38:23<58:33, 94.95s/it]Evaluating commonsenseqa :  43%|███████████████                    | 27/63 [40:00<57:23, 95.65s/it]Evaluating commonsenseqa :  44%|███████████████▌                   | 28/63 [41:11<51:25, 88.16s/it]Evaluating commonsenseqa :  46%|████████████████                   | 29/63 [42:46<51:07, 90.21s/it]Evaluating commonsenseqa :  48%|████████████████▋                  | 30/63 [44:21<50:25, 91.68s/it]Evaluating commonsenseqa :  49%|█████████████████▏                 | 31/63 [45:57<49:36, 93.02s/it]Evaluating commonsenseqa :  51%|█████████████████▊                 | 32/63 [47:33<48:30, 93.90s/it]Evaluating commonsenseqa :  52%|██████████████████▎                | 33/63 [48:14<38:55, 77.85s/it]Evaluating commonsenseqa :  54%|██████████████████▉                | 34/63 [49:49<40:09, 83.08s/it]Evaluating commonsenseqa :  56%|███████████████████▍               | 35/63 [50:35<33:33, 71.91s/it]Evaluating commonsenseqa :  57%|████████████████████               | 36/63 [52:11<35:38, 79.22s/it]Evaluating commonsenseqa :  59%|████████████████████▌              | 37/63 [53:49<36:48, 84.94s/it]Evaluating commonsenseqa :  60%|█████████████████████              | 38/63 [55:26<36:53, 88.52s/it]Evaluating commonsenseqa :  62%|█████████████████████▋             | 39/63 [57:00<36:01, 90.08s/it]Evaluating commonsenseqa :  63%|██████████████████████▏            | 40/63 [58:13<32:34, 84.97s/it]Evaluating commonsenseqa :  65%|██████████████████████▊            | 41/63 [59:38<31:08, 84.92s/it]Evaluating commonsenseqa :  67%|██████████████████████           | 42/63 [1:01:15<31:00, 88.62s/it]Evaluating commonsenseqa :  68%|██████████████████████▌          | 43/63 [1:02:48<29:58, 89.93s/it]Evaluating commonsenseqa :  70%|███████████████████████          | 44/63 [1:04:22<28:54, 91.27s/it]Evaluating commonsenseqa :  71%|███████████████████████▌         | 45/63 [1:05:56<27:33, 91.87s/it]Evaluating commonsenseqa :  73%|████████████████████████         | 46/63 [1:07:12<24:42, 87.23s/it]Evaluating commonsenseqa :  75%|████████████████████████▌        | 47/63 [1:08:48<23:58, 89.93s/it]Evaluating commonsenseqa :  76%|█████████████████████████▏       | 48/63 [1:10:22<22:48, 91.20s/it]Evaluating commonsenseqa :  78%|█████████████████████████▋       | 49/63 [1:11:18<18:45, 80.41s/it]Evaluating commonsenseqa :  79%|██████████████████████████▏      | 50/63 [1:12:53<18:22, 84.77s/it]Evaluating commonsenseqa :  81%|██████████████████████████▋      | 51/63 [1:14:29<17:39, 88.26s/it]Evaluating commonsenseqa :  83%|███████████████████████████▏     | 52/63 [1:16:04<16:31, 90.13s/it]Evaluating commonsenseqa :  84%|███████████████████████████▊     | 53/63 [1:17:38<15:13, 91.30s/it]Evaluating commonsenseqa :  86%|████████████████████████████▎    | 54/63 [1:19:14<13:56, 92.93s/it]Evaluating commonsenseqa :  87%|████████████████████████████▊    | 55/63 [1:20:50<12:30, 93.77s/it]Evaluating commonsenseqa :  89%|█████████████████████████████▎   | 56/63 [1:22:28<11:04, 94.92s/it]Evaluating commonsenseqa :  90%|█████████████████████████████▊   | 57/63 [1:24:04<09:32, 95.40s/it]Evaluating commonsenseqa :  92%|██████████████████████████████▍  | 58/63 [1:25:41<07:58, 95.78s/it]Evaluating commonsenseqa :  94%|██████████████████████████████▉  | 59/63 [1:27:15<06:21, 95.27s/it]Evaluating commonsenseqa :  95%|███████████████████████████████▍ | 60/63 [1:27:51<03:52, 77.64s/it]Evaluating commonsenseqa :  97%|███████████████████████████████▉ | 61/63 [1:29:21<02:42, 81.31s/it]Evaluating commonsenseqa :  98%|████████████████████████████████▍| 62/63 [1:30:25<01:15, 75.99s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:31:04<00:00, 65.00s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:31:04<00:00, 86.74s/it]
name: commonsenseqa | avg. gen lenth: 268.844 | time: 5465.021106719971s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 29501 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 4 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o9-tgsm8k-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 9
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 02:32:54,755] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 02:32:54,766] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 02:32:54,783] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 02:32:54,799] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o9-tgsm8k-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 9
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o9-tgsm8k-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                       | 0/9741 [00:00<?, ?it/s]Loading data: 100%|████████████████████████████████████████| 9741/9741 [00:00<00:00, 648632.54it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.42s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.48s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.50s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:09<00:09,  9.05s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.36s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.82s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.40s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.87s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.43s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.89s/it]
[2023-08-28 02:33:05,810] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-28 02:33:05,890] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 02:33:05,912] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:11<00:00,  4.93s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:11<00:00,  5.54s/it]
[2023-08-28 02:33:07,229] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 02:33:07,859] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-28 02:33:07,860] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-28 02:33:07,861] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-28 02:33:07,861] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-28 02:33:07,861] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-28 02:33:07,861] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-28 02:33:07,861] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-28 02:33:07,861] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-28 02:33:07,861] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-28 02:33:07,861] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-28 02:33:07,861] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-28 02:33:07,861] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fcb91b5b1f0>
[2023-08-28 02:33:07,861] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-28 02:33:07,861] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-28 02:33:07,862] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-28 02:33:07,863] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-28 02:33:07,863] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                            | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: 240

Input: Sarah, Mary, and Tuan decided to go to the restaurant for a meal. They decided to split the cost of the meal evenly. If the total price of the meal comes to $67 and they have a coupon for $4, how much does each person need to contribute to the bill?
Output: 21

Input: Tom's brother is 4 times as old as Tom's dog. If in 6 years, Tom's brother will be 30 years, how old is Tom's dog going to be in six years?
Output: 12

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o9-tgsm8k-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▌                                 | 1/63 [01:34<1:37:32, 94.39s/it]Evaluating commonsenseqa :   3%|█                                 | 2/63 [03:10<1:36:47, 95.20s/it]Evaluating commonsenseqa :   5%|█▌                                | 3/63 [04:43<1:34:30, 94.50s/it]Evaluating commonsenseqa :   6%|██▏                               | 4/63 [06:19<1:33:12, 94.79s/it]Evaluating commonsenseqa :   8%|██▋                               | 5/63 [07:54<1:31:59, 95.16s/it]Evaluating commonsenseqa :  10%|███▏                              | 6/63 [09:28<1:30:03, 94.80s/it]Evaluating commonsenseqa :  11%|███▊                              | 7/63 [10:21<1:15:26, 80.83s/it]Evaluating commonsenseqa :  13%|████▎                             | 8/63 [11:55<1:18:11, 85.30s/it]Evaluating commonsenseqa :  14%|████▊                             | 9/63 [13:29<1:19:11, 88.00s/it]Evaluating commonsenseqa :  16%|█████▏                           | 10/63 [15:05<1:19:48, 90.35s/it]Evaluating commonsenseqa :  17%|█████▊                           | 11/63 [16:40<1:19:31, 91.76s/it]Evaluating commonsenseqa :  19%|██████▎                          | 12/63 [17:46<1:11:22, 83.98s/it]Evaluating commonsenseqa :  21%|██████▊                          | 13/63 [19:20<1:12:24, 86.89s/it]Evaluating commonsenseqa :  22%|███████▎                         | 14/63 [20:55<1:13:05, 89.51s/it]Evaluating commonsenseqa :  24%|███████▊                         | 15/63 [22:31<1:13:01, 91.29s/it]Evaluating commonsenseqa :  25%|████████▍                        | 16/63 [24:07<1:12:39, 92.75s/it]Evaluating commonsenseqa :  27%|████████▉                        | 17/63 [25:40<1:11:07, 92.78s/it]Evaluating commonsenseqa :  29%|█████████▍                       | 18/63 [27:15<1:10:08, 93.52s/it]Evaluating commonsenseqa :  30%|█████████▉                       | 19/63 [28:48<1:08:25, 93.32s/it]Evaluating commonsenseqa :  32%|██████████▍                      | 20/63 [30:23<1:07:13, 93.79s/it]Evaluating commonsenseqa :  33%|███████████                      | 21/63 [31:56<1:05:38, 93.77s/it]Evaluating commonsenseqa :  35%|███████████▌                     | 22/63 [33:19<1:01:52, 90.55s/it]Evaluating commonsenseqa :  37%|████████████                     | 23/63 [34:56<1:01:28, 92.22s/it]Evaluating commonsenseqa :  38%|████████████▌                    | 24/63 [36:31<1:00:34, 93.19s/it]Evaluating commonsenseqa :  40%|█████████████▉                     | 25/63 [37:50<56:18, 88.91s/it]Evaluating commonsenseqa :  41%|██████████████▍                    | 26/63 [39:25<55:56, 90.71s/it]Evaluating commonsenseqa :  43%|███████████████                    | 27/63 [40:59<55:01, 91.72s/it]Evaluating commonsenseqa :  44%|███████████████▌                   | 28/63 [42:10<49:48, 85.39s/it]Evaluating commonsenseqa :  46%|████████████████                   | 29/63 [42:54<41:30, 73.25s/it]Evaluating commonsenseqa :  48%|████████████████▋                  | 30/63 [44:27<43:30, 79.10s/it]Evaluating commonsenseqa :  49%|█████████████████▏                 | 31/63 [45:59<44:10, 82.82s/it]Evaluating commonsenseqa :  51%|█████████████████▊                 | 32/63 [46:49<37:41, 72.95s/it]Evaluating commonsenseqa :  52%|██████████████████▎                | 33/63 [48:22<39:34, 79.14s/it]Evaluating commonsenseqa :  54%|██████████████████▉                | 34/63 [49:57<40:29, 83.76s/it]Evaluating commonsenseqa :  56%|███████████████████▍               | 35/63 [51:15<38:22, 82.23s/it]Evaluating commonsenseqa :  57%|████████████████████               | 36/63 [52:51<38:50, 86.30s/it]Evaluating commonsenseqa :  59%|████████████████████▌              | 37/63 [54:25<38:25, 88.67s/it]Evaluating commonsenseqa :  60%|█████████████████████              | 38/63 [55:57<37:19, 89.60s/it]Evaluating commonsenseqa :  62%|█████████████████████▋             | 39/63 [57:32<36:25, 91.08s/it]Evaluating commonsenseqa :  63%|██████████████████████▏            | 40/63 [59:06<35:14, 91.93s/it]Evaluating commonsenseqa :  65%|█████████████████████▍           | 41/63 [1:00:22<31:57, 87.17s/it]Evaluating commonsenseqa :  67%|██████████████████████           | 42/63 [1:01:56<31:18, 89.47s/it]Evaluating commonsenseqa :  68%|██████████████████████▌          | 43/63 [1:03:31<30:21, 91.09s/it]Evaluating commonsenseqa :  70%|███████████████████████          | 44/63 [1:05:05<29:06, 91.90s/it]Evaluating commonsenseqa :  71%|███████████████████████▌         | 45/63 [1:06:38<27:41, 92.32s/it]Evaluating commonsenseqa :  73%|████████████████████████         | 46/63 [1:07:16<21:32, 76.01s/it]Evaluating commonsenseqa :  75%|████████████████████████▌        | 47/63 [1:08:52<21:51, 81.95s/it]Evaluating commonsenseqa :  76%|█████████████████████████▏       | 48/63 [1:10:02<19:35, 78.38s/it]Evaluating commonsenseqa :  78%|█████████████████████████▋       | 49/63 [1:11:38<19:29, 83.51s/it]Evaluating commonsenseqa :  79%|██████████████████████████▏      | 50/63 [1:13:09<18:36, 85.88s/it]Evaluating commonsenseqa :  81%|██████████████████████████▋      | 51/63 [1:14:43<17:37, 88.16s/it]Evaluating commonsenseqa :  83%|███████████████████████████▏     | 52/63 [1:15:37<14:17, 77.98s/it]Evaluating commonsenseqa :  84%|███████████████████████████▊     | 53/63 [1:17:10<13:45, 82.55s/it]Evaluating commonsenseqa :  86%|████████████████████████████▎    | 54/63 [1:18:42<12:48, 85.43s/it]Evaluating commonsenseqa :  87%|████████████████████████████▊    | 55/63 [1:20:18<11:48, 88.59s/it]Evaluating commonsenseqa :  89%|█████████████████████████████▎   | 56/63 [1:21:51<10:29, 89.95s/it]Evaluating commonsenseqa :  90%|█████████████████████████████▊   | 57/63 [1:23:27<09:10, 91.82s/it]Evaluating commonsenseqa :  92%|██████████████████████████████▍  | 58/63 [1:24:11<06:26, 77.37s/it]Evaluating commonsenseqa :  94%|██████████████████████████████▉  | 59/63 [1:25:44<05:28, 82.14s/it]Evaluating commonsenseqa :  95%|███████████████████████████████▍ | 60/63 [1:27:19<04:17, 85.83s/it]Evaluating commonsenseqa :  97%|███████████████████████████████▉ | 61/63 [1:28:53<02:56, 88.41s/it]Evaluating commonsenseqa :  98%|████████████████████████████████▍| 62/63 [1:30:27<01:29, 89.92s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:31:09<00:00, 75.68s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:31:09<00:00, 86.82s/it]
name: commonsenseqa | avg. gen lenth: 274.584 | time: 5469.996410608292s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 29501 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 4 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o10-tgsm8k-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 10
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 04:04:26,967] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 04:04:27,324] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 04:04:27,469] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 04:04:27,510] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o10-tgsm8k-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 10
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o10-tgsm8k-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                       | 0/9741 [00:00<?, ?it/s]Loading data: 100%|████████████████████████████████████████| 9741/9741 [00:00<00:00, 626857.87it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.06s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.49s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.17s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.40s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.32s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.73s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.21s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.66s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.42s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.88s/it]
 > number of parameters: 6738415616
[2023-08-28 04:04:38,851] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.36s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.82s/it]
[2023-08-28 04:04:38,901] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 04:04:38,913] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 04:04:38,975] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 04:04:39,526] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-28 04:04:39,527] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-28 04:04:39,527] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-28 04:04:39,527] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-28 04:04:39,527] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-28 04:04:39,527] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f366bfcb2e0>
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-28 04:04:39,528] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-28 04:04:39,529] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-28 04:04:39,529] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                            | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: 240

Input: Sarah, Mary, and Tuan decided to go to the restaurant for a meal. They decided to split the cost of the meal evenly. If the total price of the meal comes to $67 and they have a coupon for $4, how much does each person need to contribute to the bill?
Output: 21

Input: Tom's brother is 4 times as old as Tom's dog. If in 6 years, Tom's brother will be 30 years, how old is Tom's dog going to be in six years?
Output: 12

Input: There are 50 children at the party. Three-fifths of them are boys. How many of the children are girls?
Output: 20

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o10-tgsm8k-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▌                                 | 1/63 [01:33<1:37:01, 93.90s/it]Evaluating commonsenseqa :   3%|█                                 | 2/63 [03:07<1:35:10, 93.62s/it]Evaluating commonsenseqa :   5%|█▌                                | 3/63 [04:40<1:33:16, 93.27s/it]Evaluating commonsenseqa :   6%|██▏                               | 4/63 [06:13<1:31:51, 93.41s/it]Evaluating commonsenseqa :   8%|██▋                               | 5/63 [07:47<1:30:30, 93.62s/it]Evaluating commonsenseqa :  10%|███▏                              | 6/63 [09:19<1:28:25, 93.08s/it]Evaluating commonsenseqa :  11%|███▊                              | 7/63 [10:52<1:26:44, 92.93s/it]Evaluating commonsenseqa :  13%|████▎                             | 8/63 [12:27<1:25:40, 93.47s/it]Evaluating commonsenseqa :  14%|████▊                             | 9/63 [14:00<1:24:13, 93.57s/it]Evaluating commonsenseqa :  16%|█████▏                           | 10/63 [15:34<1:22:34, 93.47s/it]Evaluating commonsenseqa :  17%|█████▊                           | 11/63 [17:07<1:20:56, 93.40s/it]Evaluating commonsenseqa :  19%|██████▎                          | 12/63 [18:39<1:19:09, 93.13s/it]Evaluating commonsenseqa :  21%|██████▊                          | 13/63 [19:33<1:07:41, 81.23s/it]Evaluating commonsenseqa :  22%|███████▎                         | 14/63 [21:06<1:09:04, 84.57s/it]Evaluating commonsenseqa :  24%|███████▊                         | 15/63 [22:38<1:09:37, 87.03s/it]Evaluating commonsenseqa :  25%|████████▍                        | 16/63 [24:12<1:09:46, 89.07s/it]Evaluating commonsenseqa :  27%|████████▉                        | 17/63 [25:48<1:09:46, 91.02s/it]Evaluating commonsenseqa :  29%|█████████▍                       | 18/63 [27:20<1:08:39, 91.54s/it]Evaluating commonsenseqa :  30%|█████████▉                       | 19/63 [28:54<1:07:32, 92.11s/it]Evaluating commonsenseqa :  32%|██████████▍                      | 20/63 [30:27<1:06:19, 92.55s/it]Evaluating commonsenseqa :  33%|███████████                      | 21/63 [32:01<1:04:56, 92.77s/it]Evaluating commonsenseqa :  35%|████████████▏                      | 22/63 [32:23<48:53, 71.54s/it]Evaluating commonsenseqa :  37%|████████████▊                      | 23/63 [33:55<51:53, 77.85s/it]Evaluating commonsenseqa :  38%|█████████████▎                     | 24/63 [35:14<50:42, 78.00s/it]Evaluating commonsenseqa :  40%|█████████████▉                     | 25/63 [36:47<52:18, 82.60s/it]Evaluating commonsenseqa :  41%|██████████████▍                    | 26/63 [38:18<52:32, 85.21s/it]Evaluating commonsenseqa :  43%|███████████████                    | 27/63 [39:51<52:27, 87.42s/it]Evaluating commonsenseqa :  44%|███████████████▌                   | 28/63 [41:16<50:38, 86.83s/it]Evaluating commonsenseqa :  46%|████████████████                   | 29/63 [42:49<50:09, 88.51s/it]Evaluating commonsenseqa :  48%|████████████████▋                  | 30/63 [44:22<49:25, 89.85s/it]Evaluating commonsenseqa :  49%|█████████████████▏                 | 31/63 [45:11<41:26, 77.71s/it]Evaluating commonsenseqa :  51%|█████████████████▊                 | 32/63 [46:43<42:24, 82.09s/it]Evaluating commonsenseqa :  52%|██████████████████▎                | 33/63 [48:15<42:32, 85.07s/it]Evaluating commonsenseqa :  54%|██████████████████▉                | 34/63 [49:51<42:41, 88.34s/it]Evaluating commonsenseqa :  56%|███████████████████▍               | 35/63 [50:47<36:42, 78.67s/it]Evaluating commonsenseqa :  57%|████████████████████               | 36/63 [52:19<37:10, 82.62s/it]Evaluating commonsenseqa :  59%|████████████████████▌              | 37/63 [53:52<37:04, 85.55s/it]Evaluating commonsenseqa :  60%|█████████████████████              | 38/63 [55:24<36:32, 87.71s/it]Evaluating commonsenseqa :  62%|█████████████████████▋             | 39/63 [56:58<35:46, 89.45s/it]Evaluating commonsenseqa :  63%|██████████████████████▏            | 40/63 [58:32<34:45, 90.68s/it]Evaluating commonsenseqa :  65%|██████████████████████▊            | 41/63 [59:40<30:49, 84.05s/it]Evaluating commonsenseqa :  67%|██████████████████████           | 42/63 [1:01:13<30:17, 86.56s/it]Evaluating commonsenseqa :  68%|██████████████████████▌          | 43/63 [1:02:45<29:28, 88.42s/it]Evaluating commonsenseqa :  70%|███████████████████████          | 44/63 [1:03:57<26:24, 83.42s/it]Evaluating commonsenseqa :  71%|███████████████████████▌         | 45/63 [1:05:29<25:46, 85.92s/it]Evaluating commonsenseqa :  73%|████████████████████████         | 46/63 [1:07:03<25:00, 88.27s/it]Evaluating commonsenseqa :  75%|████████████████████████▌        | 47/63 [1:08:38<24:05, 90.32s/it]Evaluating commonsenseqa :  76%|█████████████████████████▏       | 48/63 [1:10:09<22:38, 90.55s/it]Evaluating commonsenseqa :  78%|█████████████████████████▋       | 49/63 [1:11:23<20:01, 85.79s/it]Evaluating commonsenseqa :  79%|██████████████████████████▏      | 50/63 [1:12:55<18:57, 87.54s/it]Evaluating commonsenseqa :  81%|██████████████████████████▋      | 51/63 [1:14:23<17:31, 87.63s/it]Evaluating commonsenseqa :  83%|███████████████████████████▏     | 52/63 [1:15:49<15:57, 87.08s/it]Evaluating commonsenseqa :  84%|███████████████████████████▊     | 53/63 [1:17:23<14:52, 89.28s/it]Evaluating commonsenseqa :  86%|████████████████████████████▎    | 54/63 [1:18:55<13:31, 90.21s/it]Evaluating commonsenseqa :  87%|████████████████████████████▊    | 55/63 [1:20:07<11:17, 84.67s/it]Evaluating commonsenseqa :  89%|█████████████████████████████▎   | 56/63 [1:21:42<10:14, 87.84s/it]Evaluating commonsenseqa :  90%|█████████████████████████████▊   | 57/63 [1:23:03<08:33, 85.54s/it]Evaluating commonsenseqa :  92%|██████████████████████████████▍  | 58/63 [1:24:08<06:38, 79.63s/it]Evaluating commonsenseqa :  94%|██████████████████████████████▉  | 59/63 [1:24:51<04:33, 68.39s/it]Evaluating commonsenseqa :  95%|███████████████████████████████▍ | 60/63 [1:26:27<03:50, 76.74s/it]Evaluating commonsenseqa :  97%|███████████████████████████████▉ | 61/63 [1:28:01<02:43, 81.86s/it]Evaluating commonsenseqa :  98%|████████████████████████████████▍| 62/63 [1:29:32<01:24, 84.60s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:30:24<00:00, 74.78s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:30:24<00:00, 86.10s/it]
name: commonsenseqa | avg. gen lenth: 283.728 | time: 5424.347620248795s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 29501 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 4 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o11-tgsm8k-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 11
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 05:35:10,364] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 05:35:10,410] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 05:35:10,430] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 05:35:10,435] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o11-tgsm8k-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 11
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o11-tgsm8k-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                       | 0/9741 [00:00<?, ?it/s]Loading data: 100%|████████████████████████████████████████| 9741/9741 [00:00<00:00, 610987.22it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.07s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.13s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.23s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.26s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.71s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.28s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.70s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.30s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.73s/it]
[2023-08-28 05:35:21,166] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 05:35:21,199] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
 > number of parameters: 6738415616
[2023-08-28 05:35:21,206] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:10<00:10, 10.03s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:12<00:00,  5.42s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:12<00:00,  6.11s/it]
[2023-08-28 05:35:23,961] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 05:35:24,728] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-28 05:35:24,730] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-28 05:35:24,730] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-28 05:35:24,730] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-28 05:35:24,730] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-28 05:35:24,730] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ffae1e47640>
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-28 05:35:24,731] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-28 05:35:24,732] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-28 05:35:24,732] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                            | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: 240

Input: Sarah, Mary, and Tuan decided to go to the restaurant for a meal. They decided to split the cost of the meal evenly. If the total price of the meal comes to $67 and they have a coupon for $4, how much does each person need to contribute to the bill?
Output: 21

Input: Tom's brother is 4 times as old as Tom's dog. If in 6 years, Tom's brother will be 30 years, how old is Tom's dog going to be in six years?
Output: 12

Input: There are 50 children at the party. Three-fifths of them are boys. How many of the children are girls?
Output: 20

Input: Gail has some bills in her wallet which amount to $100. She has four $5 bills and three $20 bills, and the rest are $10 bills. How many $10 bills are in her wallet?
Output: 2

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o11-tgsm8k-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▌                                 | 1/63 [01:31<1:34:04, 91.04s/it]Evaluating commonsenseqa :   3%|█                                 | 2/63 [03:02<1:32:43, 91.20s/it]Evaluating commonsenseqa :   5%|█▌                                | 3/63 [04:33<1:31:07, 91.13s/it]Evaluating commonsenseqa :   6%|██▏                               | 4/63 [05:54<1:25:35, 87.04s/it]Evaluating commonsenseqa :   8%|██▋                               | 5/63 [07:26<1:25:57, 88.92s/it]Evaluating commonsenseqa :  10%|███▏                              | 6/63 [08:41<1:20:07, 84.35s/it]Evaluating commonsenseqa :  11%|███▊                              | 7/63 [10:11<1:20:13, 85.95s/it]Evaluating commonsenseqa :  13%|████▎                             | 8/63 [11:43<1:20:41, 88.03s/it]Evaluating commonsenseqa :  14%|████▊                             | 9/63 [13:16<1:20:31, 89.48s/it]Evaluating commonsenseqa :  16%|█████▏                           | 10/63 [14:48<1:19:47, 90.33s/it]Evaluating commonsenseqa :  17%|█████▊                           | 11/63 [16:22<1:19:15, 91.45s/it]Evaluating commonsenseqa :  19%|██████▎                          | 12/63 [17:53<1:17:36, 91.30s/it]Evaluating commonsenseqa :  21%|██████▊                          | 13/63 [19:23<1:15:52, 91.05s/it]Evaluating commonsenseqa :  22%|███████▊                           | 14/63 [19:47<57:48, 70.78s/it]Evaluating commonsenseqa :  24%|████████▎                          | 15/63 [21:01<57:17, 71.61s/it]Evaluating commonsenseqa :  25%|████████▍                        | 16/63 [22:32<1:00:39, 77.44s/it]Evaluating commonsenseqa :  27%|█████████▍                         | 17/63 [23:38<56:45, 74.02s/it]Evaluating commonsenseqa :  29%|██████████                         | 18/63 [25:11<59:41, 79.58s/it]Evaluating commonsenseqa :  30%|█████████▉                       | 19/63 [26:43<1:01:09, 83.39s/it]Evaluating commonsenseqa :  32%|██████████▍                      | 20/63 [28:12<1:01:03, 85.20s/it]Evaluating commonsenseqa :  33%|███████████                      | 21/63 [29:46<1:01:30, 87.86s/it]Evaluating commonsenseqa :  35%|████████████▏                      | 22/63 [31:13<59:47, 87.51s/it]Evaluating commonsenseqa :  37%|████████████▊                      | 23/63 [32:44<59:00, 88.52s/it]Evaluating commonsenseqa :  38%|█████████████▎                     | 24/63 [33:33<49:50, 76.68s/it]Evaluating commonsenseqa :  40%|█████████████▉                     | 25/63 [35:06<51:42, 81.65s/it]Evaluating commonsenseqa :  41%|██████████████▍                    | 26/63 [36:36<51:51, 84.10s/it]Evaluating commonsenseqa :  43%|███████████████                    | 27/63 [38:07<51:43, 86.20s/it]Evaluating commonsenseqa :  44%|███████████████▌                   | 28/63 [39:23<48:26, 83.03s/it]Evaluating commonsenseqa :  46%|████████████████                   | 29/63 [40:55<48:37, 85.80s/it]Evaluating commonsenseqa :  48%|████████████████▋                  | 30/63 [41:52<42:26, 77.17s/it]Evaluating commonsenseqa :  49%|█████████████████▏                 | 31/63 [43:23<43:19, 81.23s/it]Evaluating commonsenseqa :  51%|█████████████████▊                 | 32/63 [44:54<43:34, 84.33s/it]Evaluating commonsenseqa :  52%|██████████████████▎                | 33/63 [46:25<43:08, 86.29s/it]Evaluating commonsenseqa :  54%|██████████████████▉                | 34/63 [47:57<42:30, 87.96s/it]Evaluating commonsenseqa :  56%|███████████████████▍               | 35/63 [49:26<41:12, 88.30s/it]Evaluating commonsenseqa :  57%|████████████████████               | 36/63 [50:44<38:24, 85.34s/it]Evaluating commonsenseqa :  59%|████████████████████▌              | 37/63 [52:15<37:37, 86.83s/it]Evaluating commonsenseqa :  60%|█████████████████████              | 38/63 [53:02<31:16, 75.04s/it]Evaluating commonsenseqa :  62%|█████████████████████▋             | 39/63 [54:31<31:40, 79.19s/it]Evaluating commonsenseqa :  63%|██████████████████████▏            | 40/63 [56:02<31:40, 82.63s/it]Evaluating commonsenseqa :  65%|██████████████████████▊            | 41/63 [56:17<22:55, 62.51s/it]Evaluating commonsenseqa :  67%|███████████████████████▎           | 42/63 [57:50<25:03, 71.61s/it]Evaluating commonsenseqa :  68%|███████████████████████▉           | 43/63 [59:23<26:01, 78.10s/it]Evaluating commonsenseqa :  70%|███████████████████████          | 44/63 [1:00:55<26:01, 82.19s/it]Evaluating commonsenseqa :  71%|███████████████████████▌         | 45/63 [1:02:26<25:24, 84.71s/it]Evaluating commonsenseqa :  73%|████████████████████████         | 46/63 [1:03:56<24:27, 86.33s/it]Evaluating commonsenseqa :  75%|████████████████████████▌        | 47/63 [1:05:10<22:01, 82.60s/it]Evaluating commonsenseqa :  76%|█████████████████████████▏       | 48/63 [1:06:39<21:08, 84.59s/it]Evaluating commonsenseqa :  78%|█████████████████████████▋       | 49/63 [1:08:11<20:14, 86.75s/it]Evaluating commonsenseqa :  79%|██████████████████████████▏      | 50/63 [1:08:59<16:17, 75.21s/it]Evaluating commonsenseqa :  81%|██████████████████████████▋      | 51/63 [1:10:30<15:58, 79.88s/it]Evaluating commonsenseqa :  83%|███████████████████████████▏     | 52/63 [1:12:01<15:15, 83.21s/it]Evaluating commonsenseqa :  84%|███████████████████████████▊     | 53/63 [1:13:31<14:12, 85.23s/it]Evaluating commonsenseqa :  86%|████████████████████████████▎    | 54/63 [1:15:02<13:04, 87.11s/it]Evaluating commonsenseqa :  87%|████████████████████████████▊    | 55/63 [1:16:34<11:47, 88.45s/it]Evaluating commonsenseqa :  89%|█████████████████████████████▎   | 56/63 [1:18:03<10:20, 88.66s/it]Evaluating commonsenseqa :  90%|█████████████████████████████▊   | 57/63 [1:19:34<08:55, 89.29s/it]Evaluating commonsenseqa :  92%|██████████████████████████████▍  | 58/63 [1:21:04<07:28, 89.65s/it]Evaluating commonsenseqa :  94%|██████████████████████████████▉  | 59/63 [1:22:37<06:01, 90.42s/it]Evaluating commonsenseqa :  95%|███████████████████████████████▍ | 60/63 [1:24:09<04:32, 90.93s/it]Evaluating commonsenseqa :  97%|███████████████████████████████▉ | 61/63 [1:25:39<03:01, 90.64s/it]Evaluating commonsenseqa :  98%|████████████████████████████████▍| 62/63 [1:27:09<01:30, 90.67s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:27:24<00:00, 67.75s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:27:24<00:00, 83.24s/it]
name: commonsenseqa | avg. gen lenth: 280.556 | time: 5244.439510583878s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 29501 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 4 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o12-tgsm8k-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 12
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 07:05:38,264] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 07:05:38,301] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 07:05:38,302] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 07:05:38,302] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o12-tgsm8k-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 12
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o12-tgsm8k-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                       | 0/9741 [00:00<?, ?it/s]Loading data: 100%|████████████████████████████████████████| 9741/9741 [00:00<00:00, 597862.32it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:06<00:06,  6.81s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.39s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.54s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:08<00:00,  4.09s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:08<00:00,  4.50s/it]
[2023-08-28 07:05:48,690] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.34s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.80s/it]
[2023-08-28 07:05:49,326] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.34s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.82s/it]
 > number of parameters: 6738415616
[2023-08-28 07:05:49,527] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:10<00:10, 10.05s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:12<00:00,  5.33s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:12<00:00,  6.04s/it]
[2023-08-28 07:05:51,790] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 07:05:52,355] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-28 07:05:52,356] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-28 07:05:52,356] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-28 07:05:52,356] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-28 07:05:52,356] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-28 07:05:52,356] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff65de53520>
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-28 07:05:52,357] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-28 07:05:52,358] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-28 07:05:52,358] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                            | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: 240

Input: Sarah, Mary, and Tuan decided to go to the restaurant for a meal. They decided to split the cost of the meal evenly. If the total price of the meal comes to $67 and they have a coupon for $4, how much does each person need to contribute to the bill?
Output: 21

Input: Tom's brother is 4 times as old as Tom's dog. If in 6 years, Tom's brother will be 30 years, how old is Tom's dog going to be in six years?
Output: 12

Input: There are 50 children at the party. Three-fifths of them are boys. How many of the children are girls?
Output: 20

Input: Gail has some bills in her wallet which amount to $100. She has four $5 bills and three $20 bills, and the rest are $10 bills. How many $10 bills are in her wallet?
Output: 2

Input: A 220-liter barrel has a small leak. It lost 10% of its contents before anyone noticed. How many liters are left in the barrel?
Output: 198

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o12-tgsm8k-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▌                                 | 1/63 [01:30<1:33:25, 90.42s/it]Evaluating commonsenseqa :   3%|█                                 | 2/63 [02:59<1:31:00, 89.52s/it]Evaluating commonsenseqa :   5%|█▌                                | 3/63 [04:28<1:29:18, 89.31s/it]Evaluating commonsenseqa :   6%|██▏                               | 4/63 [05:59<1:28:32, 90.04s/it]Evaluating commonsenseqa :   8%|██▋                               | 5/63 [07:29<1:26:53, 89.90s/it]Evaluating commonsenseqa :  10%|███▏                              | 6/63 [08:58<1:25:14, 89.72s/it]Evaluating commonsenseqa :  11%|███▊                              | 7/63 [10:28<1:23:56, 89.93s/it]Evaluating commonsenseqa :  13%|████▎                             | 8/63 [11:33<1:15:07, 81.95s/it]Evaluating commonsenseqa :  14%|████▊                             | 9/63 [13:03<1:15:58, 84.41s/it]Evaluating commonsenseqa :  16%|█████▏                           | 10/63 [14:33<1:16:00, 86.04s/it]Evaluating commonsenseqa :  17%|█████▊                           | 11/63 [16:03<1:15:45, 87.41s/it]Evaluating commonsenseqa :  19%|██████▎                          | 12/63 [17:28<1:13:30, 86.47s/it]Evaluating commonsenseqa :  21%|██████▊                          | 13/63 [18:59<1:13:13, 87.86s/it]Evaluating commonsenseqa :  22%|███████▎                         | 14/63 [20:29<1:12:18, 88.54s/it]Evaluating commonsenseqa :  24%|███████▊                         | 15/63 [21:58<1:11:06, 88.88s/it]Evaluating commonsenseqa :  25%|████████▍                        | 16/63 [23:29<1:10:03, 89.44s/it]Evaluating commonsenseqa :  27%|████████▉                        | 17/63 [25:02<1:09:17, 90.39s/it]Evaluating commonsenseqa :  29%|█████████▍                       | 18/63 [26:31<1:07:35, 90.12s/it]Evaluating commonsenseqa :  30%|█████████▉                       | 19/63 [27:59<1:05:32, 89.37s/it]Evaluating commonsenseqa :  32%|███████████                        | 20/63 [28:42<54:10, 75.59s/it]Evaluating commonsenseqa :  33%|███████████▋                       | 21/63 [30:13<56:04, 80.11s/it]Evaluating commonsenseqa :  35%|████████████▏                      | 22/63 [31:43<56:48, 83.14s/it]Evaluating commonsenseqa :  37%|████████████▊                      | 23/63 [33:12<56:37, 84.95s/it]Evaluating commonsenseqa :  38%|█████████████▎                     | 24/63 [34:42<56:01, 86.19s/it]Evaluating commonsenseqa :  40%|█████████████▉                     | 25/63 [36:11<55:17, 87.30s/it]Evaluating commonsenseqa :  41%|██████████████▍                    | 26/63 [37:43<54:41, 88.68s/it]Evaluating commonsenseqa :  43%|███████████████                    | 27/63 [38:13<42:33, 70.93s/it]Evaluating commonsenseqa :  44%|███████████████▌                   | 28/63 [39:44<44:50, 76.87s/it]Evaluating commonsenseqa :  46%|████████████████                   | 29/63 [41:13<45:37, 80.51s/it]Evaluating commonsenseqa :  48%|████████████████▋                  | 30/63 [42:44<46:08, 83.90s/it]Evaluating commonsenseqa :  49%|█████████████████▏                 | 31/63 [44:14<45:38, 85.58s/it]Evaluating commonsenseqa :  51%|█████████████████▊                 | 32/63 [45:44<44:57, 87.02s/it]Evaluating commonsenseqa :  52%|██████████████████▎                | 33/63 [47:14<43:52, 87.76s/it]Evaluating commonsenseqa :  54%|██████████████████▉                | 34/63 [48:16<38:40, 80.02s/it]Evaluating commonsenseqa :  56%|███████████████████▍               | 35/63 [49:45<38:39, 82.85s/it]Evaluating commonsenseqa :  57%|████████████████████               | 36/63 [51:16<38:20, 85.19s/it]Evaluating commonsenseqa :  59%|████████████████████▌              | 37/63 [52:44<37:18, 86.08s/it]Evaluating commonsenseqa :  60%|█████████████████████              | 38/63 [54:14<36:22, 87.30s/it]Evaluating commonsenseqa :  62%|█████████████████████▋             | 39/63 [55:43<35:04, 87.67s/it]Evaluating commonsenseqa :  63%|██████████████████████▏            | 40/63 [57:14<34:02, 88.79s/it]Evaluating commonsenseqa :  65%|██████████████████████▊            | 41/63 [58:45<32:47, 89.44s/it]Evaluating commonsenseqa :  67%|██████████████████████           | 42/63 [1:00:15<31:21, 89.60s/it]Evaluating commonsenseqa :  68%|██████████████████████▌          | 43/63 [1:01:45<29:53, 89.69s/it]Evaluating commonsenseqa :  70%|███████████████████████          | 44/63 [1:03:14<28:20, 89.52s/it]Evaluating commonsenseqa :  71%|███████████████████████▌         | 45/63 [1:04:44<26:51, 89.53s/it]Evaluating commonsenseqa :  73%|████████████████████████         | 46/63 [1:06:15<25:30, 90.00s/it]Evaluating commonsenseqa :  75%|████████████████████████▌        | 47/63 [1:07:44<23:57, 89.84s/it]Evaluating commonsenseqa :  76%|█████████████████████████▏       | 48/63 [1:09:15<22:33, 90.21s/it]Evaluating commonsenseqa :  78%|█████████████████████████▋       | 49/63 [1:10:43<20:54, 89.60s/it]Evaluating commonsenseqa :  79%|██████████████████████████▏      | 50/63 [1:12:15<19:33, 90.26s/it]Evaluating commonsenseqa :  81%|██████████████████████████▋      | 51/63 [1:13:45<18:03, 90.27s/it]Evaluating commonsenseqa :  83%|███████████████████████████▏     | 52/63 [1:15:17<16:38, 90.80s/it]Evaluating commonsenseqa :  84%|███████████████████████████▊     | 53/63 [1:16:46<15:00, 90.08s/it]Evaluating commonsenseqa :  86%|████████████████████████████▎    | 54/63 [1:18:15<13:28, 89.84s/it]Evaluating commonsenseqa :  87%|████████████████████████████▊    | 55/63 [1:19:45<11:59, 89.95s/it]Evaluating commonsenseqa :  89%|█████████████████████████████▎   | 56/63 [1:21:15<10:29, 89.96s/it]Evaluating commonsenseqa :  90%|█████████████████████████████▊   | 57/63 [1:22:46<09:01, 90.28s/it]Evaluating commonsenseqa :  92%|██████████████████████████████▍  | 58/63 [1:24:17<07:32, 90.50s/it]Evaluating commonsenseqa :  94%|██████████████████████████████▉  | 59/63 [1:25:46<06:00, 90.04s/it]Evaluating commonsenseqa :  95%|███████████████████████████████▍ | 60/63 [1:27:16<04:30, 90.04s/it]Evaluating commonsenseqa :  97%|███████████████████████████████▉ | 61/63 [1:28:47<03:00, 90.11s/it]Evaluating commonsenseqa :  98%|████████████████████████████████▍| 62/63 [1:30:16<01:29, 89.79s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:31:06<00:00, 77.84s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:31:06<00:00, 86.76s/it]
name: commonsenseqa | avg. gen lenth: 301.576 | time: 5466.525060176849s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 29501 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 4 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o13-tgsm8k-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 13
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 08:37:05,972] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 08:37:06,282] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 08:37:06,304] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 08:37:06,308] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o13-tgsm8k-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 13
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tgsm8k-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                       | 0/9741 [00:00<?, ?it/s]Loading data: 100%|████████████████████████████████████████| 9741/9741 [00:00<00:00, 586558.26it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.28s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.32s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.34s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.37s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.24s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.69s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.26s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.72s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.27s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.73s/it]
[2023-08-28 08:37:17,040] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.26s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.73s/it]
 > number of parameters: 6738415616
[2023-08-28 08:37:17,127] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 08:37:17,147] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 08:37:17,498] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 08:37:18,068] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-28 08:37:18,069] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-28 08:37:18,069] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4030c53520>
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-28 08:37:18,070] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-28 08:37:18,071] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-28 08:37:18,071] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                            | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: 240

Input: Sarah, Mary, and Tuan decided to go to the restaurant for a meal. They decided to split the cost of the meal evenly. If the total price of the meal comes to $67 and they have a coupon for $4, how much does each person need to contribute to the bill?
Output: 21

Input: Tom's brother is 4 times as old as Tom's dog. If in 6 years, Tom's brother will be 30 years, how old is Tom's dog going to be in six years?
Output: 12

Input: There are 50 children at the party. Three-fifths of them are boys. How many of the children are girls?
Output: 20

Input: Gail has some bills in her wallet which amount to $100. She has four $5 bills and three $20 bills, and the rest are $10 bills. How many $10 bills are in her wallet?
Output: 2

Input: A 220-liter barrel has a small leak. It lost 10% of its contents before anyone noticed. How many liters are left in the barrel?
Output: 198

Input: Markese earned 5 fewer dollars than Evan. Together they earned $37. How many dollars did Markese earn? Use E to represent how many dollars Evan earned.
Output: 16

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o13-tgsm8k-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▌                                 | 1/63 [01:29<1:32:44, 89.76s/it]Evaluating commonsenseqa :   3%|█                                 | 2/63 [02:59<1:31:07, 89.64s/it]Evaluating commonsenseqa :   5%|█▌                                | 3/63 [04:15<1:23:35, 83.59s/it]Evaluating commonsenseqa :   6%|██▏                               | 4/63 [05:44<1:24:07, 85.54s/it]Evaluating commonsenseqa :   8%|██▋                               | 5/63 [07:12<1:23:43, 86.61s/it]Evaluating commonsenseqa :  10%|███▏                              | 6/63 [08:41<1:23:01, 87.40s/it]Evaluating commonsenseqa :  11%|███▊                              | 7/63 [10:10<1:21:59, 87.85s/it]Evaluating commonsenseqa :  13%|████▎                             | 8/63 [11:29<1:17:56, 85.03s/it]Evaluating commonsenseqa :  14%|████▊                             | 9/63 [12:37<1:11:49, 79.80s/it]Evaluating commonsenseqa :  16%|█████▏                           | 10/63 [14:06<1:12:49, 82.44s/it]Evaluating commonsenseqa :  17%|█████▊                           | 11/63 [15:34<1:13:05, 84.34s/it]Evaluating commonsenseqa :  19%|██████▎                          | 12/63 [17:02<1:12:36, 85.42s/it]Evaluating commonsenseqa :  21%|██████▊                          | 13/63 [17:56<1:03:15, 75.92s/it]Evaluating commonsenseqa :  22%|███████▎                         | 14/63 [19:09<1:01:09, 74.89s/it]Evaluating commonsenseqa :  24%|███████▊                         | 15/63 [20:38<1:03:23, 79.24s/it]Evaluating commonsenseqa :  25%|████████▍                        | 16/63 [22:06<1:04:09, 81.91s/it]Evaluating commonsenseqa :  27%|████████▉                        | 17/63 [23:35<1:04:24, 84.01s/it]Evaluating commonsenseqa :  29%|█████████▍                       | 18/63 [25:03<1:03:50, 85.11s/it]Evaluating commonsenseqa :  30%|█████████▉                       | 19/63 [26:33<1:03:36, 86.74s/it]Evaluating commonsenseqa :  32%|██████████▍                      | 20/63 [28:03<1:02:47, 87.62s/it]Evaluating commonsenseqa :  33%|███████████                      | 21/63 [29:31<1:01:27, 87.80s/it]Evaluating commonsenseqa :  35%|████████████▏                      | 22/63 [30:06<49:08, 71.91s/it]Evaluating commonsenseqa :  37%|████████████▊                      | 23/63 [31:35<51:16, 76.91s/it]Evaluating commonsenseqa :  38%|█████████████▎                     | 24/63 [33:04<52:21, 80.55s/it]Evaluating commonsenseqa :  40%|█████████████▉                     | 25/63 [34:32<52:28, 82.85s/it]Evaluating commonsenseqa :  41%|██████████████▍                    | 26/63 [36:01<52:17, 84.80s/it]Evaluating commonsenseqa :  43%|███████████████                    | 27/63 [37:28<51:18, 85.52s/it]Evaluating commonsenseqa :  44%|███████████████▌                   | 28/63 [38:58<50:31, 86.61s/it]Evaluating commonsenseqa :  46%|████████████████                   | 29/63 [40:25<49:15, 86.93s/it]Evaluating commonsenseqa :  48%|████████████████▋                  | 30/63 [41:53<47:57, 87.21s/it]Evaluating commonsenseqa :  49%|█████████████████▏                 | 31/63 [43:22<46:43, 87.60s/it]Evaluating commonsenseqa :  51%|█████████████████▊                 | 32/63 [44:52<45:40, 88.41s/it]Evaluating commonsenseqa :  52%|██████████████████▎                | 33/63 [46:21<44:19, 88.64s/it]Evaluating commonsenseqa :  54%|██████████████████▉                | 34/63 [47:51<43:00, 88.97s/it]Evaluating commonsenseqa :  56%|███████████████████▍               | 35/63 [49:19<41:23, 88.71s/it]Evaluating commonsenseqa :  57%|████████████████████               | 36/63 [50:48<39:59, 88.86s/it]Evaluating commonsenseqa :  59%|████████████████████▌              | 37/63 [52:06<37:06, 85.63s/it]Evaluating commonsenseqa :  60%|█████████████████████              | 38/63 [53:35<36:03, 86.55s/it]Evaluating commonsenseqa :  62%|█████████████████████▋             | 39/63 [55:02<34:41, 86.74s/it]Evaluating commonsenseqa :  63%|██████████████████████▏            | 40/63 [56:34<33:48, 88.17s/it]Evaluating commonsenseqa :  65%|██████████████████████▊            | 41/63 [58:05<32:42, 89.20s/it]Evaluating commonsenseqa :  67%|███████████████████████▎           | 42/63 [59:30<30:47, 87.99s/it]Evaluating commonsenseqa :  68%|██████████████████████▌          | 43/63 [1:00:59<29:25, 88.29s/it]Evaluating commonsenseqa :  70%|███████████████████████          | 44/63 [1:01:38<23:16, 73.49s/it]Evaluating commonsenseqa :  71%|███████████████████████▌         | 45/63 [1:03:06<23:16, 77.61s/it]Evaluating commonsenseqa :  73%|████████████████████████         | 46/63 [1:04:36<23:06, 81.57s/it]Evaluating commonsenseqa :  75%|████████████████████████▌        | 47/63 [1:06:07<22:27, 84.20s/it]Evaluating commonsenseqa :  76%|█████████████████████████▏       | 48/63 [1:07:36<21:24, 85.63s/it]Evaluating commonsenseqa :  78%|█████████████████████████▋       | 49/63 [1:09:06<20:19, 87.08s/it]Evaluating commonsenseqa :  79%|██████████████████████████▏      | 50/63 [1:10:34<18:55, 87.38s/it]Evaluating commonsenseqa :  81%|██████████████████████████▋      | 51/63 [1:12:03<17:34, 87.87s/it]Evaluating commonsenseqa :  83%|███████████████████████████▏     | 52/63 [1:13:31<16:04, 87.71s/it]Evaluating commonsenseqa :  84%|███████████████████████████▊     | 53/63 [1:14:59<14:38, 87.87s/it]Evaluating commonsenseqa :  86%|████████████████████████████▎    | 54/63 [1:16:28<13:14, 88.23s/it]Evaluating commonsenseqa :  87%|████████████████████████████▊    | 55/63 [1:17:55<11:44, 88.03s/it]Evaluating commonsenseqa :  89%|█████████████████████████████▎   | 56/63 [1:19:25<10:19, 88.51s/it]Evaluating commonsenseqa :  90%|█████████████████████████████▊   | 57/63 [1:20:55<08:52, 88.79s/it]Evaluating commonsenseqa :  92%|██████████████████████████████▍  | 58/63 [1:22:22<07:21, 88.34s/it]Evaluating commonsenseqa :  94%|██████████████████████████████▉  | 59/63 [1:23:35<05:34, 83.67s/it]Evaluating commonsenseqa :  95%|███████████████████████████████▍ | 60/63 [1:25:04<04:16, 85.51s/it]Evaluating commonsenseqa :  97%|███████████████████████████████▉ | 61/63 [1:26:18<02:43, 81.83s/it]Evaluating commonsenseqa :  98%|████████████████████████████████▍| 62/63 [1:27:35<01:20, 80.35s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:28:01<00:00, 64.30s/it]Evaluating commonsenseqa : 100%|█████████████████████████████████| 63/63 [1:28:01<00:00, 83.84s/it]
name: commonsenseqa | avg. gen lenth: 302.752 | time: 5282.203695297241s
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 29501 /home/ylu130/workspace/in-context-generalization/inference.py --model-name llama2-7b --model-type llama --model-path /scratch/ylu130/model-hf --model-hf-name meta-llama/Llama-2-7b-hf --n-gpu 4 --is-opensource --data-name commonsenseqa --num-eval 1000 --num-workers 0 --num-in-domain 0 --out-domain-data-name gsm8k --save /home/ylu130/workspace/in-context-generalization/results --do-sample --top-k 50 --top-p 1 --temperature 1 --batch-size 4 --deepspeed --deepspeed_config /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json --data-dir /scratch/ylu130/processed_data/commonsenseqa/out-domain/o14-tgsm8k-s1-rFalse --seed 1 --max-prompt-length 4096 --num-out-domain 14
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-28 10:05:29,080] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 10:05:29,100] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 10:05:29,101] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-28 10:05:29,101] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
arguments:
  model_name ................... llama2-7b
  model_type ................... llama
  model_path ................... /scratch/ylu130/model-hf
  model_hf_name ................ meta-llama/Llama-2-7b-hf
  n_gpu ........................ 4
  n_nodes ...................... 1
  is_opensource ................ True
  data_name .................... commonsenseqa
  data_dir ..................... /scratch/ylu130/processed_data/commonsenseqa/out-domain/o14-tgsm8k-s1-rFalse
  processed_data_dir ........... None
  num_eval ..................... 1000
  num_in_domain ................ 0
  num_out_domain ............... 14
  out_domain_data_name ......... gsm8k
  out_domain_data_dir .......... None
  num_workers .................. 0
  max_prompt_length ............ 4096
  max_length ................... 512
  save ......................... /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o14-tgsm8k-s1-rFalse-m4096
  rationales ................... False
  top_k ........................ 50
  top_p ........................ 1.0
  do_sample .................... True
  num_beams .................... 1
  temperature .................. 1.0
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  batch_size ................... 4
  seed ......................... 1
  deepspeed .................... True
  deepspeed_config ............. /home/ylu130/workspace/in-context-generalization/configs/deepspeed/ds_config.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  local_rank ................... 0
  rank ......................... 0
  world_size ................... 4
Loading data:   0%|                                                       | 0/9741 [00:00<?, ?it/s]Loading data: 100%|████████████████████████████████████████| 9741/9741 [00:00<00:00, 520209.26it/s]
Num instances: 1000
Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                             | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.36s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.40s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.44s/it]Loading checkpoint shards:  50%|██████████████████▌                  | 1/2 [00:07<00:07,  7.60s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.25s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.72s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.27s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.75s/it]
[2023-08-28 10:05:39,938] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 10:05:39,950] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.31s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.81s/it]
[2023-08-28 10:05:40,092] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:10<00:00,  4.70s/it]Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:10<00:00,  5.11s/it]
 > number of parameters: 6738415616
[2023-08-28 10:05:40,698] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-08-28 10:05:41,303] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-28 10:05:41,305] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-28 10:05:41,305] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-28 10:05:41,305] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-28 10:05:41,305] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-28 10:05:41,305] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fcc8cf97640>
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 2000, 'delayed_shift': 4, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-28 10:05:41,306] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   train_batch_size ............. 4
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   world_size ................... 4
[2023-08-28 10:05:41,307] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-28 10:05:41,308] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-28 10:05:41,308] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-28 10:05:41,308] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-28 10:05:41,308] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-28 10:05:41,308] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false
}
Evaluating commonsenseqa :   0%|                                            | 0/63 [00:00<?, ?it/s]############### Example ###############
### Instruction:Answer the following multiple choice question.

Input: Mary had 89 stickers.  She used 3 large stickers on the front page of her journal and 7 stickers each to 6 other pages of her journal. How many stickers does Mary have remaining?
Output: 44

Input: Zach is saving his money to buy a brand new bike that costs $100.  His weekly allowance is $5.  His parent will pay him an extra $10 to mow the lawn.  His neighbor will pay him $7 per hour to babysit their son.  He has already saved up $65.  He'll receive his allowance on Friday and he's planning on babysitting for 2 hours this Saturday after he mows the lawn.  How much more money does Zach need to earn before he can buy the bike?
Output: 6

Input: Mark has kangaroos and goats.  Kangaroos have two legs and goats have four legs.  If he has 23 kangaroos and three times as many goats as kangaroos what is the total number of legs of all his animals?
Output: 322

Input: Josh’s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil for $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?
Output: 3

Input: George's bowling team is one round away from breaking the league record for most points scored in a season. The old record is an average score per player of 287 per round. Each team has 4 players and there are 10 rounds in the season. Through the first 9 rounds, his team has scored a total of 10,440. How many points less than the current league record per game average is the minimum average they need to score, per player, in the final round to tie the league record?
Output: 27

Input: Max was doing homework in three different subjects. It took him 20 minutes to finish tasks from biology and two times more time to finish history. Geography took him the most time, three times more than history. How much time did Max spend on doing his homework?
Output: 180

Input: Sophia ate 1/6 of her pie and she put the rest on the fridge. If the pie left in the fridge weighs 1200 grams, how many grams did Sophia eat?
Output: 240

Input: Sarah, Mary, and Tuan decided to go to the restaurant for a meal. They decided to split the cost of the meal evenly. If the total price of the meal comes to $67 and they have a coupon for $4, how much does each person need to contribute to the bill?
Output: 21

Input: Tom's brother is 4 times as old as Tom's dog. If in 6 years, Tom's brother will be 30 years, how old is Tom's dog going to be in six years?
Output: 12

Input: There are 50 children at the party. Three-fifths of them are boys. How many of the children are girls?
Output: 20

Input: Gail has some bills in her wallet which amount to $100. She has four $5 bills and three $20 bills, and the rest are $10 bills. How many $10 bills are in her wallet?
Output: 2

Input: A 220-liter barrel has a small leak. It lost 10% of its contents before anyone noticed. How many liters are left in the barrel?
Output: 198

Input: Markese earned 5 fewer dollars than Evan. Together they earned $37. How many dollars did Markese earn? Use E to represent how many dollars Evan earned.
Output: 16

Input: Lou Senior took 3 cookies out of the cookie jar and ate them.  Since he didn't get caught by his wife, he went back the next day and took another 3 cookies out of the jar.  But after eating just one of the cookies, he felt guilty about it and put the other two cookies back.  His son, Louie Junior saw that his Dad was eating cookies.  So, Louie Junior took seven cookies out of the jar and hid them in his bedroom for later.  The next morning, Debra, Lou's wife looked into the cookie jar and reacted by accusing her husband of eating half of the cookies out of the cookie jar.  How many cookies remained in the jar?
Output: 11

Input:The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Choices:  A: ignore B: enforce C: authoritarian D: yell at E: avoid
Output:
############### End ###############
Experiment Save Path: /home/ylu130/workspace/in-context-generalization/results/llama2-7b/commonsenseqa/out-domain/o14-tgsm8k-s1-rFalse-m4096
Evaluating commonsenseqa :   2%|▌                                 | 1/63 [01:26<1:29:16, 86.40s/it]Evaluating commonsenseqa :   3%|█                                 | 2/63 [02:52<1:27:38, 86.20s/it]Evaluating commonsenseqa :   5%|█▌                                | 3/63 [04:17<1:25:28, 85.48s/it]Evaluating commonsenseqa :   6%|██▏                               | 4/63 [04:48<1:03:09, 64.22s/it]Evaluating commonsenseqa :   8%|██▋                               | 5/63 [06:13<1:09:06, 71.50s/it]Evaluating commonsenseqa :  10%|███▏                              | 6/63 [07:36<1:11:43, 75.50s/it]Evaluating commonsenseqa :  11%|███▊                              | 7/63 [09:00<1:13:13, 78.45s/it]Evaluating commonsenseqa :  13%|████▎                             | 8/63 [10:28<1:14:33, 81.34s/it]Evaluating commonsenseqa :  14%|████▊                             | 9/63 [11:52<1:14:00, 82.24s/it]Evaluating commonsenseqa :  16%|█████▏                           | 10/63 [13:18<1:13:29, 83.20s/it]Evaluating commonsenseqa :  17%|█████▊                           | 11/63 [14:41<1:12:12, 83.31s/it]Evaluating commonsenseqa :  19%|██████▎                          | 12/63 [16:06<1:11:16, 83.86s/it]Evaluating commonsenseqa :  21%|██████▊                          | 13/63 [17:32<1:10:29, 84.60s/it]Evaluating commonsenseqa :  22%|███████▎                         | 14/63 [18:57<1:08:56, 84.43s/it]Evaluating commonsenseqa :  24%|███████▊                         | 15/63 [20:25<1:08:26, 85.55s/it]Evaluating commonsenseqa :  25%|████████▍                        | 16/63 [21:48<1:06:29, 84.88s/it]Evaluating commonsenseqa :  27%|████████▉                        | 17/63 [23:13<1:05:09, 85.00s/it]Evaluating commonsenseqa :  29%|█████████▍                       | 18/63 [24:37<1:03:21, 84.47s/it]Evaluating commonsenseqa :  30%|██████████▌                        | 19/63 [25:37<56:38, 77.23s/it]Evaluating commonsenseqa :  32%|███████████                        | 20/63 [27:01<56:54, 79.40s/it]Evaluating commonsenseqa :  33%|███████████▋                       | 21/63 [28:25<56:32, 80.78s/it]Evaluating commonsenseqa :  35%|████████████▏                      | 22/63 [29:50<56:02, 82.01s/it]Evaluating commonsenseqa :  37%|████████████▊                      | 23/63 [31:15<55:12, 82.81s/it]Evaluating commonsenseqa :  38%|█████████████▎                     | 24/63 [32:39<54:05, 83.21s/it]Evaluating commonsenseqa :  40%|█████████████▉                     | 25/63 [34:06<53:20, 84.23s/it]Evaluating commonsenseqa :  41%|██████████████▍                    | 26/63 [35:10<48:11, 78.14s/it]Evaluating commonsenseqa :  43%|███████████████                    | 27/63 [36:34<48:02, 80.07s/it]Evaluating commonsenseqa :  44%|███████████████▌                   | 28/63 [38:01<47:53, 82.11s/it]Evaluating commonsenseqa :  46%|████████████████                   | 29/63 [39:23<46:35, 82.21s/it]Evaluating commonsenseqa :  48%|████████████████▋                  | 30/63 [40:49<45:48, 83.29s/it]Evaluating commonsenseqa :  49%|█████████████████▏                 | 31/63 [42:15<44:45, 83.93s/it]Evaluating commonsenseqa :  51%|█████████████████▊                 | 32/63 [43:39<43:26, 84.09s/it]Evaluating commonsenseqa :  52%|██████████████████▎                | 33/63 [45:04<42:13, 84.46s/it]Evaluating commonsenseqa :  54%|██████████████████▉                | 34/63 [46:28<40:39, 84.13s/it]